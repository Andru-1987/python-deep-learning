import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_iris
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.model_selection import cross_val_score
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

#features: petal length, petal width, sepal length, sepal width
#target species: versicolor, virginica, setsoa
plt.style.use('ggplot')

iris=load_iris()

#bunch lets you use a python dict like an object
print(type(iris))
print(iris.keys())
#feature and target are numpy arrays
print('features type:' + str(type(iris.data)))
print('target type:' + str(type(iris.target)))
print(iris.data.shape)

X=iris.data
y=iris.target

labels=y

df=pd.DataFrame(X,columns=iris.feature_names)
print(df.head(5))

xs=df['petal length (cm)']
ys=df['petal width (cm)']

_=plt.scatter(xs,ys,c=labels)
plt.show()

points=np.column_stack((xs,ys))
#print(points)

xs = new_points[:,0]
ys = new_points[:,1]

plt.clf()
# Make a scatter plot of xs and ys, using labels to define the colors
_=plt.scatter(xs,ys,c=labels, alpha=0.5)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_
# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]

# Make a scatter plot of centroids_x and centroids_y
_=plt.scatter(centroids_x,centroids_y, marker='D',s=50)
plt.show()

labels=iris.target
print(labels)
species=[]
for i in range(0,len(labels)):
    if(labels[i]==0):
        species.append("versicolor")
    elif(labels[i]==1):
        species.append("virginica")    
    elif(labels[i]==2):
        species.append('setsoa')
 
#print(species)

df2=pd.DataFrame({'labels':labels, 'species':species})
print(df2)
ct=pd.crosstab(df2['labels'],df2['species'])
print(ct)

scaler=StandardScaler()
kmeans=KMeans(n_clusters=3)
pipeline=make_pipeline(scaler,kmeans)

pipeline.fit(points)
labels=pipeline.predict(points)

df2 = pd.DataFrame({'labels':labels,'species':species})

ct = pd.crosstab(df2['labels'],df2['species'])

# Display ct
print(ct)


>>>>>sample using Normalizer

Daily stock price movements, the difference between opening and closing

[[ 5.8000000e-01 -2.2000500e-01 -3.4099980e+00 ... -5.3599620e+00
   8.4001900e-01 -1.9589981e+01]
 [-6.4000200e-01 -6.5000000e-01 -2.1000100e-01 ... -4.0001000e-02
  -4.0000200e-01  6.6000000e-01]
 [-2.3500060e+00  1.2600090e+00 -2.3500060e+00 ...  4.7900090e+00
  -1.7600090e+00  3.7400210e+00]
 ...
 [ 4.3000100e-01  2.2999600e-01  5.7000000e-01 ... -2.6000200e-01
   4.0000100e-01  4.8000300e-01]
 [ 9.0000000e-02  1.0000000e-02 -8.0000000e-02 ... -3.0000000e-02
   2.0000000e-02 -3.0000000e-02]
 [ 1.5999900e-01  1.0001000e-02  0.0000000e+00 ... -6.0001000e-02
   2.5999800e-01  9.9998000e-02]]

# Import Normalizer
from  sklearn.preprocessing import Normalizer

# Create a normalizer: normalizer
normalizer = Normalizer()

# Create a KMeans model with 10 clusters: kmeans
kmeans = KMeans(n_clusters=10)

pipeline=make_pipeline(normalizer,kmeans)
pipeline.fit(movements)

import pandas as pd

# Predict the cluster labels: labels
labels = pipeline.predict(movements)

# Create a DataFrame aligning labels and companies: df
df = pd.DataFrame({'labels': labels, 'companies': companies})

# Display df sorted by cluster label
print(df.sort_values('labels'))


>>>Visualizing hierarchies

1. t-sne creates a 2d map of a data set
2. hierarch of groups

sample
countries gave score to songs perfomred at eurovision 2016
2d array of scores
rows are countries and columns are songs and the intersection is the score for the song.

hierarchial clustering

1. every country begins in a separate cluster
2. at each step, the two closest clusters are merged
3. continue until all countries are a single cluster
4. this is agglomerative hierarchial clustering

Linkage performs the hierarchial clustering

import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage, dendrogram

mergings= linkage(samples, method='complete')

dendrogram(mergings,
labels=country_names,
leaf_rotation=90,
leaf_font_size=6)

plt.show()


Sample>>>

# Import normalize
from sklearn.preprocessing import normalize

# Normalize the movements: normalized_movements
normalized_movements = normalize(movements)

# Calculate the linkage: mergings
mergings = linkage(normalized_movements,method='complete')

# Plot the dendrogram
dendrogram(mergings,
labels=companies,
leaf_rotation=90,
leaf_font_size=6)

plt.show()

companies data

['Apple', 'AIG', 'Amazon', 'American express', 'Boeing', 'Bank of America', 'British American Tobacco', 'Canon', 'Caterpillar', 'Colgate-Palmolive', 'ConocoPhillips', 'Cisco', 'Chevron', 'DuPont de Nemours', 'Dell', 'Ford', 'General Electrics', 'Google/Alphabet', 'Goldman Sachs', 'GlaxoSmithKline', 'Home Depot', 'Honda', 'HP', 'IBM', 'Intel', 'Johnson & Johnson', 'JPMorgan Chase', 'Kimberly-Clark', 'Coca Cola', 'Lookheed Martin', 'MasterCard', 'McDonalds', '3M', 'Microsoft', 'Mitsubishi', 'Navistar', 'Northrop Grumman', 'Novartis', 'Pepsi', 'Pfizer', 'Procter Gamble', 'Philip Morris', 'Royal Dutch Shell', 'SAP', 'Schlumberger', 'Sony', 'Sanofi-Aventis', 'Symantec', 'Toyota', 'Total', 'Taiwan Semiconductor Manufacturing', 'Texas instruments', 'Unilever', 'Valero Energy', 'Walgreen', 'Wells Fargo', 'Wal-Mart', 'Exxon', 'Xerox', 'Yahoo']


[ 5.8000000e-01 -2.2000500e-01 -3.4099980e+00 ... -5.3599620e+00
   8.4001900e-01 -1.9589981e+01]
 [-6.4000200e-01 -6.5000000e-01 -2.1000100e-01 ... -4.0001000e-02
  -4.0000200e-01  6.6000000e-01]
 [-2.3500060e+00  1.2600090e+00 -2.3500060e+00 ...  4.7900090e+00
  -1.7600090e+00  3.7400210e+00]
 ...
 [ 4.3000100e-01  2.2999600e-01  5.7000000e-01 ... -2.6000200e-01
   4.0000100e-01  4.8000300e-01]
 [ 9.0000000e-02  1.0000000e-02 -8.0000000e-02 ... -3.0000000e-02
   2.0000000e-02 -3.0000000e-02]
 [ 1.5999900e-01  1.0001000e-02  0.0000000e+00 ... -6.0001000e-02
   2.5999800e-01  9.9998000e-02]]

>>>cluster labels in hierarchial clustering
1. not only a visualization tool
2. cluster labels at intermediate stage can be recovered
3. for use in cross-tabulations (clusters by labels through aggregation)


dendrogram height
1. height on a dendrogram is the distanced between merging clusters

The distance between merging clusters i the height.

The distance between clusters is measured using a linkage method.

The linkage between clusters is the maximum distance between their samples.

method="complete" means maximum distance


The clusters at a certain dendrogram height can be extracted using the fcluster function.


from scipy.cluster.hierarchy import linkage

mergings = linkage(samples, method='complete')

from scipy.cluster.hierarchy import fcluster

labels = fcluster(mergings, 15, criterion='distance')
print(labels)

#returns a numpy array of all the countries that are that height 15

import pandas as pd
pairs=pd.DataFrame({'labels':labels, 'countries':country_names})
print(pairs.sort_values('labels'))

#lines up the labels with the countries

>> Sample

# Perform the necessary imports
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage, dendrogram

# Calculate the linkage: mergings
mergings = linkage(samples, method='single')

# Plot the dendrogram
dendrogram(mergings,
labels=country_names,
leaf_rotation=90,
leaf_font_size=6)

plt.show()

>>>Extracting the cluster labels

# Perform the necessary imports
from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import fcluster

# Use fcluster to extract labels: labels
labels = fcluster(mergings, 6, criterion='distance')
print(labels)

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({'labels': labels, 'varieties': varieties})

# Create crosstab: ct
ct = ct=pd.crosstab(df['labels'],df['varieties'])

# Display ct
print(ct)


>>>>t-SNE for 2 dimensional maps

1. t-distributed stochastic neighbor embedding
2. maps data from a higher dimensional space say 3d to a 2 dimensional space, so it can be visualized.

Iris
1. the iris dataset has 4 measurements, so samples are in 4 dimensional space,
include petal length and width.

2. we learn there are two species of flower that are close together: versicolor and virginica

samples are in a 2D numpy array, four feature measurements per row

import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
model=TSNE(learning_rate=100)

transformed=model.fit_transform(samples)

xs=transformed[:,0]
ys=transformed[:,1]

plt.scatter(xs,ys,c=species)
plt.show


https://vincentarelbundock.github.io/Rdatasets/datasets.html


>>>Sample annotate labels to the t-sne
1. The stock price movements have been normalized

from sklearn.manifold import TSNE

# Create a TSNE instance: model
model=TSNE(learning_rate=50)

# Apply fit_transform to normalized_movements: tsne_features
tsne_features = model.fit_transform(normalized_movements)

# Select the 0th feature: xs
xs = tsne_features[:,0]

# Select the 1th feature: ys
ys = tsne_features[:,1]

# Scatter plot
plt.scatter(xs,ys,alpha=0.5)

# Annotate the points
for x, y, company in zip(xs, ys, companies):
    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)
plt.show()


















 



























