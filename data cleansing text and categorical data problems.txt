categories
1. Predefined finite set of categories
2. Text categories are converted to numeric representations

how to treat the problems
1. drop rows of data
2. remap the categories
3. infer the category

study data
name, birthday, blood_type

awesome-public datasets
https://github.com/awesomedata/awesome-public-datasets


study_data= pd.read_csv('study.csv')


anti joins (left join)
what is a and not in b

inner join
what is both a and b

inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])

print(inconsistent_categories)

outputs {'Z+'}

inconsistent_rows=study_data['blood_type'].isin(inconsistent_categories)

#returns a boolean of true for inconsistent rows

study_data[inconsistent_rows]

consistent_data=study_data[~inconsistent_rows]

>>>>Sample >> using unique to find categories

# Print categories DataFrame
print(categories)

# Print unique values of survey columns in airlines
print('Cleanliness: ', airlines['cleanliness'].unique(), "\n")
print('Safety: ', airlines['safety'].unique(), "\n")
print('Satisfaction: ', airlines['satisfaction'].unique(),"\n")

>>>Sample >>> finding inconsistencies in the categories

print(airlines['cleanliness'])
print(categories.columns)
inconsistent_categories1=set(airlines['cleanliness']).difference(categories['cleanliness'])
inconsistent_categories2=set(airlines['safety']).difference(categories['safety'])
inconsistent_categories3=set(airlines['satisfaction']).difference(categories['satisfaction'])
print(len(inconsistent_categories1),len(inconsistent_categories2),len(inconsistent_categories3))

>>>>Sample >>> finding the rows with the inconsistent data

# Find the cleanliness category in airlines not in categories
inconsistent_categories1=set(airlines['cleanliness']).difference(categories['cleanliness'])

# Find rows with that category
cat_clean_rows = airlines['cleanliness'].isin(inconsistent_categories1)

# Print rows with inconsistent category
print(airlines[cat_clean_rows])

# Print rows with consistent categories
print(airlines[~cat_clean_rows])

>>>>>>>>>>>>>>>What type of errors could we have

1. inconsistent fields
2. trailing white spaces

Collapsing too many categories to few
1. create new groups (0-20k) (20-40k) from contineous household income data
2. mapping groups to new ones

Capitalization:
married or Married or UNMARRIED or unmarried

marriage_status=demographics['marriage_status']
marriage_status.value_counts()


for a dataframe

marriage_status.groupby('marriage_status').count()

fix

marriage_status['marriage_status']=marriage_status['marriage_status'].str.upper()

leading spaces

marriage_status['marriage_status']=marriage_status['marriage_status'].str.strip()


>>>>>>Collapsing data into categories

ranges=[0, 200000,500000,np.inf]
group_names=['0-200k','200k-500k','500k+']

demographics['income_group']=pd.cut(demographics['household_income'], bins=ranges, labels=group_names)

print(demographics[['income_group','household_income']]


>>>>Collapsing data into categories

'Microsoft', 'MacOS', 'IOS', 'Android', 'Linus' are collasped into a category called 'operating sytems'

mapping={'Microsoft':'DesktopOS','MacOS':'DesktopOS',
'Linux':'DesktopOS','IOS':'MobileOS','Android':'MobileOS'}

devices['operating_systems]=devices['operating_systems'].replace(mapping)


>>>>> Sample >>>> lower and replace

# Print unique values of both columns
print(airlines['dest_region'].unique())
print(airlines['dest_size'].unique())

# Lower dest_region column and then replace "eur" with "europe"
airlines['dest_region'] = airlines['dest_region'].str.lower()
airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})



>>>>Sample >>> Strip

# Print unique values of both columns
print(airlines['dest_region'].unique())
print(airlines['dest_size'].unique())

# Lower dest_region column and then replace "eur" with "europe"
airlines['dest_region'] = airlines['dest_region'].str.lower() 
airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})

# Remove white spaces from `dest_size`
airlines['dest_size'] = airlines['dest_size'].str.strip()
airlines['dest_region'] = airlines['dest_region'].str.strip()

# Verify changes have been effected
print(airlines['dest_region'].unique())
print(airlines['dest_size'].unique())


>>>>>Sample >>> using cut and remapping

# Create ranges for categories
label_ranges = [0, 60, 180, np.inf]
label_names = ['short', 'medium', 'long']

# Create wait_type column
airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, 
                                labels = label_names)

# Create mappings and replace
mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 
            'Thursday': 'weekday', 'Friday': 'weekday', 
            'Saturday': 'weekend', 'Sunday': 'weekend'}

airlines['day_week'] = airlines['day'].replace(mappings)

print(airlines.head())

>>>>Cleaning text data

1. leading zeros on the phone number
2. phone numbers with the incorrect length

all phone numbers begin with 00
and incorrect length phone numbers are replaced with nan

phones['Phone Number']=phones['Phone Number'].str.replace("-","")

digits = phones['Phone Number'].str.len()

phones.loc[digits<10,"Phone Number"] = np.nan


#assert if minimum phone number length is 10
sanity_check=phone['Phone number'].str.len()
assert sanity_check.min()>=10

#any returns any records that are true
assert phone['Phone Number'].str.contains("+|-").any()==False


>>>>>Regular expressions

#replace any character that is not a digit with nothing

phones['Phone number']=phones['Phone number'].str.replace(r'\D+','')

>>>>>Sample >>> replace pattern with empty

# Replace "Dr." with empty string ""
airlines['full_name'] = airlines['full_name'].str.replace("Dr.","")

# Replace "Mr." with empty string ""
airlines['full_name'] = airlines['full_name'].str.replace('Mr.','')

# Replace "Miss" with empty string ""
airlines['full_name'] = airlines['full_name'].str.replace('Miss.','')

# Replace "Ms." with empty string ""
airlines['full_name'] = airlines['full_name'].str.replace('Ms.','')

# Assert that full_name has no honorifics
assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False


>>>>>Sample find strings with len > 40 and assert if not true

# Store length of each row in survey_response column
resp_length = airlines['survey_response'].str.len()

# Find rows in airlines where resp_length > 40
airlines_survey = airlines[resp_length > 40]

# Assert minimum survey_response length is > 40
assert airlines_survey['survey_response'].str.len().min() > 40

# Print new survey_response column
print(airlines_survey['survey_response'])


>>>>>>>>>>>>>>>>>>>>Uniformity
1. dealing with missing data


tempuratures in both celsius and fahrenheit
weight in kilograms and stones
date is short and long format
money in dollars and euros


>>>Fixing fahrenheit and celius

plt.scatter(x='Date', y='Temperature', data=temperatures)
plt.title('Temperature in Celsius ')
plt.xlabel('Dates')
plt.ylabel('Temperature in Celsius')
plt.show()

C=(F-32) x 5/9

temp_fah=temperatures.loc[temperatures['Temperature']>40,'Temperature]

temp_celsius=(temp_fah-32)*(5/9)

temperatures.loc[temperatures['Temperature']>40,'Temperature]=temp_celsius

assert temperatures['Temperature'].max()<40


>>>Fixing Dates

datetime is used to format dates

pandas.to_datetime()
 
%d-%m-%Y  25-12-2019
%c December 25th 2019
12-25-2019 %m-%d-%Y

birthdays['Birthday']=pd.to_datetime(birthdays['Birthday']
		,infer_datetime_format=True,
		errors='coerce')

NaT is Not a Date Time


birthdays['Birthday]=birthdays['Birthday].dt.strftime("%d-%m-%Y")


>>>Sample >>> converting euros to dollars

# Find values of acct_cur that are equal to 'euro'
acct_eu = banking['acct_cur'] == 'euro'

# Convert acct_amount where it is in euro to dollars
banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 

# Unify acct_cur column by changing 'euro' values to 'dollar'
banking.loc[acct_eu, 'acct_cur'] = 'dollar'

# Assert that only dollar currency remains
assert banking['acct_cur'].unique() == 'dollar'


>>>Sample >> converting dates

# Print the header of account_opened
print(banking['account_opened'].head())

# Convert account_opened to datetime
banking['account_opened'] = pd.to_datetime(banking['account_opened'],
                                           # Infer datetime format
                                           infer_datetime_format = True,
                                           # Return missing value for error
                                           errors = 'coerce')

# Get year of account opened
banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')

# Print acct_year
print(banking['acct_year'])

>>>>>>Cross field validation
1. The challenge from merging data from different sources is data integrity.

2. cross field validation is using multiple fields in a dataset to sanity check data integrity.

3. economy_class+business_class+first_class=total_passengers

sum_classes=flights[['economy_class','business_class','first_class']].sum(axis=1)

passenger_equal=sum_classes==flights['total_passengers']

inconsistent_pass=flights[~passenger_equal]
consistent_pass=flights[passenger_equal]

>>>>>>birthday check
import pandas as pd
import datetime as dt

users['Birthday']=pd.to_datetime(users['Birthday'])
today=dt.date.today()

age_manual=today.year - users['Birthday'].dt.year

age_equal=age_manual==users['Age']

inconsistent_age=users[~age_equal]
consistent_age=users[age_equal]

inconsistent data can be dropped, set to missing and impute
and apply rules from domain knowlege

>>>Sample >> sum funds a,b,c,d and compare inv_amount for inconsistencies

# Store fund columns to sum against
fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']

# Find rows where fund_columns row sum == inv_amount
inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']

# Store consistent and inconsistent data
consistent_inv = banking[inv_equ]
inconsistent_inv = banking[~inv_equ]

# Store consistent and inconsistent data
print("Number of inconsistent investments: ", inconsistent_inv.shape[0])

>>>>Sample >> check birthday and age consistency

# Store today's date and find ages
today = dt.date.today()
ages_manual = today.year - banking['birth_date'].dt.year

# Find rows where age column == ages_manual
age_equ = ages_manual == banking['age']

# Store consistent and inconsistent data
consistent_ages = banking[age_equ]
inconsistent_ages = banking[~age_equ]

>>>>>>>>>>>>>>Completeness
1. missing data is represented as na, nan, 0, ., or ...

caused from a technical error or human error

Temperature and co2

#find missing data
airquality.isna()

airquality.isna().sum()

import missingno as msno
import matplotlib.pyplot as plt

msno.matrix(airquality)
plt.show()

missing=airquality[airquality['CO2'].isna()]
complete=airquality[~airquality['CO2'].isna()]

missing.describe


sorted_airquality = airquality.sort_values(by='Temperature')
msno.matrix(sorted_airquality)
plt.show()

co2 are lost for extremely low temperatures

>>>>>Missingness types
1. Missing Completely at Random: No systematic relationship between a column's missing values and other or own values.

2. Missing at Random: There is a systematic relationship between a column's missing values and other observed values.

3. Missing not at Random: There is a systematic relationship between a column's missing values and unobserved values.

1. Missing completely at Random (no relationship)
2. Missing at Random (relationship with features)
3. Missing not at random (systemtic relationship causing the missing data)

>>>Dealing with missing data
1. drop missing data
2. impute with statistical measures (mean, median, mode)

drop values
airquality.dropna()

impute
airquality.fillna(mean)

feed values if we have enough knowledge of the dataset
airquality.fillna(custom)


>>>>>Sample >> using msno to visual missing values

# Print number of missing values in banking
print(banking.isna().sum())

# Visualize missingness matrix

>>>>Sample >> sort by age and display the mnso matrix

# Print number of missing values in banking
print(banking.isna().sum())

# Visualize missingness matrix
msno.matrix(banking)
plt.show()

# Isolate missing and non missing values of inv_amount
missing_investors = banking[banking['inv_amount'].isna()]
investors = banking[~banking['inv_amount'].isna()]

# Sort banking by age and visualize
banking_sorted = banking.sort_values(by='age')
msno.matrix(banking_sorted)
plt.show()

>>>>>Sample >>> imput

# Drop missing values of cust_id
banking_fullid = banking.dropna(subset = ['cust_id'])

# Compute estimated acct_amount
acct_imp = banking_fullid['inv_amount']*5
# Impute missing acct_amount with corresponding acct_imp
banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})

# Print number of missing values
print(banking_imputed.isna().sum())

msno.matrix(banking)
plt.show()


>>>Sample>>> print missing inv_amount values

# Print number of missing values in banking
print(banking.isna().sum())

# Visualize missingness matrix
msno.matrix(banking)
plt.show()

# Isolate missing and non missing values of inv_amount
missing_investors = banking[banking['inv_amount'].isna()]
investors = banking[~banking['inv_amount'].isna()]

missing.describe()

>>>>>>>>>>>>>Comparing strings

from fuzzywuzzy import fuzz

fuzz.WRatio('Reeding','Reading')
fuzz.WRatio('Houston Rockets','Rockets')






































