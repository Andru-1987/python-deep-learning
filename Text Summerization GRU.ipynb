{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnishimoto.BOISE\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dnishimoto.BOISE\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dnishimoto.BOISE\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dnishimoto.BOISE\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dnishimoto.BOISE\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dnishimoto.BOISE\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.utils import to_categorical\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, RepeatVector,Dense,Bidirectional,Embedding,GRU,TimeDistributed,LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
      "      dtype='object')\n",
      "Review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most. \n",
      "\tSummary: Good Quality Dog Food \n",
      "\n",
      "Review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\". \n",
      "\tSummary: Not as Advertised \n",
      "\n",
      "Review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch. \n",
      "\tSummary: \"Delight\" says it all \n",
      "\n",
      "Review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal. \n",
      "\tSummary: Cough Medicine \n",
      "\n",
      "Review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal. \n",
      "\tSummary: Great taffy \n",
      "\n",
      "Review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat. \n",
      "\tSummary: Nice Taffy \n",
      "\n",
      "Review: This saltwater taffy had great flavors and was very soft and chewy.  Each candy was individually wrapped well.  None of the candies were stuck together, which did happen in the expensive version, Fralinger's.  Would highly recommend this candy!  I served it at a beach-themed party and everyone loved it! \n",
      "\tSummary: Great!  Just as good as the expensive brands! \n",
      "\n",
      "Review: This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!! \n",
      "\tSummary: Wonderful, tasty taffy \n",
      "\n",
      "Review: Right now I'm mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too \n",
      "\tSummary: Yay Barley \n",
      "\n",
      "Review: This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding. \n",
      "\tSummary: Healthy Dog Food \n",
      "\n",
      "Review: I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service! \n",
      "\tSummary: The Best Hot Sauce in the World \n",
      "\n",
      "Review: One of my boys needed to lose some weight and the other didn't.  I put this food on the floor for the chubby guy, and the protein-rich, no by-product food up higher where only my skinny boy can jump.  The higher food sits going stale.  They both really go for this food.  And my chubby boy has been losing about an ounce a week. \n",
      "\tSummary: My cats LOVE this \"diet\" food better than their regular food \n",
      "\n",
      "Review: My cats have been happily eating Felidae Platinum for more than two years. I just got a new bag and the shape of the food is different. They tried the new food when I first put it in their bowls and now the bowls sit full and the kitties will not touch the food. I've noticed similar reviews related to formula changes in the past. Unfortunately, I now need to find a new food that my cats will eat. \n",
      "\tSummary: My Cats Are Not Fans of the New Food \n",
      "\n",
      "Review: good flavor! these came securely packed... they were fresh and delicious! i love these Twizzlers! \n",
      "\tSummary: fresh and greasy! \n",
      "\n",
      "Review: The Strawberry Twizzlers are my guilty pleasure - yummy. Six pounds will be around for a while with my son and I. \n",
      "\tSummary: Strawberry Twizzlers - Yummy \n",
      "\n",
      "Review: My daughter loves twizzlers and this shipment of six pounds really hit the spot. It's exactly what you would expect...six packages of strawberry twizzlers. \n",
      "\tSummary: Lots of twizzlers, just what you expect. \n",
      "\n",
      "Review: I love eating them and they are good for watching TV and looking at movies! It is not too sweet. I like to transfer them to a zip lock baggie so they stay fresh so I can take my time eating them. \n",
      "\tSummary: poor taste \n",
      "\n",
      "Review: I am very satisfied with my Twizzler purchase.  I shared these with others and we have all enjoyed them.  I will definitely be ordering more. \n",
      "\tSummary: Love it! \n",
      "\n",
      "Review: Twizzlers, Strawberry my childhood favorite candy, made in Lancaster Pennsylvania by Y & S Candies, Inc. one of the oldest confectionery Firms in the United States, now a Subsidiary of the Hershey Company, the Company was established in 1845 as Young and Smylie, they also make Apple Licorice Twists, Green Color and Blue Raspberry Licorice Twists, I like them all<br /><br />I keep it in a dry cool place because is not recommended it to put it in the fridge. According to the Guinness Book of Records, the longest Licorice Twist ever made measured 1.200 Feet (370 M) and weighted 100 Pounds (45 Kg) and was made by Y & S Candies, Inc. This Record-Breaking Twist became a Guinness World Record on July 19, 1998. This Product is Kosher! Thank You \n",
      "\tSummary: GREAT SWEET CANDY! \n",
      "\n",
      "Review: Candy was delivered very fast and was purchased at a reasonable price.  I was home bound and unable to get to a store so this was perfect for me. \n",
      "\tSummary: Home delivered twizlers \n",
      "\n",
      "sos Good Quality Dog Food eos\n",
      "sos Not as Advertised eos\n",
      "sos \"Delight\" says it all eos\n",
      "sos Cough Medicine eos\n",
      "sos Great taffy eos\n",
      "sos Nice Taffy eos\n",
      "sos Great!  Just as good as the expensive brands! eos\n",
      "sos Wonderful, tasty taffy eos\n",
      "sos Yay Barley eos\n",
      "sos Healthy Dog Food eos\n",
      "sos The Best Hot Sauce in the World eos\n",
      "sos My cats LOVE this \"diet\" food better than their regular food eos\n",
      "sos My Cats Are Not Fans of the New Food eos\n",
      "sos fresh and greasy! eos\n",
      "sos Strawberry Twizzlers - Yummy eos\n",
      "sos Lots of twizzlers, just what you expect. eos\n",
      "sos poor taste eos\n",
      "sos Love it! eos\n",
      "sos GREAT SWEET CANDY! eos\n",
      "sos Home delivered twizlers eos\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"Reviews.csv\",nrows=20)\n",
    "print(df.columns)\n",
    "df.drop_duplicates(subset=['Text'],inplace=True)  #dropping duplicates\\n\",\n",
    "df.dropna(axis=0,inplace=True)\n",
    "\n",
    "detail_sentences=[]\n",
    "summary_sentences=[]\n",
    "for detail,summary in zip(df['Text'],df['Summary']):\n",
    "    print(\"Review:\",detail,\"\\n\\tSummary:\",summary,\"\\n\")\n",
    "    detail_sentences.append(detail)\n",
    "    summary_sentences.append(summary)\n",
    "    \n",
    "data=[]\n",
    "\n",
    "for sent in df['Text']:\n",
    "    # Add sos and eos tokens using string.join\n",
    "    sent_new = \" \".join(['sos', sent, 'eos'])\n",
    "    data.append(sent_new)\n",
    "\n",
    "summary=[]\n",
    "for sent in df['Summary']:\n",
    "    # Add sos and eos tokens using string.join\n",
    "    sent_new = \" \".join(['sos', sent, 'eos'])\n",
    "    print(sent_new)\n",
    "    summary.append(sent_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sos good quality dog food eos\n",
      " sos not as advertised eos\n",
      " sos delight says it all eos\n",
      " sos cough medicine eos\n",
      " sos great taffy eos\n",
      " sos nice taffy eos\n",
      " sos great just as good as the expensive brands eos\n",
      " sos wonderful tasty taffy eos\n",
      " sos yay barley eos\n",
      " sos healthy dog food eos\n",
      " sos the best hot sauce in the world eos\n",
      " sos my cats love this diet food better than their regular food eos\n",
      " sos my cats are not fans of the new food eos\n",
      " sos fresh and greasy eos\n",
      " sos strawberry twizzlers yummy eos\n",
      " sos lots of twizzlers just what you expect eos\n",
      " sos poor taste eos\n",
      " sos love it eos\n",
      " sos great sweet candy eos\n",
      " sos home delivered twizlers eos\n",
      "[[ 1  8 18  9  3  2  0  0  0  0  0  0  0]\n",
      " [ 1 10  5 19  2  0  0  0  0  0  0  0  0]\n",
      " [ 1 20 21 11 22  2  0  0  0  0  0  0  0]\n",
      " [ 1 23 24  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  6  7  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 25  7  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  6 12  5  8  5  4 26 27  2  0  0  0]\n",
      " [ 1 28 29  7  2  0  0  0  0  0  0  0  0]\n",
      " [ 1 30 31  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 32  9  3  2  0  0  0  0  0  0  0  0]\n",
      " [ 1  4 33 34 35 36  4 37  2  0  0  0  0]\n",
      " [ 1 13 14 15 38 39  3 40 41 42 43  3  2]\n",
      " [ 1 13 14 44 10 45 16  4 46  3  2  0  0]\n",
      " [ 1 47 48 49  2  0  0  0  0  0  0  0  0]\n",
      " [ 1 50 17 51  2  0  0  0  0  0  0  0  0]\n",
      " [ 1 52 16 17 12 53 54 55  2  0  0  0  0]\n",
      " [ 1 56 57  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 15 11  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  6 58 59  2  0  0  0  0  0  0  0  0]\n",
      " [ 1 60 61 62  2  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = Tokenizer()\n",
    "en_tokenizer.fit_on_texts(list(data))\n",
    "\n",
    "x_data    =   en_tokenizer.texts_to_sequences(data) \n",
    "x_data    =   pad_sequences(x_data,  maxlen=None, padding='post')\n",
    "#reverse the sequence\n",
    "x_data=x_data[:,::-1]\n",
    "en_vocab=len(en_tokenizer.word_index)+1\n",
    "\n",
    "#print(en_tokenizer.index_word)\n",
    "\n",
    "fr_tokenizer = Tokenizer()\n",
    "fr_tokenizer.fit_on_texts(list(summary))\n",
    "\n",
    "y_summary    =   fr_tokenizer.texts_to_sequences(summary) \n",
    "y_summary    =   pad_sequences(y_summary, maxlen=None, padding='post')\n",
    "\n",
    "fr_vocab=len(fr_tokenizer.word_index)+1\n",
    "\n",
    "max_len_text=x_data.shape[1]\n",
    "max_len_summary=y_summary.shape[1]\n",
    "\n",
    "#[[print(fr_tokenizer.index_word[i]) for i in sentence if i in fr_tokenizer.index_word] for sentence in y_summary]\n",
    "for sentence_index in y_summary:\n",
    "    sentence=\"\"\n",
    "    for i in sentence_index:\n",
    "        if i in fr_tokenizer.index_word:\n",
    "            sentence+=\" \"+fr_tokenizer.index_word[i]\n",
    "    print(sentence)\n",
    "print(y_summary)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(x_data,y_summary,test_size=0.1,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dnishimoto.BOISE\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_inputs (InputLayer)  (None, 152, 458)          0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    [(None, 256), (None, 256) 549120    \n",
      "=================================================================\n",
      "Total params: 549,120\n",
      "Trainable params: 549,120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, 152, 458)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, 256), (None, 549120      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 13, 256)      0           gru[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 13, 256)      393984      repeat_vector[0][0]              \n",
      "                                                                 gru[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 13, 63)       16191       gru_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 959,295\n",
      "Trainable params: 959,295\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hsize = 256\n",
    "# Define an input layer\n",
    "en_inputs = keras.layers.Input(shape=(max_len_text,en_vocab),name='encoder_inputs')\n",
    "#en_inputs = keras.layers.Input(shape=(en_len,en_vocab),name='encoder_inputs')\n",
    "# Define a GRU layer which returns the state\n",
    "en_gru = keras.layers.GRU(hsize, return_state=True)\n",
    "# Get the output and state from the GRU\n",
    "en_out,en_state = en_gru(en_inputs)\n",
    "# Define and print the model summary\n",
    "encoder = keras.models.Model(inputs=en_inputs, outputs=en_state)\n",
    "\n",
    "# Define a RepeatVector layer\n",
    "de_inputs = RepeatVector(max_len_summary)(en_state)\n",
    "# Define a GRU model that returns all outputs\n",
    "decoder_gru = keras.layers.GRU(hsize, return_sequences=True)\n",
    "# Get the outputs of the decoder\n",
    "gru_outputs = decoder_gru(de_inputs, initial_state=en_state)\n",
    "# Define a model with the correct inputs and outputs\n",
    "\n",
    "\n",
    "de_dense = keras.layers.Dense(fr_vocab, activation='softmax')\n",
    "de_dense_time = keras.layers.TimeDistributed(de_dense)\n",
    "de_pred=de_dense_time(gru_outputs)\n",
    "\n",
    "#enc_dec = Model(inputs=en_inputs, outputs=gru_outputs)\n",
    "enc_dec = Model(inputs=en_inputs, outputs=de_pred)\n",
    "enc_dec.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "print(encoder.summary())\n",
    "print(enc_dec.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents2seqs(input_type, sentences, tokenizer,sentence_len,vocab_size, onehot=False, pad_type='post', reverse=False):     \n",
    "    encoded_text = tokenizer.texts_to_sequences(sentences)\n",
    "    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=sentence_len)\n",
    "    if reverse:\n",
    "      # Reverse the text using numpy axis reversing\n",
    "      preproc_text = preproc_text[:,::-1]\n",
    "    if onehot:\n",
    "        preproc_text = to_categorical(preproc_text, num_classes=vocab_size)\n",
    "    return preproc_text\n",
    "\n",
    "#n_epochs=10\n",
    "#data_size=len(x_data)\n",
    "#batch_size=1024\n",
    "\n",
    "#for ei in range(n_epochs):\n",
    "#    print(\"epoch\",ei)\n",
    "#    for i in range(0,data_size,batch_size):\n",
    "#        en_x=to_categorical(X_train[i:i+batch_size],en_vocab)\n",
    "#        fr_y=to_categorical(y_train[i:i+batch_size],fr_vocab)\n",
    "#        enc_dec.train_on_batch(en_x,fr_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=False)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    model.compile(loss = sparse_categorical_crossentropy, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['acc'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 150\n",
      "Max French sentence length: 11\n",
      "English vocabulary size: 455\n",
      "French vocabulary size: 60\n",
      "Record Count 20\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    x_tk = Tokenizer(char_level = False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
    "\n",
    "def preprocess_embedding(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess_embedding(detail_sentences, summary_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)\n",
    "print(\"Record Count\",len(detail_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 128)          58368     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 512)               591360    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 11, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 11, 512)           1181184   \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 11, 61)            31293     \n",
      "=================================================================\n",
      "Total params: 1,862,205\n",
      "Trainable params: 1,862,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 6s 294ms/sample - loss: 4.1130 - acc: 0.0045\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 2s 108ms/sample - loss: 3.4090 - acc: 0.6227\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 2s 99ms/sample - loss: 3.9734 - acc: 0.6227\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 3s 128ms/sample - loss: 2.1479 - acc: 0.6227\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 2s 125ms/sample - loss: 2.2536 - acc: 0.6227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x211c1aa9888>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x = pad(preproc_english_sentences)\n",
    "model = model_final(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "#model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 61, 62, 11, 1, 64, 22, 33, 65, 2, 21, 34, 66, 35, 16, 67, 11, 13, 17, 69, 70, 71, 4, 72, 36, 4, 73, 74, 2, 5, 75, 6, 77, 2, 78, 79, 3, 17, 37, 36]\n",
      "good\n",
      "quality\n",
      "dog\n",
      "food\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "en_st=\"sos I have bought several of the Vitality canned dog food products and have found them all to be of good quality.The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most. eos\"\n",
    "#en_seq=sents2seqs('source',en_st,english_tokenizer,max_english_sequence_length,english_vocab_size,onehot=True,reverse=True)\n",
    "sentence=[english_tokenizer.word_index[word] for word in en_st.split(\" \") if word in english_tokenizer.word_index]\n",
    "print(sentence)\n",
    "sentence = pad_sequences([sentence], maxlen=max_english_sequence_length, padding='post')\n",
    "#print(max_english_sequence_length)\n",
    "sentences = np.array([sentence[0], preproc_english_sentences[0]])\n",
    "#print(sentences)\n",
    "fr_predictions=model.predict(sentences,len(sentences))\n",
    "#print(french_tokenizer.index_word)\n",
    "#print(fr_predictions)\n",
    "\n",
    "for x in fr_predictions[0]:\n",
    "        index=np.argmax(x)\n",
    "        if index !=0:\n",
    "            print(french_tokenizer.index_word[index])\n",
    "#print(' '.join([french_tokenizer.index_word[np.argmax(x)] for x in fr_predictions[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
