{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\analysis for traffic stops by police officers.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\analysis for traffic stops by police officers.txt\n",
      "the dataset for stops by police officers in the state of rhode island.\n",
      "\n",
      "1. State\n",
      "2. stop_date\n",
      "3. stop_time\n",
      "4. county_name (contains nan values)\n",
      "5. driver_gender\n",
      "6. driver_race\n",
      "\n",
      "\n",
      "ri=pd.read_csv('police.csv')\n",
      "ri.isnull()\n",
      "\n",
      "ri.isnull().sum()\n",
      "county_name=91741\n",
      "\n",
      "ri.shape()\n",
      "output: 91741,15\n",
      "\n",
      "drop county_name column\n",
      "\n",
      "ri.drop('county_name',axis='columns', inplace=True)\n",
      "\n",
      ".dropna() : drops rows based on the presence of missing values.\n",
      "\n",
      "\n",
      "\n",
      "   sample  > dropping a column\n",
      "\n",
      "# Examine the shape of the DataFrame\n",
      "print(ri.shape)\n",
      "\n",
      "# Drop the 'county_name' and 'state' columns\n",
      "ri.drop(['county_name', 'state'], axis='columns', inplace=True)\n",
      "\n",
      "# Examine the shape of the DataFrame (again)\n",
      "print(ri.shape)\n",
      "\n",
      "   sample  > drop na subset\n",
      "\n",
      "# Count the number of missing values in each column\n",
      "print(ri.isnull().sum())\n",
      "\n",
      "# Drop all rows that are missing 'driver_gender'\n",
      "ri.dropna(subset=['driver_gender'], inplace=True)\n",
      "\n",
      "# Count the number of missing values in each column (again)\n",
      "print(ri.isnull().sum())\n",
      "\n",
      "# Examine the shape of the DataFrame\n",
      "print(ri.shape)\n",
      "\n",
      "\n",
      "removing columns and rows that will not be useful.\n",
      "\n",
      "\n",
      "   Examining the data types\n",
      "read_csv creates an inferred datatype\n",
      "\n",
      "print(ri.dtypes)\n",
      "\n",
      "dtype:\n",
      "1.object\n",
      "2.bool\n",
      "3.int\n",
      "4.float\n",
      "5.datetime\n",
      "6.category\n",
      "\n",
      "datatype affect opeations you can perform\n",
      "\n",
      "math operations can be performed on int and floats\n",
      "\n",
      "datetime \n",
      "category uses less memory and runs faster\n",
      "bool enables logical and mathematical operations\n",
      "\n",
      "\n",
      "apple\n",
      "1. date\n",
      "2. time\n",
      "3. price\n",
      "\n",
      "apple.price.dtype\n",
      "output dtype('O') means object\n",
      "\n",
      "apple['price']= apple.price.astype('float')\n",
      "\n",
      "\n",
      "  >Sample  > convert object dtype to bool\n",
      "\n",
      "# Examine the head of the 'is_arrested' column\n",
      "print(ri.is_arrested.dtype)\n",
      "\n",
      "# Change the data type of 'is_arrested' to 'bool'\n",
      "ri['is_arrested'] = ri.is_arrested.astype(bool)\n",
      "\n",
      "# Check the data type of 'is_arrested' \n",
      "print(ri.is_arrested.dtype)\n",
      "\n",
      "\n",
      "   sample  > value_counts and unique\n",
      "\n",
      "# Count the unique values in 'violation'\n",
      "print(ri['violation'].unique())\n",
      "\n",
      "# Express the counts as proportions\n",
      "print(ri['violation'].value_counts(normalize=True))\n",
      "\n",
      "['Equipment' 'Speeding' 'Other' 'Moving violation' 'Registration/plates'\n",
      " 'Seat belt']\n",
      "Speeding               48423\n",
      "Moving violation       16224\n",
      "Equipment              10921\n",
      "Other                   4409\n",
      "Registration/plates     3703\n",
      "Seat belt               2856\n",
      "Name: violation, dtype: int64\n",
      "\n",
      "  >normalized=True    output\n",
      "\n",
      "Speeding               0.559571\n",
      "Moving violation       0.187483\n",
      "Equipment              0.126202\n",
      "Other                  0.050950\n",
      "Registration/plates    0.042791\n",
      "Seat belt              0.033004\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "\n",
      "  >sample    women have more speeding violations\n",
      "\n",
      "# Create a DataFrame of female drivers\n",
      "female = ri[ri['driver_gender']=='F']\n",
      "\n",
      "# Create a DataFrame of male drivers\n",
      "male = ri[ri['driver_gender']=='M']\n",
      "\n",
      "print(female.violation.value_counts(normalize=True))\n",
      "\n",
      "# Compute the violations by male drivers (as proportions)\n",
      "print(male.violation.value_counts(normalize=True))\n",
      "\n",
      "output:\n",
      "\n",
      "Speeding               0.658114\n",
      "Moving violation       0.138218\n",
      "Equipment              0.105199\n",
      "Registration/plates    0.044418\n",
      "Other                  0.029738\n",
      "Seat belt              0.024312\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "Speeding               0.522243\n",
      "Moving violation       0.206144\n",
      "Equipment              0.134158\n",
      "Other                  0.058985\n",
      "Registration/plates    0.042175\n",
      "Seat belt              0.036296\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "\n",
      "Filtering a dataframe using multiple conditions\n",
      "\n",
      "female = ri[ri.driver_gender=='F']\n",
      "female.shape\n",
      "\n",
      "or\n",
      "\n",
      "female = ri[\n",
      "(ri.driver_gender=='F') &\n",
      "(ri.is_arrested==True)\n",
      "]\n",
      "female.shape\n",
      "\n",
      "\n",
      "each condition is surround by parentheses and the & separates the conditions\n",
      "\n",
      "only female drivers who were arrested\n",
      "\n",
      "| represents the or condition\n",
      "\n",
      "|| represents the and condition\n",
      "\n",
      "\n",
      " sample  > filtering\n",
      "\n",
      "ri[(ri.driver_gender=='F') & (ri.violation=='Speeding')]\n",
      "\n",
      "\n",
      " > Sample  > Stop outcomes\n",
      "\n",
      "\n",
      "# Create a DataFrame of female drivers stopped for speeding\n",
      "female_and_speeding = ri[(ri.driver_gender=='F') & (ri.violation=='Speeding')]\n",
      "\n",
      "# Create a DataFrame of male drivers stopped for speeding\n",
      "male_and_speeding = ri[(ri.driver_gender=='M') & (ri.violation=='Speeding')]\n",
      "\n",
      "print(\"male\")\n",
      "# Compute the stop outcomes for female drivers (as proportions)\n",
      "print(female_and_speeding.stop_outcome.value_counts(normalize=True))\n",
      "print(\"female\")\n",
      "# Compute the stop outcomes for male drivers (as proportions)\n",
      "print(male_and_speeding.stop_outcome.value_counts(normalize=True))\n",
      "\n",
      "Output::  (95% of stops resulting in a ticket)\n",
      "\n",
      "male\n",
      "Citation            0.952192\n",
      "Warning             0.040074\n",
      "Arrest Driver       0.005752\n",
      "N/D                 0.000959\n",
      "Arrest Passenger    0.000639\n",
      "No Action           0.000383\n",
      "Name: stop_outcome, dtype: float64\n",
      "\n",
      "female\n",
      "Citation            0.944595\n",
      "Warning             0.036184\n",
      "Arrest Driver       0.015895\n",
      "Arrest Passenger    0.001281\n",
      "No Action           0.001068\n",
      "N/D                 0.000976\n",
      "Name: stop_outcome, dtype: float64\n",
      "\n",
      "\n",
      "   >Does gender affect the vehicles that are searched?\n",
      "\n",
      "\n",
      "ri.isnull().sum()\n",
      "\n",
      "true is 1\n",
      "false is 0\n",
      "then sum the rows\n",
      "\n",
      "the mean of a boolean series represents the percentage of True values\n",
      "\n",
      "ri.is_arrested.value_counts(normalized=True)\n",
      ".03\n",
      "ri.is_arrested.mean()\n",
      ".03\n",
      "\n",
      "\n",
      "find the unique districts\n",
      "\n",
      "ri.district.unique()\n",
      "\n",
      "print(df_sas[df_sas['District'].isin(districts)]['ArrestInt'].mean())\n",
      "\n",
      "\n",
      "print(df_sas.groupby('District')['ArrestInt'].mean())\n",
      "\n",
      "print(df_sas.groupby(['District','Ward'])['ArrestInt'].mean())\n",
      "\n",
      "  >Sample    search_conducted\n",
      "\n",
      "# Check the data type of 'search_conducted'\n",
      "print(ri['search_conducted'].dtype)\n",
      "\n",
      "# Calculate the search rate by counting the values\n",
      "print(ri['search_conducted'].value_counts(normalize=True))\n",
      "\n",
      "# Calculate the search rate by taking the mean\n",
      "print(ri.search_conducted.mean())\n",
      "\n",
      "\n",
      "output\n",
      "bool\n",
      "False    0.961785\n",
      "True     0.038215\n",
      "Name: search_conducted, dtype: float64\n",
      "0.0382153092354627\n",
      "\n",
      "\n",
      "  Sample  > female\n",
      "\n",
      "# Calculate the search rate for female drivers\n",
      "print(ri[ri.driver_gender=='F'].search_conducted.mean())\n",
      "\n",
      "output: 0.019180617481282074 (female)\n",
      "output: 0.04542557598546892 (male)\n",
      "\n",
      " >Sample  > groupby\n",
      "\n",
      "# Calculate the search rate for both groups simultaneously\n",
      "print(ri.groupby('driver_gender').search_conducted.mean())\n",
      "\n",
      " >Sample  > groupby multiple column\n",
      "\n",
      "print(ri.groupby(['driver_gender','violation']).search_conducted.mean())\n",
      "\n",
      "driver_gender  violation          \n",
      "F              Equipment              0.039984\n",
      "               Moving violation       0.039257\n",
      "               Other                  0.041018\n",
      "               Registration/plates    0.054924\n",
      "               Seat belt              0.017301\n",
      "               Speeding               0.008309\n",
      "\n",
      "M              Equipment              0.071496\n",
      "               Moving violation       0.061524\n",
      "               Other                  0.046191\n",
      "               Registration/plates    0.108802\n",
      "               Seat belt              0.035119\n",
      "               Speeding               0.027885\n",
      "Name: search_conducted, dtype: float64\n",
      "\n",
      "\n",
      "       >Gender affect frisking\n",
      "\n",
      "ri.search_type.value_counts(dropna=False)\n",
      "1. Incident to Arrest\n",
      "2. Probable cause\n",
      "3. Inventory\n",
      "4. Reasonable Suspicion\n",
      "5. Protective Frisk\n",
      "6. Incident to Arrest, Inventory\n",
      "7. Incident to Arrest, Probable Cause\n",
      "\n",
      "\n",
      "ri['inventory']=ri.search_type.str.contains('Inventory',na=False)\n",
      "\n",
      "na=False means return a false when it finds a missing value\n",
      "ri.inventory.sum()\n",
      "\n",
      "\n",
      "search=ri[ri.searched_conducted==True]\n",
      "searched.inventory.mean()\n",
      "\n",
      "\n",
      " >Sample    > search type count, frisk in the search_type\n",
      "\n",
      "# Count the 'search_type' values\n",
      "print(len(ri.search_type.unique()))\n",
      "\n",
      "# Check if 'search_type' contains the string 'Protective Frisk'\n",
      "ri['frisk'] = ri.search_type.str.contains('Protective Frisk', na=False)\n",
      "\n",
      "# Check the data type of 'frisk'\n",
      "print(ri['frisk'].dtype)\n",
      "\n",
      "# Take the sum of 'frisk'\n",
      "print(ri['frisk'].sum())\n",
      "\n",
      "\n",
      " >Sample  > search conduction    frisk average per gender\n",
      "\n",
      "# Create a DataFrame of stops in which a search was conducted\n",
      "searched = ri[ri.search_conducted == True]\n",
      "\n",
      "# Calculate the overall frisk rate by taking the mean of 'frisk'\n",
      "print(searched.frisk.mean())\n",
      "\n",
      "# Calculate the frisk rate for each gender\n",
      "print(searched.groupby(\"driver_gender\").frisk.mean())\n",
      "\n",
      "      Does the time of day affect arrest rate\n",
      "\n",
      "analyzing datetime data\n",
      "\n",
      "apple\n",
      "1. price\n",
      "2. volume (shares traded)\n",
      "3. date_and_time\n",
      "\n",
      "\n",
      "dt.month\n",
      "dt.week\n",
      "dt.dayofweek\n",
      "dt.hour\n",
      "\n",
      "apple.set_index('date_and_time', inplace=True)\n",
      "apple.index.month\n",
      "apple.price.mean()\n",
      "\n",
      "month_price=apple.groupby(apple.index.month).price.mean()\n",
      "\n",
      "monthly_price.plot()\n",
      "plt.xlabel('Month')\n",
      "plt.ylabel('Price')\n",
      "\n",
      "df_sas['Year']=pd.DatetimeIndex(df_sas['date']).year\n",
      "arrest_year=df_sas.groupby(['Year'])['ArrestInt'].sum()\n",
      "\n",
      "\n",
      "  >Sample   > arrest rate as a time of day\n",
      "\n",
      "# Calculate the overall arrest rate\n",
      "print(ri.is_arrested.mean())\n",
      "\n",
      "# Calculate the hourly arrest rate\n",
      "print(ri.groupby(ri.index.hour).is_arrested.mean())\n",
      "\n",
      "# Save the hourly arrest rate\n",
      "hourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean()\n",
      "\n",
      "\n",
      "  Sample  > plot arrest time\n",
      "\n",
      "# Import matplotlib.pyplot as plt\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Create a line plot of 'hourly_arrest_rate'\n",
      "hourly_arrest_rate.plot()\n",
      "\n",
      "# Add the xlabel, ylabel, and title\n",
      "plt.xlabel('Hour')\n",
      "plt.ylabel('Arrest Rate')\n",
      "plt.title('Arrest Rate by Time of Day')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "         Are drug related stops on the rise\n",
      "1. We will use a subplot to see how two variables change over time\n",
      "\n",
      "2. Resampling is when you change the frequency of the time series\n",
      "\n",
      "monthly_price=apple.price.resample('M').mean()\n",
      "\n",
      "resample by month\n",
      "\n",
      "the output is the last day of the month rather than a number\n",
      "\n",
      "monthly_volume=apple.volume.resample('M').mean()\n",
      "\n",
      "\n",
      "pd.concat([monthly_price,monthly_volume],axis='columns')\n",
      "\n",
      "concatenates along a specified axis\n",
      "\n",
      "monthly.plot(subplots=True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  Sample    drug related stops   resampling\n",
      "\n",
      "# Calculate the annual rate of drug-related stops\n",
      "print(ri.drugs_related_stop.resample('A').mean())\n",
      "\n",
      "# Save the annual rate of drug-related stops\n",
      "annual_drug_rate = ri.drugs_related_stop.resample('A').mean()\n",
      "\n",
      "# Create a line plot of 'annual_drug_rate'\n",
      "annual_drug_rate.plot(subplots=True)\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "  Sample  > concatenate the two columns\n",
      "\n",
      "# Calculate and save the annual search rate\n",
      "annual_search_rate = ri.search_conducted.resample('A').mean()\n",
      "\n",
      "# Concatenate 'annual_drug_rate' and 'annual_search_rate'\n",
      "annual = pd.concat([annual_drug_rate,annual_search_rate], axis='columns')\n",
      "\n",
      "# Create subplots from 'annual'\n",
      "annual.plot(subplots=True)\n",
      "\n",
      "# Display the subplots\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    What violations are caught in each district\n",
      "\n",
      "result=df_sas.groupby(['Year','Month','fbi_code'])['ArrestInt'].sum().reset_index()\n",
      "#print(top20.columns)\n",
      "mask=result['ArrestInt']>30\n",
      "fbi_codes=result[mask]['fbi_code'].unique()\n",
      "\n",
      "filter=df_sas['fbi_code'].isin(fbi_codes) \n",
      "fbi_codes=df_sas['fbi_code'].unique()\n",
      "\n",
      "arrest_breakdown=df_sas[filter].groupby(['Year','Month','fbi_code'])['ArrestInt'].sum().reset_index()\n",
      "keys=arrest_breakdown.keys()\n",
      "#print(arrest_breakdown)\n",
      "\n",
      "g = sns.factorplot(data=arrest_breakdown, x='Year', y='ArrestInt', \n",
      "                  hue='fbi_code',  kind='point',size=8,aspect=2)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "         cross tab\n",
      "\n",
      "table=pd.crosstab(ri.driver_race, ri_driver_gender)\n",
      "\n",
      "creates a pivot table building a frequency table\n",
      "\n",
      "ri[(ri.driver_race=='Asian') & (ri.driver_gender=='F')].shape\n",
      "\n",
      "\n",
      "range=table.loc['Asian':'Hispanic']\n",
      "\n",
      "range.plot(kind='bar')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " > stack bar plot\n",
      "\n",
      "range.plot(kind='bar', stacked=True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    Sample\n",
      "\n",
      "# Create a frequency table of districts and violations\n",
      "print(pd.crosstab(ri.district,ri.violation))\n",
      "\n",
      "# Save the frequency table as 'all_zones'\n",
      "all_zones = pd.crosstab(ri.district,ri.violation)\n",
      "\n",
      "# Select rows 'Zone K1' through 'Zone K3'\n",
      "print(all_zones.loc['Zone K1':'Zone K3'])\n",
      "\n",
      "# Save the smaller table as 'k_zones'\n",
      "k_zones = all_zones.loc['Zone K1':'Zone K3']\n",
      "\n",
      "k_zone.plot(kind='bar', stacked=True)\n",
      "plt.show()\n",
      "\n",
      "     How long might you be stopped\n",
      "\n",
      "apple\n",
      "date_and_time\n",
      "price\n",
      "volume\n",
      "change\n",
      "\n",
      "change when  change\n",
      "\n",
      "True if the price went up\n",
      "\n",
      "calculate how often the price went up taking the column mean\n",
      "\n",
      "\n",
      "Stefan Jansen\n",
      "https://www.amazon.com/dp/B08D9SP6MB/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1\n",
      "\n",
      "mapping = {'up':True, 'down':False}\n",
      "apple['is_up']=apple.chage.map(mapping)\n",
      "\n",
      "apple.is_up.mean()\n",
      "\n",
      "  >how often searches occur after each violation\n",
      "\n",
      "search_rate=ri.groupby('violation').search_conducted.mean()\n",
      "\n",
      "search_rate.plot(kind='bar')\n",
      "plt.show()\n",
      "\n",
      "search rate is on the y axis\n",
      "the violation is on the x axis\n",
      "\n",
      "\n",
      "search_rate.plot(kind='barh')\n",
      "plt.show()\n",
      "\n",
      "  sample    mapping\n",
      "\n",
      "# Print the unique values in 'stop_duration'\n",
      "print(ri.stop_duration.unique())\n",
      "\n",
      "# Create a dictionary that maps strings to integers\n",
      "mapping = {\n",
      "    '0-15 Min': 8,\n",
      "    '16-30 Min': 23,\n",
      "    '30+ Min': 45\n",
      "    }\n",
      "\n",
      "# Convert the 'stop_duration' strings to integers using the 'mapping'\n",
      "ri['stop_minutes'] = ri.stop_duration.map(mapping)\n",
      "\n",
      "# Print the unique values in 'stop_minutes'\n",
      "print(ri['stop_minutes'].unique())\n",
      "\n",
      "\n",
      "   sample  >  groupby    average  > sort\n",
      "\n",
      "\n",
      "# Calculate the mean 'stop_minutes' for each value in 'violation_raw'\n",
      "print(ri.groupby('violation_raw')['stop_minutes'].mean())\n",
      "\n",
      "# Save the resulting Series as 'stop_length'\n",
      "stop_length = ri.groupby('violation_raw')['stop_minutes'].mean()\n",
      "\n",
      "# Sort 'stop_length' by its values and create a horizontal bar plot\n",
      "stop_length.sort_values().plot(kind='barh')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   Exploring the weather dataset\n",
      "\n",
      "do weather conditions affect police behavior\n",
      "\n",
      "noaa : national centers for environmental information\n",
      "\n",
      "single station in rhode islands to give weather information\n",
      "\n",
      "weather = pd.read_csv('weather.csv')\n",
      "\n",
      "weather.head(3)\n",
      "\n",
      "TAVG, TMIN, TMAX: Temperature\n",
      "AWND. WSF2: Wind Speed (average, fastest wind speed in a 2 minute interval)\n",
      "WT01, WT022: Bad Weather conditions\n",
      "\n",
      "https://mesonet.agron.iastate.edu/request/download.phtml?network=ID_ASOS\n",
      "\n",
      "increased convinced the data is trustworthy\n",
      "\n",
      "\n",
      "station:three or four character site identifier\n",
      "valid:timestamp of the observation\n",
      "tmpf:Air Temperature in Fahrenheit, typically @ 2 meters\n",
      "dwpf:Dew Point Temperature in Fahrenheit, typically @ 2 meters\n",
      "relh:Relative Humidity in %\n",
      "drct:Wind Direction in degrees from north\n",
      "sknt:Wind Speed in knots \n",
      "p01i:One hour precipitation for the period from the observation time to the time of the previous hourly precipitation reset. This varies slightly by site. Values are in inches. This value may or may not contain frozen precipitation melted by some device on the sensor or estimated by some other means. Unfortunately, we do not know of an authoritative database denoting which station has which sensor.\n",
      "alti:Pressure altimeter in inches\n",
      "mslp:Sea Level Pressure in millibar\n",
      "vsby:Visibility in miles\n",
      "gust:Wind Gust in knots\n",
      "skyc1:Sky Level 1 Coverage\n",
      "skyc2:Sky Level 2 Coverage\n",
      "skyc3:Sky Level 3 Coverage\n",
      "skyc4:Sky Level 4 Coverage\n",
      "skyl1:Sky Level 1 Altitude in feet\n",
      "skyl2:Sky Level 2 Altitude in feet\n",
      "skyl3:Sky Level 3 Altitude in feet\n",
      "skyl4:Sky Level 4 Altitude in feet\n",
      "wxcodes:Present Weather Codes (space seperated)\n",
      "feel:Apparent Temperature (Wind Chill or Heat Index) in Fahrenheit\n",
      "ice_accretion_1hr:Ice Accretion over 1 Hour (inches)\n",
      "ice_accretion_3hr:Ice Accretion over 3 Hours (inches)\n",
      "ice_accretion_6hr:Ice Accretion over 6 Hours (inches)\n",
      "peak_wind_gust:Peak Wind Gust (from PK WND METAR remark) (knots)\n",
      "peak_wind_drct:Peak Wind Gust Direction (from PK WND METAR remark) (deg)\n",
      "peak_wind_time:Peak Wind Gust Time (from PK WND METAR remark)\n",
      "metar:unprocessed reported observation in METAR format\n",
      "\n",
      "weather[['AWND','WSF2']].describe()\n",
      "\n",
      "create a box plot\n",
      "weather[['AWND','WSF2']].plot(kind='box')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "take the fast wind speed minus the average wind speed\n",
      "\n",
      "weather['WDIFF']= weather.WSF2-weather.AWND\n",
      "\n",
      "weather.WDIFF.plot(kind='hist', bins =20)\n",
      "plt.show()\n",
      "\n",
      "  Sample   > box plot temperatures\n",
      "\n",
      "# Read 'weather.csv' into a DataFrame named 'weather'\n",
      "df=pd.read_csv('weather.csv')\n",
      "\n",
      "# Describe the temperature columns\n",
      "print(df[['TMIN','TAVG','TMAX']].describe())\n",
      "\n",
      "# Create a box plot of the temperature columns\n",
      "df[['TMIN','TAVG','TMAX']].plot(kind='box')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "  >Sample   histogram to confirm temperature range distribution\n",
      "\n",
      "# Create a 'TDIFF' column that represents temperature difference\n",
      "weather['TDIFF']=weather.TMAX- weather.TMIN\n",
      "\n",
      "# Describe the 'TDIFF' column\n",
      "print(weather['TDIFF'].describe())\n",
      "\n",
      "# Create a histogram with 20 bins to visualize 'TDIFF'\n",
      "weather['TDIFF'].plot(kind='hist',bins=20)\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "            >Categorizing the weather\n",
      "\n",
      "\n",
      "slicing columns of the original dataframe\n",
      "\n",
      "temp=weather.loc[:,'TAVG':'TMAX']\n",
      "\n",
      "\n",
      "temp.sum(axis='columns').head()  \n",
      "\n",
      "this sums all the columns\n",
      "\n",
      "ri.stop_duration.unique()\n",
      "\n",
      "mapping = {\n",
      "    '0-15 Min': 'short',\n",
      "    '16-30 Min': 'medium',\n",
      "    '30+ Min': 'long'\n",
      "    }\n",
      "\n",
      "# Convert the 'stop_duration' strings to integers using the 'mapping'\n",
      "ri['stop_length'] = ri.stop_duration.map(mapping)\n",
      "\n",
      "ri.stop_length.dtype\n",
      "outputs object type because it contains string data\n",
      "\n",
      "ri.stop_length.unique()\n",
      "\n",
      "\n",
      "cats=['short','medium','long']\n",
      "ri.stop_length.astype('category',ordered=True,categories=cats)\n",
      "\n",
      "\n",
      "1. stores more efficiently\n",
      "2. allows a logical order\n",
      "\n",
      "ri.stop_length.memory_usage(deep=True)  #memory used to store the column\n",
      "\n",
      "cats=['short','medium','long']\n",
      "\n",
      "ri[ri.stop_length>'short']\n",
      "output data with categories of medium or long stop_length\n",
      "\n",
      "ri.groupby('stop_length').is_arrested.mean()\n",
      "\n",
      "\n",
      "   Sample  > bad weather conditions\n",
      "\n",
      "# Copy 'WT01' through 'WT22' to a new DataFrame\n",
      "WT = weather.loc[:,'WT01':'WT22']\n",
      "\n",
      "# Calculate the sum of each row in 'WT'\n",
      "weather['bad_conditions'] = WT.sum(axis='columns')\n",
      "\n",
      "# Replace missing values in 'bad_conditions' with '0'\n",
      "weather['bad_conditions'] = weather.bad_conditions.fillna(0).astype('int')\n",
      "\n",
      "# Create a histogram to visualize 'bad_conditions'\n",
      "\n",
      "\n",
      "weather['bad_conditions'].plot(kind='hist')\n",
      "plt.show()\n",
      "# Display the plot\n",
      "\n",
      "   sample   > bad_conditions by category\n",
      "\n",
      "# Count the unique values in 'bad_conditions' and sort the index\n",
      "print(weather.bad_conditions.value_counts().sort_index())\n",
      "\n",
      "# Create a dictionary that maps integers to strings\n",
      "mapping = {0:'good', 1:'bad', 2:'bad', 3:'bad', 4:'bad', 5:'worse', 6:'worse', 7:'worse', 8:'worse', 9:'worse'}\n",
      "\n",
      "# Convert the 'bad_conditions' integers to strings using the 'mapping'\n",
      "weather['rating'] = weather.bad_conditions.map(mapping)\n",
      "\n",
      "# Count the unique values in 'rating'\n",
      "print(weather['rating'].unique())\n",
      "\n",
      "print(weather.rating.value_counts())\n",
      "\n",
      "output:\n",
      "bad      1836\n",
      "good     1749\n",
      "worse     432\n",
      "Name: rating, dtype: int64\n",
      "\n",
      "  Sample   > create a column as a category\n",
      "\n",
      "# Create a list of weather ratings in logical order\n",
      "cats=['good','bad','worse']\n",
      "\n",
      "# Change the data type of 'rating' to category\n",
      "weather['rating'] = weather.rating.astype('category', ordered=True, categories=cats)\n",
      "\n",
      "# Examine the head of 'rating'\n",
      "print(weather['rating'].head())\n",
      "\n",
      "\n",
      "       Merging Datasets\n",
      "\n",
      "reset_index returns the index to an autonumber\n",
      "\n",
      "\n",
      "high=high_low[['DATE','HIGH']]\n",
      "\n",
      "we only need the high column\n",
      "\n",
      "apple_high=pd.merge(left=apple, right=high, left_on='date', right_on='DATE', how=left)\n",
      "\n",
      "apple_high.set_index('date_and_time', inplace=True)\n",
      "\n",
      "\n",
      "\n",
      "<<<<<<<Sample  > reset the index to the autonumber,  extract the DATE and rating columns from the weather dataframe\n",
      "\n",
      "# Reset the index of 'ri'\n",
      "ri.reset_index(inplace=True)\n",
      "\n",
      "# Examine the head of 'ri'\n",
      "print(ri.head())\n",
      "\n",
      "# Create a DataFrame from the 'DATE' and 'rating' columns\n",
      "weather_rating=weather[['DATE','rating']]\n",
      "\n",
      "# Examine the head of 'weather_rating'\n",
      "print(weather_rating.head())\n",
      "\n",
      "\n",
      "   Sample  > merge columns on stop_date and date\n",
      "\n",
      "# Examine the shape of 'ri'\n",
      "print(ri.shape)\n",
      "\n",
      "# Merge 'ri' and 'weather_rating' using a left join\n",
      "ri_weather = pd.merge(left=ri, right=weather_rating, left_on='stop_date', right_on='DATE', how='left')\n",
      "\n",
      "# Examine the shape of 'ri_weather'\n",
      "print(ri_weather.shape)\n",
      "\n",
      "# Set 'stop_datetime' as the index of 'ri_weather'\n",
      "ri_weather.set_index('stop_datetime', inplace=True)\n",
      "\n",
      "\n",
      "https://datatofish.com/multiple-linear-regression-python/\n",
      "\n",
      "\n",
      " > weather and behavior\n",
      "\n",
      "search_rate = ri.groupby(['violation','driver_gender']).search_conducted.mean()\n",
      "\n",
      "multi-index\n",
      "pandas.core.indexes.multi.multiindex (second dimension)\n",
      "\n",
      "level=0\n",
      "level=1\n",
      "\n",
      "search_rate.loc['Equipment'] #level 0\n",
      "search_rate.loc['Equipment','M'] #level 1\n",
      "\n",
      "search_rate.unstack()\n",
      "\n",
      "results in a dataframe\n",
      "\n",
      "ri.pivot_table(index='violation',\n",
      "\tcolumns='driver_gender',\n",
      "\tvalues='search_conducted')\n",
      "\n",
      "  >Sample\n",
      "print(ri_weather.is_arrested.mean())\n",
      "0.0355690117407784\n",
      "\n",
      "overall arrest rate\n",
      "\n",
      "\n",
      "# Calculate the arrest rate for each 'rating'\n",
      "print(ri_weather.groupby('rating').is_arrested.mean())\n",
      "\n",
      "stop_minutes  \n",
      "rating                \n",
      "good     0.033715\n",
      "bad      0.036261\n",
      "worse    0.041667\n",
      "\n",
      "  Sample   > create a multi index series    violation and rating for is_arrested mean\n",
      "\n",
      "# Calculate the arrest rate for each 'violation' and 'rating'\n",
      "print(ri_weather.groupby(['violation','rating']).is_arrested.mean())\n",
      "\n",
      "violation            rating\n",
      "*Equipment            good      0.059007\n",
      "                     bad       0.066311\n",
      "                     worse     0.097357\n",
      "*Moving violation     good      0.056227\n",
      "                     bad       0.058050\n",
      "                     worse     0.065860\n",
      "*Other                good      0.076966\n",
      "                     bad       0.087443\n",
      "                     worse     0.062893\n",
      "*Registration/plates  good      0.081574\n",
      "                     bad       0.098160\n",
      "                     worse     0.115625\n",
      "Seat belt            good      0.028587\n",
      "                     bad       0.022493\n",
      "                     worse     0.000000\n",
      "Speeding             good      0.013405\n",
      "                     bad       0.013314\n",
      "                     worse     0.016886\n",
      "\n",
      "  sample  > slicing the multi-index series\n",
      "\n",
      "# Save the output of the groupby operation from the last exercise\n",
      "arrest_rate = ri_weather.groupby(['violation', 'rating']).is_arrested.mean()\n",
      "\n",
      "# Print the 'arrest_rate' Series\n",
      "print(arrest_rate)\n",
      "\n",
      "# Print the arrest rate for moving violations in bad weather\n",
      "print(arrest_rate.loc['Moving violation','bad'])\n",
      "\n",
      "# Print the arrest rates for speeding violations in all three weather conditions\n",
      "print(arrest_rate.loc['Speeding'])\n",
      "\n",
      "\n",
      "  sample  > unstack and pivot\n",
      "\n",
      "# Unstack the 'arrest_rate' Series into a DataFrame\n",
      "print(arrest_rate.unstack())\n",
      "\n",
      "# Create the same DataFrame using a pivot table\n",
      "print(ri_weather.pivot_table(index='violation', columns='rating', values='is_arrested'))\n",
      "\n",
      "practice answering questions using data\n",
      "\n",
      "https://openpolicing.stanford.edu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\create a datetimeindex.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\create a datetimeindex.txt\n",
      "   >Create a DateTimeIndex\n",
      "\n",
      "1. combine stop_date and stop_time into one column\n",
      "\n",
      "2. convert to datetime format\n",
      "\n",
      "\n",
      "apple.date.str.replace('/','-')\n",
      "\n",
      "combined=apple.date.str.cat(apple.time, sep=' ')\n",
      "#concatenate and separate with a space\n",
      "\n",
      "appled['date_and_time']=pd.to_datetime(combined)\n",
      "\n",
      "stored in a more standard way\n",
      "\n",
      "setting the index\n",
      "\n",
      "\n",
      "apple.set_index('date_and_time', inplace=True)\n",
      "\n",
      "when a column becomes an index, it is not longer considered a dataframe column\n",
      "\n",
      "  sample  > combining a date and time columns into a datetime column\n",
      "\n",
      "# Concatenate 'stop_date' and 'stop_time' (separated by a space)\n",
      "combined = ri.stop_date.str.cat(ri.stop_time, sep=' ')\n",
      "\n",
      "# Convert 'combined' to datetime format\n",
      "ri['stop_datetime'] = pd.to_datetime(combined)\n",
      "\n",
      "# Examine the data types of the DataFrame\n",
      "print(ri.dtypes)\n",
      "\n",
      "  >sample  > set_index\n",
      "\n",
      "# Set 'stop_datetime' as the index\n",
      "ri.set_index('stop_datetime', inplace=True)\n",
      "\n",
      "# Examine the index\n",
      "print(ri.index)\n",
      "\n",
      "# Examine the columns\n",
      "print(ri.columns)\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\data cleansing and sql data analysis.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\data cleansing and sql data analysis.txt\n",
      "\n",
      "\n",
      "# Print the information of ride_sharing\n",
      "print(ride_sharing.info())\n",
      "\n",
      "# Print summary statistics of user_type column\n",
      "print(ride_sharing['user_type'].describe())\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "# Print the information of ride_sharing\n",
      "print(ride_sharing.info())\n",
      "\n",
      "# Print summary statistics of user_type column\n",
      "print(ride_sharing['user_type'].describe())\n",
      "\n",
      "# Convert user_type from integer to category\n",
      "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
      "\n",
      "# Write an assert statement confirming the change\n",
      "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
      "\n",
      "# Print new summary statistics \n",
      "print(ride_sharing['user_type_cat'].describe())\n",
      "\n",
      "print(ride_sharing['user_type_cat'])\n",
      "\n",
      " Sample\n",
      "\n",
      "# Strip duration of minutes\n",
      "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes') \n",
      "\n",
      "# Convert duration to integer\n",
      "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
      "\n",
      "# Write an assert statement making sure of conversion\n",
      "assert ride_sharing['duration_time'].dtype == 'int'\n",
      "\n",
      "# Print formed columns and calculate average ride duration \n",
      "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
      "print(ride_sharing['duration_time'].mean())\n",
      "\n",
      "\n",
      " >Data range constraints\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.hist(movies['avg_rating'])\n",
      "plt.title('Average rating of movies (1-5)')\n",
      "\n",
      "or signups in the future\n",
      "\n",
      "import datetime as dt\n",
      "today_date=dt.date.today()\n",
      "user_signups[user_signups['subscription_date']> today_date]\n",
      "\n",
      " >dealing with out of range data\n",
      "1. drop the data\n",
      "2. set custom minimums and maximums\n",
      "3. treat a s missing and impute\n",
      "4. Set custom value depending on business assumptions\n",
      "\n",
      "\n",
      " > dropping data\n",
      "\n",
      "movies.drop(movies[movies['avg_rating']>5].index, inplace=True)\n",
      "\n",
      "assert movies['avg_rating'].max()<=5\n",
      "\n",
      "  Setting to a hard limit\n",
      "\n",
      "movie.loc[movies['avg_rating']>5, 'avg rating']=5\n",
      "\n",
      "\n",
      "user_signups.dtypes\n",
      "\n",
      "ouput: subscription_date object\n",
      "\n",
      "user_signups['subscription_date']= pd.to_datetime(user_signups['subscription_date'])\n",
      "\n",
      "assert user_signups['subscriptions_date'].dtype == 'datetime64[ns]'\n",
      "\n",
      "today_date=dt.date.today()\n",
      "\n",
      "assert user_signups.subscription_date.max().date() <= today_date\n",
      "\n",
      "\n",
      " >Sample   > convert to categorical\n",
      "\n",
      "# Convert tire_sizes to integer\n",
      "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
      "\n",
      "# Set all values above 27 to 27\n",
      "ride_sharing.loc[ride_sharing.tire_sizes > 27,'tire_sizes'] = 27\n",
      "\n",
      "# Reconvert tire_sizes back to categorical\n",
      "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
      "\n",
      "print(ride_sharing['tire_sizes'])\n",
      "# Print tire size description\n",
      "print(ride_sharing['tire_sizes'].describe())\n",
      "\n",
      " >Sample   > convert to datetime\n",
      "\n",
      "# Convert ride_date to datetime\n",
      "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])\n",
      "\n",
      "# Save today's date\n",
      "today = dt.date.today()\n",
      "\n",
      "# Set all in the future to today's date\n",
      "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
      "\n",
      "# Print maximum of ride_dt column\n",
      "print(ride_sharing['ride_dt'].max())\n",
      "\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "#Correct! Subsetting on metadata and keeping all duplicate records gives you a better bird-eye's view over your data and how to duplicate it!\n",
      "\n",
      "\n",
      "# Find duplicates\n",
      "duplicates = ride_sharing.duplicated('ride_id', keep=False)\n",
      "print(duplicates)\n",
      "\n",
      "# Sort your duplicated rides\n",
      "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
      "\n",
      "# Print relevant columns of duplicated_rides\n",
      "print(duplicated_rides[['ride_id','duration','user_birth_year']])\n",
      "\n",
      " Sample dropping duplicates\n",
      "\n",
      "# Drop complete duplicates from ride_sharing\n",
      "ride_dup = ride_sharing.drop_duplicates()\n",
      "\n",
      "# Create statistics dictionary for aggregation function\n",
      "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
      "\n",
      "# Group by ride_id and compute new statistics\n",
      "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
      "\n",
      "# Find duplicated values again\n",
      "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
      "duplicated_rides = ride_unique[duplicates == True]\n",
      "\n",
      "# Assert duplicates are processed\n",
      "assert duplicated_rides.shape[0] == 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\data cleansing text and categorical data problems.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\data cleansing text and categorical data problems.txt\n",
      "categories\n",
      "1. Predefined finite set of categories\n",
      "2. Text categories are converted to numeric representations\n",
      "\n",
      "how to treat the problems\n",
      "1. drop rows of data\n",
      "2. remap the categories\n",
      "3. infer the category\n",
      "\n",
      "study data\n",
      "name, birthday, blood_type\n",
      "\n",
      "awesome-public datasets\n",
      "https://github.com/awesomedata/awesome-public-datasets\n",
      "\n",
      "\n",
      "study_data= pd.read_csv('study.csv')\n",
      "\n",
      "\n",
      "anti joins (left join)\n",
      "what is a and not in b\n",
      "\n",
      "inner join\n",
      "what is both a and b\n",
      "\n",
      "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
      "\n",
      "print(inconsistent_categories)\n",
      "\n",
      "outputs {'Z+'}\n",
      "\n",
      "inconsistent_rows=study_data['blood_type'].isin(inconsistent_categories)\n",
      "\n",
      "#returns a boolean of true for inconsistent rows\n",
      "\n",
      "study_data[inconsistent_rows]\n",
      "\n",
      "consistent_data=study_data[~inconsistent_rows]\n",
      "\n",
      "  Sample   using unique to find categories\n",
      "\n",
      "# Print categories DataFrame\n",
      "print(categories)\n",
      "\n",
      "# Print unique values of survey columns in airlines\n",
      "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
      "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
      "print('Satisfaction: ', airlines['satisfaction'].unique(),\"\\n\")\n",
      "\n",
      " >Sample  > finding inconsistencies in the categories\n",
      "\n",
      "print(airlines['cleanliness'])\n",
      "print(categories.columns)\n",
      "inconsistent_categories1=set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
      "inconsistent_categories2=set(airlines['safety']).difference(categories['safety'])\n",
      "inconsistent_categories3=set(airlines['satisfaction']).difference(categories['satisfaction'])\n",
      "print(len(inconsistent_categories1),len(inconsistent_categories2),len(inconsistent_categories3))\n",
      "\n",
      "  Sample  > finding the rows with the inconsistent data\n",
      "\n",
      "# Find the cleanliness category in airlines not in categories\n",
      "inconsistent_categories1=set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
      "\n",
      "# Find rows with that category\n",
      "cat_clean_rows = airlines['cleanliness'].isin(inconsistent_categories1)\n",
      "\n",
      "# Print rows with inconsistent category\n",
      "print(airlines[cat_clean_rows])\n",
      "\n",
      "# Print rows with consistent categories\n",
      "print(airlines[~cat_clean_rows])\n",
      "\n",
      "       >What type of errors could we have\n",
      "\n",
      "1. inconsistent fields\n",
      "2. trailing white spaces\n",
      "\n",
      "Collapsing too many categories to few\n",
      "1. create new groups (0-20k) (20-40k) from contineous household income data\n",
      "2. mapping groups to new ones\n",
      "\n",
      "Capitalization:\n",
      "married or Married or UNMARRIED or unmarried\n",
      "\n",
      "marriage_status=demographics['marriage_status']\n",
      "marriage_status.value_counts()\n",
      "\n",
      "\n",
      "for a dataframe\n",
      "\n",
      "marriage_status.groupby('marriage_status').count()\n",
      "\n",
      "fix\n",
      "\n",
      "marriage_status['marriage_status']=marriage_status['marriage_status'].str.upper()\n",
      "\n",
      "leading spaces\n",
      "\n",
      "marriage_status['marriage_status']=marriage_status['marriage_status'].str.strip()\n",
      "\n",
      "\n",
      "   Collapsing data into categories\n",
      "\n",
      "ranges=[0, 200000,500000,np.inf]\n",
      "group_names=['0-200k','200k-500k','500k+']\n",
      "\n",
      "demographics['income_group']=pd.cut(demographics['household_income'], bins=ranges, labels=group_names)\n",
      "\n",
      "print(demographics[['income_group','household_income']]\n",
      "\n",
      "\n",
      "  Collapsing data into categories\n",
      "\n",
      "'Microsoft', 'MacOS', 'IOS', 'Android', 'Linus' are collasped into a category called 'operating sytems'\n",
      "\n",
      "mapping={'Microsoft':'DesktopOS','MacOS':'DesktopOS',\n",
      "'Linux':'DesktopOS','IOS':'MobileOS','Android':'MobileOS'}\n",
      "\n",
      "devices['operating_systems]=devices['operating_systems'].replace(mapping)\n",
      "\n",
      "\n",
      "  > Sample    lower and replace\n",
      "\n",
      "# Print unique values of both columns\n",
      "print(airlines['dest_region'].unique())\n",
      "print(airlines['dest_size'].unique())\n",
      "\n",
      "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
      "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
      "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
      "\n",
      "\n",
      "\n",
      "  Sample  > Strip\n",
      "\n",
      "# Print unique values of both columns\n",
      "print(airlines['dest_region'].unique())\n",
      "print(airlines['dest_size'].unique())\n",
      "\n",
      "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
      "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
      "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
      "\n",
      "# Remove white spaces from `dest_size`\n",
      "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
      "airlines['dest_region'] = airlines['dest_region'].str.strip()\n",
      "\n",
      "# Verify changes have been effected\n",
      "print(airlines['dest_region'].unique())\n",
      "print(airlines['dest_size'].unique())\n",
      "\n",
      "\n",
      "  >Sample  > using cut and remapping\n",
      "\n",
      "# Create ranges for categories\n",
      "label_ranges = [0, 60, 180, np.inf]\n",
      "label_names = ['short', 'medium', 'long']\n",
      "\n",
      "# Create wait_type column\n",
      "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
      "                                labels = label_names)\n",
      "\n",
      "# Create mappings and replace\n",
      "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
      "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
      "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
      "\n",
      "airlines['day_week'] = airlines['day'].replace(mappings)\n",
      "\n",
      "print(airlines.head())\n",
      "\n",
      "  Cleaning text data\n",
      "\n",
      "1. leading zeros on the phone number\n",
      "2. phone numbers with the incorrect length\n",
      "\n",
      "all phone numbers begin with 00\n",
      "and incorrect length phone numbers are replaced with nan\n",
      "\n",
      "phones['Phone Number']=phones['Phone Number'].str.replace(\"-\",\"\")\n",
      "\n",
      "digits = phones['Phone Number'].str.len()\n",
      "\n",
      "phones.loc[digits<10,\"Phone Number\"] = np.nan\n",
      "\n",
      "\n",
      "#assert if minimum phone number length is 10\n",
      "sanity_check=phone['Phone number'].str.len()\n",
      "assert sanity_check.min()>=10\n",
      "\n",
      "#any returns any records that are true\n",
      "assert phone['Phone Number'].str.contains(\"+|-\").any()==False\n",
      "\n",
      "\n",
      "  >Regular expressions\n",
      "\n",
      "#replace any character that is not a digit with nothing\n",
      "\n",
      "phones['Phone number']=phones['Phone number'].str.replace(r'\\D+','')\n",
      "\n",
      "  >Sample  > replace pattern with empty\n",
      "\n",
      "# Replace \"Dr.\" with empty string \"\"\n",
      "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
      "\n",
      "# Replace \"Mr.\" with empty string \"\"\n",
      "airlines['full_name'] = airlines['full_name'].str.replace('Mr.','')\n",
      "\n",
      "# Replace \"Miss\" with empty string \"\"\n",
      "airlines['full_name'] = airlines['full_name'].str.replace('Miss.','')\n",
      "\n",
      "# Replace \"Ms.\" with empty string \"\"\n",
      "airlines['full_name'] = airlines['full_name'].str.replace('Ms.','')\n",
      "\n",
      "# Assert that full_name has no honorifics\n",
      "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False\n",
      "\n",
      "\n",
      "  >Sample find strings with len > 40 and assert if not true\n",
      "\n",
      "# Store length of each row in survey_response column\n",
      "resp_length = airlines['survey_response'].str.len()\n",
      "\n",
      "# Find rows in airlines where resp_length > 40\n",
      "airlines_survey = airlines[resp_length > 40]\n",
      "\n",
      "# Assert minimum survey_response length is > 40\n",
      "assert airlines_survey['survey_response'].str.len().min() > 40\n",
      "\n",
      "# Print new survey_response column\n",
      "print(airlines_survey['survey_response'])\n",
      "\n",
      "\n",
      "          Uniformity\n",
      "1. dealing with missing data\n",
      "\n",
      "\n",
      "tempuratures in both celsius and fahrenheit\n",
      "weight in kilograms and stones\n",
      "date is short and long format\n",
      "money in dollars and euros\n",
      "\n",
      "\n",
      " >Fixing fahrenheit and celius\n",
      "\n",
      "plt.scatter(x='Date', y='Temperature', data=temperatures)\n",
      "plt.title('Temperature in Celsius ')\n",
      "plt.xlabel('Dates')\n",
      "plt.ylabel('Temperature in Celsius')\n",
      "plt.show()\n",
      "\n",
      "C=(F-32) x 5/9\n",
      "\n",
      "temp_fah=temperatures.loc[temperatures['Temperature']>40,'Temperature]\n",
      "\n",
      "temp_celsius=(temp_fah-32)*(5/9)\n",
      "\n",
      "temperatures.loc[temperatures['Temperature']>40,'Temperature]=temp_celsius\n",
      "\n",
      "assert temperatures['Temperature'].max()<40\n",
      "\n",
      "\n",
      " >Fixing Dates\n",
      "\n",
      "datetime is used to format dates\n",
      "\n",
      "pandas.to_datetime()\n",
      " \n",
      "%d-%m-%Y  25-12-2019\n",
      "%c December 25th 2019\n",
      "12-25-2019 %m-%d-%Y\n",
      "\n",
      "birthdays['Birthday']=pd.to_datetime(birthdays['Birthday']\n",
      "\t\t,infer_datetime_format=True,\n",
      "\t\terrors='coerce')\n",
      "\n",
      "NaT is Not a Date Time\n",
      "\n",
      "\n",
      "birthdays['Birthday]=birthdays['Birthday].dt.strftime(\"%d-%m-%Y\")\n",
      "\n",
      "\n",
      " >Sample  > converting euros to dollars\n",
      "\n",
      "# Find values of acct_cur that are equal to 'euro'\n",
      "acct_eu = banking['acct_cur'] == 'euro'\n",
      "\n",
      "# Convert acct_amount where it is in euro to dollars\n",
      "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 \n",
      "\n",
      "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
      "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
      "\n",
      "# Assert that only dollar currency remains\n",
      "assert banking['acct_cur'].unique() == 'dollar'\n",
      "\n",
      "\n",
      " >Sample   converting dates\n",
      "\n",
      "# Print the header of account_opened\n",
      "print(banking['account_opened'].head())\n",
      "\n",
      "# Convert account_opened to datetime\n",
      "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
      "                                           # Infer datetime format\n",
      "                                           infer_datetime_format = True,\n",
      "                                           # Return missing value for error\n",
      "                                           errors = 'coerce')\n",
      "\n",
      "# Get year of account opened\n",
      "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
      "\n",
      "# Print acct_year\n",
      "print(banking['acct_year'])\n",
      "\n",
      "   Cross field validation\n",
      "1. The challenge from merging data from different sources is data integrity.\n",
      "\n",
      "2. cross field validation is using multiple fields in a dataset to sanity check data integrity.\n",
      "\n",
      "3. economy_class+business_class+first_class=total_passengers\n",
      "\n",
      "sum_classes=flights[['economy_class','business_class','first_class']].sum(axis=1)\n",
      "\n",
      "passenger_equal=sum_classes==flights['total_passengers']\n",
      "\n",
      "inconsistent_pass=flights[~passenger_equal]\n",
      "consistent_pass=flights[passenger_equal]\n",
      "\n",
      "   birthday check\n",
      "import pandas as pd\n",
      "import datetime as dt\n",
      "\n",
      "users['Birthday']=pd.to_datetime(users['Birthday'])\n",
      "today=dt.date.today()\n",
      "\n",
      "age_manual=today.year - users['Birthday'].dt.year\n",
      "\n",
      "age_equal=age_manual==users['Age']\n",
      "\n",
      "inconsistent_age=users[~age_equal]\n",
      "consistent_age=users[age_equal]\n",
      "\n",
      "inconsistent data can be dropped, set to missing and impute\n",
      "and apply rules from domain knowlege\n",
      "\n",
      " >Sample   sum funds a,b,c,d and compare inv_amount for inconsistencies\n",
      "\n",
      "# Store fund columns to sum against\n",
      "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
      "\n",
      "# Find rows where fund_columns row sum == inv_amount\n",
      "inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']\n",
      "\n",
      "# Store consistent and inconsistent data\n",
      "consistent_inv = banking[inv_equ]\n",
      "inconsistent_inv = banking[~inv_equ]\n",
      "\n",
      "# Store consistent and inconsistent data\n",
      "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])\n",
      "\n",
      "  Sample   check birthday and age consistency\n",
      "\n",
      "# Store today's date and find ages\n",
      "today = dt.date.today()\n",
      "ages_manual = today.year - banking['birth_date'].dt.year\n",
      "\n",
      "# Find rows where age column == ages_manual\n",
      "age_equ = ages_manual == banking['age']\n",
      "\n",
      "# Store consistent and inconsistent data\n",
      "consistent_ages = banking[age_equ]\n",
      "inconsistent_ages = banking[~age_equ]\n",
      "\n",
      "       Completeness\n",
      "1. missing data is represented as na, nan, 0, ., or ...\n",
      "\n",
      "caused from a technical error or human error\n",
      "\n",
      "Temperature and co2\n",
      "\n",
      "#find missing data\n",
      "airquality.isna()\n",
      "\n",
      "airquality.isna().sum()\n",
      "\n",
      "import missingno as msno\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "msno.matrix(airquality)\n",
      "plt.show()\n",
      "\n",
      "missing=airquality[airquality['CO2'].isna()]\n",
      "complete=airquality[~airquality['CO2'].isna()]\n",
      "\n",
      "missing.describe\n",
      "\n",
      "\n",
      "sorted_airquality = airquality.sort_values(by='Temperature')\n",
      "msno.matrix(sorted_airquality)\n",
      "plt.show()\n",
      "\n",
      "co2 are lost for extremely low temperatures\n",
      "\n",
      "  >Missingness types\n",
      "1. Missing Completely at Random: No systematic relationship between a column's missing values and other or own values.\n",
      "\n",
      "2. Missing at Random: There is a systematic relationship between a column's missing values and other observed values.\n",
      "\n",
      "3. Missing not at Random: There is a systematic relationship between a column's missing values and unobserved values.\n",
      "\n",
      "1. Missing completely at Random (no relationship)\n",
      "2. Missing at Random (relationship with features)\n",
      "3. Missing not at random (systemtic relationship causing the missing data)\n",
      "\n",
      " >Dealing with missing data\n",
      "1. drop missing data\n",
      "2. impute with statistical measures (mean, median, mode)\n",
      "\n",
      "drop values\n",
      "airquality.dropna()\n",
      "\n",
      "impute\n",
      "airquality.fillna(mean)\n",
      "\n",
      "feed values if we have enough knowledge of the dataset\n",
      "airquality.fillna(custom)\n",
      "\n",
      "\n",
      "  >Sample   using msno to visual missing values\n",
      "\n",
      "# Print number of missing values in banking\n",
      "print(banking.isna().sum())\n",
      "\n",
      "# Visualize missingness matrix\n",
      "\n",
      "  Sample   sort by age and display the mnso matrix\n",
      "\n",
      "# Print number of missing values in banking\n",
      "print(banking.isna().sum())\n",
      "\n",
      "# Visualize missingness matrix\n",
      "msno.matrix(banking)\n",
      "plt.show()\n",
      "\n",
      "# Isolate missing and non missing values of inv_amount\n",
      "missing_investors = banking[banking['inv_amount'].isna()]\n",
      "investors = banking[~banking['inv_amount'].isna()]\n",
      "\n",
      "# Sort banking by age and visualize\n",
      "banking_sorted = banking.sort_values(by='age')\n",
      "msno.matrix(banking_sorted)\n",
      "plt.show()\n",
      "\n",
      "  >Sample  > imput\n",
      "\n",
      "# Drop missing values of cust_id\n",
      "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
      "\n",
      "# Compute estimated acct_amount\n",
      "acct_imp = banking_fullid['inv_amount']*5\n",
      "# Impute missing acct_amount with corresponding acct_imp\n",
      "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
      "\n",
      "# Print number of missing values\n",
      "print(banking_imputed.isna().sum())\n",
      "\n",
      "msno.matrix(banking)\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample > print missing inv_amount values\n",
      "\n",
      "# Print number of missing values in banking\n",
      "print(banking.isna().sum())\n",
      "\n",
      "# Visualize missingness matrix\n",
      "msno.matrix(banking)\n",
      "plt.show()\n",
      "\n",
      "# Isolate missing and non missing values of inv_amount\n",
      "missing_investors = banking[banking['inv_amount'].isna()]\n",
      "investors = banking[~banking['inv_amount'].isna()]\n",
      "\n",
      "missing.describe()\n",
      "\n",
      "      >Comparing strings\n",
      "\n",
      "from fuzzywuzzy import fuzz\n",
      "\n",
      "fuzz.WRatio('Reeding','Reading')\n",
      "fuzz.WRatio('Houston Rockets','Rockets')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\datetime.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\datetime.txt\n",
      "from datetime import date\n",
      "\n",
      "two_hurricane_dates=[date(2016,10,7), date(2017,6,21)]\n",
      "\n",
      "print (two_hurricane_dates[0].weekday())\n",
      "\n",
      "   sample  >  date\n",
      "\n",
      "# Import date from datetime\n",
      "from datetime import date\n",
      "\n",
      "# Create a date object\n",
      "hurricane_andrew = date(1992, 8, 24)\n",
      "\n",
      "# Which day of the week is the date?\n",
      "print(hurricane_andrew.weekday())\n",
      "\n",
      "\n",
      "  >sample    month\n",
      "\n",
      "# Counter for how many before June 1\n",
      "early_hurricanes = 0\n",
      "\n",
      "# We loop over the dates\n",
      "for hurricane in florida_hurricane_dates:\n",
      "  # Check if the month is before June (month number 6)\n",
      "  if hurricane.month < 6:\n",
      "    early_hurricanes = early_hurricanes + 1\n",
      "    \n",
      "print(early_hurricanes)\n",
      "\n",
      "<<<<<<<<<Math with Dates\n",
      "\n",
      "2017-11-05  and 2017-12-04\n",
      "\n",
      "from datetime import date\n",
      "\n",
      "d1=date(2017,11,5)\n",
      "d2=date(2017,12,4)\n",
      "\n",
      "delta= d2 - d1\n",
      "\n",
      "print(delta.days)\n",
      "\n",
      "output: 29 days elapsed\n",
      "\n",
      "from datetime import timedelta\n",
      "\n",
      "td=timedelta(days=29)\n",
      "print(d1+td)\n",
      "\n",
      "output 2017-12-04\n",
      "\n",
      "\n",
      " >sample   > subtract two dates\n",
      "\n",
      "# Import date\n",
      "from datetime import date\n",
      "\n",
      "# Create a date object for May 9th, 2007\n",
      "start = date(2007,5,9)\n",
      "\n",
      "# Create a date object for December 13th, 2007\n",
      "end = date(2007,12,13)\n",
      "\n",
      "# Subtract the two dates and print the number of days\n",
      "print((end-start).days)\n",
      "\n",
      "\n",
      " >sample  > month as an index into a dictionary\n",
      "\n",
      "# A dictionary to count hurricanes per calendar month\n",
      "hurricanes_each_month = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6:0,\n",
      "\t\t  \t\t\t\t 7: 0, 8:0, 9:0, 10:0, 11:0, 12:0}\n",
      "\n",
      "# Loop over all hurricanes\n",
      "for hurricane in florida_hurricane_dates:\n",
      "  # Pull out the month\n",
      "  month = hurricane.month\n",
      "  # Increment the count in your dictionary by one\n",
      "  hurricanes_each_month[month] +=1\n",
      "  \n",
      "print(hurricanes_each_month)\n",
      "\n",
      "   sample  print the first and last scrambled dates\n",
      "\n",
      " # Print the first and last scrambled dates\n",
      "print(dates_scrambled)\n",
      "print(dates_scrambled[0])\n",
      "print(dates_scrambled[-1])\n",
      "\n",
      "\n",
      "  > sample  > sort dates\n",
      "\n",
      "# Print the first and last scrambled dates\n",
      "print(dates_scrambled[0])\n",
      "print(dates_scrambled[-1])\n",
      "\n",
      "# Put the dates in order\n",
      "dates_ordered =sorted(dates_scrambled)\n",
      "\n",
      "# Print the first and last ordered dates\n",
      "print(dates_ordered[0])\n",
      "print(dates_ordered[-1])\n",
      "\n",
      "   turning dates into strings\n",
      "\n",
      "from datetime import date\n",
      "\n",
      "d+date(2017,11,5)\n",
      "\n",
      "print(d)\n",
      "\n",
      "output:2017-11-05\n",
      "\n",
      "YYYY-MM-DD (ISO 8601 format)\n",
      "\n",
      "some_dates=['2001-01-01','1999-12-31']\n",
      "\n",
      "iso 8601 strings sort correctly\n",
      "\n",
      "\n",
      "print(sorted(some_dates))\n",
      "\n",
      "d.strftime()\n",
      "\n",
      "d=date(2017,1,5)\n",
      "\n",
      "print(d.strftime(\"%Y\"))\n",
      "\n",
      "\n",
      "base = datetime.datetime.today()\n",
      "date_list = [base - datetime.timedelta(days=x) for x in range(numdays)]\n",
      "\n",
      " datetime.datetime.strptime\n",
      "\n",
      "\n",
      " >Sample  > format string\n",
      "\n",
      "import datetime\n",
      "#https://strftime.org/\n",
      "d=datetime.datetime.now()\n",
      "\n",
      "dateList=['2017-12-31','2001-01-01','2010-11-21']\n",
      "\n",
      "for item in dateList:\n",
      "    diff_days=(d-datetime.datetime.strptime(item,'%Y-%m-%d')).days\n",
      "    if (diff_days/365)>5:\n",
      "        print(item,\" found\")\n",
      "\n",
      "\n",
      "  > sample  > isoformat\n",
      "\n",
      "# Assign the earliest date to first_date\n",
      "first_date = min(florida_hurricane_dates)\n",
      "\n",
      "# Convert to ISO and US formats\n",
      "iso = \"Our earliest hurricane date: \" + first_date.isoformat()\n",
      "us = \"Our earliest hurricane date: \" + first_date.strftime(\"%m/%d/%Y\")\n",
      "\n",
      "print(\"ISO: \" + iso)\n",
      "print(\"US: \" + us)\n",
      "\n",
      "    sample  > YYYY-MM\n",
      "\n",
      "# Import date\n",
      "from datetime import date\n",
      "\n",
      "# Create a date object\n",
      "andrew = date(1992, 8, 26)\n",
      "\n",
      "# Print the date in the format 'YYYY-MM'\n",
      "print(andrew.strftime('%Y-%m'))\n",
      "\n",
      "   > sample Month and Year\n",
      "\n",
      "# Import date\n",
      "from datetime import date\n",
      "\n",
      "# Create a date object\n",
      "andrew = date(1992, 8, 26)\n",
      "\n",
      "# Print the date in the format 'MONTH (YYYY)'\n",
      "print(andrew.strftime('%B (%Y)'))\n",
      "\n",
      "\n",
      "     sample Julian day\n",
      "\n",
      "from datetime import date\n",
      "\n",
      "# Create a date object\n",
      "andrew = date(1992, 8, 26)\n",
      "\n",
      "# Print the date in the format 'YYYY-DDD'\n",
      "print(andrew.strftime('%Y-%j'))\n",
      "\n",
      "     > working with dates and times\n",
      "\n",
      "october 1, 2017 3:23:25 PM\n",
      "\n",
      "from datetime import datetime\n",
      "\n",
      "dt=datetime(2017,10,1,15,23,25)\n",
      "\n",
      "\n",
      "dt=datetime(2017,10,1,15,23,25,500000)\n",
      "\n",
      "python breaks down secords to a millionth of second\n",
      "\n",
      "\n",
      "dt=datetime(year=2017,\n",
      "month=10,\n",
      "day=1,\n",
      "hour=15,\n",
      "minute=23,\n",
      "second=25,\n",
      "microsecond=500000)\n",
      "\n",
      " >replace\n",
      "\n",
      "dt_hr = dt.replace(minute=0, second=0, microsecond=0\n",
      "\n",
      "print(dt_hr)\n",
      "\n",
      "output: 2017-10-01 15:00:00\n",
      "\n",
      "    sample  > isoformat()\n",
      "\n",
      "# Import datetime\n",
      "from datetime import datetime\n",
      "\n",
      "# Create a datetime object\n",
      "dt = datetime(2017,10,1,15,26,26)\n",
      "\n",
      "# Print the results in ISO 8601 format\n",
      "print(dt.isoformat())\n",
      "\n",
      "\n",
      "dt = datetime(2017, 12, 31, 15, 19, 13)\n",
      "\n",
      "dt_old = dt.replace(year=1917)\n",
      "\n",
      "   sample using a dictionary to count time occurrences\n",
      "\n",
      "\n",
      "# Create dictionary to hold results\n",
      "trip_counts = {'AM': 0, 'PM': 0}\n",
      "\n",
      "\n",
      "# Loop over all trips\n",
      "for trip in onebike_datetimes:\n",
      "  # Check to see if the trip starts before noon\n",
      "  if trip['start'].hour < 12:\n",
      "    # Increment the counter for before noon\n",
      "    trip_counts['AM'] += 1\n",
      "  else:\n",
      "    # Increment the counter for after noon\n",
      "    trip_counts['PM'] += 1\n",
      "  \n",
      "print(trip_counts)\n",
      "\n",
      "    Printing and parsing datetimes\n",
      "\n",
      "convert a string into datetime\n",
      "\n",
      "dt=datetime(2017,12,30,15,19,13)\n",
      "\n",
      "print(dt.strftime('%Y-%m-%d'))\n",
      "\n",
      "output:2017-12-30\n",
      "\n",
      "print(dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
      "output:2017-12-30 15:19:13\n",
      "\n",
      "print(dt.isoformat())\n",
      "\n",
      "     strptime   > string parse time\n",
      "\n",
      "dt=datetime.strptime(\"12/30/2017 15:19:13\",\"%m/%d/%Y %H:%M:%S)\n",
      "\n",
      "\n",
      "number of seconds from jan 1 1970\n",
      "\n",
      "ts=1514665153.0\n",
      "\n",
      "print(datetime.formattimestamp(ts))\n",
      "\n",
      "\n",
      "  > Sample  > strptime\n",
      "\n",
      "# Import the datetime class\n",
      "from datetime import datetime\n",
      "\n",
      "# Starting string, in YYYY-MM-DD HH:MM:SS format\n",
      "s = '2017-02-03 00:00:01'\n",
      "\n",
      "# Write a format string to parse s\n",
      "fmt = '%Y-%m-%d %H:%M:%S'\n",
      "\n",
      "# Create a datetime object d\n",
      "d = datetime.strptime(s, fmt)\n",
      "\n",
      "# Print d\n",
      "print(d)\n",
      "\n",
      "or\n",
      "\n",
      "\n",
      "# Import the datetime class\n",
      "from datetime import datetime\n",
      "\n",
      "# Starting string, in YYYY-MM-DD format\n",
      "s = '2030-10-15'\n",
      "\n",
      "# Write a format string to parse s\n",
      "fmt = '%Y-%m-%d'\n",
      "\n",
      "# Create a datetime object d\n",
      "d = datetime.strptime(s, fmt)\n",
      "\n",
      "# Print d\n",
      "print(d)\n",
      "\n",
      "or\n",
      "\n",
      "# Import the datetime class\n",
      "from datetime import datetime\n",
      "\n",
      "# Starting string, in MM/DD/YYYY HH:MM:SS format\n",
      "s = '12/15/1986 08:00:00'\n",
      "\n",
      "# Write a format string to parse s\n",
      "fmt = '%m/%d/%Y %H:%M:%S'\n",
      "\n",
      "# Create a datetime object d\n",
      "d = datetime.strptime(s, fmt)\n",
      "\n",
      "# Print d\n",
      "print(d)\n",
      "\n",
      "  > Sample  > cleanup strings to dates\n",
      "\n",
      "# Write down the format string\n",
      "fmt = \"%Y-%m-%d %H:%M:%S\"\n",
      "\n",
      "# Initialize a list for holding the pairs of datetime objects\n",
      "onebike_datetimes = []\n",
      "\n",
      "# Loop over all trips\n",
      "for (start, end) in onebike_datetime_strings:\n",
      "  trip = {'start': datetime.strptime(start, fmt),\n",
      "          'end': datetime.strptime(end, fmt)}\n",
      "  \n",
      "  # Append the trip\n",
      "  onebike_datetimes.append(trip)\n",
      "\n",
      "\n",
      "<<<< sample  > isoformat using strftime\n",
      "\n",
      "\n",
      "# Import datetime\n",
      "from datetime import datetime\n",
      "\n",
      "# Pull out the start of the first trip\n",
      "first_start = onebike_datetimes[0]['start']\n",
      "\n",
      "# Format to feed to strftime()\n",
      "fmt = \"%Y-%m-%dT%H:%M:%S\"\n",
      "\n",
      "# Print out date with .isoformat(), then with .strftime() to compare\n",
      "print(first_start.isoformat())\n",
      "print(datetime.strftime(first_start,fmt))\n",
      "\n",
      "\n",
      "   sample    fromtimestamp\n",
      "\n",
      "# Import datetime\n",
      "from datetime import datetime\n",
      "\n",
      "# Starting timestamps\n",
      "timestamps = [1514665153, 1514664543]\n",
      "\n",
      "# Datetime objects\n",
      "dts = []\n",
      "\n",
      "# Loop\n",
      "for ts in timestamps:\n",
      "  dts.append(datetime.fromtimestamp(ts))\n",
      "  \n",
      "# Print results\n",
      "print(dts)\n",
      "\n",
      "      working with durations\n",
      "\n",
      "working with durations\n",
      "\n",
      "\n",
      "\n",
      "start= datetime(2017,10,8,23,46,47)\n",
      "end=datetime(2017,10,9,0,10,57)\n",
      "\n",
      "duration=end-start\n",
      "\n",
      "timedelta is a duration\n",
      "\n",
      "print(duration.total_seconds())\n",
      "\n",
      "output: 1450\n",
      "\n",
      "from datetime import timedelta\n",
      "\n",
      "delta = timedelta(seconds=1)\n",
      "\n",
      "print(start + delta1)\n",
      "\n",
      "output: 2017-10-08 23:46:48\n",
      "\n",
      "one second later\n",
      "\n",
      "delta2= timedelta(days=1, seconds=1)\n",
      "\n",
      "print(start+delta2)\n",
      "output: 2017-10-09 23:46:48\n",
      "\n",
      "\n",
      "delta3 = timedelta(week=-1)\n",
      "\n",
      "    sample    duration in seconds\n",
      "\n",
      "# Initialize a list for all the trip durations\n",
      "onebike_durations = []\n",
      "\n",
      "for trip in onebike_datetimes:\n",
      "  # Create a timedelta object corresponding to the length of the trip\n",
      "  trip_duration = trip['end'] - trip['start']\n",
      "  \n",
      "  # Get the total elapsed seconds in trip_duration\n",
      "  trip_length_seconds = trip_duration.total_seconds()\n",
      "  \n",
      "  # Append the results to our list\n",
      "  onebike_durations.append(trip_length_seconds)\n",
      "\n",
      "\n",
      "# What was the total duration of all trips?\n",
      "total_elapsed_time = sum(onebike_durations)\n",
      "\n",
      "# What was the total number of trips?\n",
      "number_of_trips = len(onebike_durations)\n",
      "  \n",
      "# Divide the total duration by the number of trips\n",
      "print(total_elapsed_time / number_of_trips)\n",
      "\n",
      "  >sample  > finding min and max durations\n",
      "\n",
      "\n",
      "# Calculate shortest and longest trips\n",
      "shortest_trip = min(onebike_durations)\n",
      "longest_trip = max(onebike_durations)\n",
      "\n",
      "# Print out the results\n",
      "print(\"The shortest trip was \" + str(shortest_trip) + \" seconds\")\n",
      "print(\"The longest trip was \" + str(longest_trip) + \" seconds\")\n",
      "\n",
      "\n",
      "      UTC offsets\n",
      "\n",
      "comparing dates across different parts of the world\n",
      "\n",
      "time zones\n",
      "pacific 3 pm\n",
      "mountain 4 pm\n",
      "central 5 pm\n",
      "eastern 6 pm\n",
      "\n",
      "uk standard time is utc\n",
      "\n",
      "utc-x (North and South America)\n",
      "utc+x (Africa, Russia, China, and Australia)\n",
      "\n",
      "\n",
      "from datetime import datetime, timedelta, timezone\n",
      "\n",
      "ET=timezone(timedelta(hours=-5))\n",
      "\n",
      "dt=datetime(2017,12,30,15,9,3,tzinfo=ET)\n",
      "\n",
      "output: 2017-12-30 15:09:03-05:00\n",
      "\n",
      "includes the utc offset\n",
      "\n",
      "   change the close to India Standard time zone\n",
      "\n",
      "IST= timezone(timedelta(hours=5,minutes=30))\n",
      "\n",
      "print(dt.astimezone(ITS))\n",
      "\n",
      "print(dt.replace(tzinfo=timezone.utc))\n",
      "\n",
      "timezone.utc has 0 timezone offset\n",
      "\n",
      "print(dt.replace(tzinfo=timezone.utc))\n",
      "\n",
      "the clock stays the same, but the utc has shifted\n",
      "\n",
      "\n",
      "   adjusting timezones and changing the tzinfo\n",
      "\n",
      "import os, time\n",
      "time.strftime('%X %x %Z')\n",
      "'12:45:20 08/19/09 CDT'\n",
      "os.environ['TZ'] = 'Europe/London'\n",
      "time.tzset()\n",
      "\n",
      "year = time.strftime('%Y')\n",
      "month = time.strftime('%m')\n",
      "day = time.strftime('%d')\n",
      "hour = time.strftime('%H')\n",
      "minute = time.strftime('%M')\n",
      "\n",
      "astimezone changes the clock and the utc offset\n",
      "\n",
      "   sample  set the tzinfo\n",
      "\n",
      "the datetime object does not supply any concrete subclass of tzinfo.  The tzinfo object reveals the local time from UTC.  the tzinfo object contains the offset, the name of the time zone, and the dst offset.\n",
      "\n",
      "# Import datetime, timezone\n",
      "from datetime import datetime, timezone\n",
      "\n",
      "# October 1, 2017 at 15:26:26, UTC\n",
      "dt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=timezone.utc)\n",
      "\n",
      "# Print results\n",
      "print(dt.isoformat())\n",
      "\n",
      "   sample  > datetime in pacific time zone\n",
      "\n",
      "# Import datetime, timedelta, timezone\n",
      "from datetime import datetime, timedelta, timezone\n",
      "\n",
      "# Create a timezone for Pacific Standard Time, or UTC-8\n",
      "pst = timezone(timedelta(hours=-8))\n",
      "\n",
      "# October 1, 2017 at 15:26:26, UTC-8\n",
      "dt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=pst)\n",
      "\n",
      "# Print results\n",
      "print(dt.isoformat())\n",
      "\n",
      " > sample  > australia time zone\n",
      "\n",
      "# Import datetime, timedelta, timezone\n",
      "from datetime import datetime, timedelta, timezone\n",
      "\n",
      "# Create a timezone for Australian Eastern Daylight Time, or UTC+11\n",
      "aedt = timezone(timedelta(hours=11))\n",
      "\n",
      "# October 1, 2017 at 15:26:26, UTC+11\n",
      "dt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=aedt)\n",
      "\n",
      "# Print results\n",
      "print(dt.isoformat())\n",
      "\n",
      "\n",
      "   sample  > replace the time as utc time\n",
      "\n",
      "# Loop over the trips\n",
      "for trip in onebike_datetimes[:10]:\n",
      "  # Pull out the start\n",
      "  dt = trip['start']\n",
      "  # Move dt to be in UTC\n",
      "  dt = dt.astimezone(timezone.utc)\n",
      "  \n",
      "  # Print the start time in UTC\n",
      "  print('Original:', trip['start'], '| UTC:', dt.isoformat())\n",
      "\n",
      "Original: 2017-10-01 15:23:25-04:00 | UTC: 2017-10-01T15:23:25+00:00\n",
      "    Original: 2017-10-01 15:42:57-04:00 | UTC: 2017-10-01T15:42:57+00:00\n",
      "\n",
      " .replace() just changes the timezone whereas .astimezone() actually moves the hours and days to match.\n",
      "\n",
      "         >Time Zone Database\n",
      "\n",
      "how to align your data to utc\n",
      "\n",
      "from datetime import datetime\n",
      "for dateutil import tz\n",
      "\n",
      "#eastern time zone\n",
      "et=tz.gettz('America/New_York')\n",
      "\n",
      "#continent slash major city\n",
      "\n",
      "'America/New_York'\n",
      "'America/Mexico_City'\n",
      "'Europe/London'\n",
      "'Africa/Accra'\n",
      "\n",
      "\n",
      "last=datetime(2017,12,30.15,9,3,tzinfo=et)\n",
      "\n",
      "\n",
      "in some place the clocks change twice a year\n",
      "\n",
      "  sample  > using dateutil\n",
      "\n",
      "from dateutil import tz\n",
      "\n",
      "# Create a timezone object for Eastern Time\n",
      "et = tz.gettz('America/New_York')\n",
      "\n",
      "# Loop over trips, updating the datetimes to be in Eastern Time\n",
      "for trip in onebike_datetimes[:10]:\n",
      "  # Update trip['start'] and trip['end']\n",
      "  trip['start'] = trip['start'].replace(tzinfo=et)\n",
      "  trip['end'] = trip['end'].replace(tzinfo=et)\n",
      "\n",
      "\n",
      "  sample  > uk time and local time\n",
      "\n",
      "# Create the timezone object\n",
      "uk = tz.gettz('Europe/London')\n",
      "\n",
      "# Pull out the start of the first trip\n",
      "local = onebike_datetimes[0]['start']\n",
      "\n",
      "# What time was it in the UK?\n",
      "notlocal = local.astimezone(uk)\n",
      "\n",
      "# Print them out and see the difference\n",
      "print(local.isoformat())\n",
      "print(notlocal.isoformat())\n",
      "\n",
      "  > sample    asia time vs local time\n",
      "\n",
      "# Create the timezone object\n",
      "ist = tz.gettz('Asia/Kolkata')\n",
      "\n",
      "# Pull out the start of the first trip\n",
      "local = onebike_datetimes[0]['start']\n",
      "\n",
      "# What time was it in India?\n",
      "notlocal = local.astimezone(ist)\n",
      "\n",
      "# Print them out and see the difference\n",
      "print(local.isoformat())\n",
      "print(notlocal.isoformat())\n",
      "\n",
      "  > sample  > pacific time zone vs local\n",
      "\n",
      "# Create the timezone object\n",
      "sm = tz.gettz('Pacific/Apia')\n",
      "\n",
      "# Pull out the start of the first trip\n",
      "local = onebike_datetimes[0]['start']\n",
      "\n",
      "# What time was it in Samoa?\n",
      "notlocal = local.astimezone(sm)\n",
      "\n",
      "# Print them out and see the difference\n",
      "print(local.isoformat())\n",
      "print(notlocal.isoformat())\n",
      "\n",
      "\n",
      "    Daylight saving time\n",
      "\n",
      "forward in the spring\n",
      "back in the fall\n",
      "\n",
      "\n",
      "\n",
      "2017-03-12 01:59:59 springs forward 2017-03-12 03:00:00\n",
      "\n",
      "spring_ahead_159am = datetime(2017,3,12,1,59,59)\n",
      "spring_ahead_159am=isoformat()\n",
      "\n",
      "spring_ahead_3am=datetime(2017,3,12,3,0,0)\n",
      "spring_ahead_3am.isoformat()\n",
      "\n",
      "(spring_ahead_3am - spring_ahead_159am).total_seconds()\n",
      "\n",
      "EST = timezone(timedelta(hours=-5))\n",
      "EDT = timezone(timedelta(hours=-4))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\kaggle.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\kaggle.txt\n",
      "1. problem\n",
      "2. data\n",
      "3. model\n",
      "4. submission\n",
      "5. leaderboard\n",
      "\n",
      "download the data\n",
      "build your models\n",
      "\n",
      "\n",
      "taxi_train=pd.read_csv('taxi_train.csv')\n",
      "\n",
      "taxi_train.columns.to_list()\n",
      "\n",
      "taxi_test=pd.read_csv('taxi_test.csv')\n",
      "\n",
      "taxi_test.columns.to_list()\n",
      "\n",
      "submission file:\n",
      "\n",
      "taxi_sample_submission.csv\n",
      "\n",
      "taxi_sample_sub=pd.read_csv('taxi_sample_submission.csv')\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Read train data\n",
      "train = pd.read_csv('train.csv')\n",
      "\n",
      "# Look at the shape of the data\n",
      "print('Train shape:', train.shape)\n",
      "\n",
      "# Look at the head() of the data\n",
      "print(train.head())\n",
      "\n",
      " Read the test data\n",
      "test = pd.read_csv('test.csv')\n",
      "\n",
      "# Print train and test columns\n",
      "print('Train columns:', train.columns.tolist())\n",
      "print('Test columns:', test.columns.tolist())\n",
      "\n",
      "output:\n",
      "Train columns: ['id', 'date', 'store', 'item', 'sales']\n",
      "Test columns: ['id', 'date', 'store', 'item']\n",
      "\n",
      "\n",
      "# Read the sample submission file\n",
      "sample_submission = pd.read_csv('sample_submission.csv')\n",
      "\n",
      "# Look at the head() of the sample submission\n",
      "print(sample_submission.head())\n",
      "\n",
      "\n",
      "            >Prepare your first submission\n",
      "\n",
      "taxi_train=pd.read_csv('taxi_train.csv')\n",
      "taxi_train.columns.to_list()\n",
      "\n",
      "what is the problem type (regression, classification)\n",
      "\n",
      "\n",
      "taxi_train.fare_amount.hist(bins=30, alpha=0.5)\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lr=LinearRegression()\n",
      "\n",
      "lr.fit(X=taxi_train[['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']],\n",
      "\ty=taxi_train['fare_amount'])\n",
      "\n",
      "features=['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']\n",
      "\n",
      "taxi_test['fare_amount']=lr.predict(taxi_test[features])\n",
      "\n",
      "  submission file\n",
      "\n",
      "key\n",
      "fare_amount\n",
      "\n",
      "taxi_submission=taxi_test[['key','fare_amount']]\n",
      "\n",
      "taxi_submission.to_csv('first_sub.csv',index=False)\n",
      "\n",
      "\n",
      "   sample submission\n",
      "\n",
      "features=['id', 'date', 'store', 'item', 'sales']\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "# Read the train data\n",
      "train = pd.read_csv('train.csv')\n",
      "\n",
      "print(train.columns.to_list())\n",
      "\n",
      "# Create a Random Forest object\n",
      "rf = RandomForestRegressor()\n",
      "\n",
      "# Train a model\n",
      "rf.fit(X=train[['store', 'item']], y=train['sales'])\n",
      "\n",
      "# Read test and sample submission data\n",
      "test = pd.read_csv('test.csv')\n",
      "sample_submission = pd.read_csv('sample_submission.csv')\n",
      "\n",
      "# Show the head() of the sample_submission\n",
      "print(sample_submission.head())\n",
      "\n",
      "# Get predictions for the test set\n",
      "test['sales'] = rf.predict(test[['store', 'item']])\n",
      "\n",
      "# Write test predictions using the sample_submission format\n",
      "test[['id', 'sales']].to_csv('kaggle_submission.csv', index=False)\n",
      "\n",
      "\n",
      "  > public vs private leaderboard\n",
      "\n",
      "evaluation metric\n",
      "\n",
      "1. area under the roc (auc) (classification)\n",
      "2. f1 score (classification)\n",
      "3. mean log loss (logloss) (classification)\n",
      "4. mean absolute error (mae) (regression)\n",
      "5. mean squared error (mse) (regression)\n",
      "6. mean average precision Ranking (ranking)\n",
      "\n",
      "test split\n",
      "1. public test\n",
      "2. private test\n",
      "\n",
      "\n",
      "submission[['id','target']].to_csv('submission_1.csv',index=False)\n",
      "\n",
      "\n",
      "as model complexity increases the train data error goes down but the test data error goes up\n",
      "\n",
      "private LB\n",
      "public LB\n",
      "\n",
      "shake-up\n",
      "\n",
      "   > sample  > xgboost  > max_depth=2\n",
      "\n",
      "import xgboost as xgb\n",
      "\n",
      "# Create DMatrix on train data\n",
      "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
      "                     label=train['sales'])\n",
      "\n",
      "# Define xgboost parameters\n",
      "params = {'objective': 'reg:linear',\n",
      "          'max_depth': 2,\n",
      "          'silent': 1}\n",
      "\n",
      "# Train xgboost model\n",
      "xg_depth_2 = xgb.train(params=params, dtrain=dtrain)\n",
      "\n",
      "\n",
      "     sample  > xgboost  > max_depth=8\n",
      "\n",
      "import xgboost as xgb\n",
      "\n",
      "# Create DMatrix on train data\n",
      "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
      "                     label=train['sales'])\n",
      "\n",
      "# Define xgboost parameters\n",
      "params = {'objective': 'reg:linear',\n",
      "          'max_depth': 8,\n",
      "          'silent': 1}\n",
      "\n",
      "# Train xgboost model\n",
      "xg_depth_8 = xgb.train(params=params, dtrain=dtrain)\n",
      "\n",
      "dtrain = xgb.DMatrix(data=train[['store', 'item']])\n",
      "dtest = xgb.DMatrix(data=test[['store', 'item']])\n",
      "\n",
      "# For each of 3 trained models\n",
      "for model in [xg_depth_2, xg_depth_8, xg_depth_15]:\n",
      "    # Make predictions\n",
      "    train_pred = model.predict(dtrain)     \n",
      "    test_pred = model.predict(dtest)          \n",
      "    \n",
      "    # Calculate metrics\n",
      "    mse_train = mse(train['sales'], train_pred)                  \n",
      "    mse_test =mse(test['sales'], test_pred)\n",
      "    print('MSE Train: {:.3f}. MSE Test: {:.3f}'.format(mse_train, mse_test))\n",
      "\n",
      "output:\n",
      "\n",
      "MSE Train: 631.275. MSE Test: 558.522\n",
      "MSE Train: 183.771. MSE Test: 337.337\n",
      "MSE Train: 134.984. MSE Test: 355.534\n",
      "\n",
      "       Understand the problem\n",
      "\n",
      "solution workflow\n",
      "1. understand the problem\n",
      "2. eda - explore data analysis\n",
      "3. local validation\n",
      "4. modeling\n",
      "\n",
      "\n",
      "data type: tabular, time series, image, text\n",
      "\n",
      "problem type: classification, regression, ranking\n",
      "\n",
      "evaluation metric: roc au, f1 score, mae, mse\n",
      "\n",
      "from sklearn.metrics import roc_auc_score, f1_score, mean_squared_error\n",
      "\n",
      "def rmsl(y_true, y_pred):\n",
      "\tdiffs=np.log(y_true+1) - np.log(y_pred+1)\n",
      "\tsquares=np.power(diffs,2)\n",
      "\terr=np.sqrt(np.mean(squares))\n",
      "\treturn err\n",
      "\n",
      "\n",
      "def own_mse(y_true, y_pred):\n",
      "  \t# Raise differences to the power of 2\n",
      "    squares = np.power(y_true - y_pred, 2)\n",
      "    # Find mean over all observations\n",
      "    err = np.mean(squares)\n",
      "    return err\n",
      "\n",
      "\n",
      "   sample  > log loss\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "# Import log_loss from sklearn\n",
      "from sklearn.metrics import log_loss\n",
      "\n",
      "# Define your own LogLoss function\n",
      "def own_logloss(y_true, prob_pred):\n",
      "  \t# Find loss for each observation\n",
      "    terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1 - prob_pred)\n",
      "    # Find mean over all observations\n",
      "    err = np.mean(terms) \n",
      "    return -err\n",
      "\n",
      "print('Sklearn LogLoss: {:.5f}'.format(log_loss(y_classification_true, y_classification_pred)))\n",
      "print('Your LogLoss: {:.5f}'.format(own_logloss(y_classification_true, y_classification_pred)))\n",
      "\n",
      "\n",
      "\n",
      "        Initial EDA\n",
      "\n",
      "Exploratory Data Analysis\n",
      "\n",
      "1. size of the data\n",
      "2. properties of the target variable\n",
      "3. properties of the features\n",
      "4. generate ideas for feature engineering\n",
      "\n",
      "predict the popularity of an apartment rental listing\n",
      "\n",
      "target_variable\n",
      "1. interest_level\n",
      "\n",
      "two sigma connect\n",
      "\n",
      "id\n",
      "bathrooms\n",
      "bedrooms\n",
      "building_id\n",
      "latitude\n",
      "longitude\n",
      "manager_id\n",
      "price\n",
      "interest_level\n",
      "\n",
      "df.interest_level.value_counts()\n",
      "\n",
      "df.describe()\n",
      "1. count\n",
      "2. std\n",
      "3. min\n",
      "4. 25%\n",
      "5. 50%\n",
      "6. 75%\n",
      "7. max\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.style.use('ggplot')\n",
      "\n",
      "prices = df.groupby('interest_level', as_index=False)['price'].median()\n",
      "\n",
      "fig=plt.figure(figsize=(7,5))\n",
      "\n",
      "plt.bar(prices.interest_level, prices.price, width=0.5, alpha=0.8)\n",
      "\n",
      "plt.xlabel('Interest level')\n",
      "plt.ylabel('Median price')\n",
      "plt.title('Median listing price across interest level')\n",
      "\n",
      "plt.show()\n",
      "\n",
      " > sample \n",
      "\n",
      "# Shapes of train and test data\n",
      "print('Train shape:', train.shape)\n",
      "print('Test shape:', test.shape)\n",
      "\n",
      "# Train head()\n",
      "print(train.head())\n",
      "\n",
      "# Describe the target variable\n",
      "print(train.fare_amount.describe())\n",
      "\n",
      "# Train distribution of passengers within rides\n",
      "print(train.passenger_count.value_counts())\n",
      "\n",
      "\n",
      "# Calculate the ride distance\n",
      "train['distance_km'] = haversine_distance(train)\n",
      "\n",
      "# Draw a scatterplot\n",
      "plt.scatter(x=train['fare_amount'], y=train['distance_km'], alpha=0.5)\n",
      "plt.xlabel('Fare amount')\n",
      "plt.ylabel('Distance, km')\n",
      "plt.title('Fare amount based on the distance')\n",
      "\n",
      "# Limit on the distance\n",
      "plt.ylim(0, 50)\n",
      "plt.show()\n",
      "\n",
      "  > sample fare amount on day time\n",
      "\n",
      "# Create hour feature\n",
      "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n",
      "train['hour'] = train.pickup_datetime.dt.hour\n",
      "\n",
      "# Find median fare_amount for each hour\n",
      "hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()\n",
      "\n",
      "# Plot the line plot\n",
      "plt.plot(hour_price['hour'], hour_price['fare_amount'], marker='o')\n",
      "plt.xlabel('Hour of the day')\n",
      "plt.ylabel('Median fare amount')\n",
      "plt.title('Fare amount based on day time')\n",
      "plt.xticks(range(24))\n",
      "plt.show()\n",
      "\n",
      "     >Local validation\n",
      "\n",
      "private LB overfitting\n",
      "\n",
      "holdout set\n",
      "\n",
      "train data-> train set and holdout set\n",
      "\n",
      "train set -> training ->model ->predicting -> holdout set   (assess model quality)\n",
      "\n",
      "       cross validation using k-folding\n",
      "\n",
      "the test on each fold is on data the model has never seen before\n",
      "\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "kdf=KFold(n_splits=5, shuffle=True, random_state=123)\n",
      "\n",
      "for train_index, test_index = kdf.split(train):\n",
      "\tcv_train,cv_test=train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "str_kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
      "\n",
      "for train_index, test_index = str_kf.split(train):\n",
      "\tcv_train,cv_test=train.iloc[train_index], train.iloc[test_index]\n",
      "\t\n",
      "\t\n",
      "\n",
      "    sample    KFold\n",
      "\n",
      "# Import KFold\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "# Create a KFold object\n",
      "kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
      "\n",
      "# Loop through each split\n",
      "fold = 0\n",
      "for train_index, test_index in kf.split(train):\n",
      "    # Obtain training and testing folds\n",
      "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "    print('Fold: {}'.format(fold))\n",
      "    print('CV train shape: {}'.format(cv_train.shape))\n",
      "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
      "    fold += 1\n",
      "\n",
      "   > sample    stratified Fold\n",
      "\n",
      "# Import StratifiedKFold\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "# Create a StratifiedKFold object\n",
      "str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
      "\n",
      "# Loop through each split\n",
      "fold = 0\n",
      "for train_index, test_index in str_kf.split(train, train['interest_level']):\n",
      "    # Obtain training and testing folds\n",
      "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "    print('Fold: {}'.format(fold))\n",
      "    print('CV train shape: {}'.format(cv_train.shape))\n",
      "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
      "    fold += 1\n",
      "\n",
      "\n",
      "       >Validation Usage\n",
      "\n",
      "data leakage\n",
      "1. leak in the features: using data that will not be available in the real setting\n",
      "\n",
      "2. leak in validation strategy - validation strategy differs from the real-world situation\n",
      "\n",
      "\n",
      "    Time K-fold cross validation\n",
      "\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "\n",
      "time_kfold=TimeSeriesSpit(n_splits=5)\n",
      "\n",
      "train=train.sort_values('date')\n",
      "\n",
      "from train_index, test_index in time_kfold.split(train):\n",
      "\t cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "\n",
      "    validation pipeline CV_STRATEGY\n",
      "\n",
      "#list for results\n",
      "\n",
      "fold_metrics=[]\n",
      "\n",
      "for train_index, test_index in CV_STRATEGY.split(train):\n",
      "\t cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "\tmodel.fit(cv_train)\n",
      "\tpredictions=model.predict(cv_test)\n",
      "\t\n",
      "\tmetric=evaluate(cv_test, predictions)\n",
      "\n",
      "\tfold_metrics.append(metric)\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "mean_score = np.mean(fold_metrics)\n",
      "\n",
      "overall_score_minimizing = no.mean(fold_metrics)+ np.std(fold_metrics)\n",
      "\n",
      "overall_score_maximizing = no.mean(fold_metrics)- np.std(fold_metrics)\n",
      "\n",
      "\n",
      "  > sample  > timeseries fold\n",
      "\n",
      "# Create TimeSeriesSplit object\n",
      "time_kfold = TimeSeriesSplit(n_splits=3)\n",
      "\n",
      "# Sort train data by date\n",
      "train = train.sort_values('date')\n",
      "\n",
      "# Iterate through each split\n",
      "fold = 0\n",
      "for train_index, test_index in time_kfold.split(train):\n",
      "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "    \n",
      "    print('Fold :', fold)\n",
      "    print('Train date range: from {} to {}'.format(cv_train.date.min(), cv_train.date.max()))\n",
      "    print('Test date range: from {} to {}\\n'.format(cv_test.date.min(), cv_test.date.max()))\n",
      "    fold += 1\n",
      "\n",
      "\n",
      "\n",
      "Fold : 0\n",
      "Train date range: from 2017-12-01 to 2017-12-08\n",
      "Test date range: from 2017-12-08 to 2017-12-16\n",
      "\n",
      "Fold : 1\n",
      "Train date range: from 2017-12-01 to 2017-12-16\n",
      "Test date range: from 2017-12-16 to 2017-12-24\n",
      "\n",
      "Fold : 2\n",
      "Train date range: from 2017-12-01 to 2017-12-24\n",
      "Test date range: from 2017-12-24 to 2017-12-31\n",
      "\n",
      "\n",
      "the test date ranges do not overlap\n",
      "\n",
      "\n",
      "  > sample get the mean validation mse:\n",
      "\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "import numpy as np\n",
      "\n",
      "# Sort train data by date\n",
      "train = train.sort_values('date')\n",
      "\n",
      "# Initialize 3-fold time cross-validation\n",
      "kf = TimeSeriesSplit(n_splits=3)\n",
      "\n",
      "# Get MSE scores for each cross-validation split\n",
      "mse_scores = get_fold_mse(train, kf)\n",
      "\n",
      "print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))\n",
      "\n",
      "print('MSE by fold: {}'.format(mse_scores))\n",
      "\n",
      "print('Overall validation MSE: {:.5f}'.format(np.mean(mse_scores) + np.std(mse_scores)))\n",
      "\n",
      "output:\n",
      "Mean validation MSE: 955.49186\n",
      "MSE by fold: [890.30336, 961.65797, 1014.51424]\n",
      "Overall validation MSE: 1006.38784\n",
      "\n",
      "\n",
      "        feature engineering\n",
      "\n",
      "modeling\n",
      "\n",
      "1. create new features\n",
      "2. improve models\n",
      "3. apply tricks\n",
      "4. preprocess data\n",
      "\n",
      "feature engineering is creating new features\n",
      "1. numerical\n",
      "2. categorical\n",
      "3. datetime\n",
      "4. coordinates\n",
      "5. text\n",
      "6. images\n",
      "\n",
      "data = pd.concat([train,test])\n",
      "\n",
      "train=data[data.id.isin(train_id)]\n",
      "test=data[data.id.isin(test_id)]\n",
      "\n",
      "dem['date']=pd.to_datetime(dem['date'])\n",
      "\n",
      "dem['year']=dem['date'].dt.year\n",
      "dem['month']=dem['date'].dt.month\n",
      "dem['day']=dem['date'].dt.day\n",
      "dem['dayofweek']=dem['date'].dt.dayofweek\n",
      "\n",
      "\n",
      "  sample    create a new feature\n",
      "\n",
      "# Look at the initial RMSE\n",
      "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
      "\n",
      "# Find the total area of the house\n",
      "train['TotalArea'] = train[\"TotalBsmtSF\"] + train[\"FirstFlrSF\"] + train[\"SecondFlrSF\"]\n",
      "\n",
      "# Look at the updated RMSE\n",
      "print('RMSE with total area:', get_kfold_rmse(train))\n",
      "\n",
      "# Find the area of the garden\n",
      "train['GardenArea'] = train[\"LotArea\"] - train[\"FirstFlrSF\"]\n",
      "print('RMSE with garden area:', get_kfold_rmse(train))\n",
      "\n",
      "RMSE before feature engineering: 36029.39\n",
      "RMSE with total area: 35073.2\n",
      "RMSE with garden area: 34413.55\n",
      "\n",
      "# Find total number of bathrooms\n",
      "train['TotalBath'] = train['FullBath']+train['HalfBath']\n",
      "print('RMSE with number of bathrooms:', get_kfold_rmse(train))\n",
      "\n",
      "RMSE with number of bathrooms: 34506.78\n",
      "\n",
      "Here you see that house area improved the RMSE by almost $1,000. Adding garden area improved the RMSE by another $600. However, with the total number of bathrooms, the RMSE has increased. It means that you keep the new area features, but do not add \"TotalBath\" as a new feature. Let's now work with the datetime features!\n",
      "\n",
      "\n",
      "    sample  > new datetime feature\n",
      "\n",
      "# Concatenate train and test together\n",
      "taxi = pd.concat([train, test])\n",
      "\n",
      "# Convert pickup date to datetime object\n",
      "taxi['pickup_datetime'] = pd.to_datetime(taxi['pickup_datetime'])\n",
      "\n",
      "# Create a day of week feature\n",
      "taxi['dayofweek'] = taxi['pickup_datetime'].dt.dayofweek\n",
      "\n",
      "# Create an hour feature\n",
      "taxi['hour'] = taxi['pickup_datetime'].dt.hour\n",
      "\n",
      "# Split back into train and test\n",
      "new_train = taxi[taxi['id'].isin(train['id'])]\n",
      "new_test = taxi[taxi['id'].isin(test['id'])]\n",
      "\n",
      "\n",
      "        categorical features\n",
      "\n",
      "label encoding\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "4 a\n",
      "\n",
      "encoded\n",
      "1 0\n",
      "2 1\n",
      "3 2\n",
      "4 0\n",
      "\n",
      "\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "le = LabelEncoder()\n",
      "\n",
      "df['cat_encoded'] = le.fit_transform(df['cat'])\n",
      "\n",
      "to overcome the independency between categories, one hot encoding was developed\n",
      "\n",
      "ohe=pd.get_dummies(df['cat'], prefix='ohe_cat')\n",
      "\n",
      "df.drop('cat',axis=1,inplace=True)\n",
      "df=pd.concat([df,ohe],axis=1)\n",
      "\n",
      "  > binary features\n",
      "yes or no\n",
      "\n",
      "le=LabelEncoder()\n",
      "\n",
      "binary_feature['binary_encoded']=le.fit_transform(binary_feature['binary_feat'])\n",
      "\n",
      " > other encoders\n",
      "\n",
      "target encoder\n",
      "\n",
      "  > sample  > labelEncoder\n",
      "\n",
      "# Concatenate train and test together\n",
      "houses = pd.concat([train, test])\n",
      "\n",
      "# Label encoder\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "le = LabelEncoder()\n",
      "\n",
      "# Create new features\n",
      "houses['RoofStyle_enc'] = le.fit_transform(houses[\"RoofStyle\"])\n",
      "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
      "\n",
      "# Look at new features\n",
      "print(houses[['RoofStyle', 'RoofStyle_enc', 'CentralAir', 'CentralAir_enc']].head())\n",
      "\n",
      "\n",
      "\n",
      "    problem with label encoding\n",
      "\n",
      "The problem with label encoding is that it implicitly assumes that there is a ranking dependency between the categories.\n",
      "\n",
      "\n",
      "  > sample  value counts between roof style and central air\n",
      "\n",
      "# Concatenate train and test together\n",
      "houses = pd.concat([train, test])\n",
      "\n",
      "# Look at feature distributions\n",
      "print(houses['RoofStyle'].value_counts(), '\\n')\n",
      "print(houses['CentralAir'].value_counts())\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "Name: RoofStyle, dtype: int64 \n",
      "\n",
      "Y    2723\n",
      "N     196\n",
      "\n",
      "Name: CentralAir, dtype: int64\n",
      "\n",
      "  > encode CentralAir as binary 0 or 1\n",
      "\n",
      "# Concatenate train and test together\n",
      "houses = pd.concat([train, test])\n",
      "\n",
      "# Label encode binary 'CentralAir' feature\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "le = LabelEncoder()\n",
      "houses['CentralAir_enc'] = le.fit_transform(houses[\"CentralAir\"])\n",
      "\n",
      " >one hot encode\n",
      "\n",
      "# Create One-Hot encoded features\n",
      "ohe = pd.get_dummies(houses['RoofStyle'], prefix='RoofStyle')\n",
      "\n",
      "# Concatenate OHE features to houses\n",
      "houses = pd.concat([houses, ohe], axis=1)\n",
      "\n",
      "# Look at OHE features\n",
      "print(houses[[col for col in houses.columns if 'RoofStyle' in col]].head(3))\n",
      "\n",
      "\n",
      "     >Target encoding\n",
      "\n",
      "1. label encoder provides distinct number for each category\n",
      "\n",
      "2. one-hot encoder creates new features for each category value\n",
      "\n",
      "target encoding creates a single column\n",
      "\n",
      "1. calculate mean on the train, apply to the test\n",
      "2. split train into K folds.  calculate mean on k-1 folds, apply to the k-th fold.  this prevents overfitting\n",
      "3. add mean target encoded feature to the model\n",
      "\n",
      "\n",
      "  > sample    categorical  target\n",
      "def train_mean_target_encoding(train, target, categorical, alpha=5):\n",
      "    # Create 5-fold cross-validation\n",
      "    kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
      "    train_feature = pd.Series(index=train.index)\n",
      "\n",
      "    # For each folds split\n",
      "    for train_index, test_index in kf.split(train):\n",
      "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "        # Calculate out-of-fold statistics and apply to cv_test\n",
      "        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)\n",
      "        # Save new feature for this particular fold\n",
      "        train_feature.iloc[test_index] = cv_test_feature       \n",
      "    return train_feature.values\n",
      "\n",
      "def test_mean_target_encoding(train, test, target, categorical, alpha=5):\n",
      "    # Calculate global mean on the train data\n",
      "    global_mean = train[target].mean()\n",
      "    \n",
      "    # Group by the categorical feature and calculate its properties\n",
      "    train_groups = train.groupby(categorical)\n",
      "    category_sum = train_groups[target].sum()\n",
      "    category_size = train_groups.size()\n",
      "    \n",
      "    # Calculate smoothed mean target statistics\n",
      "    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)\n",
      "    \n",
      "    # Apply statistics to the test data and fill new categories\n",
      "    test_feature = test[categorical].map(train_statistics).fillna(global_mean)\n",
      "    return test_feature.values\n",
      "\n",
      "\n",
      "def mean_target_encoding(train, test, target, categorical, alpha=5):\n",
      "  \n",
      "    # Get the train feature\n",
      "    train_feature = train_mean_target_encoding(train, target, categorical, alpha)\n",
      "  \n",
      "    # Get the test feature\n",
      "    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)\n",
      "    \n",
      "    # Return new features to add to the model\n",
      "    return train_features, test_features\n",
      "\n",
      "\n",
      " code\n",
      "\n",
      "# For each folds split\n",
      "    for train_index, test_index in kf.split(train):\n",
      "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "      \n",
      "        # Calculate out-of-fold statistics and apply to cv_test\n",
      "\n",
      "        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)\n",
      "        \n",
      "        # Save new feature for this particular fold\n",
      "        train_feature.iloc[test_index] = cv_test_feature       \n",
      "\n",
      "    return train_feature.values\n",
      "\n",
      "\n",
      "  > target categorical\n",
      "\n",
      "# Create 5-fold cross-validation\n",
      "kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
      "\n",
      "# For each folds split\n",
      "for train_index, test_index in kf.split(bryant_shots):\n",
      "    cv_train, cv_test = bryant_shots.iloc[train_index], bryant_shots.iloc[test_index]\n",
      "\n",
      "    # Create mean target encoded feature\n",
      "    cv_train['game_id_enc'], cv_test['game_id_enc'] = mean_target_encoding(train=cv_train,\n",
      "                                                                           test=cv_test,\n",
      "                                                                           target='shot_made_flag',\n",
      "                                                                           categorical='game_id',\n",
      "                                                                           alpha=5)\n",
      "    # Look at the encoding\n",
      "    print(cv_train[['game_id', 'shot_made_flag', 'game_id_enc']].sample(n=1))\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "<script.py> output:\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    7106  20500532             0.0     0.361914\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    5084  20301100             0.0     0.568395\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    6687  20500228             0.0      0.48131\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    5046  20301075             0.0     0.252103\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    4662  20300515             1.0     0.452637\n",
      "\n",
      "\n",
      "The main conclusion you should make: while using local cross-validation, you need to repeat mean target encoding procedure inside each folds split separately. Go on to try other problem types beyond binary classification!\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "# Create mean target encoded feature\n",
      "train['RoofStyle_enc'], test['RoofStyle_enc'] = mean_target_encoding(train=train,\n",
      "                                                                     test=test,\n",
      "                                                                     target='SalePrice',\n",
      "                                                                     categorical='RoofStyle',\n",
      "                                                                     alpha=10)\n",
      "\n",
      "# Look at the encoding\n",
      "print(test[['RoofStyle', 'RoofStyle_enc']].drop_duplicates())\n",
      "\n",
      "\n",
      "\n",
      "output:\n",
      "<script.py> output:\n",
      "         RoofStyle  RoofStyle_enc\n",
      "    0        Gable  171565.947836\n",
      "    1          Hip  217594.645131\n",
      "    98     Gambrel  164152.950424\n",
      "    133       Flat  188703.563431\n",
      "    362    Mansard  180775.938759\n",
      "    1053      Shed  188267.663242\n",
      "\n",
      "\n",
      "So, you observe that houses with the Hip roof are the most pricy, while houses with the Gambrel roof are the cheapest.\n",
      "\n",
      "\n",
      "   > Missing data\n",
      "\n",
      "Mean/Median imputation\n",
      "\n",
      "categorical missing data replaced with the most frequent value\n",
      "\n",
      "new category imputation\n",
      "\n",
      "df.isnull().head(1)\n",
      "\n",
      "\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "mean_imputer = SimpleImputer(strategy='mean')\n",
      "\n",
      "constant_imputer = SimpleImputer(strategy='constant', fill_value=-999)\n",
      "\n",
      "\n",
      "df[['num']] = mean_imputer.fit_transform(df[['num']])\n",
      "\n",
      "\n",
      "constant_imputer = SimpleImputer(strategy='constant', fill_value='MISS')\n",
      "\n",
      "   sample  > find columns with missing data\n",
      "\n",
      "# Read dataframe\n",
      "twosigma = pd.read_csv(\"twosigma_train.csv\")\n",
      "\n",
      "# Find the number of missing values in each column\n",
      "print(twosigma.isnull().sum())\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "id                 0\n",
      "bathrooms          0\n",
      "bedrooms           0\n",
      "building_id       13\n",
      "latitude           0\n",
      "longitude          0\n",
      "manager_id         0\n",
      "price             32\n",
      "interest_level     0\n",
      "dtype: int64\n",
      "\n",
      " # Look at the columns with the missing values\n",
      "print(twosigma[['building_id', 'price']].head())\n",
      "\n",
      "\n",
      " > sample  > imputer mean\n",
      "\n",
      "# Import SimpleImputer\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "# Create mean imputer\n",
      "mean_imputer = SimpleImputer(strategy='mean')\n",
      "\n",
      "# Price imputation\n",
      "rental_listings[['price']] = mean_imputer.fit_transform(rental_listings[['price']])\n",
      "\n",
      "   sample  > simple imputer constant\n",
      "\n",
      "# Import SimpleImputer\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "# Create constant imputer\n",
      "constant_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
      "\n",
      "# building_id imputation\n",
      "rental_listings[['building_id']] = constant_imputer.fit_transform(rental_listings[['building_id']])\n",
      "\n",
      "     >baseline model\n",
      "\n",
      "taxi_train=pd.read_csv('taxi_train.csv')\n",
      "taxi_test=pd.read_csv('taxi_test.csv')\n",
      "\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "validation_train, validation_test=train_test_split(taxi_train, test_size=0.3, random_state=123)\n",
      "\n",
      "taxi_test['fare_amount']=np.mean(taxi_train.fair_amount)\n",
      "\n",
      "#### mean_sub\n",
      "taxi_test[['id','fare_amount']].to_csv('mean_sub.csv',index=False)\n",
      "\n",
      "naive_prediction_groups=taxi_train.groupby('passenger_count').fare_amount.mean()\n",
      "\n",
      "taxi_test['fare_amount']=taxi_test.passenger_count.map(naive_prediction_groups)\n",
      "\n",
      "#map- Used for substituting each value in a Series with another value\n",
      "\n",
      "### mean group sub\n",
      "\n",
      "taxi_test[['id','fare_amount']].to_csv('mean_group_sub.csv', index=False)\n",
      "\n",
      "#select only numeric features\n",
      "\n",
      "features['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']\n",
      "\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "gb=GradientBoostingRegressor()\n",
      "\n",
      "gb.fit(taxi_train[features], taxi_train.fare_amount)\n",
      "\n",
      "taxi_test['fare_amount']=gb.predict(taxi_test[features])\n",
      "\n",
      "### gradient boost\n",
      "\n",
      "taxi_test[['id','fare_amount']].to_csv('gb_sub.csv',index=False)\n",
      "\n",
      "\n",
      "model\t\tvalidation RMSE  public LB RMSE\n",
      "\n",
      "simple mean\t9.986\t9.409\n",
      "group mean\t9.978\t9.407\n",
      "gradient boost\t5.996\t4.595\n",
      "\n",
      "\n",
      "   sample  > hold out\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from math import sqrt\n",
      "\n",
      "# Calculate the mean fare_amount on the validation_train data\n",
      "naive_prediction = np.mean(validation_train['fare_amount'])\n",
      "\n",
      "# Assign naive prediction to all the holdout observations\n",
      "validation_test['pred'] = naive_prediction\n",
      "\n",
      "# Measure the local RMSE\n",
      "rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))\n",
      "print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))\n",
      "\n",
      "Validation RMSE for Baseline I model: 9.986\n",
      "\n",
      "\n",
      "   sample Group by hour\n",
      "\n",
      "# Get pickup hour from the pickup_datetime column\n",
      "train['hour'] = train['pickup_datetime'].dt.hour\n",
      "test['hour'] = test['pickup_datetime'].dt.hour\n",
      "\n",
      "# Calculate average fare_amount grouped by pickup hour \n",
      "hour_groups = train.groupby('hour')['fare_amount'].mean()\n",
      "\n",
      "# Make predictions on the test set\n",
      "test['fare_amount'] = test.hour.map(hour_groups)\n",
      "\n",
      "# Write predictions\n",
      "test[['id','fare_amount']].to_csv('hour_mean_sub.csv', index=False)\n",
      "\n",
      "\n",
      "   hyperparameter tuning\n",
      "\n",
      "add hour feature: validation rmse 5.553\n",
      "\n",
      "add distance feature: validation rmse 5.268\n",
      "\n",
      "deep learning does not require feature engineering\n",
      "\n",
      "least squares linear regression\n",
      "1. loss = (y_i-yhat_i)**2 -> min\n",
      "\n",
      "ridge regression\n",
      "loss =  (y_i-yhat_i)**2 + alpha * weights**2\n",
      "\n",
      "popular approachs\n",
      "1. Grid Search\n",
      "2. Random Grid Search\n",
      "3. Bayesian optimization\n",
      "\n",
      "\n",
      "     >Grid search\n",
      "\n",
      "alpha_grid=[0.01,0.1,1,10]\n",
      "\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "results={}\n",
      "\n",
      "for candidate_alpha in alpha_grid:\n",
      "\tridge_regression=Ridge(alpha=candidate_alpha)\n",
      "\n",
      "\tresults[candidate_alpha]=validation_score\n",
      "\n",
      "  > sample max depth\n",
      "\n",
      "# Possible max depth values\n",
      "max_depth_grid = [3,6,9,12,15]\n",
      "results = {}\n",
      "\n",
      "# For each value in the grid\n",
      "for max_depth_candidate in max_depth_grid:\n",
      "    # Specify parameters for the model\n",
      "    params = {'max_depth': max_depth_candidate}\n",
      "\n",
      "    # Calculate validation score for a particular hyperparameter\n",
      "    validation_score = get_cv_score(train, params)\n",
      "\n",
      "    # Save the results for each max depth value\n",
      "    results[max_depth_candidate] = validation_score   \n",
      "print(results)\n",
      "\n",
      "output:\n",
      "\n",
      "{3: 6.50509, 6: 6.52138, 9: 6.64181, 12: 6.8819, 15: 6.99156}\n",
      "\n",
      "\n",
      "\n",
      "The drawback of tuning each hyperparameter independently is a potential dependency between different hyperparameters. The better approach is to try all the possible hyperparameter combinations.\n",
      "\n",
      "\n",
      "       Sample Product with parameters\n",
      "\n",
      "import itertools\n",
      "\n",
      "# Hyperparameter grids\n",
      "max_depth_grid = [3, 5, 7]\n",
      "subsample_grid = [0.8, 0.9, 1.0]\n",
      "results = {}\n",
      "\n",
      "# For each couple in the grid\n",
      "for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid):\n",
      "    params = {'max_depth': max_depth_candidate,\n",
      "              'subsample': subsample_candidate}\n",
      "    validation_score = get_cv_score(train, params)\n",
      "    # Save the results for each couple\n",
      "    results[(max_depth_candidate, subsample_candidate)] = validation_score   \n",
      "print(results)\n",
      "\n",
      "\n",
      "{(3, 0.8): 6.33917, (3, 0.9): 6.43642, (3, 1.0): 6.50509, (5, 0.8): 6.26977, (5, 0.9): 6.35116, (5, 1.0): 6.45468, (7, 0.8): 6.1635, (7, 0.9): 6.34018, (7, 1.0): 6.48436}\n",
      "\n",
      "With max_depth equal to 7 and subsample equal to 0.8, the best RMSE is now $6.16.\n",
      "\n",
      "(grid_df_class.cv_results_)\n",
      "\n",
      "applies only to classification\n",
      "\n",
      "    Model Ensembling\n",
      "\n",
      "input data:\n",
      "1. categorical\n",
      "2. one hote encoded categorical\n",
      "3. numerics\n",
      "\n",
      "inputs into\n",
      "500 modesl\n",
      "inputs into \n",
      "125 xgboost models\n",
      "\n",
      "different subsets\n",
      "40 models and 60 models\n",
      "\n",
      "input into \n",
      "5 models of keras\n",
      "\n",
      "weighted Rank Average\n",
      "\n",
      "\n",
      "  > Regression problem\n",
      "\n",
      "regression classifier\n",
      "\n",
      "train two models a and b\n",
      "\n",
      "\n",
      "  > model stacking\n",
      "\n",
      "1. split train data into two parts\n",
      "2. train multiple models on part 1\n",
      "3. make predictions on part 2\n",
      "4. make predictions on the test data\n",
      "\n",
      "5. train a new model on part 2 using predictions as features\n",
      "\n",
      "6. make predictions on the test data using the 2nd level model.\n",
      "\n",
      "train models A, b, c on part 1\n",
      "\n",
      "trainid   feature1 feature2 Target A_pred, B_pred, c_pred\n",
      "\n",
      "4\t.10\t2.87\t1\t0.71\t0.52\t.098\n",
      "\n",
      "make predictions of the test data as well\n",
      "\n",
      "testid   feature1 feature2 Target A_pred, B_pred, c_pred\n",
      "\n",
      "\n",
      "train 2nd level model on part 2 using the train and test data set from part 1\n",
      "\n",
      "resulting in a stacking prediction\n",
      "\n",
      "    sample add new feature gb and rf part 1\n",
      "\n",
      "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
      "\n",
      "# Train a Gradient Boosting model\n",
      "gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\n",
      "\n",
      "# Train a Random Forest model\n",
      "rf = RandomForestRegressor().fit(train[features], train.fare_amount)\n",
      "\n",
      "# Make predictions on the test data\n",
      "test['gb_pred'] = gb.predict(test[features])\n",
      "test['rf_pred'] = rf.predict(test[features])\n",
      "\n",
      "# Find mean of model predictions\n",
      "test['blend'] = (test['gb_pred'] + test['rf_pred']) / 2\n",
      "print(test[['gb_pred', 'rf_pred', 'blend']].head(3))\n",
      "\n",
      "\n",
      "# Split train data into two parts\n",
      "part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n",
      "\n",
      "# Train a Gradient Boosting model on Part 1\n",
      "gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n",
      "\n",
      "# Train a Random Forest model on Part 1\n",
      "rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)\n",
      "\n",
      "# Make predictions on the Part 2 data\n",
      "part_2['gb_pred'] = gb.predict(part_2[features])\n",
      "part_2['rf_pred'] = rf.predict(part_2[features])\n",
      "\n",
      "# Make predictions on the test data\n",
      "test['gb_pred'] = gb.predict(test[features])\n",
      "test['rf_pred'] = rf.predict(test[features])\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Create linear regression model without the intercept\n",
      "lr = LinearRegression(fit_intercept=False)\n",
      "\n",
      "# Train 2nd level model on the Part 2 data\n",
      "lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\n",
      "\n",
      "# Make stacking predictions on the test data\n",
      "test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\n",
      "\n",
      "# Look at the model coefficients\n",
      "print(lr.coef_)\n",
      "\n",
      "output:\n",
      "[0.72504358 0.27647395]\n",
      "\n",
      "Looking at the coefficients, it's clear that 2nd level model has more trust to the Gradient Boosting: 0.7 versus 0.3 for the Random Forest model. \n",
      "\n",
      "        >.Save Information\n",
      "\n",
      "1. save folds to the disk\n",
      "2. save model runs\n",
      "3. save model predictions to the disk\n",
      "4. save performance results\n",
      "\n",
      "\n",
      "forums\n",
      "\n",
      "Competition discussion by the participants\n",
      "\n",
      "Kaggle kernels\n",
      "scripts and notebooks shared by the participants\n",
      "\n",
      "cloud computational environment\n",
      "\n",
      "competitions last 2 to 3 months\n",
      "\n",
      "  > sample drop column and score\n",
      "\n",
      "# Drop passenger_count column\n",
      "new_train_1 = train.drop('passenger_count', axis=1)\n",
      "\n",
      "# Compare validation scores\n",
      "initial_score = get_cv_score(train)\n",
      "new_score = get_cv_score(new_train_1)\n",
      "\n",
      "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))\n",
      "\n",
      "\n",
      " Initial score is 6.50509 and the new score is 6.41902\n",
      "\n",
      "\n",
      "# Create copy of the initial train DataFrame\n",
      "new_train_2 = train.copy()\n",
      "\n",
      "# Find sum of pickup latitude and ride distance\n",
      "new_train_2['weird_feature'] = new_train_2['pickup_latitude'] + new_train_2['distance_km']\n",
      "\n",
      "# Compare validation scores\n",
      "initial_score = get_cv_score(train)\n",
      "new_score = get_cv_score(new_train_2)\n",
      "\n",
      "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))\n",
      "\n",
      "Initial score is 6.50509 and the new score is 6.5121\n",
      "\n",
      " In this particular case, dropping the \"passenger_count\" feature helped, while finding the sum of pickup latitude and ride distance did not. \n",
      "\n",
      "Machine learning models\n",
      "\n",
      "1. talk to business.  Define the problem\n",
      "2. collect the data\n",
      "3. select the metric\n",
      "4. make train and test split\n",
      "5. create the model\n",
      "6. move the model to production\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\kpi.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\kpi.txt\n",
      "import pandas as pd\n",
      "\n",
      "customer_demographics=pd.read_csv('customer_demographics.csv')\n",
      "\n",
      "uid\n",
      "reg_date\n",
      "device\n",
      "gender\n",
      "country\n",
      "age\n",
      "\n",
      "\n",
      "#customer actions\n",
      "customer_subscriptions=pd.read_csv('customer_subscriptions.csv')\n",
      "\n",
      "print(customer_subscriptions.head())\n",
      "\n",
      "uid\n",
      "lapse_date\n",
      "subscription_date\n",
      "price\n",
      "\n",
      "KPI : conversion rate\n",
      "\n",
      "importance across different user groups\n",
      "\n",
      "sub_data_demo=customer_demographics.merge(\n",
      "\tcustomer_subscriptions,\n",
      "\thow='inner',\n",
      "\ton=['uid']\n",
      "\t)\n",
      "\n",
      "\n",
      "\n",
      "   > sample\n",
      "\n",
      "# Import pandas \n",
      "import pandas as pd\n",
      "\n",
      "# Load the customer_data\n",
      "customer_data = pd.read_csv('customer_data.csv')\n",
      "\n",
      "# Load the app_purchases\n",
      "app_purchases = pd.read_csv('inapp_purchases.csv')\n",
      "\n",
      "# Print the columns of customer data\n",
      "print(customer_data.columns)\n",
      "\n",
      "# Print the columns of app_purchases\n",
      "print(app_purchases.columns)\n",
      "\n",
      "\n",
      "Index(['uid', 'reg_date', 'device', 'gender', 'country', 'age'], dtype='object')\n",
      "\n",
      "Index(['date', 'uid', 'sku', 'price'], dtype='object')\n",
      "\n",
      "# Merge on the 'uid' field\n",
      "uid_combined_data = app_purchases.merge(customer_data, on=['uid'], how='inner')\n",
      "\n",
      "# Examine the results \n",
      "print(uid_combined_data.head())\n",
      "print(len(uid_combined_data))\n",
      "\n",
      "\n",
      "date_x       uid            sku  price      date_y device gender country  age\n",
      "0  2017-07-10  41195147  sku_three_499    499  2017-06-26    and      M     BRA   17\n",
      "1  2017-07-15  41195147  sku_three_499    499  2017-06-26    and      M     BRA   17\n",
      "2  2017-11-12  41195147   sku_four_599    599  2017-06-26    and      M     BRA   17\n",
      "3  2017-09-26  91591874    sku_two_299    299  2017-01-05    and      M     TUR   17\n",
      "4  2017-12-01  91591874   sku_four_599    599  2017-01-05    and      M     TUR   17\n",
      "9006\n",
      "In [1]:\n",
      "\n",
      "\n",
      "# Merge on the 'uid' and 'date' field\n",
      "uid_date_combined_data = app_purchases.merge(customer_data, on=['uid', 'date'], how='inner')\n",
      "\n",
      "# Examine the results \n",
      "print(uid_date_combined_data.head())\n",
      "print(len(uid_date_combined_data))\n",
      "\n",
      "\n",
      " uid             sku  price device gender country  age\n",
      "0  2016-03-30  94055095    sku_four_599    599    iOS      F     BRA   16\n",
      "1  2015-10-28  69627745     sku_one_199    199    and      F     BRA   18\n",
      "2  2017-02-02  11604973  sku_seven_1499    499    and      F     USA   16\n",
      "3  2016-06-05  22495315    sku_four_599    599    and      F     USA   19\n",
      "4  2018-02-17  51365662     sku_two_299    299    iOS      M     TUR   16\n",
      "\n",
      "      . exploratory analysis of kpi\n",
      "\n",
      "1. most companies will have many kpis\n",
      "2. each serves a different purpose\n",
      "\n",
      "#axis=0 is columns\n",
      "#as_index will use group labels as index\n",
      "\n",
      "sub_data_grp=sub_data_deep.groupby(by=['country','device'], axis=0, as_index=False)\n",
      "\n",
      "sub_data_grp.mean()\n",
      "or\n",
      "sub_data_grp.agg('mean')\n",
      "or\n",
      "sub_data_grp.agg(['mean','median'])\n",
      "or\n",
      "sub_data_grp.agg({'price':['mean','median','max'],\n",
      "\t'age':['mean','median','max']\n",
      "\t})\n",
      "\n",
      "def truncate_mean(data):\n",
      "\ttop_val=data.quantile(.9)\n",
      "\tbot_val=data.quantile(.1)\n",
      "\ttrunc_data=data[(data<=top_val) & (data>=bot_val)]\n",
      "\tmean=trunc_data.mean()\n",
      "\treturn (mean)\n",
      "\n",
      "\n",
      "sub_data_grp.agg({'age':[truncated_mean]})\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "# Calculate the mean and median purchase price \n",
      "purchase_price_summary = purchase_data.price.agg(['mean', 'median'])\n",
      "\n",
      "# Examine the output \n",
      "print(purchase_price_summary)\n",
      "\n",
      "mean      406.772596\n",
      "median    299.000000\n",
      "\n",
      "# Calculate the mean and median of price and age\n",
      "purchase_summary = purchase_data.agg({'price': ['mean', 'median'], 'age': ['mean', 'median']})\n",
      "\n",
      "# Examine the output \n",
      "print(purchase_summary)\n",
      "\n",
      "             price        age\n",
      "mean    406.772596  23.922274\n",
      "median  299.000000  21.000000\n",
      "\n",
      "\n",
      "Notice how the mean is higher than the median? This suggests that we have some users who are making a lot of purchases!\n",
      "\n",
      "# Group the data \n",
      "grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])\n",
      "\n",
      "# Aggregate the data\n",
      "purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})\n",
      "\n",
      "# Examine the results\n",
      "print(purchase_summary)\n",
      "\n",
      "\n",
      "price                   \n",
      "                     mean median         std\n",
      "device gender                               \n",
      "and    F       400.747504    299  179.984378\n",
      "       M       416.237308    499  195.001520\n",
      "iOS    F       404.435330    299  181.524952\n",
      "       M       405.272401    299  196.843197\n",
      "\n",
      "       calculating a conversion rate\n",
      "\n",
      "import pandas as pd\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "current_date=pd.to_datetime('2018-03-17')\n",
      "\n",
      "#what is the maximum lapse date in our dataset\n",
      "\n",
      "max_lapse_date = current_date - timedelta(days=7)\n",
      "\n",
      "conv_sub_data=sub_data_demo[(sub_data_demo.lapse_date<max_lapse_date)]\n",
      "\n",
      "\n",
      "total_users_count=conv_sub_data.price.count()\n",
      "print(total_users_count)\n",
      "\n",
      "max_sub_date=conv_sub_data.lapse_date+timedelta(days=7)\n",
      "\n",
      "total_subs=conv_sub_data[\n",
      "(conv_sub_data.price>0) &\n",
      "(conv_sub_data.subscription_data<=max_sub_data)\n",
      "]\n",
      "\n",
      "total_sub_count=total_sub.price.count()\n",
      "print(total_subs_count)\n",
      "\n",
      "conversion rate = Total subscribers/potential subscribers\n",
      "\n",
      "conversion_rate = total_subs_count / total_users_count\n",
      "print(conversion_rate)\n",
      "\n",
      "\n",
      "      cohort conversion rate\n",
      "\n",
      "conv_sub_data = conv_sub_data.copy()\n",
      "\n",
      "#keep users who lapsed prior to the last 14 days\n",
      "\n",
      "max_lapse_date = current_date - timedelta(days=14)\n",
      "\n",
      "conv_sub_data = sub_data_demo[\n",
      " (sub_data_demo.lapse_date <=max_lapse_date)\n",
      "]\n",
      "\n",
      "sub time is the number of days been the lapse date and the subscription date\n",
      "\n",
      "np.where receives a number to return a true and one to return a false\n",
      "\n",
      "sub_time = np. where(\n",
      "\tconv_sub_data.subscription_date.notnull(),\n",
      "\t#then find how many days since their lapse\n",
      "\t(conv_sub_data.scription_date - conv_sub_data.lapse_date).dt.days,\n",
      "\t#else set the value to pd.NaT\n",
      "\tpd.NaT)\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "\n",
      "find the conversion rate gcr7() and gcr14()\n",
      "\n",
      "purchase_cohorts=conv_sub_data.groupby(by=['gender','device'],as_index=False)\n",
      "\n",
      "#find the conversion rate for each cohort using gcr7 and gcr14\n",
      "\n",
      "purchase_cohorts.agg({sub_time:[gcr7,gcr14]})\n",
      "\n",
      "     How to choose KPI metrics\n",
      "\n",
      "how long does it take to gain insight on a metric\n",
      "\n",
      "what is an actionable time scale\n",
      "\n",
      "monthly conversion rate = 1 month wait time\n",
      "\n",
      "leverage exploratory data analysis\n",
      "* reveals relationships between metrics and key results\n",
      "\n",
      "KPI should measure strong growth\n",
      "* potential early warning sign of problems\n",
      "* senstive to changes in the overall ecosystem\n",
      "\n",
      "       sample\n",
      "\n",
      "# Compute max_purchase_date \n",
      "max_purchase_date = current_date - timedelta(days=28)\n",
      "\n",
      "# Filter to only include users who registered before our max date\n",
      "purchase_data_filt = purchase_data[purchase_data.reg_date < max_purchase_date]\n",
      "\n",
      "# Filter to contain only purchases within the first 28 days of registration\n",
      "purchase_data_filt = purchase_data_filt[(purchase_data_filt.date <=\n",
      "                                         purchase_data_filt.reg_date + \n",
      "                                         timedelta(days=28))]\n",
      "\n",
      "# Output the mean price paid per purchase\n",
      "print(purchase_data_filt.price.mean())\n",
      "\n",
      "414.4237288135593\n",
      "\n",
      "\n",
      "      find a 1 month of data\n",
      "\n",
      "# Set the max registration date to be one month before today\n",
      "max_reg_date = current_date - timedelta(days=28)\n",
      "\n",
      "# Find the month 1 values:\n",
      "month1 = np.where((purchase_data.reg_date < max_reg_date) &\n",
      "                    (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),\n",
      "                  purchase_data.price, \n",
      "                  np.NaN)\n",
      "                 \n",
      "# Update the value in the DataFrame \n",
      "purchase_data['month1'] = month1\n",
      "\n",
      "print(month1)\n",
      "\n",
      "# Group the data by gender and device \n",
      "purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False)\n",
      "\n",
      "# Aggregate the month1 and price data \n",
      "purchase_summary = purchase_data_upd.agg(\n",
      "                        {'month1': ['mean', 'median'],\n",
      "                        'price': ['mean', 'median']})\n",
      "\n",
      "# Examine the results \n",
      "print(purchase_summary)\n",
      "\n",
      "gender device      month1              price       \n",
      "                       mean median        mean median\n",
      "0      F    and  388.204545  299.0  400.747504    299\n",
      "1      F    iOS  432.587786  499.0  404.435330    299\n",
      "2      M    and  413.705882  399.0  416.237308    499\n",
      "3      M    iOS  433.313725  499.0  405.272401    299\n",
      "\n",
      "\n",
      "\n",
      "      >.Working with time series\n",
      "\n",
      "exploratory data analysis\n",
      "\n",
      "2nd week subscribers\n",
      "\n",
      "exclude customers who have not been on the platform for two weeks\n",
      "\n",
      "\n",
      "current_date=pd.to_datetime('2018-03-17')\n",
      "max_lapse_date=current_date - timedelta(days=14)\n",
      "\n",
      "#find all uses who have not been on the platform in the last two weeks\n",
      "\n",
      "conv_sub_data= sub_data_demo[\n",
      "  sub_data_demo.lapse_date < max_lapse_date\n",
      "]\n",
      "\n",
      "#how many days before the user subscribed\n",
      "\n",
      "sub_time = conv_sub_data.subscription_date -\n",
      "conv_sub_data.lapse_date\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "or\n",
      "conv_sub_data['sub_time']= conv_sub_data.sub_time.dt.days\n",
      "\n",
      "#find the users who have not subscribed in week one but have been on the platform for two or more weeks\n",
      "\n",
      "conv_base=conv_sub_data[\n",
      " (conv_sub_data.sub_time.notnull()) |\n",
      "\t(conv_sub_data.sub_time > 7)]\n",
      "\n",
      "total_users=len(conv_base)\n",
      "\n",
      "total_sub=np.where(conv_sub_data.sub_time.notnull()) &\n",
      "\t(conv_sub_data.sub_time <= 14,1,0)\n",
      "\n",
      "total_subs=sum(total_subs)\n",
      "\n",
      "conversion_rate = total_subs/total_users\n",
      "\n",
      "output\n",
      "0.009\n",
      "\n",
      "    pandas date parser on read_csv\n",
      "\n",
      "pandas.read_csv(\n",
      "\n",
      "\tparse_dates=False\n",
      "\tinfer_datetime_format=False\n",
      "\tkeep_date_col=False\n",
      "\tdate_parser=None\n",
      "\tdayFirst=False\n",
      "\t)\n",
      "\n",
      "strftime\n",
      "\"%Y-%m-%d\"\n",
      "\"%H:%M:%S\"\n",
      "\n",
      "\"%B %d, %Y\"\n",
      "\n",
      "to_datetime\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_one = pd.to_datetime(date_data_one, format=\"%A %B %d, %Y\")\n",
      "print(date_data_one)\n",
      "\n",
      "output:\n",
      "DatetimeIndex(['2017-01-27', '2017-12-02'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_two = pd.to_datetime(date_data_two, format=\"%Y-%m-%d\")\n",
      "print(date_data_two)\n",
      "\n",
      "output:\n",
      "'2017-01-01', '2016-05-03']\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_three = pd.to_datetime(date_data_three, format=\"%m/%d/%Y\")\n",
      "print(date_data_three)\n",
      "\n",
      "output:\n",
      "'1978-08-17', '1976-01-07'\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_four = pd.to_datetime(date_data_four, format=\"%Y %B %d %H:%M\")\n",
      "print(date_data_four)\n",
      "\n",
      "output:\n",
      "2016-03-01 01:56:00', '2016-01-04 02:16:00'\n",
      "\n",
      "    Creating time series graphs with matplotlib\n",
      "\n",
      "#find all uses who have not been on the platform in the last two weeks\n",
      "\n",
      "conv_sub_data= sub_data_demo[\n",
      "  sub_data_demo.lapse_date < max_lapse_date\n",
      "]\n",
      "\n",
      "#how many days before the user subscribed\n",
      "\n",
      "sub_time = conv_sub_data.subscription_date -\n",
      "conv_sub_data.lapse_date\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "or\n",
      "conv_sub_data['sub_time']= conv_sub_data.sub_time.dt.days\n",
      "\n",
      "#find the users who have not subscribed in week one but have been on the platform for two or more weeks\n",
      "\n",
      "conv_base=conv_sub_data[\n",
      " (conv_sub_data.sub_time.notnull()) |\n",
      "\t(conv_sub_data.sub_time > 7)]\n",
      "\n",
      "total_users=len(conv_base)\n",
      "\n",
      "total_sub=np.where(conv_sub_data.sub_time.notnull()) &\n",
      "\t(conv_sub_data.sub_time <= 14,1,0)\n",
      "\n",
      "total_subs=sum(total_subs)\n",
      "\n",
      "conversion_rate = total_subs/total_users\n",
      "\n",
      "   new stuff\n",
      "conversion_data = conv_sub_data.groupby(\n",
      "\tby=['lapse_date'], as_index=False\n",
      ").agg('sub_time': [gc7]})\n",
      "\n",
      "#produces the week one conversion rate by conversion date.\n",
      "\n",
      "\n",
      "conversion_data.plot(x='lapse_date',y='sub_time')\n",
      "\n",
      "* compare users of different genders\n",
      "* evaluate the impact of a change across regions\n",
      "* see the impact for different devices\n",
      "\n",
      "reformatted_cntry_data=pd.pivot_table(\n",
      "\tconversion_data,\n",
      "\tvalues=['sub_time'],\n",
      "\tcolumns=['country'],\n",
      "\tindex=['reg_data'],\n",
      "\tfill_value=0\n",
      ")\n",
      "\n",
      "reformat_cntry_data.plot(\n",
      "\tx='reg_date',\n",
      "\ty=['BRA','FRA','DEU','TUR','USA','CAN']\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >sample   graph reg_date by first_week_purchases\n",
      "\n",
      "# Group the data and aggregate first_week_purchases\n",
      "\n",
      "user_purchases columns: 'reg_date', 'first_week_purchases'\n",
      "\n",
      "user_purchases = user_purchases.groupby(by=['reg_date', 'uid']).agg({'first_week_purchases': ['sum']})\n",
      "\n",
      "# Reset the indexes\n",
      "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
      "user_purchases.reset_index(inplace=True)\n",
      "\n",
      "# Find the average number of purchases per day by first-week users\n",
      "user_purchases = user_purchases.groupby(by=['reg_date']).agg({'first_week_purchases': ['mean']})\n",
      "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
      "user_purchases.reset_index(inplace=True)\n",
      "\n",
      "# Plot the results\n",
      "user_purchases.plot(x='reg_date', y='first_week_purchases')\n",
      "plt.show()\n",
      "\t\n",
      "\n",
      "   sample pivot table on the first_week_purchases by country\n",
      "\n",
      "# Pivot the data\n",
      "country_pivot = pd.pivot_table(user_purchases_country, values=['first_week_purchases'], columns=['country'], index=['reg_date'])\n",
      "print(country_pivot.head())\n",
      "\n",
      "\n",
      "# Pivot the data\n",
      "device_pivot = pd.pivot_table(user_purchases_device, values=['first_week_purchases'], columns=['device'], index=['reg_date'])\n",
      "print(device_pivot.head())\n",
      "\n",
      "\n",
      "          first_week_purchases          \n",
      "device                      and       iOS\n",
      "reg_date                                 \n",
      "2017-06-01             0.714286  1.000000\n",
      "2017-06-02             1.400000  1.285714\n",
      "2017-06-03             1.545455  1.000000\n",
      "2017-06-04             1.600000  1.833333\n",
      "2017-06-05             1.625000  2.000000\n",
      "\n",
      "\n",
      "# Plot the average first week purchases for each country by registration date\n",
      "country_pivot.plot(x='reg_date', y=['USA', 'CAN', 'FRA', 'BRA', 'TUR', 'DEU'])\n",
      "plt.show()\n",
      "\n",
      "# Plot the average first week purchases for each device by registration date\n",
      "device_pivot.plot(x='reg_date', y=['and', 'iOS'])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     understanding and visualizing trends in customer data\n",
      "\n",
      "usa_subscriptions['sub_day']=(usa_subscriptions.sub_date - usa_subscriptions.lapse_date).dt.days\n",
      "\n",
      "\n",
      "usa_subscriptions = usa_subscriptions[usa_subscriptions.sub_day <=7]\n",
      "\n",
      "usa_subscriptions = usa_subscriptions.groupby(\n",
      "\tby=['sub_date'],as_index=False\n",
      ").agg({'subs':['sum']})\n",
      "\n",
      "\n",
      "     >looking for seasonal change in buying movement\n",
      "\n",
      "Trailing average smoothing technique that averages over a lagging window\n",
      "1. reveal hidden trends by smoothing out seasonality\n",
      "2. average across the period of seasonality\n",
      "3. 7-day window to smooth weekly seasonality\n",
      "4. average out day level effects to produce the average week effect\n",
      "\n",
      "calculate the rolling average over the usa subscribers data with .rolling()\n",
      "\n",
      "rolling_subs = usa_subscriptions.subs.rolling(\n",
      "\twindow=7,\n",
      "\t#specify to average backwards\n",
      "\tcenter=False\n",
      ")\n",
      "\n",
      "usa_subscriptions['rolling_subs']\n",
      "\t=rolling_subs.mean()\n",
      "usa_subscriptions.tail()\n",
      "\n",
      "high_sku_purchases = pd.read_csv(\n",
      "\t'high_sku_purchases.csv',\n",
      "\tparse_dates=True,\n",
      "\tinfer_datetime_format=True\n",
      ")\n",
      "\n",
      "high_sku_purchases.plot(x='date', y='purchases')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "       exponential moving average\n",
      "\n",
      "1. weighted moving (rolling) average\n",
      "\n",
      "* weights more recent items in the window more\n",
      "* applies weights according to an exponential distribution\n",
      "* average back to a central trend without masking any recent movements\n",
      "\n",
      ".ewm() : exponential weighting function\n",
      "\n",
      "\n",
      "window to apply weights over\n",
      "\n",
      "exp_mean=high_sku_purchases.purchases.ewm(span=30)\n",
      "\n",
      "high_sku_purchases['exp_mean'] = exp_mean.mean()\n",
      "\n",
      "\n",
      "   >  sample  > rolling window 7, 28, 365\n",
      "\n",
      "# Compute 7_day_rev\n",
      "daily_revenue['7_day_rev'] = daily_revenue.revenue.rolling(window=7,center=False).mean()\n",
      "\n",
      "# Compute 28_day_rev\n",
      "daily_revenue['28_day_rev'] = daily_revenue.revenue.rolling(window=28,center=False).mean()\n",
      "    \n",
      "# Compute 365_day_rev\n",
      "daily_revenue['365_day_rev'] = daily_revenue.revenue.rolling(window=365,center=False).mean()\n",
      "    \n",
      "# Plot date, and revenue, along with the 3 rolling functions (in order)    \n",
      "daily_revenue.plot(x='date', y=['revenue', '7_day_rev', '28_day_rev', '365_day_rev', ])\n",
      "plt.show()\n",
      "\n",
      "   > sample ewm\n",
      "\n",
      "# Calculate 'small_scale'\n",
      "daily_revenue['small_scale'] = daily_revenue.revenue.ewm(span=10).mean()\n",
      "\n",
      "# Calculate 'medium_scale'\n",
      "daily_revenue['medium_scale'] = daily_revenue.revenue.ewm(span=100).mean()\n",
      "\n",
      "# Calculate 'large_scale'\n",
      "daily_revenue['large_scale'] = daily_revenue.revenue.ewm(span=500).mean()\n",
      "\n",
      "# Plot 'date' on the x-axis and, our three averages and 'revenue'\n",
      "# on the y-axis\n",
      "daily_revenue.plot(x = 'date', y =['revenue', 'small_scale', 'medium_scale', 'large_scale'])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >Events and releases\n",
      "\n",
      "discover the cause of an issue\n",
      "\n",
      "visualizing the drop in conversion rate (3 years)\n",
      "\n",
      "we notice a dip in new user retention\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "conv_sub_data = sub_data_demo(\n",
      "\tsub_data_demo.lapse_date <= max_lapse_date]\n",
      "\n",
      "sub_time = (conv_sub_data.subscription_date -\n",
      "\tconv_sub_data.lapse_date).dt.days\n",
      "\n",
      "conv_sub_date['sub_time']=sub_time\n",
      "\n",
      "conversion_data = conv_sub_data.groupby(\n",
      "\tby=['lapse_data'], as_index=False)\n",
      ".agg({sub_time':[gc7]})\n",
      "\n",
      "conversion_data.plot()\n",
      "plt.show()\n",
      "\n",
      "    >look at the recent six months\n",
      "\n",
      "current_date = pd.to_date('2018-03-17')\n",
      "\n",
      "start_date=current_date - timedelta(days=(6*28))\n",
      "\n",
      "conv_filter=(\n",
      "\tconversion_data.lapse_date >= start_date)\n",
      "\t& (conversion_data.lapse_date <= current_date)\n",
      ")\n",
      "\n",
      "con_data_filt=conversion_data[conv_filter]\n",
      "\n",
      "conv_data_filt.plot(x='lapse_date', y='sub_time')\n",
      "plt.show()\n",
      "\n",
      "* is this drop impacting all users or just specific cohort\n",
      "\n",
      "* this could provide clues on what the issue may be\n",
      "\n",
      "* ecosystems within our data\n",
      "1. distinct countries\n",
      "2. specific device (android or ios)\n",
      "\n",
      "\n",
      "\n",
      "#pivot the results to have one column per country\n",
      "\n",
      "conv_data_cntry = pd.pivot_table(\n",
      "\tconv_data_cntry, values=['sub_time'],\n",
      "\tcolumns=['country'], index=['lapse_date'], fill_value=0\n",
      ")\n",
      "\n",
      "#pivot the results to have one column per device\n",
      "\n",
      "\n",
      "conv_data_device = pd.pivot_table(\n",
      "\tconv_data_cntry, values=['sub_time'],\n",
      "\tcolumns=['device'], index=['lapse_date'], fill_value=0\n",
      ")\n",
      "\n",
      "* all countries experience the drop\n",
      "\n",
      "* most pronounced in Brazil & Turkey\n",
      "\n",
      "* breaking out by device\n",
      "1 the drop only appears on android devices\n",
      "\n",
      "events: holidays and events impacting user behavior\n",
      "\n",
      "events=pd.read_csv('events.csv')\n",
      "1. Date\n",
      "2. Event\n",
      "\n",
      "releases: ios and android software releases\n",
      "\n",
      "releases = pd.read_csv('releases.csv')\n",
      "\n",
      "     >Plot the conversion rate trend per device\n",
      "\n",
      "conv_data_dev.plot(\n",
      "\tx=['lapse_date'], y=['iOS','and']\n",
      ")\n",
      "\n",
      "events.Date = pd.to_datetime(events.Date)\n",
      "\n",
      "#iterate through events and plot each one\n",
      "\n",
      "for row in events.iterrows():\n",
      "\ttmp=row[1]\n",
      "\tplt.axvline(\n",
      "\tx=tmp.Date, color='k', linestyle='---'\n",
      ")\n",
      "\n",
      "\n",
      "#iterate through the releases and plot each one\n",
      "\n",
      "releases.Date = pd.to_datetime(releases.Date)\n",
      "\n",
      "\n",
      "for row in releases.iterrows():\n",
      "\ttmp=row[1]\n",
      "\tif tmp.Event== 'iOS Release':\n",
      "\n",
      "\t\tplt.axvline(\n",
      "\t\tx=tmp.Date, color='b', linestyle='---'\n",
      ")\n",
      "\telse:\n",
      "\t\tplt.axvline(\n",
      "\t\tx=tmp.Date, color='r', linestyle='---'\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "There was an android release in feb/mar aligns with our dip in conversion rate\n",
      "\n",
      "\n",
      "visualizing data over time to uncover hidden trends\n",
      "\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "user_revenue:\n",
      "1. device\n",
      "2. gender\n",
      "3. country\n",
      "4. date \n",
      "5. revenue\n",
      "6. month\n",
      "\n",
      "\n",
      "# Pivot user_revenue\n",
      "pivoted_data = pd.pivot_table(user_revenue, values ='revenue', columns=['device', 'gender'], index='month')\n",
      "pivoted_data = pivoted_data[1:(len(pivoted_data) -1 )]\n",
      "\n",
      "# Create and show the plot\n",
      "pivoted_data.plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "more female bought ios devices\n",
      "\n",
      "      Introduction to A/B testing\n",
      "\n",
      "discoverying causal relationships\n",
      "\n",
      "test two or more variants against each other\n",
      "\n",
      "to evaluate which one performs best\n",
      "\n",
      "in context of a randomized experiment\n",
      "\n",
      "testing two more ideas against each other\n",
      "\n",
      "control: the current state of your product\n",
      "\n",
      "treatment: the variant that you want to test\n",
      "\n",
      "current paywall: I hope you enjoyed your free-trial please consider subscribing\n",
      "\n",
      "proposed paywall: your free-trial has ended, don't miss out, subscribe today\n",
      "\n",
      "randomly select a subset of users and show one set the control and on e the treatment\n",
      "\n",
      "monitor the conversion rates of each group to see which is better\n",
      "\n",
      "by randomly assigning the user we isolate the impact of the change and reduce the potential impact of confounding variables\n",
      "\n",
      "using an assignment criteria may introduce confounders\n",
      "\n",
      "A/B testing can be used to \n",
      "1. improve sales within a mobile application\n",
      "2. increase user interactions with a website\n",
      "3. identify the impact of a medical treatment\n",
      "4. optimize an assembly lines efficiency\n",
      "\n",
      "good problems for ab testing\n",
      "1. where users are being impacted individually\n",
      "2. testing changes that can directly impact their behavior\n",
      "\n",
      "bad problems for ab testing\n",
      "1. challenging to segment the users into groups\n",
      "2. difficult to untangle the impact of the test\n",
      "\n",
      "\n",
      "     >initial ab test design\n",
      "\n",
      "increasing our apps revenue with a/b testing\n",
      "\n",
      "1. test change to our consumable purchase paywall\n",
      "2. increase revenue by increasing the purchase rate\n",
      "\n",
      "general concepts\n",
      "1. a/b testing techniques transfer across a variety of context\n",
      "2. keep in mind how you would apply these techniques\n",
      "\n",
      "    paywall views & demographics data\n",
      "\n",
      "demographics_data = pd.read_csv('user_demographics.csv')\n",
      "demographics_data.head(n=2)\n",
      "\n",
      "1.uid\n",
      "2.reg_date\n",
      "3.device\n",
      "4.gender\n",
      "5.country\n",
      "6.age\n",
      "\n",
      "\n",
      "paywall_views = pd.read_csv('paywall_views.csv')\n",
      "\n",
      "1.uid\n",
      "2.date\n",
      "3.purchase\n",
      "4.sku\n",
      "5.price\n",
      "\n",
      "\n",
      "   >Response variable\n",
      "1. A response variable is used to measure the impact of your change\n",
      "2. should either be a kpi or directly related to a kpi\n",
      "3. something that is easy to measure\n",
      "\n",
      "factors:\n",
      "1. the paywall color\n",
      "\n",
      "variants:\n",
      "1. particular changes you are testing\n",
      "\n",
      "Experimental unit of our test\n",
      "1. the smallest unit you are measuring the change over\n",
      "2. Individual users make a convenient experimental unit\n",
      "\n",
      "purchase_data = demographics_data.merge(\n",
      "\tpaywall_views, how='left', on=['uid'])\n",
      "\n",
      "#find the total purchases for each user\n",
      "\n",
      "total_purchases = purchase_data.groupby(\n",
      "\tby=['uid'], as_index=False).purchase.sum()\n",
      "\n",
      "#find the mean number of purchase per user\n",
      "total_purchases.purchase.mean()\n",
      "\n",
      "print('total purchases average does not make alot of sense, instead try min and max')\n",
      "\n",
      "\n",
      "#find the min and max number of purchases per users in the time period\n",
      "\n",
      "total_purchases.purchase.min()\n",
      "total_purchases.purchase.max()\n",
      "\n",
      "    user days\n",
      "\n",
      "user interactions on a given day\n",
      "1. more convenient than users by itself\n",
      "2. not required to track users actions across time\n",
      "3. can treat simpler actions as responses to the test\n",
      "\n",
      "\n",
      "total_purchases = purchase_data.groupby(\n",
      "\tby=['uid','date'], as_index=False).purchase.sum()\n",
      "\n",
      "total_purchases.purchase.mean()\n",
      "users in the time period\n",
      "total_purchases.purchase.min()\n",
      "total_purchases.purchase.max()\n",
      "\n",
      "\n",
      "   Randomize by user\n",
      "1. best to randomize by individuals regardless of our experimental unit\n",
      "2. otherwise users can have inconsistent experience\n",
      "\n",
      "important to build intuition about your users and data overall\n",
      "\n",
      "\n",
      "   sample  > calculate the user average purchase per day\n",
      "\n",
      "# Extract the 'day'; value from the timestamp\n",
      "purchase_data.date = purchase_data.date.dt.floor('d')\n",
      "\n",
      "# Replace the NaN price values with 0 \n",
      "purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)\n",
      "\n",
      "# Aggregate the data by 'uid' & 'date'\n",
      "purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)\n",
      "revenue_user_day = purchase_data_agg.sum()\n",
      "\n",
      "# Calculate the final average\n",
      "revenue_user_day = revenue_user_day.price.mean()\n",
      "print(revenue_user_day) \n",
      "\n",
      "\n",
      "output:\n",
      "407.33800579385104\n",
      "\n",
      "\n",
      "    Preparing to run an ab test\n",
      "\n",
      "current paywall: \"I hope you are enjoying the relaxing benefits of our app.  Consider making a purchase\"\n",
      "\n",
      "proposed Paywall: \"don't miss out! try one of our new products!\"\n",
      "\n",
      "Questions:\n",
      "Will updating the paywall text impact our revenue\n",
      "How do our three different consumable prices impact this?\n",
      "\n",
      "Considerations in test design\n",
      "1. can our test be run well in practice\n",
      "2. will we be able to derive meaningful results from it\n",
      "\n",
      "Test sensitivity\n",
      "1. What size of impact is meaningful to detect\n",
      "\n",
      "smaller changes are more difficult to detect and can be hidden by randomness\n",
      "\n",
      "Sensitivity is the minimum level of change we want to be able to detect in our tests\n",
      "\n",
      "     Calculating the revenue per user\n",
      "\n",
      "purchase_data = demographics_data.merge(\n",
      "\tpaywall_views, how='left', on=['uid'])\n",
      "\n",
      "total_revenue = purchase_data.groupby(by=['uid'], as_index=False).price.sum()\n",
      "\n",
      "total_revenue.price = np.where(\n",
      "\tnp.isnan(total_revenue.price),0, total_revenue.price)\n",
      "\n",
      "#calculate the average revenue per user\n",
      "\n",
      "avg_revenue = total_revenue.price.mean()\n",
      "\n",
      "print(avg_revenue)\n",
      "16\n",
      "\n",
      "#find the 1% 10% and 20% change in revenue\n",
      "\n",
      "avg_revenue *1 1.01\n",
      "16.32\n",
      "avg_revenue *1 1.10\n",
      "17.77\n",
      "avg_revenue *1 1.20\n",
      "19.39\n",
      "\n",
      "    Data variability\n",
      "1. important to understand the variability in the data\n",
      "2. does the amount spent vary alot among users\n",
      "a. if it does not then it will be easier to detect a change\n",
      "\n",
      "\n",
      "#calculate the standard deviation of revenue per user\n",
      "\n",
      "revenue_variation = total_revenue.price.std()\n",
      "\n",
      "print(revenue_variation)\n",
      "\n",
      "17.520\n",
      "\n",
      "notice the standard deviation is roughly 100% of what the mean average of 16 is.\n",
      "\n",
      "revenue_variation/avg_revenue\n",
      "1.084\n",
      "\n",
      "\n",
      "#find the average number of purchases per user\n",
      "avg_purchases = total_purchases.purchase.mean()\n",
      "3.15\n",
      "\n",
      "purchase_variation = total_purchases.purchase.std()\n",
      "2.68\n",
      "\n",
      "purchase_variation/avg_purchases\n",
      "0.850\n",
      "\n",
      "Primary goal is the increase revenue\n",
      "1. paywall view to purchase conversion rate\n",
      "a. more granular than overall revenue\n",
      "b. directly related to our test\n",
      "\n",
      "Experimental unit: paywall views\n",
      "1. simplest to work with\n",
      "2. assuming these interactions are independent\n",
      "\n",
      "\n",
      "     finding the baseline conversion rate\n",
      "\n",
      "purchase_data = demographic_data.merge(\n",
      "\tpaywall_views, how='inner', on=['uid']\n",
      ")\n",
      "\n",
      "conversion_rate = (sum(purchase_data.purchase) /\n",
      "\tpurchase_data.purchase.count())\n",
      "\n",
      "print(conversion_rate)\n",
      "\n",
      "0.347\n",
      "\n",
      "      sample get the sum and count\n",
      "\n",
      "# Merge and group the datasets\n",
      "purchase_data = demographics_data.merge(paywall_views,  how='left', on=['uid'])\n",
      "purchase_data.date = purchase_data.date.dt.floor('d')\n",
      "\n",
      "# Group and aggregate our combined dataset \n",
      "daily_purchase_data = purchase_data.groupby(by=['uid'], as_index=False)\n",
      "daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})\n",
      "\n",
      "# Find the mean of each field and then multiply by 1000 to scale the result\n",
      "daily_purchases = daily_purchase_data.purchase['sum'].mean()\n",
      "daily_paywall_views = daily_purchase_data.purchase['count'].mean()\n",
      "daily_purchases = daily_purchases * 1000\n",
      "daily_paywall_views = daily_paywall_views * 1000\n",
      "\n",
      "print(daily_purchases)\n",
      "print(daily_paywall_views)\n",
      "\n",
      "3150.0 (purchases)\n",
      "90814.54545454546 (number of views)\n",
      "\n",
      "\n",
      "        calculating lift dependent upon sensitivity\n",
      "\n",
      "small_sensitivity = 0.1 \n",
      "\n",
      "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
      "small_conversion_rate = conversion_rate * (1 + small_sensitivity) \n",
      "\n",
      "# Apply the new conversion rate to find how many more users per day that translates to\n",
      "small_purchasers = daily_paywall_views * small_conversion_rate\n",
      "\n",
      "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
      "purchaser_lift = small_purchasers - daily_purchases\n",
      "\n",
      "print(small_conversion_rate)\n",
      "print(small_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "\n",
      "0.03814800000000001 (small conversion rate)\n",
      "3499.384706400001 (small purchasers)\n",
      "317.58470640000087 (lift)\n",
      "\n",
      "  > medium sensitivity\n",
      "\n",
      "medium_sensitivity = 0.2\n",
      "\n",
      "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
      "medium_conversion_rate = conversion_rate * (1 + medium_sensitivity) \n",
      "\n",
      "# Apply the new conversion rate to find how many more users per day that translates to\n",
      "medium_purchasers = daily_paywall_views * medium_conversion_rate\n",
      "\n",
      "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
      "purchaser_lift = medium_purchasers - daily_purchases\n",
      "\n",
      "print(medium_conversion_rate)\n",
      "print(medium_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "0.041616 (4% conversion rate)\n",
      "3817.5105888000003 (purchasers)\n",
      "635.7105888000001 (lift)\n",
      "\n",
      "     large sensitivity\n",
      "\n",
      "large_sensitivity = 0.5\n",
      "\n",
      "# Find the conversion rate lift with the sensitivity above\n",
      "large_conversion_rate = conversion_rate * (1 + large_sensitivity)\n",
      "\n",
      "# Find how many more users per day that translates to\n",
      "large_purchasers = daily_paywall_views * large_conversion_rate\n",
      "purchaser_lift = large_purchasers - daily_purchases\n",
      "\n",
      "print(large_conversion_rate)\n",
      "print(large_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "0.052020000000000004\n",
      "4771.888236000001\n",
      "1590.0882360000005\n",
      "\n",
      "Awesome! While it seems that a 50% increase may be too drastic and unreasonable to expect, the small and medium sensitivities both seem very reasonable.\n",
      "\n",
      "\n",
      "       standard error\n",
      "\n",
      "\n",
      "# Find the n & v quantities\n",
      "n = purchase_data.purchase.count()\n",
      "\n",
      "# Calculate the quantity \"v\"\n",
      "v = conversion_rate * (1 - conversion_rate) \n",
      "\n",
      "# Calculate the variance and standard error of the estimate\n",
      "var = v / n \n",
      "se = var**0.5\n",
      "\n",
      "print(var)\n",
      "print(se)\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "3.351780834114284e-07\n",
      "0.0005789456653360731\n",
      "\n",
      "     >calculating sample size\n",
      "\n",
      "what is the null hypothesis\n",
      "\n",
      "1. hypothesis that control and treatment have the same impact on response\n",
      "a. updated paywall does not improve conversion rate\n",
      "b. any observed difference is due to randomness\n",
      "\n",
      "rejecting the null hypothesis\n",
      "a. determine their is a difference between the treatment and control\n",
      "b. we say the test has statistical significances\n",
      "\n",
      "\n",
      "\n",
      "Null hypothesis\n",
      "\n",
      "     \ttrue   \t\tfalse\n",
      "accept\tcorrect\t\ttype II error\n",
      "reject\ttype I error\tcorrect\n",
      "\n",
      "types of error & confidence level\n",
      "1. probablilty of not making type 1 error\n",
      "2. higher this value, larger the test sample needed\n",
      "\n",
      "common values is 0.95\n",
      "\n",
      "     >Statistical power\n",
      "\n",
      "statistical power is the probability of finding a statistically siginificant result when the null hypothesis is false\n",
      "\n",
      "confidence level\n",
      "standard error\n",
      "statistical power\n",
      "test sensitivity\n",
      "\n",
      "\n",
      "as the sample size increases so does our power increase\n",
      "\n",
      "\n",
      "    calculating our needed sample size\n",
      "\n",
      "baseline conversion rate 0.3468\n",
      "confidence level: 0.95\n",
      "desired power: 0.80\n",
      "sensitivity=0.1\n",
      "\n",
      "sample_size_group=get_sample(size(0.8, conversion_rate *1.1, 0.95)\n",
      "\n",
      "print(sample_size_per_group)\n",
      "\n",
      "output:\n",
      "45788\n",
      "\n",
      "\n",
      "      >generality of this function\n",
      "\n",
      "function shown specific to conversion rate calculations\n",
      "\n",
      "different response variables have different buy analogous formulas\n",
      "\n",
      "\n",
      "  > decreasing the need sample size\n",
      "\n",
      "* choose a unit of observation with lower variability\n",
      "\n",
      "* excluding users irrelevant to the process/change\n",
      "\n",
      "* think through how different factors relate to the sample size\n",
      "\n",
      "\n",
      "\n",
      "       increase the confidence level\n",
      "\n",
      "# Look at the impact of sample size increase on power\n",
      "n_param_one = get_power(n=1000, p1=p1, p2=p2, cl=cl)\n",
      "n_param_two = get_power(n=2000, p1=p1, p2=p2, cl=cl)\n",
      "\n",
      "# Look at the impact of confidence level increase on power\n",
      "alpha_param_one = get_power(n=n1, p1=p1, p2=p2, cl=0.8)\n",
      "alpha_param_two = get_power(n=n1, p1=p1, p2=p2, cl=0.95)\n",
      "    \n",
      "# Compare the ratios\n",
      "print(n_param_two / n_param_one)\n",
      "print(alpha_param_one / alpha_param_two)\n",
      "\n",
      "\n",
      "1.7596440001351992  (change sample size)\n",
      "1.8857367092232278  (change confidence levels)\n",
      "\n",
      "\n",
      "\n",
      "With these particular values it looks like decreasing our confidence level has a slightly larger impact on the power than increasing our sample size\n",
      "\n",
      "      calculate the conversion rate\n",
      "\n",
      "# Merge the demographics and purchase data to only include paywall views\n",
      "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
      "                            \n",
      "# Find the conversion rate\n",
      "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
      "            \n",
      "print(conversion_rate)\n",
      "\n",
      "0.03468607351645712\n",
      "\n",
      "    > calculate sample size\n",
      "\n",
      "# Merge the demographics and purchase data to only include paywall views\n",
      "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
      "                            \n",
      "# Find the conversion rate\n",
      "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
      "\n",
      "# Desired Power: 0.8\n",
      "# CL: 0.90\n",
      "# Percent Lift: 0.1\n",
      "p2 = conversion_rate * (1 + 0.1)\n",
      "sample_size = get_sample_size(0.8, conversion_rate, p2, 0.90)\n",
      "print(sample_size)\n",
      "\n",
      "36101\n",
      "\n",
      "\n",
      "# Desired Power: 0.95\n",
      "# CL 0.90\n",
      "# Percent Lift: 0.1\n",
      "p2 = conversion_rate * (1 + 0.1)\n",
      "sample_size = get_sample_size(0.95, conversion_rate, p2, 0.90)\n",
      "print(sample_size)\n",
      "\n",
      "63201\n",
      "\n",
      "\n",
      "      analyzing the ab test results\n",
      "\n",
      "compare the two groups purchase rates\n",
      "\n",
      "test_demographics = pd.read_csv('test_demographics.csv')\n",
      "\n",
      "#results for our ab test\n",
      "#group column c for control | v for variant\n",
      "\n",
      "test_results=pd.read_csv('ab_test_results.csv')\n",
      "test_results.head()\n",
      "\n",
      "uid\n",
      "date\n",
      "purchase\n",
      "sku\n",
      "price\n",
      "group\n",
      "\n",
      "\n",
      "    confirming our test results\n",
      "\n",
      "does the data look reasonable\n",
      "\n",
      "\n",
      "test_results_grpd = test_results.groupby(\n",
      "\tby=['group'], as_index=False)\n",
      "\n",
      "test_results_grpd.uid.count()\n",
      "\n",
      "48236\n",
      "49867\n",
      "\n",
      "\n",
      "test_results_demo = test_results.merge(\n",
      "\ttest_demo, how='inner', on='uid')\n",
      "\n",
      "test_results_grpd = test_results_demo.groupby(\n",
      "\tby=['country','gender','device','group'],\n",
      "as_index=False)\n",
      "\n",
      "test_results_grd.uid.count()\n",
      "\n",
      "\n",
      "    > find the mean conversion\n",
      "\n",
      "test_results_summary= test_results_demo.groupby(\n",
      "\tby=['group'], as_index=False\n",
      ").agg({'purchase':['count','sum']})\n",
      "\n",
      "test_results_summary['conv'] = (test_results_summary.purchase['sum']/\n",
      "\ttest_results_summary.purchase['count'])\n",
      "\n",
      "test_results_summary\n",
      "\n",
      "grp  sum   count   conversion\n",
      "c    48236 1657    0.034351\n",
      "v    49867 2094    0.041984\n",
      "\n",
      "Is the result statistically significant\n",
      "1. are the conversion rates different enough\n",
      "2. if yes then reject the null hypothesis\n",
      "3. conclude that the paywalls have different effects\n",
      "4. if no then it may just be randomness\n",
      "\n",
      "    p -value\n",
      "\n",
      "probability if the null hypothesis is true\n",
      "\n",
      "of observing a value as or more extreme\n",
      " \n",
      "what does a low p-value mean\n",
      "1. the power is low\n",
      "2. the observation is unlikely to happen due to randomness\n",
      "\n",
      "\n",
      "<0.01 very strong evidence against the null hypothesis\n",
      "\n",
      "0.01-0.5 strong evidence against the null hypothesis\n",
      "0.05-1. very weak evidence against the null hypothesis\n",
      ">0.1 small or no evidence against the null hypothesis\n",
      "\n",
      "\n",
      "?     sample test the null hypothesis\n",
      "\n",
      "# Compute and print the results\n",
      "results = ab_test_results.groupby('group').agg({'uid':pd.Series.nunique}) \n",
      "print(results)\n",
      "\n",
      "\n",
      "   uid\n",
      "group        \n",
      "C      2825.0\n",
      "V      2834.0\n",
      "\n",
      "\n",
      "# Find the unique users in each group \n",
      "results = ab_test_results.groupby('group').agg({'uid': pd.Series.nunique}) \n",
      "\n",
      "# Find the overall number of unique users using \"len\" and \"unique\"\n",
      "unique_users = len(ab_test_results.uid.unique()) \n",
      "\n",
      "# Find the percentage in each group\n",
      "results = results / unique_users * 100\n",
      "print(results)\n",
      "\n",
      "     uid\n",
      "group           \n",
      "C      49.920481\n",
      "V      50.079519\n",
      "\n",
      "\n",
      "   find the number of users in group device and gender\n",
      "\n",
      "# Find the unique users in each group, by device and gender\n",
      "results = ab_test_results.groupby(by=['group', 'device', 'gender']).agg({'uid': pd.Series.nunique}) \n",
      "\n",
      "# Find the overall number of unique users using \"len\" and \"unique\"\n",
      "unique_users = len(ab_test_results.uid.unique())\n",
      "\n",
      "# Find the percentage in each group\n",
      "results = results / unique_users * 100\n",
      "print(results)\n",
      "\n",
      "uid\n",
      "group device gender           \n",
      "C     and    F       14.896625\n",
      "             M       13.518289\n",
      "      iOS    F       11.309419\n",
      "             M       10.196148\n",
      "V     and    F       14.861283\n",
      "             M       13.659657\n",
      "      iOS    F       10.920657\n",
      "             M       10.637922\n",
      "\n",
      "\n",
      "     understanding statistical significance\n",
      "\n",
      "distribution of expected difference between control and test groups _if_ the null hypothesis is true\n",
      "\n",
      "The red line is the observed difference in the conversion rates from our tests\n",
      "\n",
      "p-value: probability of being as or more extreme than the red line on either side of the distribution.\n",
      "\n",
      "\n",
      "def get_pvalue ( con_conv, test_conv, con_size, test_size):\n",
      "\n",
      "\tlift= - abs(test_conv - con_conv)\n",
      "\tscale_one = con_conv * (1-con_conv) * (1/con_size)\n",
      "\tscale_two= test_conv * (1-test_conv) * (1/test_size)\n",
      "\tscale_val = (scale_one + scale_two) **0.5\n",
      "\tp_value=2*stats.norm.cdf(lift, loc=0, scale=scale_val)\n",
      "\treturn p_value\n",
      "\n",
      "\n",
      "con_conv=0.034351\n",
      "test_conv=0.041984\n",
      "con_size=48236\n",
      "test_size=49867\n",
      "\n",
      "p_value=get_pvalue(con_conv,test_conv,con_size,test_size)\n",
      "print(p_value)\n",
      "\n",
      "4.2572974 e-10  (extremely small p-value)\n",
      "\n",
      "accept the null hypothesis\n",
      "\n",
      "\n",
      "    find the power of the test\n",
      "\n",
      "def get_power(n, p1, p2, cl):\n",
      "    alpha = 1 - cl\n",
      "    qu = stats.norm.ppf(1 - alpha/2)\n",
      "    diff = abs(p2-p1)\n",
      "    bp = (p1+p2) / 2\n",
      "    \n",
      "    v1 = p1 * (1-p1)\n",
      "    v2 = p2 * (1-p2)\n",
      "    bv = bp * (1-bp)\n",
      "    \n",
      "    power_part_one = stats.norm.cdf((n**0.5 * diff - qu * (2 * bv)**0.5) / (v1+v2) ** 0.5)\n",
      "    power_part_two = 1 - stats.norm.cdf((n**0.5 * diff + qu * (2 * bv)**0.5) / (v1+v2) ** 0.5)\n",
      "    \n",
      "    power = power_part_one + power_part_two\n",
      "    \n",
      "    return (power)\n",
      "\n",
      "\n",
      "power= get_power (test_size, con_conv, test_conv, 0.95)\n",
      "print(power)\n",
      "0.9999925941372282\n",
      "\n",
      "\n",
      "small p-value and nearly perfect power\n",
      "\n",
      "        confidence interval\n",
      "\n",
      "ranges of values for our estimation rather than a single number\n",
      "\n",
      "provides context for our estimation process\n",
      "\n",
      "series of repeated experiments\n",
      "1. the calculated intervals will contain the true parameter x% of the time\n",
      "2. the true conversion rate is fixed quantity, it is the interval that is random not the conversion rate.\n",
      "\n",
      "\n",
      "The estimated parameter or difference in conversion rate follows a normal distribution\n",
      "\n",
      "1. we can estimate the standard deviation\n",
      "2. the mean of this distribution\n",
      "\n",
      "alpha is the desired confidence interval width\n",
      "\n",
      "bounds containing X% of hte probabilty around the mean (95%) of that distribution\n",
      "\n",
      "\n",
      "from scipy import stats\n",
      "\n",
      "def get_ci(test_conv, con_conv, test_size, con_size, ci):\n",
      "\n",
      "\tsd=((test_conv * (1-test_conv))/test_size+\n",
      "\t(con_conv * (1-con_conv)) / con_size)**0.5\n",
      "\n",
      "\tlift=test_conv - con_conv\n",
      "\n",
      "\tval=stats.norm.isf((1-ci)/2)\n",
      "\tlwr_bnd=lift - val *sd\n",
      "\tupr_bnd=lift+ val*sd\n",
      "\treturn ((lwr_bnd,upr_bnd))\n",
      "\n",
      "\n",
      "    get p-value\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=0.1, test_conv=0.17, con_size=1000, test_size=1000)\n",
      "print(p_value)\n",
      "\n",
      "4.131297741047306e-06\n",
      "\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=.1, test_conv=.15, con_size=100, test_size=100)\n",
      "print(p_value) \n",
      "\n",
      "0.28366948940702086\n",
      "\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=.48, test_conv=.5, con_size=1000, test_size=1000)\n",
      "print(p_value)\n",
      "\n",
      "0.370901935824383\n",
      "\n",
      "\n",
      "To recap we observed that a large lift makes us confident in our observed result, while a small sample size makes us less so, and ultimately high variance can lead to a high p-value!\n",
      "\n",
      "\n",
      "    check for statistically signficant\n",
      "\n",
      "\n",
      "cont_conv=0.09096495570387314 \n",
      "test_conv=0.1020053238686779 \n",
      "con_size=5329 \n",
      "test_size=5748\n",
      "\n",
      "# Compute the p-value\n",
      "p_value = get_pvalue(con_conv=cont_conv, test_conv=test_conv, con_size=cont_size, test_size=test_size)\n",
      "print(p_value)\n",
      "\n",
      "# Check for statistical significance\n",
      "if p_value >= 0.05:\n",
      "    print(\"Not Significant\")\n",
      "else:\n",
      "    print(\"Significant Result\")\n",
      "\n",
      "\n",
      "  > confidence interval\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, 0.975, 0.5)\n",
      "print(confidence_interval)\n",
      "\n",
      "(0.9755040421682947, 1.0244959578317054)\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, .95, 2)\n",
      "print(confidence_interval)\n",
      "\n",
      "2 standard deviations\n",
      "\n",
      "(0.6690506448818785, 1.3309493551181215)\n",
      "\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, 0.95, .001)\n",
      "print(confidence_interval)\n",
      "\n",
      "(1.0, 1.0)\n",
      "\n",
      "\n",
      "As our standard deviation decreases so too does the width of our confidence interval. Great work!\n",
      "\n",
      "\n",
      "con_conv=0.034351\n",
      "test_conv=0.041984\n",
      "con_size=48236\n",
      "test_size=49867\n",
      "ci=.95\n",
      "\n",
      "# Calculate the mean of our lift distribution \n",
      "lift_mean = test_conv -cont_conv\n",
      "\n",
      "# Calculate variance and standard deviation \n",
      "lift_variance = (1 - test_conv) * test_conv /test_size + (1 - cont_conv) * cont_conv / cont_size\n",
      "lift_sd = lift_variance**0.5\n",
      "\n",
      "# Find the confidence intervals with cl = 0.95\n",
      "confidence_interval = get_ci(lift_mean, 0.95,lift_sd)\n",
      "print(confidence_interval)\n",
      "\n",
      "confidence interval:\n",
      "(0.011039999822042502, 0.011040000177957487)\n",
      "\n",
      "Notice that our interval is very narrow thanks to our substantial lift and large sample size.\n",
      "\n",
      "      interpreting your results\n",
      "\n",
      "report \n",
      "\t\tTest Group\tControl Group\n",
      "1. Sample size  \t7030\t6970\n",
      "2. run time\t2 weeks\t\t2weeks\n",
      "3. mean\t\t3.12\t\t2.69\n",
      "4. variance\t3.20\t\t2.64\n",
      "5. est lift\t0.56\n",
      "6. conf level\t0.56 += 0.4\n",
      "\n",
      "* significant at the 0.05 level\n",
      "\n",
      "visualization\n",
      "\n",
      "histograms: bucketed counts of observations across values\n",
      "\n",
      "user data rolled up to group and user level\n",
      "uid\n",
      "group\n",
      "purchase\n",
      "\n",
      "var=results[results.group=='V']\n",
      "con=results[results.group=='C']\n",
      "\n",
      "plt.hist(var['purchase'],color='yellow',\n",
      "\talpha=0.8, bins=50, label='Test')\n",
      "plt.hist(con['purchase'], color='blue',\n",
      "\talpha=0.8, bins=50, label='Control')\n",
      "plt.legend(loc='upper right')\n",
      "\n",
      "\n",
      "plt.axvline(x= np.mean(results.purchase),\n",
      "\tcolor='red')\n",
      "plt.axvline(x=np.mean(results.purchase),\n",
      "\tcolor='green')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >plotting a distribution\n",
      "\n",
      "mean_con=0.090965\n",
      "mean_test=0.102005\n",
      "var_con=(mean_con * (1-mean_con))/58583\n",
      "var_test=(mean-test *(1-mean_test))/56350\n",
      "\n",
      "con_line = np.linspace(-3*var_con**0.5+mean_con,\n",
      "\t3*vr_con**0.5+mean_con, 100)\n",
      "\n",
      "test_linenp.linspace(-3*var_test**0.5+mean_test,\n",
      "\t3*vr_test**0.5+mean_test, 100)\n",
      "\n",
      "\n",
      "\n",
      "#plot the probabilities across the distribution of conversion rates\n",
      "\n",
      "plt.plot(con_line, mlab.normpdf(\n",
      "\tcon_line, mean_con, var_con**0.5)\n",
      ")\n",
      "\n",
      "plt.plot(test_line, mlab.normpdf(\n",
      "\ttest_line, mean_test, var_test**0.5)\n",
      ")\n",
      "plt.show()\n",
      "\n",
      "mlab.normpdf(): converts values to probablities from Normal Distribution\n",
      "\n",
      "   plotting the difference of conversion rates\n",
      "\n",
      "lift= mean_test - mean_control\n",
      "var = var_test + var_control\n",
      "\n",
      "variance is the sum of variances\n",
      "\n",
      "diff_line = np.linspace(-3*var**0.5 + lift,\n",
      "3*var**0.5 + lift, 100)\n",
      "\n",
      "plt.plot(diff_line, mlab.normpdf(\n",
      "\tdiff_line, lift, var**0.5)\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\object oriented programming constructors.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\object oriented programming constructors.txt\n",
      "class Employee:\n",
      "\tdef __init__(self, name, salary):\n",
      "\t\tself.name=name\n",
      "\t\tself.salary=salary\n",
      "\n",
      "emp1 = Employee(\"Teo Mille\",50000)\n",
      "emp2 = Employee(\"Marta Popov\", 65000)\n",
      "\n",
      "how to share data among all the instances of the class?\n",
      "\n",
      "class attribute\n",
      "\n",
      "class MyClass\n",
      "\tCLASS_ATTR_NAME = attr_value\n",
      "\n",
      "serves as a global variable in the class\n",
      "\n",
      "class Employee:\n",
      "\tMIN_SALARY=30000\n",
      "\n",
      "\tdef __init__(self, name,salary):\n",
      "\t\tself.name=name\n",
      "\n",
      "\n",
      "\t\tif salary>=Employee.MIN_SALARY:\n",
      "\t\t\tself.salary=salary\n",
      "\t\telse\n",
      "\t\t\tself.salary=Employee.MIN_SALARY\n",
      "\n",
      "\n",
      "\tdef my_awesome_method(cls, args)\n",
      "\t\tprint(cls)\n",
      "\n",
      "class methods can be called statically\n",
      "\n",
      "Employee.my_awesome_method(cls, args)\n",
      "\n",
      "why?\n",
      "alternative constructors\n",
      "\n",
      "an instantiated class can only have one constructor\n",
      "\n",
      "\n",
      "\tdef from_file(cls, filename):\n",
      "\t\twith open(filename, \"r\") as f:\n",
      "\t\t\tname=f.readline()\n",
      "\n",
      "\t\treturn cls(name)\n",
      "\n",
      "\n",
      "emp = Employee.from_file(\"employee_data.txt\")\n",
      "\n",
      "type(emp)\n",
      "\n",
      "creates the employee instance without calling the constructor\n",
      "\n",
      "\n",
      " > sample  > print the shared variable MAX_POSITION from the player class\n",
      "\n",
      "# Create a Player class\n",
      "class Player:\n",
      "        MAX_POSITION=30\n",
      "        def __init__(self):\n",
      "            self.position=0\n",
      "\n",
      "\n",
      "# Print Player.MAX_POSITION       \n",
      "print(Player.MAX_POSITION)\n",
      "\n",
      "# Create a player p and print its MAX_POSITITON\n",
      "p=Player\n",
      "print(p.MAX_POSITION)\n",
      "\n",
      "   sample appling an where clause using the shared variable\n",
      "\n",
      "class Player:\n",
      "    MAX_POSITION = 10\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.position = 0\n",
      "\n",
      "    # Add a move() method with steps parameter\n",
      "    def move(self,steps):\n",
      "            if steps<Player.MAX_POSITION:\n",
      "                self.position+=steps\n",
      "            else:\n",
      "                self.position=Player.MAX_POSITION\n",
      "    \n",
      "\n",
      "       \n",
      "    # This method provides a rudimentary visualization in the console    \n",
      "    def draw(self):\n",
      "        drawing = \"-\" * self.position + \"|\" +\"-\"*(Player.MAX_POSITION - self.position)\n",
      "        print(drawing)\n",
      "\n",
      "p = Player(); p.draw()\n",
      "p.move(4); p.draw()\n",
      "p.move(5); p.draw()\n",
      "p.move(3); p.draw()\n",
      "\n",
      "\n",
      "   > sample  assigning to a shared variable by instance and by class\n",
      "\n",
      "# Create Players p1 and p2\n",
      "p1=Player()\n",
      "p2=Player()\n",
      "\n",
      "print(\"MAX_SPEED of p1 and p2 before assignment:\")\n",
      "# Print p1.MAX_SPEED and p2.MAX_SPEED\n",
      "print(p1.MAX_SPEED)\n",
      "print(p2.MAX_SPEED)\n",
      "\n",
      "# Assign 7 to p1.MAX_SPEED\n",
      "p1.MAX_SPEED=7\n",
      "\n",
      "print(\"MAX_SPEED of p1 and p2 after assignment:\")\n",
      "# Print p1.MAX_SPEED and p2.MAX_SPEED\n",
      "print(p1.MAX_SPEED)\n",
      "print(p2.MAX_SPEED)\n",
      "\n",
      "print(\"MAX_SPEED of Player:\")\n",
      "# Print Player.MAX_SPEED\n",
      "print(Player.MAX_SPEED)\n",
      "\n",
      "output:\n",
      "\n",
      "MAX_SPEED of p1 and p2 before assignment:\n",
      "3\n",
      "3\n",
      "MAX_SPEED of p1 and p2 after assignment:\n",
      "7\n",
      "3\n",
      "MAX_SPEED of Player:\n",
      "3\n",
      "\n",
      "\n",
      "You shouldn't be able to change the data in all the instances of the class through a single instance.\n",
      "\n",
      "   sample  > using a decorator with a method\n",
      "\n",
      "class BetterDate:    \n",
      "    # Constructor\n",
      "    def __init__(self, year, month, day):\n",
      "      # Recall that Python allows multiple variable assignments in one line\n",
      "      self.year, self.month, self.day = year, month, day\n",
      "    \n",
      "    # Define a class method from_str\n",
      "    @classmethod\n",
      "    def from_str(cls, datestr):\n",
      "        # Split the string at \"-\" and convert each part to integer\n",
      "        parts = datestr.split(\"-\")\n",
      "        year, month, day = int(parts[0]), int(parts[1]), int(parts[2])\n",
      "        # Return the class instance\n",
      "        return BetterDate(year,month,day)\n",
      "        \n",
      "bd = BetterDate.from_str('2020-04-30')   \n",
      "print(bd.year)\n",
      "print(bd.month)\n",
      "print(bd.day)\n",
      "\n",
      "    sample return a class using a from_datetime method call of a class\n",
      "\n",
      "# import datetime from datetime\n",
      "from datetime import datetime\n",
      "\n",
      "class BetterDate:\n",
      "    def __init__(self, year, month, day):\n",
      "      self.year, self.month, self.day = year, month, day\n",
      "      \n",
      "    @classmethod\n",
      "    def from_str(cls, datestr):\n",
      "        year, month, day = map(int, datestr.split(\"-\"))\n",
      "        return cls(year, month, day)\n",
      "      \n",
      "    # Define a class method from_datetime accepting a datetime object\n",
      "    def from_datetime(datetime):\n",
      "      return BetterDate(datetime.year,datetime.month, datetime.day)\n",
      "\n",
      "# You should be able to run the code below with no errors: \n",
      "today = datetime.today()     \n",
      "bd = BetterDate.from_datetime(today)   \n",
      "print(bd.year)\n",
      "print(bd.month)\n",
      "print(bd.day)\n",
      "\n",
      "\n",
      "        >Class inheritance\n",
      "\n",
      "some one may have solved the problem\n",
      "\n",
      "1. modules are great for fixed functionality\n",
      "\n",
      "BankAccount\n",
      "a. balance\n",
      "b. interest\n",
      "\n",
      "SavingAccount\n",
      "1. withdraw()\n",
      "2. compute_interest()\n",
      "\n",
      "CheckingAccount\n",
      "a. balance\n",
      "b. limit\n",
      "1. withdraw()\n",
      "2. deposit()\n",
      "\n",
      "class BankAccount:\n",
      "\tdef __init__(self, balance):\n",
      "\t\tself.balance=balance\n",
      "\tdef withdraw(self,amount):\n",
      "\t\tself.balance-=amount\n",
      "\n",
      "class SavingsAccount(BankAccount):\n",
      "\tpass\n",
      "\n",
      "BankAccount functionality is being extended or inherited\n",
      "\n",
      "SavingsAccount will inherit the functionality and add more of its own functionality\n",
      "\n",
      "savings_account = SavingsAccount(1000)\n",
      "type(savings_acct)\n",
      "\n",
      "savings_acct.balance\n",
      "\n",
      "savings_acct.withdraw(300)\n",
      "\n",
      "savingAccount is a BankAccount\n",
      "\n",
      "isinstance(savings_acct, SavingsAccount)\n",
      "\n",
      "output: True\n",
      "\n",
      "bank_acct=BankAccount(300)\n",
      "\n",
      "isinstance(bank_acct,SavingsAccount)\n",
      "\n",
      "output: False\n",
      "\n",
      "\n",
      " > sample  > inheritance\n",
      "\n",
      "class Employee:\n",
      "  MIN_SALARY = 30000    \n",
      "\n",
      "  def __init__(self, name, salary=MIN_SALARY):\n",
      "      self.name = name\n",
      "      if salary >= Employee.MIN_SALARY:\n",
      "        self.salary = salary\n",
      "      else:\n",
      "        self.salary = Employee.MIN_SALARY\n",
      "        \n",
      "  def give_raise(self, amount):\n",
      "      self.salary += amount      \n",
      "        \n",
      "# Define a new class Manager inheriting from Employee\n",
      "class Manager(Employee):\n",
      "  pass\n",
      "\n",
      "  def display(self):\n",
      "      return(\"Manager \"+self.name)\n",
      "\n",
      "# Define a Manager object\n",
      "mng = Manager(\"Debbie Lashko\",86500)\n",
      "\n",
      "# Print mng's name\n",
      "print(mng.name)\n",
      "\n",
      "# Call mng.display()\n",
      "mng.display()\n",
      "\n",
      "output:\n",
      "\n",
      "Debbie Lashko\n",
      "'Manager Debbie Lashko'\n",
      "\n",
      "      >Customizing functionality via inheritance\n",
      "\n",
      "class SavingsAccount(BankAccount):\n",
      "\n",
      "\tdef __init__(self, balance, interest_rate):\n",
      "\n",
      "\t\tBankAccount.__init(self, balance)\n",
      "\t\tself.interest_rate= interest_rate\n",
      "\n",
      "\n",
      "\tdef compute_interest(self, n_periods=1):\n",
      "\t\treturn self.balance *((1+self.interest_rate)**n_periods-1)\n",
      "\n",
      "\n",
      "acct=SavingsAccount(1000,0.03)\n",
      "\n",
      "acct.interest_rate\n",
      "\n",
      "output: 0.03\n",
      "\n",
      "Adding Functionality:\n",
      "1. add methods as usual\n",
      "2. use data from the parent and the child class\n",
      "\n",
      "CheckingAccount\n",
      "1. balance\n",
      "2. limit\n",
      "\n",
      "withdraw()\n",
      "deposit()\n",
      "\n",
      "\n",
      "\n",
      "class CheckingAccount(BankAccount):\n",
      "\tdef __init__(self, balance, limit):\n",
      "\t\tBankAccount.__init__(self, content)\n",
      "\t\tself.limit=limit\n",
      "\n",
      "\tdef deposit(self, amount):\n",
      "\t\tself.balance+=amount\n",
      "\n",
      "\tdef withdraw(self,amount, fee=0):\n",
      "\t\tif fee<=self.limit:\n",
      "\t\t\tBankAccount.withdraw(self, amount-fee)\n",
      "\t\telse:\n",
      "\t\t\tBankAccount.withdraw(self, amount-self.limit)\n",
      "\n",
      "  > sample Employee and Manager\n",
      "\n",
      "class Employee:\n",
      "    def __init__(self, name, salary=30000):\n",
      "        self.name = name\n",
      "        self.salary = salary\n",
      "\n",
      "    def give_raise(self, amount):\n",
      "        self.salary += amount\n",
      "\n",
      "        \n",
      "class Manager(Employee):\n",
      "    def display(self):\n",
      "\tprint(\"Manager \", self.name)\n",
      "\n",
      "  # Add a constructor\n",
      "    def __init__(self, name, salarysalary=50000, project=None):\n",
      "\n",
      "        # Call the parent's constructor   \n",
      "        Employee.__init__(self, name, salary)\n",
      "\n",
      "        # Assign project attribute\n",
      "        self.project=project\n",
      "\n",
      "  \t# Add a give_raise method\n",
      "\tdef give_raise(self, amount,bonus=1.05):\n",
      "        new_amount=amount * bonus\n",
      "        Employee.give_raise(self,new_amount)\n",
      "    \n",
      "    \n",
      "mngr=Manager(\"Ashta Dunbar\", 78500)\n",
      "mngr.give_raise(1000)\n",
      "print(mngr.salary)\n",
      "mngr.give_raise(2000, bonus=1.03)\n",
      "print(mngr.salary)\n",
      "\n",
      "\n",
      "output:\n",
      "78500\n",
      "79550.0\n",
      "81610.0\n",
      "\n",
      "     > sample    racer\n",
      "\n",
      "# Create a Racer class and set MAX_SPEED to 5\n",
      "class Racer(Player):\n",
      "    MAX_SPEED=5\n",
      "# Create a Player and a Racer objects\n",
      "p = Player()\n",
      "r = Racer()\n",
      "\n",
      "print(\"p.MAX_SPEED = \", p.MAX_SPEED)\n",
      "print(\"r.MAX_SPEED = \", r.MAX_SPEED)\n",
      "\n",
      "print(\"p.MAX_POSITION = \", p.MAX_POSITION)\n",
      "print(\"r.MAX_POSITION = \", r.MAX_POSITION)\n",
      "\n",
      "p.MAX_SPEED =  3\n",
      "r.MAX_SPEED =  5\n",
      "p.MAX_POSITION =  10\n",
      "r.MAX_POSITION =  10\n",
      "\n",
      "\n",
      "    sample inheriting from pd.DataFrame and adding the attribute created_at\n",
      "\n",
      "# Import pandas as pd\n",
      "import pandas as pd\n",
      "\n",
      "# Define LoggedDF inherited from pd.DataFrame and add the constructor\n",
      "class LoggedDF(pd.DataFrame):\n",
      "        def __init__(self,*args,**kwargs):\n",
      "            pd.DataFrame.__init__(self,*args,**kwargs)\n",
      "            self.created_at = datetime.today()\n",
      "\n",
      "def to_csv(self, *args, **kwargs):\n",
      "    # Copy self to a temporary DataFrame\n",
      "    temp = self.copy()\n",
      "    \n",
      "    # Create a new column filled with self.created at\n",
      "    temp[\"created_at\"] = self.created_at\n",
      "    \n",
      "    # Call pd.DataFrame.to_csv on temp with *args and **kwargs\n",
      "        pd.DataFrame.to_csv(temp, *args,**kwargs)\n",
      "\n",
      "\n",
      "Using *args and **kwargs allows you to not worry about keeping the signature of your customized method compatible\n",
      "\n",
      "\n",
      "      >overloading comparison\n",
      "\n",
      "class Customer:\n",
      "\tdef __init__(self, id, name):\n",
      "\t\tself.id,self.name=id,name\n",
      "\n",
      "\tdef __eq__(self, other):\n",
      "\t\treturn(self.id==other.id and\n",
      "\t\t\tself.name==other.name)\n",
      "\n",
      "eq == should return a boolean value\n",
      "\n",
      "comparison operations\n",
      "\n",
      "== __eq__()\n",
      "!= __ne__()\n",
      ">= __ge__()\n",
      "<= __le__()\n",
      ">  __gt__()\n",
      "<  __lt__()\n",
      "\n",
      "objects as dictionaries __hash__()\n",
      "\n",
      "  > sample   > override __eq__\n",
      "\n",
      "class BankAccount:\n",
      "   # MODIFY to initialize a number attribute\n",
      "    def __init__(self, number, balance=0):\n",
      "        self.number=number\n",
      "        self.balance = balance\n",
      "      \n",
      "    def withdraw(self, amount):\n",
      "        self.balance -= amount \n",
      "    \n",
      "    # Define __eq__ that returns True if the number attributes are equal \n",
      "    def __eq__(self, other):\n",
      "        return self.number == other.number\n",
      "\n",
      "# Create accounts and compare them       \n",
      "acct1 = BankAccount(123, 1000)\n",
      "acct2 = BankAccount(123, 1000)\n",
      "acct3 = BankAccount(456, 1000)\n",
      "print(acct1 == acct2)\n",
      "print(acct1 == acct3)\n",
      "\n",
      "\n",
      "   sample  > override __eq__ and check type of the object\n",
      "\n",
      "class BankAccount:\n",
      "    def __init__(self, number, balance=0):\n",
      "        self.number, self.balance = number, balance\n",
      "      \n",
      "    def withdraw(self, amount):\n",
      "        self.balance -= amount \n",
      "\n",
      "    # MODIFY to add a check for the type()\n",
      "    def __eq__(self, other):\n",
      "        return ((self.number == other.number) \\\n",
      "            & (type(self) == type(other))\n",
      "        )\n",
      "\n",
      "acct = BankAccount(873555333)\n",
      "pn = Phone(873555333)\n",
      "print(acct == pn)\n",
      "\n",
      "\n",
      "Python always calls the child's __eq__() method when comparing a child object to a parent\n",
      " \n",
      "\n",
      "# Import pandas as pd\n",
      "import pandas as pd\n",
      "\n",
      "# Define LoggedDF inherited from pd.DataFrame and add the constructor\n",
      "class LoggedDF(pd.DataFrame):\n",
      "  \n",
      "  def __init__(self, *args, **kwargs):\n",
      "    pd.DataFrame.__init__(self, *args, **kwargs)\n",
      "    self.created_at = datetime.today()\n",
      "    \n",
      "    \n",
      "ldf = LoggedDF({\"col1\": [1,2], \"col2\": [3,4]})\n",
      "print(ldf.values)\n",
      "print(ldf.created_at)\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<< overloading\n",
      "\n",
      "class Customer:\n",
      "\tdef __init__(self, name, balance):\n",
      "\t\tself.name,self.balance=name,balance\n",
      "\n",
      "\tdef __str__(self):\n",
      "\tcust_str=\"\"\"\n",
      "\t\tCustomer:\n",
      "\t\t\tname:{name}\n",
      "\t\t\tbalance:{balance}\n",
      "\t\t\t\"\"\".format(name=self.name, balance=self.balance)\n",
      "\treturn cust_str\n",
      "\n",
      "\tdef __repr__(self):\n",
      "\treturn \"Customer('{name}',{balance})\".format(name=self.name,balance=self.balance)\n",
      "\n",
      "\n",
      "cust=Customer(\"Maryam Azur\",3000)\n",
      "\n",
      "print(cust)\n",
      "\n",
      "output: see an user friendly\n",
      "   getting a printable representation of an object\n",
      "\n",
      "__str__()\n",
      "__repr__()\n",
      "\n",
      "print(obj), str(obj)\n",
      "\n",
      "repr(obj)\n",
      "\n",
      "\n",
      "   sample  > __str__\n",
      "\n",
      "class Employee:\n",
      "    def __init__(self, name, salary=30000):\n",
      "        self.name, self.salary = name, salary\n",
      "            \n",
      "    # Add the __str__() method\n",
      "    def __str__(self):\n",
      "        s = \"Employee name: {name}\\nEmployee salary: {salary}\".format(name=self.name, salary=self.salary)      \n",
      "        return s\n",
      "\n",
      "emp1 = Employee(\"Amar Howard\", 30000)\n",
      "print(emp1)\n",
      "emp2 = Employee(\"Carolyn Ramirez\", 35000)\n",
      "print(emp2)\n",
      "\n",
      "   sample  > __repr__\n",
      "\n",
      "class Employee:\n",
      "    def __init__(self, name, salary=30000):\n",
      "        self.name, self.salary = name, salary\n",
      "      \n",
      "\n",
      "    def __str__(self):\n",
      "        s = \"Employee name: {name}\\nEmployee salary: {salary}\".format(name=self.name, salary=self.salary)      \n",
      "        return s\n",
      "      \n",
      "    # Add the __repr__method  \n",
      "    def __repr__(self):\n",
      "        s = \"Employee(\\\"{name}\\\", {salary})\".format(name=self.name, salary=self.salary)      \n",
      "        return s       \n",
      "\n",
      "emp1 = Employee(\"Amar Howard\", 30000)\n",
      "print(repr(emp1))\n",
      "emp2 = Employee(\"Carolyn Ramirez\", 35000)\n",
      "print(repr(emp2))\n",
      "\n",
      "\n",
      "     > sample    exceptions\n",
      "\n",
      "\n",
      "exceptions will stop the execution of the program.\n",
      "\n",
      "try except finally\n",
      "\n",
      "\n",
      "try:\n",
      " #try running some code\n",
      "except ExceptionNameHere:\n",
      " #run this code if exceptionNamehere happens\n",
      "except AnotherExceptionHere:\n",
      " #run this code if anotherexceptionhere happens\n",
      "\n",
      "finally:\n",
      " #run this code no matter what\n",
      "\n",
      "\n",
      "raising exceptions:\n",
      "\n",
      "def make_list_of_ones(length):\n",
      "\tif length<=0:\n",
      "\t\traise ValueError(\"Invalid length!\")\n",
      "\treturn[1]*length\n",
      "\n",
      "make_list_of_ones(-1)\n",
      "\n",
      "\n",
      "exceptions are classes\n",
      "\n",
      "BaseException\n",
      "\tException\n",
      "\t\tArithemticError\n",
      "\t\t\tFloatingPointError\n",
      "\t\t\tOverflowError\n",
      "\t\t\tZeroDivisionError\n",
      "\t\tTypeError\n",
      "\t\tValueError\n",
      "\t\t\tUnicodeError\n",
      "\t\t\t\tUnicodeDecodeError\n",
      "\t\t\tUnicodeEncodeError\n",
      "\t\t\tUnicodeTranslateError\n",
      "\n",
      "\t\tRuntimeError\n",
      "\n",
      "\tSystemExit\n",
      "\n",
      "\n",
      "\n",
      "class BalanceError(Exception): pass\n",
      "\n",
      "\n",
      "class Customer:\n",
      "\tdef __init__(self,name, balance):\n",
      "\tif balance < 0:\n",
      "\t\traise BalanceError(\"Balance has to be non-negative!\")\n",
      "\telse:\n",
      "\tself.name, self.balance=name,balance\n",
      "\n",
      "cust=Customer(\"Larry Torres\",-100)\n",
      "\n",
      "\n",
      "    sample adding try except to a function\n",
      "\n",
      "# MODIFY the function to catch exceptions\n",
      "def invert_at_index(x, ind):\n",
      "  try:\n",
      "    return 1/x[ind]\n",
      "  except ZeroDivisionError:\n",
      "    print(\"Cannot divide by zero!\")\n",
      "  except IndexError:\n",
      "    print(\"Index out of range!\")\n",
      " \n",
      "a = [5,6,0,7]\n",
      "\n",
      "# Works okay\n",
      "print(invert_at_index(a, 1))\n",
      "\n",
      "# Potential ZeroDivisionError\n",
      "print(invert_at_index(a, 2))\n",
      "\n",
      "# Potential IndexError\n",
      "print(invert_at_index(a, 5))\n",
      "\n",
      "\n",
      " > sample   > inherit ValueError\n",
      "\n",
      "class SalaryError(ValueError): pass\n",
      "class BonusError(SalaryError): pass\n",
      "\n",
      "class Employee:\n",
      "  MIN_SALARY = 30000\n",
      "  MAX_RAISE = 5000\n",
      "\n",
      "  def __init__(self, name, salary = 30000):\n",
      "    self.name = name\n",
      "    \n",
      "    # If salary is too low\n",
      "    if  salary < Employee.MIN_SALARY:\n",
      "      # Raise a SalaryError exception\n",
      "      raise SalaryError(\"Salary is too low\")\n",
      "      \n",
      "    self.salary = salary\n",
      "\n",
      "emp=Employee(\"bob\",100)  \n",
      "\n",
      "\n",
      "   sample  > BonusError\n",
      "\n",
      "class SalaryError(ValueError): pass\n",
      "class BonusError(SalaryError): pass\n",
      "\n",
      "class Employee:\n",
      "  MIN_SALARY = 30000\n",
      "  MAX_BONUS = 5000\n",
      "\n",
      "  def __init__(self, name, salary = 30000):\n",
      "    self.name = name    \n",
      "    if salary < Employee.MIN_SALARY:\n",
      "      raise SalaryError(\"Salary is too low!\")      \n",
      "    self.salary = salary\n",
      "    \n",
      "  # Rewrite using exceptions  \n",
      "  def give_bonus(self, amount):\n",
      "    if amount > Employee.MAX_BONUS:\n",
      "      raise BonusError(\"The bonus amount is too high!\")\n",
      "        \n",
      "    elif self.salary + amount <  Employee.MIN_SALARY:\n",
      "       raise SalaryError(\"The salary after bonus is too low!\")\n",
      "      \n",
      "    else:  \n",
      "      self.salary += amount\n",
      "\n",
      "\n",
      "\n",
      "t's better to list the except blocks in the increasing order of specificity, i.e. children before parents, otherwise the child exception will be called in the parent except block.\n",
      "\n",
      "\n",
      "       >Polymorphism\n",
      "\n",
      "polymorphism: using an unified interface to operate on objects of different classes\n",
      "\n",
      "def batch_withdraw(list_of_accounts, amount):\n",
      "\tfor acct in list_of_accounts:\n",
      "\t\tacct.withdraw(amount)\n",
      "\n",
      "b,c,s= BankAccount(1000), CheckingAccount(2000), SavingsAccount(3000)\n",
      "\n",
      "batch_withdraw([b,c,s])\n",
      "\n",
      "liskov substitution principle\n",
      "\n",
      "base class should be interchangeable with any of its subclasses without altering any properties of the program\n",
      "\n",
      "\n",
      "Wherever BankAccount works, CheckingAccount should work as well\n",
      "\n",
      "syntactically\n",
      "* function signatures are compatible arguments and returned values\n",
      "\n",
      "semantically\n",
      "* the state of the object and the program remains consistent\n",
      "* subclass method doesn't strengthen input conditions\n",
      "* subclass method doesn't weaken output conditions\n",
      "* no additional exceptions\n",
      "\n",
      "violation of lsp\n",
      "\n",
      "BankAccount.withdraw() requires 1 parameter, but CheckingAccount.withdraw() requires 2\n",
      "\n",
      "violation subclass strengthening input conditions\n",
      "\n",
      "BankAccount.withdraw() accepts any amount, but CheckingAccount.withdraw() assumes that the amount is limited\n",
      "\n",
      "violation subclass weakening output conditions\n",
      "\n",
      "Bankaccount.withdraw() can only leave a positive balance or cause an error,\n",
      "\n",
      "CheckingAccount.withdraw() can leave balance negative\n",
      "\n",
      "violating lsp\n",
      "1. changing additional attributes in subclass's method\n",
      "2. throwing additional exceptions in subclasses method\n",
      "\n",
      "if the inheritance violates the lsp principle, you should not be using inheritance\n",
      "\n",
      "  > sample\n",
      "\n",
      "class Parent:\n",
      "    def talk(self):\n",
      "        print(\"Parent talking!\")     \n",
      "\n",
      "class Child(Parent):\n",
      "    def talk(self):\n",
      "        print(\"Child talking!\")          \n",
      "\n",
      "class TalkativeChild(Parent):\n",
      "    def talk(self):\n",
      "        print(\"TalkativeChild talking!\")\n",
      "        Parent.talk(self)\n",
      "\n",
      "\n",
      "p, c, tc = Parent(), Child(), TalkativeChild()\n",
      "\n",
      "for obj in (p, c, tc):\n",
      "    obj.talk()\n",
      "\n",
      "\n",
      "  > sample rectangle and square\n",
      "\n",
      "# Define a Rectangle class\n",
      "class Rectangle():\n",
      "    def __init__(self,h,w):\n",
      "        self.h,self.w=h,w\n",
      "\n",
      "# Define a Square class\n",
      "class Square(Rectangle):\n",
      "    def __init__(self,w):\n",
      "        self.h,self.w=w,w\n",
      "\n",
      "A Square inherited from a Rectangle will always have both the h and w attributes, but we can't allow them to change independently of each other.\n",
      "\n",
      "  > sample   square violates the liskov principle\n",
      "\n",
      "  preserve consistent state\n",
      "\n",
      "class Rectangle:\n",
      "    def __init__(self, w,h):\n",
      "      self.w, self.h = w,h\n",
      "\n",
      "# Define set_h to set h      \n",
      "    def set_h(self, h):\n",
      "      self.h = h\n",
      "      \n",
      "# Define set_w to set w          \n",
      "    def set_w(self, w):\n",
      "      self.w=w\n",
      "      \n",
      "      \n",
      "class Square(Rectangle):\n",
      "    def __init__(self, w):\n",
      "      self.w, self.h = w, w \n",
      "\n",
      "# Define set_h to set w and h\n",
      "    def set_h(self, h):\n",
      "      self.h = h\n",
      "      self.w = h\n",
      "\n",
      "# Define set_w to set w and h      \n",
      "    def set_w(self,w):\n",
      "      self.h = w\n",
      "      self.w = w\n",
      "\n",
      "\n",
      "Remember that the substitution principle requires the substitution to preserve the oversall state of the program. An example of a program that would fail when this substitution is made is a unit test for a setter functions in Rectangle class.\n",
      "\n",
      "    data access  > private attributes\n",
      "\n",
      "1. all class data is public\n",
      "2. restricting access\n",
      "\n",
      "use @property to customize access\n",
      "\n",
      "override __getattr__() and __setattr__()\n",
      "\n",
      "start with a sing _ > \"internal\"\n",
      "not a part of the public api\n",
      "\n",
      "if the variable starts with __ then it is private\n",
      "\n",
      "name mangling obj.__attr_name is intrepreted as obj._MyClass__attr_name\n",
      "\n",
      "Use to prevent name clashes in inherited classes\n",
      "\n",
      "   sample create an internal method called _is_valid\n",
      "\n",
      "# MODIFY to add class attributes for max number of days and months\n",
      "class BetterDate:\n",
      "    _MAX_DAYS = 30\n",
      "    _MAX_MONTHS = 12\n",
      "    \n",
      "    def __init__(self, year, month, day):\n",
      "      self.year, self.month, self.day = year, month, day\n",
      "      \n",
      "    @classmethod\n",
      "    def from_str(cls, datestr):\n",
      "        year, month, day = map(int, datestr.split(\"-\"))\n",
      "        return cls(year, month, day)\n",
      "    \n",
      "    # Add _is_valid() checking day and month values\n",
      "    def _is_valid(self):\n",
      "        return (self.day <= BetterDate._MAX_DAYS) and \\\n",
      "               (self.month <= BetterDate._MAX_MONTHS)\n",
      "         \n",
      "bd1 = BetterDate(2020, 4, 30)\n",
      "print(bd1._is_valid())\n",
      "\n",
      "bd2 = BetterDate(2020, 6, 45)\n",
      "print(bd2._is_valid())\n",
      "\n",
      "\n",
      "The single underscore naming convention is purely a convention, and Python doesn't do anything special with such attributes and methods behind the scenes\n",
      "\n",
      " That convention is widely followed, though, so if you see an attribute name with one leading underscore in someone's class - don't use it\n",
      "\n",
      "\n",
      "    >Properties\n",
      "\n",
      "\n",
      "df.shape\n",
      "\n",
      "attribute can not be changed\n",
      "\n",
      "\n",
      "class Employer:\n",
      "\tdef __init__(self,name, new_salary):\n",
      "\t\tself._salary=new_salary\n",
      "\n",
      "\n",
      "\t@property\n",
      "\tdef salary(self):\n",
      "\t\treturn self._salary\n",
      "\n",
      "\t@salary.setter\n",
      "\tdef salary(self,new_salary):\n",
      "\t\tif new_salary<0:\n",
      "\t\t\traise ValueError(\"Invalid salary\")\n",
      "\t\tself._salary=new_salary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emp.salary=60000\n",
      "\n",
      "without a setter, the property will be read only\n",
      "\n",
      "read only\n",
      "@salary.getter (read only)\n",
      "\n",
      "@salary.deleter\n",
      "\n",
      "\n",
      "    sample customer\n",
      "\n",
      "# Create a Customer class\n",
      "class Customer():\n",
      "    def __init__(self,name,new_bal):\n",
      "            self._name=name\n",
      "          if new_bal<0:\n",
      "                raise ValueError(\"Invalid new balance\")\n",
      "            else:\n",
      "                self._new_bal=new_bal\n",
      "\n",
      "    @property\n",
      "    def balance(self):\n",
      "        return self._balance\n",
      "\n",
      "       @balance.setter\n",
      "    def balance(self,balance):\n",
      "        if balance<0:\n",
      "            raise ValueError(\"Invalid new balance\")\n",
      "        else:\n",
      "            self._balance=balance\n",
      "            print(\"Setter method is called\")\n",
      "\n",
      "# Create a Customer        \n",
      "cust = Customer(\"Belinda Lutz\",2000)\n",
      "# Assign 3000 to the balance property\n",
      "cust.balance=3000\n",
      "\n",
      "# Print the balance property\n",
      "print(cust.balance)\n",
      "\n",
      "\n",
      "   > sample   read only property\n",
      "\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "\n",
      "# MODIFY the class to turn created_at into a read-only property\n",
      "class LoggedDF(pd.DataFrame):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        pd.DataFrame.__init__(self, *args, **kwargs)\n",
      "        self._created_at = datetime.today()\n",
      "\n",
      "    def to_csv(self, *args, **kwargs):\n",
      "        temp = self.copy()\n",
      "        temp[\"created_at\"] = self.created_at\n",
      "        pd.DataFrame.to_csv(temp, *args, **kwargs) \n",
      "\n",
      "    @property  \n",
      "    def created_at(self):\n",
      "        return self._created_at\n",
      "\n",
      "ldf = LoggedDF({\"col1\": [1,2], \"col2\":[3,4]}) \n",
      "\n",
      "# Put into try-except block to catch AtributeError and print a message\n",
      "try:\n",
      "    ldf.created_at = '2035-07-13'\n",
      "except AttributeError:\n",
      "    print(\"Could not set attribute\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\object oriented programming.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\object oriented programming.txt\n",
      "think object oriented\n",
      "\n",
      "start thinking of patterns of behavior\n",
      "\n",
      "code as interactions of objects\n",
      "code is more usable and maintainable\n",
      "\n",
      "object= state+ behavior  (encapsulation)\n",
      "\n",
      "classes are blueprints for objects outlining possible states and behaviors\n",
      "\n",
      "in python everything is an object\n",
      "every object has a class\n",
      "\n",
      "type() to find the class\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "a=np.array([1,2,3,4])\n",
      "a.shape\n",
      "a.reshape(2,2)\n",
      "\n",
      " >Class\n",
      "\n",
      "class Customer:\n",
      "\tpass #blank template\n",
      "\n",
      "c1=Customer()\n",
      "\n",
      "you can create instances of a template class\n",
      "\n",
      "\n",
      "class Customer:\n",
      "\n",
      "\tdef identify(self, name):\n",
      "\t\tprint(\"I am a customer \"+name)\n",
      "\n",
      "cust = Customer()\n",
      "\n",
      "cust.identify(\"Laura\")\n",
      "\n",
      "self is a stand-in for a particular object used in class definition\n",
      "\n",
      "attributes customer name should be name\n",
      "attributes are created by assignment\n",
      "\n",
      "\n",
      "class Customer:\n",
      "\n",
      "\tdef set_name(self, new_name):\n",
      "\t\tself.name=new_name\n",
      "\n",
      "\tdef identify(self):\n",
      "\t\tprint(\"I am Customer\"+self.name)\n",
      "\n",
      "\n",
      "self reference the object and name comes into existence by assignment\n",
      "\n",
      "customer=Customer()\n",
      "customer.set_name(\"Laura\")\n",
      "print(customer.name)\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "class Employee:\n",
      "  \n",
      "  def set_name(self, new_name):\n",
      "    self.name = new_name\n",
      "  \n",
      "  # Add set_salary() method\n",
      "  def set_salary(self,new_salary):\n",
      "    self.salary=new_salary\n",
      "  \n",
      "  \n",
      "# Create an object emp of class Employee  \n",
      "emp = Employee()\n",
      "\n",
      "# Use set_name to set the name of emp to 'Korel Rossi'\n",
      "emp.set_name('Korel Rossi')\n",
      "\n",
      "# Set the salary of emp to 50000\n",
      "emp.set_salary(50000)\n",
      "\n",
      "print(emp.name,emp.salary)\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "class Employee:\n",
      "    def set_name(self, new_name):\n",
      "        self.name = new_name\n",
      "\n",
      "    def set_salary(self, new_salary):\n",
      "        self.salary = new_salary \n",
      "\n",
      "    # Add a give_raise() method with raise amount as a parameter\n",
      "    def give_raise(self, new_raise):\n",
      "        self.salary += new_raise\n",
      "\n",
      "\n",
      "emp = Employee()\n",
      "emp.set_name('Korel Rossi')\n",
      "emp.set_salary(50000)\n",
      "\n",
      "print(emp.salary)\n",
      "emp.give_raise(1500)\n",
      "print(emp.salary)\n",
      "\n",
      " >Sample\n",
      "\n",
      "class Employee:\n",
      "    def set_name(self, new_name):\n",
      "        self.name = new_name\n",
      "\n",
      "    def set_salary(self, new_salary):\n",
      "        self.salary = new_salary \n",
      "\n",
      "    def give_raise(self, amount):\n",
      "        self.salary = self.salary + amount\n",
      "\n",
      "    # Add monthly_salary method that returns 1/12th of salary attribute\n",
      "    def monthly_salary(self):\n",
      "        return self.salary/12\n",
      "\n",
      "    \n",
      "emp = Employee()\n",
      "emp.set_name('Korel Rossi')\n",
      "emp.set_salary(50000)\n",
      "\n",
      "# Get monthly salary of emp and assign to mon_sal\n",
      "mon_sal = emp.monthly_salary()\n",
      "\n",
      "# Print mon_sal\n",
      "print(mon_sal)\n",
      "\n",
      "\n",
      " >init constructor\n",
      "\n",
      "class Customer:\n",
      "\tdef __init__(self, name, balance):\n",
      "\t\tself.name=name\n",
      "\t\tself.balance=balance\n",
      "\t\tprint(\" the constructor method was called\n",
      "\n",
      "customer = Customer(\"Bob\", 1000)\n",
      "print(customer.name, customer.balance)\n",
      "\n",
      "1. try defining attribute in the constructor\n",
      "2. defining attributes in the constructor puts all the attributes in one place\n",
      "3. work to create usable and maintainable code\n",
      "4. name your classes using camelcase low case for functions and attributes\n",
      "5. \"\"\"Doc strings are comments in python\"\"\"\n",
      "\n",
      " >Sample\n",
      "\n",
      "class Employee:\n",
      "    # Create __init__() method\n",
      "    def __init__(self, name, salary=0):\n",
      "        # Create the name and salary attributes\n",
      "        self.name = name\n",
      "        self.salary = salary\n",
      "    \n",
      "    # From the previous lesson\n",
      "    def give_raise(self, amount):\n",
      "        self.salary += amount\n",
      "\n",
      "    def monthly_salary(self):\n",
      "        return self.salary/12\n",
      "        \n",
      "emp = Employee(\"Korel Rossi\")\n",
      "\n",
      " >Sample\n",
      "\n",
      "class Employee:\n",
      "  \n",
      "    def __init__(self, name, salary=0):\n",
      "        self.name = name\n",
      "        # Modify code below to check if salary is positive\n",
      "        if salary>0:\n",
      "            self.salary = salary \n",
      "        else:\n",
      "            self.salary=0\n",
      "   \n",
      "   # ...Other methods omitted for brevity ...\n",
      "      \n",
      "emp = Employee(\"Korel Rossi\", -1000)\n",
      "print(emp.name)\n",
      "print(emp.salary)\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Import datetime from datetime\n",
      "from datetime import datetime\n",
      "\n",
      "class Employee:\n",
      "    \n",
      "    def __init__(self, name, salary=0):\n",
      "        self.name = name\n",
      "        if salary > 0:\n",
      "          self.salary = salary\n",
      "        else:\n",
      "          self.salary = 0\n",
      "          print(\"Invalid salary!\")\n",
      "          \n",
      "        # Add the hire_date attribute and set it to today's date\n",
      "        self.hire_date=datetime.today()\n",
      "        \n",
      "   # ...Other methods omitted for brevity ...\n",
      "      \n",
      "emp = Employee(\"Korel Rossi\", -1000)\n",
      "print(emp.name)\n",
      "print(emp.salary)\n",
      "\n",
      " >Sample\n",
      "\n",
      "import numpy as np\n",
      "class Point:\n",
      "    def __init__(self,x=0.0,y=0.0):\n",
      "        self.x=x\n",
      "        self.y=y\n",
      "        \n",
      "    def reflect(self,axis):\n",
      "        if axis=='x':\n",
      "            self.y=self.y*-1\n",
      "        elif axis=='y':\n",
      "            self.x=self.x*-1\n",
      "\n",
      "    def distance_to_origin(self):\n",
      "        return np.sqrt(self.x**2+self.y**2)\n",
      "        \n",
      "pt = Point(x=3.0)\n",
      "pt.reflect(\"y\")\n",
      "print((pt.x, pt.y))\n",
      "pt.y = 4.0\n",
      "print(pt.distance_to_origin())\n",
      "\n",
      " How to define inheritance in python\n",
      "https://www.w3schools.com/python/python_inheritance.asp\n",
      "\n",
      "inheritance\n",
      "\n",
      "class Person:\n",
      "  def __init__(self, fname, lname):\n",
      "    self.firstname = fname\n",
      "    self.lastname = lname\n",
      "\n",
      "  def printname(self):\n",
      "    print(self.firstname, self.lastname)\n",
      "\n",
      "class Student(Person):\n",
      "  def __init__(self, fname, lname):\n",
      "    super().__init__(fname, lname)\n",
      "    self.graduationyear = 2019\n",
      "\n",
      "x = Student(\"Mike\", \"Olsen\")\n",
      "print(x.graduationyear)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pandas time series.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pandas time series.txt\n",
      "How to plot data without a date\n",
      "\n",
      "data={'key':[0,1,2,3],'data_values':[100,150,400,200]}\n",
      "data=pd.DataFrame(data)\n",
      "data2={'key':[0,1,2,3],'data_values':[45,98,200,300]}\n",
      "data2=pd.DataFrame(data2)\n",
      "data.set_index('key')\n",
      "data2.set_index('key')\n",
      "print(data)\n",
      "\n",
      "fig, axs = plt.subplots(2,1, figsize=(5,10))\n",
      "data.iloc[:,1].plot(y='data_values',ax=axs[0])\n",
      "data2.iloc[:,1].plot(y='data_values',ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "\n",
      " adding an x time stamp\n",
      "\n",
      "data={'time':['2020-04-01','2020-04-02','2020-04-03','2020-04-04'],'data_values':[100,150,400,200]}\n",
      "data=pd.DataFrame(data)\n",
      "data2={'time':['2020-04-01','2020-04-02','2020-04-03','2020-04-04'],'data_values':[45,98,200,300]}\n",
      "\n",
      "data2=pd.DataFrame(data2)\n",
      "data.set_index('time')\n",
      "data2.set_index('time')\n",
      "print(data)\n",
      "\n",
      "fig, axs = plt.subplots(2,1, figsize=(5,10))\n",
      "data.plot(x='time',y='data_values',ax=axs[0])\n",
      "data2.plot(x='time', y='data_values',ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "\n",
      " linear regression\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "# Prepare input and output DataFrames\n",
      "X = boston[['AGE']]\n",
      "y = boston[['RM']]\n",
      "\n",
      "# Fit the model\n",
      "model = linear_model.LinearRegression()\n",
      "model.fit(X, y)\n",
      "\n",
      "print(new_inputs.reshape(-1,1))\n",
      "predictions = model.predict(new_inputs.reshape(-1,1))\n",
      "\n",
      "# Visualize the inputs and predicted values\n",
      "plt.scatter(new_inputs, predictions, color='r', s=3)\n",
      "plt.xlabel('inputs')\n",
      "plt.ylabel('predictions')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " generating time\n",
      "\n",
      "generates 11 numbers \n",
      "\n",
      "indices=np.arange(0,10)  \n",
      "print(indices)\n",
      "\n",
      "creates 10 evenly spaced numbers starting with 1 and ending with 10\n",
      "print(np.linspace(1,10,10))\n",
      "\n",
      "\n",
      "import librosa as lr\n",
      "from glob import glob\n",
      "\n",
      "# List all the wav files in the folder\n",
      "audio_files = glob(data_dir + '/*.wav')\n",
      "\n",
      "# Read in the first audio file, create the time array\n",
      "audio, sfreq = lr.load(audio_files[0])\n",
      "time = np.arange(0, len(audio)) / sfreq\n",
      "\n",
      "\n",
      "# Plot audio over time\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(time, audio)\n",
      "ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      " >read in stock prices per year\n",
      "\n",
      "# Read in the data\n",
      "data = pd.read_csv('prices.csv', index_col=0)\n",
      "\n",
      "# Convert the index of the DataFrame to datetime\n",
      "data.index = pd.to_datetime(data.index)\n",
      "print(data.head())\n",
      "\n",
      "# Loop through each column, plot its values over time\n",
      "fig, ax = plt.subplots()\n",
      "for column in data:\n",
      "    data[column].plot(ax=ax, label=column)\n",
      "ax.legend()\n",
      "plt.show()\n",
      "\n",
      " classification and feature engineering\n",
      "\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "X=np.column_stack([means,maxs,stds])\n",
      "y=labels.reshape([-1,1])\n",
      "model=LinearSVC()\n",
      "model.fit(X,y)\n",
      "\n",
      "predictions=model.predict(X_test)\n",
      "\n",
      "percent_score= sum(predictions==labels_test)/len(labels_test)\n",
      "percent_score= accuracy_score(labels_test,predictions)\n",
      "\n",
      "\n",
      " heartbeat analysis\n",
      "\n",
      "fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True)\n",
      "\n",
      "# Calculate the time array\n",
      "time = np.arange(normal.shape[0]) / sfreq\n",
      "\n",
      "# Stack the normal/abnormal audio so you can loop and plot\n",
      "stacked_audio = np.hstack([normal, abnormal]).T\n",
      "\n",
      "# Loop through each audio file / ax object and plot\n",
      "# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
      "for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
      "    ax.plot(time, iaudio)\n",
      "show_plot_and_make_titles()\n",
      "\n",
      " using the mean to smooth the noise\n",
      "\n",
      "mean_normal = np.mean(normal, axis=1)\n",
      "mean_abnormal = np.mean(abnormal, axis=1)\n",
      "\n",
      "# Plot each average over time\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
      "ax1.plot(time, mean_normal)\n",
      "ax1.set(title=\"Normal Data\")\n",
      "ax2.plot(time, mean_abnormal)\n",
      "ax2.set(title=\"Abnormal Data\")\n",
      "plt.show()\n",
      "\n",
      " Test the data\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "# Initialize and fit the model\n",
      "model = LinearSVC()\n",
      "model.fit(X_train,y_train)\n",
      "\n",
      "# Generate predictions and score them manually\n",
      "predictions = model.predict(X_test)\n",
      "print(sum(predictions == y_test.squeeze()) / len(y_test))\n",
      "\n",
      " Smoothing signal\n",
      "\n",
      "# Rectify the audio signal\n",
      "audio_rectified = audio.apply(np.abs)\n",
      "\n",
      "# Plot the result\n",
      "# figsize parameter 1 is the width and parameter 2 is the height\n",
      "audio_rectified.plot(figsize=(10, 5))\n",
      "plt.show() \n",
      "\n",
      " Rolling and mean\n",
      "\n",
      "# Smooth by applying a rolling mean\n",
      "audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
      "\n",
      "# Plot the result\n",
      "audio_rectified_smooth.plot(figsize=(10, 5))\n",
      "plt.show()\n",
      "\n",
      "\n",
      " cross_val_score\n",
      "\n",
      "means = np.mean(audio_rectified_smooth, axis=0)\n",
      "stds = np.std(audio_rectified_smooth, axis=0)\n",
      "maxs = np.max(audio_rectified_smooth, axis=0)\n",
      "\n",
      "# Create the X and y arrays\n",
      "X = np.column_stack([means, stds, maxs])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "from sklearn.model_selection import cross_val_score\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "print(percent_score)\n",
      "\n",
      " tempo\n",
      "\n",
      "tempos = []\n",
      "for col, i_audio in audio.items():\n",
      "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
      "\n",
      "# Convert the list to an array so you can manipulate it more easily\n",
      "tempos = np.array(tempos)\n",
      "\n",
      "# Calculate statistics of each tempo\n",
      "tempos_mean = tempos.mean(axis=-1)\n",
      "tempos_std = tempos.std(axis=-1)\n",
      "tempos_max = tempos.max(axis=-1)\n",
      "\n",
      "  column_stack\n",
      "\n",
      "# Create the X and y arrays\n",
      "X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "\n",
      "\n",
      "\n",
      "  converting date index to a datetime\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "  grid=True\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "# Plot the entire time series diet and show gridlines\n",
      "diet.plot(grid=True)\n",
      "plt.show()\n",
      "\n",
      " filter on index\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "# Slice the dataset to keep only 2012\n",
      "diet2012 = diet['2012']\n",
      "\n",
      "# Plot 2012 data\n",
      "diet2012.plot(grid=True)\n",
      "plt.show()\n",
      "\n",
      " Differences between two sets of dates\n",
      "\n",
      "set_stock_dates = set(stocks.index)\n",
      "set_bond_dates = set(bonds.index)\n",
      "\n",
      "\n",
      "differences=set_stock_dates - set_bond_dates\n",
      "# Take the difference between the sets and print\n",
      "print(differences)\n",
      "\n",
      "# Merge stocks and bonds DataFrames using join()\n",
      "stocks_and_bonds = stocks.join(bonds, how='inner')\n",
      "\n",
      "\n",
      " Correlation between two variables\n",
      "\n",
      "# Compute percent change using pct_change()\n",
      "returns = stocks_and_bonds.pct_change()\n",
      "\n",
      "# Compute correlation using corr()\n",
      "correlation = returns['SP500'].corr(returns['US10Y'])\n",
      "print(\"Correlation of stocks and interest rates: \", correlation)\n",
      "\n",
      "# Make scatter plot\n",
      "plt.scatter(returns['SP500'],returns['US10Y'])\n",
      "plt.show()\n",
      "\n",
      "        Spectrogram\n",
      "\n",
      "fourier transforms\n",
      "fast or slow moving waves\n",
      "fft show a series of fast and slow wave osciliations in a time series.\n",
      "\n",
      "short-time fourier transform is calculating a fft over a time frame then sliding the window over by one\n",
      "\n",
      "The spectrogram is the square of each sfft\n",
      "\n",
      "we can calculate the stft with librosa\n",
      "\n",
      "the sound frequencies are converted in to decibels which normalizes the average values of all the frequencies\n",
      "\n",
      "we can visualize it with specshow() function\n",
      "\n",
      "from librosa.core import stft, amplitude_to_db\n",
      "from librosa.display import specshow\n",
      "\n",
      "HOP_LENGTH = 2**4\n",
      "SIZE_WINDOW= 2**7\n",
      "\n",
      "#calculate the short fast fourier transform\n",
      "\n",
      "audio_spec = stft(audio, hop_length=HOP_LENGTH, n_fft=SIZE_WINDOW)\n",
      "\n",
      "#convert into decibels\n",
      "spec_db=amplitude_to_db(audio_spec)\n",
      "\n",
      "#visualize\n",
      "specshow(spec_db, sr=sfreq, x_axis='time',\n",
      "\ty_axis='hz', hop_length=HOP_LENGTH)\n",
      "\n",
      "bandwidths=lr.feature.spectral_bandwidth(S=spec)[0]\n",
      "centroids=lr.feature.spectral_centroid(S=spec)[0]\n",
      "\n",
      "ax= spectshow(spec, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "ax.plot(times_spec, centroids)\n",
      "ax.fill_between(times_spec, centroids-bandwidths/2, centroids+bandwidths/2, alpha=0.5)\n",
      "\n",
      "\n",
      "each spectral has different patterns\n",
      "we can use these patterns to distinquish spectrals from one another\n",
      "\n",
      "for example spectral bandwidth and spectral centroids describe where most of the energy is at each moment in time.\n",
      "\n",
      "centroids_all=[]\n",
      "bandwidths_all=[]\n",
      "\n",
      "for spec in spectrograms:\n",
      "\tbandwidths=lr.feature.spectral_bandwidth(S=lr.db_to_amplitude(spec))\n",
      "\tcentroids=lr.feature.spectral_centroid(S=lr.db_to_amplitude(spec))\n",
      "\tbandwidths_all.append(np.mean(bandwidths))\n",
      "\tcentroids_all.appen(np.mean(centroids))\n",
      "\n",
      "#input matrix\n",
      "\n",
      "X= np.column_stack([means,stds,maxs,tempo_mean,tempo_max,tempo_std, bandwidths_all, centroids_all])\n",
      "\n",
      "\n",
      "\n",
      "   sample   >  short term fourier transform heart beat audio\n",
      "#Spectral engineering is one of the most common techniques in machine learning for time series data\n",
      "\n",
      "# Import the stft function\n",
      "from librosa.core import stft\n",
      "\n",
      "# Prepare the STFT\n",
      "HOP_LENGTH = 2**4\n",
      "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n",
      "\n",
      " > sample  > convert the spectral to decibals\n",
      "\n",
      "# Convert into decibels\n",
      "spec_db = amplitude_to_db(spec)\n",
      "\n",
      "# Compare the raw audio to the spectrogram of the audio\n",
      "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
      "axs[0].plot(time, audio)\n",
      "\n",
      "specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "plt.show()\n",
      "\n",
      "#the heartbeats come in pairs as seen by the vertical lines in the spectrogram\n",
      "\n",
      "\n",
      "   >sample  > calculate the bandwidths and centroids\n",
      "\n",
      "# By computing the spectral features, you have a much better idea of what's going on. \t\n",
      "\n",
      "import librosa as lr\n",
      "from librosa.core import amplitude_to_db\n",
      "from librosa.display import specshow\n",
      "\n",
      "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
      "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
      "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
      "\n",
      "\n",
      "# Convert spectrogram to decibels for visualization\n",
      "spec_db = amplitude_to_db(spec)\n",
      "\n",
      "# Display these features on top of the spectrogram\n",
      "fig, ax = plt.subplots(figsize=(10, 5))\n",
      "ax = specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "ax.plot(times_spec, centroids)\n",
      "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
      "ax.set(ylim=[None, 6000])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#You've spent this lesson engineering many features from the audio data - some contain information about how the audio changes in time\n",
      "\n",
      "#Combine all of them into an array that can be fed into the classifier, and see how it does.\n",
      "\n",
      " > sample build the final array for the classifier\n",
      "\n",
      "# Loop through each spectrogram\n",
      "bandwidths = []\n",
      "centroids = []\n",
      "\n",
      "for spec in spectrograms:\n",
      "    # Calculate the mean spectral bandwidth\n",
      "    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
      "    # Calculate the mean spectral centroid\n",
      "    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
      "    # Collect the values\n",
      "    bandwidths.append(this_mean_bandwidth)  \n",
      "    centroids.append(this_mean_centroid)\n",
      "\n",
      "# Create X and y arrays\n",
      "X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "\n",
      "output .48\n",
      "\n",
      "#To improve the accuracy, you want to find the right features that provide relevant information and also build models on much larger data\n",
      "\n",
      "   >Regression\n",
      "\n",
      "regression model predict continueous models\n",
      "\n",
      "regression: a process that results in a formal model of the data\n",
      "correlation: a statistic that describe the data.  how two features correlate between each other.\n",
      "\n",
      "down or up together or an inverse relationship\n",
      "\n",
      "timeseries often have patterns that change over time\n",
      "\n",
      "two timeseries that seem correlated at one moment may not remain so over time\n",
      "\n",
      "fig, axs=plt.subplots(1,2)\n",
      "\n",
      "axs[0].plot(x,c='k',lw=3,alpha=.2)\n",
      "axs[0].plot(y)\n",
      "axs[0].set(xlabel='time',title='X values=time')\n",
      "\n",
      "#encode time as acolor in a scatterplot\n",
      "\n",
      "axs[1].scatter(x_long,y_long, c=np.arange(len(x_long)),cmap='viridis')\n",
      "axs[1].set(xlabel='x',ylabel='y',title='Color=time')\n",
      "\n",
      "  >regression models\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "model=LinearRegression()\n",
      "model.fit(X,y)\n",
      "model.predict(X)\n",
      "\n",
      "  >Ridge (see introductory course on skilearn)\n",
      "\n",
      "alphas=[.1,1e2,1e3]\n",
      "\n",
      "ax.plot(y_test,color='k', alpha=.3,lw=3)\n",
      "\n",
      "for ii, alpha in enumerate(alphas):\n",
      "\ty_predicted=Ridge(alpha=alpha).fit(X_train,y_train).predict(X_test)\n",
      "\tax.plot(y_predict, c=cmap(ii/len(alphas)))\n",
      "\n",
      "ax.legend(['True values','Model 1', 'Model 2', 'Model 3'])\n",
      "ax.set(xlabel='Time')\n",
      "\n",
      "  >Scoring a regression model\n",
      "\n",
      "Correlation (r)\n",
      "Coefficient of Determination(R2)\n",
      "\n",
      "\n",
      "Coefficient of Determination R2\n",
      "1- error(model)/variance(testdata)\n",
      "\n",
      "Error is actual - predicted of the model\n",
      "\n",
      "Variance is the mean squared distance of the data from their mean\n",
      "(x-x_mean) ** 2 / n\n",
      "\n",
      "or\n",
      "\n",
      "np.var(versicolor_petal_length)\n",
      "\n",
      "\n",
      "deviations = np.mean(y_data) - y_data\n",
      "\n",
      "VAR = np.sum(np.square(deviations))\n",
      "\n",
      "R-Squared : what fraction of variation is linear\n",
      "\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "print(r2_score(y_predicted, y_test))\n",
      "\n",
      "\n",
      "   Sample  > plot the prices for Ebay and Yhoo over time\n",
      "\n",
      "# Plot the raw values over time\n",
      "print(prices.columns)\n",
      "prices.plot()\n",
      "plt.show()\n",
      "\n",
      "   SAMPLE  > plot a scatter plot for ebay and yahoo prices\n",
      "\n",
      "# Scatterplot with one company per axis\n",
      "prices.plot.scatter('EBAY', 'YHOO')\n",
      "plt.show()\n",
      "\n",
      "   sample  > plot the scatter plot with color showing the price index\n",
      "# Scatterplot with color relating to time\n",
      "prices.plot.scatter('EBAY', 'YHOO', c=prices.index, \n",
      "                    cmap=plt.cm.viridis, colorbar=False)\n",
      "plt.show()\n",
      "\n",
      "The prices.index are dates.   The color changes over time.\n",
      "\n",
      "<<<<<sample  > add a ridge regressor and do 3 fold cross validation to check the accuracy.\n",
      "\n",
      "\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "# Use stock symbols to extract training data\n",
      "X = all_prices[['EBAY','NVDA','YHOO']]\n",
      "y = all_prices[['AAPL']]\n",
      "\n",
      "# Fit and score the model with cross-validation\n",
      "scores = cross_val_score(Ridge(), X, y, cv=3)\n",
      "print(scores)\n",
      "\n",
      "If Measure of fit of the model is a small value that means model is well fit to the data.\n",
      "\n",
      "If Measure of magnitude of coefficient is a small value that means model is not overfit.\n",
      "\n",
      "  >sample  > calculating the R2 factor\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "# Split our data into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, \n",
      "                                                    train_size=.8, shuffle=False, random_state=1)\n",
      "\n",
      "# Fit our model and generate predictions\n",
      "model = Ridge()\n",
      "model.fit(X_train,y_train)\n",
      "predictions = model.predict(X_test)\n",
      "score = r2_score(y_test, predictions)\n",
      "print(score)\n",
      "\n",
      "output: -5.70939901949\n",
      "\n",
      "ebay, nvda, yhoo are not linear predicters for apple prices\n",
      "\n",
      "  >sample    y_test is falling then predicted apple price is climbing\n",
      "\n",
      "# Visualize our predictions along with the \"true\" values, and print the score\n",
      "fig, ax = plt.subplots(figsize=(15, 5))\n",
      "ax.plot(y_test, color='k', lw=3)\n",
      "ax.plot(predictions, color='r', lw=2)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "The poor r2 score reflects a deviation between the predicted and true time series values.\n",
      "\n",
      "\n",
      "       cleaning and improving the data\n",
      "\n",
      "time data errors often happens because of human error, machine sensor malfunctions, and database failures.\n",
      "\n",
      "\n",
      "interpolation is using time to fill in missing data\n",
      "\n",
      "you can use time to assist in interpolation\n",
      "\n",
      "interpolation means using known values on either side of a gap in the data to make assumptions about what missing\n",
      "\n",
      "#create a boolean mask to find where the missing values are\n",
      "\n",
      "missing= prices.isna()\n",
      "\n",
      "#create a list of interpolated missing values\n",
      "prices_interp = prices.interpolate('linear')\n",
      "\n",
      "ax=prices_interp.plot(c='r')\n",
      "prices.plot(c='k',ax=ax, lw=2)\n",
      "\n",
      "    another way to fix missing data is to transform it so it more better behaved\n",
      "\n",
      "1. smooth the data\n",
      "2. use more complex transformations\n",
      "\n",
      "a common transformation is to standardize the mean and variance over time\n",
      "\n",
      "1. convert the dataset so each point represents the % change over a previous window\n",
      "2. this makes timepoints more comparable to one another if absolute values of data change a lot\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def percent_change(values):\n",
      "\t\"\"\"Calculates the % change between the last value and the mean of previous values\"\"\"\n",
      "\n",
      "\tprevious_values=values[:-1]\n",
      "\tlast_value=values[-1]\n",
      "\tpercent_change=(last_value-np.mean(previous_values))/np.mean(previous_values)\n",
      "\treturn percent_change\n",
      "\n",
      "\n",
      " > pass the percent_change function as a input to rolling prices\n",
      "\n",
      "fig,axs=plt.subplots(1,2,figsize=(10,5))\n",
      "ax=prices.plot(ax=axs[0])\n",
      "\n",
      "ax=prices.rolling(window=20).aggregate(percent_change).plot(ax=axs[1])\n",
      "ax.legend_.set_visible(False)\n",
      "\n",
      "#periods of high or low changes are easier to spot.\n",
      "\n",
      "    Outliers\n",
      "\n",
      "outliers are datapoints that are significantly statistically different from the dataset\n",
      "\n",
      "they have negative effects on the predictive power of your model, biasing it away from its true value\n",
      "\n",
      "One solution: remove or replace outliers with a more representative vlaue\n",
      "\n",
      "** there can be legitimate extreme values\n",
      "\n",
      "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
      "\n",
      "for data, ax in zip([prices,prices_perc_change],axs):\n",
      "\tthis_mean=data.mean()\n",
      "\tthis.std=data.std()\n",
      "\n",
      "\tdata.plot(ax=ax)\n",
      "\tax.axhline(this_mean+this_std*3, ls='--',c='r')\n",
      "\tax.axhline(this_mean-this_std*3, ls='--',c='r')\n",
      "\n",
      "\n",
      "find price outside of 3 std range from the mean\n",
      "\n",
      "#replace the outliers with the median\n",
      "\n",
      "prices_outlier_centered=prices_outlier_perc - prices_outlier_perc.mean()\n",
      "\n",
      "std=prices_outlier_perc.std()\n",
      "\n",
      "outliers=np.abs(prices_outlier_centered)>(std*3)\n",
      "\n",
      "prices_outlier_fixed=prices_outlier_centered.copy()\n",
      "prices_outlier_fixed[outliers]=np.nanmedian(prices_outlier_fixed)\n",
      "\n",
      "fig,axs = plt.subplots(1,2,figsize=(10,5))\n",
      "prices_outlier_centered.plot(ax=axs[0])\n",
      "prices_outlier_fixed.plot(ax=axs[1])\n",
      "\n",
      "\n",
      "   >sample      visualize missing values in continueous data\n",
      "\n",
      "# Visualize the dataset\n",
      "prices.plot(legend=False)\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "# Count the missing values of each time series\n",
      "missing_values = prices.isna().sum()\n",
      "print(missing_values)\n",
      "\n",
      "   sample  > interpolate\n",
      "\n",
      "# Create a function we'll use to interpolate and plot\n",
      "def interpolate_and_plot(prices, interpolation_type):\n",
      "\n",
      "    # Create a boolean mask for missing values\n",
      "    missing_values = prices.isna()\n",
      "\n",
      "    # Interpolate the missing values\n",
      "    prices_interp = prices.interpolate(interpolation_type)\n",
      "\n",
      "    # Plot the results, highlighting the interpolated values in black\n",
      "    fig, ax = plt.subplots(figsize=(10, 5))\n",
      "    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
      "    \n",
      "    # Now plot the interpolated values on top in red\n",
      "    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "# Interpolate using the latest non-missing value\n",
      "interpolation_type = 'zero' #'linear' or 'quadratic'\n",
      "interpolate_and_plot(prices, interpolation_type)\n",
      "\n",
      "\n",
      "  > sample  > rolling percent change\n",
      "\n",
      "# Your custom function\n",
      "def percent_change(series):\n",
      "    # Collect all *but* the last value of this window, then the final value\n",
      "    previous_values = series[:-1]\n",
      "    last_value = series[-1]\n",
      "\n",
      "    # Calculate the % difference between the last value and the mean of earlier values\n",
      "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
      "    return percent_change\n",
      "\n",
      "# Apply your custom function and plot\n",
      "prices_perc = prices.rolling(20).aggregate(percent_change)\n",
      "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
      "plt.show()\n",
      "\n",
      "    sample  > apply  replace_outliers\n",
      "\n",
      "def replace_outliers(series):\n",
      "    # Calculate the absolute difference of each timepoint from the series mean\n",
      "    absolute_differences_from_mean = np.abs(series - np.mean(series))\n",
      "\n",
      "    # Calculate a mask for the differences that are > 3 standard deviations from the mean\n",
      "    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n",
      "    \n",
      "    # Replace these values with the median accross the data\n",
      "    series[this_mask] = np.nanmedian(series)\n",
      "    return series\n",
      "\n",
      "# Apply your preprocessing function to the timeseries and plot the results\n",
      "prices_perc = prices_perc.apply(replace_outliers)\n",
      "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "          creating features over time\n",
      "\n",
      "extract features as they change over time\n",
      "\n",
      "feats=prices.rolling(20).aggregate([np.std,np.max]).dropna()\n",
      "print(feats.head())\n",
      "\n",
      "\n",
      "\n",
      "rolling is a rolling window\n",
      "\n",
      "AIG: std, amax\n",
      "ABt: std, amax\n",
      "\n",
      "always plot properties of your features\n",
      "it will help you spot noise data and outliers\n",
      "\n",
      "    >partial function\n",
      "\n",
      "lets you define a new function with parts of the old one\n",
      "\n",
      "\n",
      "from functools import partial\n",
      "\n",
      "mean_over_first_axis = partial(np.mean, axis=0)\n",
      "\n",
      "print(mean_over_first_axis(a))\n",
      "\n",
      "#the mean function always operates on the first axis\n",
      "\n",
      "percentiles give fine grained summaries of your data\n",
      "\n",
      "print(np.percentile(np.linespace(0,200),q=20))\n",
      "\n",
      "percentiles first input is an array\n",
      "q, the second input is an integer between 0 and 100\n",
      "\n",
      "returns the values of the first input as a percentile of the second input\n",
      "\n",
      "\n",
      "data = np.linspace(0,100)\n",
      "\n",
      "percentile_funcs= [partial(np.percentile, q=ii) for ii in [20,40,60]]\n",
      "\n",
      "percentiles = [i_func(data) for i_func in percentile_funcs]\n",
      "print(percentiles)\n",
      "\n",
      "output: [20,40,60]\n",
      "\n",
      "data.rolling(20).aggregrate(percentiles)\n",
      "\n",
      "         Calculating date-based features      \n",
      "\n",
      "statistical features: are numerical features like mean and standard deviation.\n",
      "human features like days of the week, holidays\n",
      "these features can span multiple years.\n",
      "\n",
      "\n",
      "#ensure index is datetime\n",
      "prices.index=pd.to_datetime(prices.index)\n",
      "\n",
      "#extract datetime features\n",
      "\n",
      "day_of_week_num = prices.index.weekday\n",
      "print(day_of_week_num[:10])\n",
      "output:[0 1 2 3 4 0 1 2 3 4]\n",
      "\n",
      "day_of_week = prices.index.weekday_name\n",
      "print(day_of_week[:10])\n",
      "output:['Monday','Tuesday'...'Friday']\n",
      "\n",
      "\n",
      "   >sample  > rolling window   > visualize min, max,mean, std for ebay\n",
      "\n",
      "# Define a rolling window with Pandas, excluding the right-most datapoint of the window\n",
      "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
      "\n",
      "# Define the features you'll calculate for each window\n",
      "features_to_calculate = [np.min, np.max, np.mean, np.std]\n",
      "\n",
      "# Calculate these features for your rolling window object\n",
      "features = prices_perc_rolling.aggregate(features_to_calculate)\n",
      "\n",
      "# Plot the results\n",
      "ax = features.loc[:\"2011-01\"].plot()\n",
      "prices_perc.loc[:\"2011-01\"].plot(ax=ax, color='k', alpha=.2, lw=3)\n",
      "ax.legend(loc=(1.01, .6))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   percentiles and partial functions\n",
      "\n",
      "# Import partial from functools\n",
      "from functools import partial\n",
      "percentiles = [1, 10, 25, 50, 75, 90, 99]\n",
      "\n",
      "# Use a list comprehension to create a partial function for each quantile\n",
      "percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n",
      "\n",
      "# Calculate each of these quantiles on the data using a rolling window\n",
      "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
      "features_percentiles = prices_perc_rolling.aggregate(percentile_functions)\n",
      "\n",
      "# Plot a subset of the result\n",
      "ax = features_percentiles.loc[:\"2011-01\"].plot(cmap=plt.cm.viridis)\n",
      "ax.legend(percentiles, loc=(1.01, .5))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Extract date features from the data, add them as columns\n",
      "prices_perc['day_of_week'] = prices_perc.index.dayofweek\n",
      "prices_perc['week_of_year'] = prices_perc.index.weekofyear\n",
      "prices_perc['month_of_year'] = prices_perc.index.month\n",
      "\n",
      "# Print prices_perc\n",
      "print(prices_perc)\n",
      "\n",
      "\n",
      "       >Feature extraction\n",
      "\n",
      "time series has a linear flow with relationships between the data\n",
      "\n",
      "information in the past can help predict what happens in the future\n",
      "\n",
      "often the features best-suited to predict a timeseries are previous values of the same timeseries\n",
      "\n",
      "the smoothness of the data help determine how correlated a timepoint is with its neighboring timepoints\n",
      "\n",
      "the amount of auto-correlation in data will impact your models\n",
      "\n",
      "data= pd.Series()\n",
      "\n",
      "shifts=[0,1,2,3,4,5,6,7]\n",
      "\n",
      "many_shifts={'lag_{}'.format(ii): data.shift(ii) for ii in shifts}\n",
      "\n",
      "many_shifts=pd.DataFrame(many_shifts)\n",
      "\n",
      "model=Ridge()\n",
      "model.fit(many_shifts,data)\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots()\n",
      "ax.bar(many_shifts.columns, model_coef_)\n",
      "ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
      "\n",
      "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "\n",
      "    Sample  > time shifted features\n",
      "In machine learning for time series, it's common to use information about previous time points to predict a subsequent time point.\n",
      "\n",
      "# These are the \"time lags\"\n",
      "shifts = np.arange(1, 11).astype(int)\n",
      "\n",
      "print(prices_perc)\n",
      "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
      "shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
      "\n",
      "# Convert into a DataFrame for subsequent use\n",
      "prices_perc_shifted = pd.DataFrame(shifted_data)\n",
      "\n",
      "# Plot the first 100 samples of each\n",
      "ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis)\n",
      "prices_perc.iloc[:100].plot(color='r', lw=2)\n",
      "ax.legend(loc='best')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Replace missing values with the median for each column\n",
      "X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
      "y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
      "\n",
      "# Fit the model\n",
      "model = Ridge()\n",
      "model.fit(X, y)\n",
      "\n",
      "def visualize_coefficients(coefs, names, ax):\n",
      "    # Make a bar plot for the coefficients, including their names on the x-axis\n",
      "    ax.bar(names, coefs)\n",
      "    ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
      "    \n",
      "    # Set formatting so it looks nice\n",
      "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "    return ax\n",
      "\n",
      "# Visualize the output data up to \"2011-01\"\n",
      "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
      "y.loc[:'2011-01'].plot(ax=axs[0])\n",
      "\n",
      "# Run the function to visualize model's coefficients\n",
      "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "Increase the data window from 20 to 40\n",
      "\n",
      "As you can see here, by transforming your data with a larger window, you've also changed the relationship between each timepoint and the ones that come just before it. This model's coefficients gradually go down to zero, which means that the signal itself is smoother over time.\n",
      "\n",
      "\n",
      "     >Cross validating time series data\n",
      "\n",
      "KFold is the most common cross validation\n",
      "\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "cv=KFold(n_splits=5)\n",
      "for tr,tt in cv.split(X,y):\n",
      "\n",
      "always visualize your models behavior during cross validation\n",
      "\n",
      "fig, axs = plt.subplots(2,1)\n",
      "\n",
      "axs[0].scatter(tt,[0]*len(tt),marker='_',s=2,lw=40)\n",
      "axs[0].set(ylim=[-.1,.1],title='Test set indices (color=CV loop)', xlabel='Index of raw data')\n",
      "\n",
      "axs[1].plot(model.predict(X[tt]))\n",
      "axs[1].set(title='Test set predictions on each CV loop', xlabel('Prediction index')\n",
      "\n",
      "\n",
      "from sklearn.model_selection import ShuffleSplit\n",
      "\n",
      "cv=ShuffleSplit(n_splits=3)\n",
      "for tr,tt in cv.split(X,y):\n",
      "\n",
      "\n",
      "    >time series cv iterator  (use only the past to validate)\n",
      "\n",
      "1. generally you should not use datapoints in the future to predict data in the past\n",
      "\n",
      "2. Always use training data from the past to predict the future\n",
      "\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "cv=TimeSeriesSplit(n_splits=10)\n",
      "\n",
      "fig,ax=plt.subplots(figsize=(10,5))\n",
      "\n",
      "for ii,(tr,tt) in enumerate(cv.split(X,y)):\n",
      "\tl1=ax.scatter(tr,[ii]*len(tr), c=[plt.cm.coolwarm(.1)],marker='_',lw=6)\n",
      "\n",
      "\tl2=ax.scatter(tt,[ii]*len(tt), c=[plt.cm.coolwarm(.9)],marker='_',lw=6)\n",
      "\n",
      "\tax.set(ylim[10,-1],title='TimeSeriesSplit behavior', xlabel='data index', ylabel='CV iteration')\n",
      "\tax.legend([l1,l2],['Training','Validation'])\n",
      "\n",
      "\n",
      "only the past is use to validate the data\n",
      "\n",
      "def myfunction(estimator, X,y):\n",
      "\ty_pred=estimator.predict(X)\n",
      "\tmy_custom_score=my_custom_function(y_pred,y)\n",
      "\treturn my_custom_score\n",
      "\n",
      "def my_pearsonr(est,X,y):\n",
      "\ty_pred=est.predict(X).squeeze()\n",
      "\tmy_corrcoef_matrix=np.corrcoef(y_pred,y.squeeze())\n",
      "\tmy_corrcoef = my_corrcoef[1,0]\n",
      "\treturn my_corrcoef\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Sample     Shufflesplit    > visualization by time\n",
      "\n",
      "# Import ShuffleSplit and create the cross-validation object\n",
      "from sklearn.model_selection import ShuffleSplit\n",
      "cv = ShuffleSplit(n_splits=3, random_state=1)\n",
      "\n",
      "# Iterate through CV splits\n",
      "results = []\n",
      "\n",
      "for tr, tt in cv.split(X, y):\n",
      "    # Fit the model on training data\n",
      "    model.fit(X[tr], y[tr])\n",
      "    \n",
      "    # Generate predictions on the test data, score the predictions, and collect\n",
      "    prediction = model.predict(X[tt])\n",
      "    score = r2_score(y[tt], prediction)\n",
      "    results.append((prediction, score, tt))\n",
      "\n",
      "# Custom function to quickly visualize predictions\n",
      "visualize_predictions(results)\n",
      "\n",
      "https://goodboychan.github.io/chans_jupyter/python/datacamp/time_series_analysis/machine_learning/2020/06/18/02-Validating-and-Inspecting-Time-Series-Models.html\n",
      "\n",
      "def visualize_predictions(results):\n",
      "    fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
      "\n",
      "    # Loop through our model results to visualize them\n",
      "    for ii, (prediction, score, indices) in enumerate(results):\n",
      "        # Plot the predictions of the model in the order they were generated\n",
      "        offset = len(prediction) * ii\n",
      "        axs[0].scatter(np.arange(len(prediction)) + offset, prediction, \n",
      "                       label='Iteration {}'.format(ii))\n",
      "\n",
      "        # Plot the predictions of the model according to how time was ordered\n",
      "        axs[1].scatter(indices, prediction)\n",
      "    axs[0].legend(loc=\"best\")\n",
      "    axs[0].set(xlabel=\"Test prediction number\", title=\"Predictions ordered by test prediction number\")\n",
      "    axs[1].set(xlabel=\"Time\", title=\"Predictions ordered by time\")\n",
      "\n",
      "\n",
      "            Sample    using KFold\n",
      "\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "# Create KFold cross-validation object\n",
      "from sklearn.model_selection import KFold\n",
      "cv = KFold(n_splits=10, shuffle=False, random_state=1)\n",
      "\n",
      "# Iterate through CV splits\n",
      "results = []\n",
      "for tr, tt in cv.split(X, y):\n",
      "    # Fit the model on training data\n",
      "    model.fit(X[tr],y[tr])\n",
      "    \n",
      "    # Generate predictions on the test data and collect\n",
      "    prediction = model.predict(X[tt])\n",
      "    results.append((prediction, tt))\n",
      "    \n",
      "# Custom function to quickly visualize predictions\n",
      "visualize_predictions(results)\n",
      "\n",
      "#This time, the predictions generated within each CV loop look 'smoother' than they were before - they look more like a real time series because you didn't shuffle the data\n",
      "\n",
      "\n",
      "     Sample   > Timeseries Split\n",
      "\n",
      "# Import TimeSeriesSplit\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "\n",
      "# Create time-series cross-validation object\n",
      "cv = TimeSeriesSplit(n_splits=10)\n",
      "\n",
      "# Iterate through CV splits\n",
      "fig, ax = plt.subplots()\n",
      "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
      "    # Plot the training data on each iteration, to see the behavior of the CV\n",
      "    ax.plot(tr, ii + y[tr])\n",
      "\n",
      "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
      "plt.show()\n",
      "\n",
      "#Note that the size of the training set grew each time when you used the time series cross-validation object\n",
      "\n",
      "\n",
      "      >Stationary and stability\n",
      "\n",
      "a stationary time series is one that does not change their statistical properties over time.\n",
      "\n",
      "most time series are non-stationary to some extent\n",
      "\n",
      "it has the same mean, standard deviation, and trends\n",
      "\n",
      "cross validation to quantify parameter stability\n",
      "\n",
      "calculate model parameter on each iteration\n",
      "\n",
      "assess parameter stability across all cv split\n",
      "\n",
      "bootstrapping is a way to estimate the confidence using the mean of a group of numbers\n",
      "\n",
      "1. take a random sample of data with replacement\n",
      "2. calculate the mean of the sample\n",
      "3. repeat the process many times\n",
      "4. caclulate the percentiles of the result\n",
      "\n",
      "the result is a 95% confidence interval of the mean of each coefficent\n",
      "\n",
      "from sklearn.utils import resample\n",
      "\n",
      "n_boots=100\n",
      "\n",
      "bootstrap_means=np.zeros(n_boots, n_coefficients)\n",
      "for ii in range(n_boots):\n",
      "\trandom_sample=resample(cv_coefficients)\n",
      "\tbootstrap_means[ii]=random_sample.mean(axis=0)\n",
      "\n",
      "percentiles=np.percentiles(bootstrap_means,(2.5,97.5),axis=0)\n",
      "\n",
      "fig,ax=plt.subplots()\n",
      "\n",
      "ax.scatter(many_shifts.columns,percentiles[0], marker='_',s=200)\n",
      "ax.scatter(many_shifts.columns,percentiles[1], marker='_',s=200)\n",
      "\n",
      "this gives an idea of the variability of the mean across all cross validation iterations\n",
      "\n",
      "      Assessing model performance stability\n",
      "\n",
      "if your using the TimeSeriesSplit, you can plot the models score over time.\n",
      "\n",
      "This is helpful to find certain regions of time that hurt the score\n",
      "\n",
      "it is also import to find non-stationary signals\n",
      "\n",
      "\n",
      "def my_corrcoef(est,X,y):\n",
      "\t\"\"\"return the correlation coefficient between model predictions and a validation set\"\"\"\n",
      "\treturn np.corrcoef(y,est,predict(X))[1,0]\n",
      "\n",
      "first_indices=[data.index[tt[0]] for tr,tt in cv.split(X,y)]\n",
      "\n",
      "cv_scores=cross_val_score(model, X,y,cv=cv, scoring=my_corrcoef)\n",
      "cv_scores=pd.Series(cv_scores, index=first_indices)\n",
      "\n",
      "\n",
      "cv.split rturns a ndarray train set indices and test ndarray set indices\n",
      "\n",
      "find the beginning of each validation block using a list comprehension\n",
      "\n",
      "collect the score and convert them into a pandas series\n",
      "\n",
      "visualize the results as a timeseries\n",
      "\n",
      "fig, axs=plt.subplots(2,1, figsize=(10,5), sharex=True)\n",
      "\n",
      "cv_scores_mean=cv_scores.rolling(10, min_periods=1).mean()\n",
      "\n",
      "cv_scores.plot(ax=axs[0])\n",
      "axs[0].set(title='Validation scores (correlation)', ylim=[0,1])\n",
      "\n",
      "data.plot(ax=axs[1])\n",
      "axs[1].set(title='Validation data')\n",
      "\n",
      "   >restrict to the latest time points to be used in training\n",
      "\n",
      "window=100\n",
      "\n",
      "cv=TimeSeries(n_splits=10, max_train_size=window)\n",
      "\n",
      "\n",
      "      >Sample  > boot strap the data    build the function\n",
      "\n",
      "from sklearn.utils import resample\n",
      "\n",
      "def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
      "    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
      "    # Create our empty array to fill the results\n",
      "    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
      "    for ii in range(n_boots):\n",
      "        # Generate random indices for our data *with* replacement, then take the sample mean\n",
      "        random_sample = resample(data)\n",
      "        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
      "        \n",
      "    # Compute the percentiles of choice for the bootstrapped means\n",
      "    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
      "    return percentiles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pyspark pipelines.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pyspark pipelines.txt\n",
      "components of a data platform\n",
      "\n",
      "ingest using singer\n",
      "\n",
      "deploy spark transformation pipelines\n",
      "\n",
      "test your code automatically\n",
      "\n",
      "apply common data cleaning operations\n",
      "\n",
      "gain insights by combining data with pyspark\n",
      "\n",
      "operational systems:\n",
      "1. messaging services\n",
      "2. payment systems\n",
      "3. google analytics\n",
      "4. crm\n",
      "5. clickstream\n",
      "6. location services\n",
      "\n",
      "data lake\n",
      "1. organized by zones\n",
      "(operational data is the landing zone)\n",
      "2. services to clean (clean zone)\n",
      "3. business zone\n",
      "\n",
      "extract transform and load\n",
      "\n",
      "      Singer\n",
      "\n",
      "1. connecting to multiple data sources\n",
      "2. open source\n",
      "\n",
      "communicates using json\n",
      "\n",
      "tap and targets\n",
      "\n",
      "1. schema\n",
      "2. state\n",
      "3. record\n",
      "\n",
      "import singer\n",
      "singer.write_schema(schema=json_schema,\n",
      "\tstream_name='DC_employees',\n",
      "\tkey_properties=[\"id\"]\n",
      "\n",
      "import json\n",
      "\n",
      "json.dumps(json_schema['properties']['age'])\n",
      "\n",
      "with open('foo.json', mode='w') as fh:\n",
      "\tjson.dump(obj=json_schema, fp=fh)\n",
      "\n",
      "#writes the json-serialized object to the open file handle\n",
      "\n",
      "  > sample  > json.dump\n",
      "\n",
      "# Import json\n",
      "import json\n",
      "\n",
      "database_address = {\n",
      "  \"host\": \"10.0.0.5\",\n",
      "  \"port\": 8456\n",
      "}\n",
      "\n",
      "# Open the configuration file in writable mode\n",
      "with open(\"database_config.json\", mode='w') as fh:\n",
      "  # Serialize the object in this file handle\n",
      "  json.dump(obj=database_address, fp=fh) \n",
      "\n",
      "\n",
      "    > sample singer write_schema\n",
      "\n",
      "# Complete the JSON schema\n",
      "schema = {'properties': {\n",
      "    'brand': {'type': 'string'},\n",
      "    'model': {'type': 'string'},\n",
      "    'price': {'type': 'number'},\n",
      "    'currency': {'type': 'string'},\n",
      "    'quantity': {'type': 'integer', 'minimum': 1},  \n",
      "    'date': {'type': 'string', 'format': 'date'},\n",
      "    'countrycode': {'type': 'string', 'pattern': \"^[A-Z]{2}$\"}, \n",
      "    'store_name': {'type': 'string'}}}\n",
      "\n",
      "\n",
      "# Write the schema\n",
      "singer.write_schema(stream_name='products', schema=schema, key_properties=[])\n",
      "\n",
      "\n",
      "     >ingestion pipeline with Singer\n",
      "\n",
      "columns=(\"id\",\"name\",\"age\",\"has_children\")\n",
      "\n",
      "users={\n",
      "(1,\"adrian\",32,False),\n",
      "(2,\"ruanne\",28,True)\n",
      "}\n",
      "\n",
      "\n",
      "singer.write_record(stream_name=\"DC_employees\",\n",
      "record=dict(zip(columns,users.pop())))\n",
      "\n",
      "fixed_dict = {\"type\":\"RECORD\",\"stream\":\"DC_employees\"}\n",
      "\n",
      "record_msg={**fixed_dict,\"record\":dict(zip(columns, users.pop()))}\n",
      "\n",
      "print(json.dumps(record_msg))\n",
      "\n",
      "** unpacks the dictionary\n",
      "\n",
      "\n",
      "import singer\n",
      "\n",
      "singer.write_schema(stream_name=\"foo\", schema=...)\n",
      "\n",
      "singer.write_records(stream_name=\"foo\", records=...)\n",
      "\n",
      "     introduction to pipes\n",
      "\n",
      "python my_tap.py | target-csv\n",
      "\n",
      "python my_tap.py | target-csv --config userconfig.cfg\n",
      "\n",
      "      >keeping track with state messages\n",
      "\n",
      "last_update_on\n",
      "\n",
      "extract after last_update_on\n",
      "\n",
      "update after tap\n",
      "\n",
      "singer.write_state(value={\"max-last-updated-on\": some_variable})\n",
      "\n",
      "\n",
      "Youre running a Singer tap daily at midnight, to synchronize changes between databases. Your tap, called tap-mydelta, extracts only the records that were updated in this database since your last retrieval. To do so, your tap keeps state: it keeps track of the last record it reported on, which can be derived from the tables last_updated_on field.\n",
      "\n",
      "   sample   get from local host rest end point\n",
      "\n",
      "endpoint = \"http://localhost:5000\"\n",
      "\n",
      "# Fill in the correct API key\n",
      "api_key = \"scientist007\"\n",
      "\n",
      "# Create the web APIs URL\n",
      "authenticated_endpoint = \"{}/{}\".format(\"http://localhost:5000\", api_key)\n",
      "\n",
      "print(authenticated_endpoint)\n",
      "\n",
      "# Get the web APIs reply to the endpoint\n",
      "api_response = requests.get(authenticated_endpoint).json()\n",
      "pprint.pprint(api_response)\n",
      "\n",
      "output:\n",
      "\n",
      "<script.py> output:\n",
      "    http://localhost:5000/scientist007\n",
      "    {'apis': [{'description': 'list the shops available',\n",
      "               'url': '<api_key>/diaper/api/v1.0/shops'},\n",
      "              {'description': 'list the items available in shop',\n",
      "               'url': '<api_key>/diaper/api/v1.0/items/<shop_name>'}]}\n",
      "    {'apis': [{'url': '<api_key>/diaper/api/v1.0/shops', 'description': 'list the shops available'}, {'url': '<api_key>/diaper/api/v1.0/items/<shop_name>', 'description': 'list the items available in shop'}]}\n",
      "\n",
      "/shops\n",
      "items/<shop_name>\n",
      "\n",
      "\n",
      "   sample get a list of shops\n",
      "\n",
      "# Create the APIs endpoint for the shops\n",
      "shops_endpoint = \"{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"shops\")\n",
      "\n",
      "shops = requests.get(shops_endpoint).json()\n",
      "print(shops)\n",
      "\n",
      "{'shops': ['Aldi', 'Kruidvat', 'Carrefour', 'Tesco', 'DM']}\n",
      "\n",
      "items_endpoint = \"{}/{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"items\",\"Aldi\")\n",
      "items = requests.get(items_endpoint).json()\n",
      "\n",
      "\n",
      "{'items': [{'countrycode': 'BE', 'brand': 'Diapers-R-Us', 'model': '6months', 'price': 6.8, 'currency': 'EUR', 'quantity': 40, 'date': '2019-02-03'}]}\n",
      "\n",
      "\n",
      "\n",
      "dm\n",
      "\n",
      "{'items': [{'brand': 'Huggies',\n",
      "            'countrycode': 'DE',\n",
      "            'currency': 'EUR',\n",
      "            'date': '2019-02-01',\n",
      "            'model': 'newborn',\n",
      "            'price': 6.8,\n",
      "            'quantity': 40},\n",
      "           {'brand': 'Huggies',\n",
      "            'countrycode': 'AT',\n",
      "            'currency': 'EUR',\n",
      "            'date': '2019-02-01',\n",
      "            'model': 'newborn',\n",
      "            'price': 7.2,\n",
      "            'quantity': 40}]}\n",
      "\n",
      "\n",
      "    sample   > extract the schema\n",
      "\n",
      "\n",
      "# Use the convenience function to query the API\n",
      "tesco_items = retrieve_products(\"Tesco\")\n",
      "\n",
      "singer.write_schema(stream_name=\"products\", schema=schema,\n",
      "                    key_properties=[])\n",
      "\n",
      "\n",
      "# Write a single record to the stream, that adheres to the schema\n",
      "singer.write_record(stream_name=\"products\", \n",
      "            record={**tesco_items[0], 'store_name': \"Tesco\"})\n",
      "\n",
      "\n",
      "      >sample  write the store_name to the database\n",
      "\n",
      "\n",
      "# Use the convenience function to query the API\n",
      "tesco_items = retrieve_products(\"Tesco\")\n",
      "\n",
      "singer.write_schema(stream_name=\"products\", schema=schema,\n",
      "                    key_properties=[])\n",
      "\n",
      "# Write a single record to the stream, that adheres to the schema\n",
      "singer.write_record(stream_name=\"products\", \n",
      "                    record={**tesco_items[0], \"store_name\": \"Tesco\"})\n",
      "\n",
      "for shop in requests.get(SHOPS_URL).json()[\"shops\"]:\n",
      "    # Write all of the records that you retrieve from the API\n",
      "    singer.write_records(\n",
      "      stream_name=\"products\", # Use the same stream name that you used in the schema\n",
      "      records=({**tesco_items[0], \"store_name\": shop}\n",
      "               for item in retrieve_products(shop))\n",
      "    )  \n",
      "\n",
      "    tap to drain\n",
      "\n",
      "tap-marketing-api | target-csv --config data_lake.conf\n",
      "\n",
      "\n",
      "     > spark \n",
      "\n",
      "1. spark sql\n",
      "2. spark streaming\n",
      "3. MLlib\n",
      "4. GraphX\n",
      "\n",
      "api is pyspark\n",
      "\n",
      "data processing at scale\n",
      "interactive analytics\n",
      "\n",
      "validate hypothesis\n",
      "machine learning and score models\n",
      "\n",
      "spark is not used for little data\n",
      "\n",
      "prices.csv\n",
      "\n",
      "ratings.csv\n",
      "\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.getOrCreate()\n",
      "\n",
      "prices-spark.read.\n",
      "options(header=\"true\").\n",
      "csv('mnt/data_lake/landing/prices.csv\")\n",
      "\n",
      "prices.show()\n",
      "\n",
      "\n",
      "from pprint import pprint\n",
      "\n",
      "pprint(prices.dtypes)\n",
      "\n",
      "\n",
      "schema=StructType(\n",
      "StructField(\"store\",StringType(),nullable=False),\n",
      "\n",
      "StructField(\"price\",FloatType(), nullable=False)\n",
      "\n",
      "StructField(\"date\",DateType(),nullable=False\n",
      ")\n",
      "\n",
      "\n",
      "prices-spark.read.\n",
      "options(header=\"true\").\n",
      "schema(schema).\n",
      "csv('mnt/data_lake/landing/prices.csv\")\n",
      "\n",
      "\n",
      "\n",
      "# Read a csv file and set the headers\n",
      "df = (spark.read\n",
      "      .options(header=True)\n",
      "      .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
      "\n",
      "df.show()\n",
      "\n",
      "brand\n",
      "model\n",
      "absorption_rate\n",
      "comfort\n",
      "\n",
      "ham or spam csv\n",
      "\n",
      "https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/sms_spam.csv\n",
      "\n",
      "\n",
      "        read.csv\n",
      "\n",
      "# Read a csv file and set the headers\n",
      "df = (spark.read\n",
      "      .options(header=True)\n",
      "      .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
      "\n",
      "df.show()\n",
      "\n",
      "  >schema\n",
      "\n",
      "# Define the schema\n",
      "schema = StructType([\n",
      "  StructField(\"brand\", StringType(), nullable=False),\n",
      "  StructField(\"model\", StringType(), nullable=False),\n",
      "  StructField(\"absorption_rate\", ByteType(), nullable=True),\n",
      "  StructField(\"comfort\", ByteType(), nullable=True)\n",
      "])\n",
      "\n",
      "better_df = (spark\n",
      "             .read\n",
      "             .options(header=\"true\")\n",
      "             # Pass the predefined schema to the Reader\n",
      "             .schema(schema)\n",
      "             .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
      "pprint(better_df.dtypes)\n",
      "\n",
      "\n",
      "     >cleaning data\n",
      "\n",
      "handling invalid rows\n",
      "\n",
      "prices=(spark.read\n",
      "\t.options(header='true',mode'DROPMALFORMED')\n",
      "\t.csv('landing/prices.csv'))\n",
      "\n",
      "the significance of null\n",
      "1. keep the row\n",
      "2. fill the blanks with null\n",
      "\n",
      "prices.fillna(25, subset=['quantity']).show()\n",
      "\n",
      "\n",
      "   >badly chosen placeholders\n",
      "\n",
      "employees= spark.read.options(header='true').\n",
      "schema(schema).csv('employees.csv')\n",
      "\n",
      "   replace with condition in the when function\n",
      "\n",
      "from pyspark.sql.functions import col, when\n",
      "from datetime import date, timedelta\n",
      "\n",
      "one_year_from_now = date.today().replace(year=date.today().year+1)\n",
      "better_frame = employees.withColumn('end_date',\n",
      "\twhen(col('end_date')> one_year_from_now,None).otherwise(col('end_date')))\n",
      "\n",
      "better_frame.show()\n",
      "\n",
      "none is translated to null\n",
      "\n",
      "bytetype range is -128 to 127\n",
      "\n",
      "\n",
      " > fillna to replace missing values\n",
      "\n",
      "print(\"BEFORE\")\n",
      "ratings.show()\n",
      "\n",
      "print(\"AFTER\")\n",
      "# Replace nulls with arbitrary value on column subset\n",
      "ratings = ratings.fillna(4, subset=[\"comfort\"])\n",
      "ratings.show()\n",
      "\n",
      "   > drop invalid rows\n",
      "PERMISSIVE is the default mode\n",
      "\n",
      "# Specify the option to drop invalid rows\n",
      "ratings = (spark\n",
      "           .read\n",
      "           .options(header=True, mode='DROPMALFORMED')\n",
      "           .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings_with_invalid_rows.csv\"))\n",
      "ratings.show()\n",
      "\n",
      "  >  conditionally replacing data\n",
      "\n",
      "from pyspark.sql.functions import col, when\n",
      "\n",
      "# Add/relabel the column\n",
      "categorized_ratings = ratings.withColumn(\n",
      "    \"comfort\",\n",
      "    # Express the condition in terms of column operations\n",
      "    when(col(\"comfort\") > 3, \"sufficient\").otherwise(\"insufficient\"))\n",
      "\n",
      "categorized_ratings.show()\n",
      "\n",
      "\n",
      "           >Transforming data with spark\n",
      "deriving insights\n",
      "\n",
      "standardizing names and normalizing numerical data\n",
      "\n",
      "filtering rows\n",
      "selecting and renaming columns\n",
      "grouping and aggregation\n",
      "ordering results\n",
      "\n",
      "prices=spark.read.options(header='true').schema(schema).csv('landing/prices.csv')\n",
      "\n",
      "filter is passed boolean values\n",
      "\n",
      "prices_in_beligium = prices.filter(col('countrycode')=='BE).orderBy(col('date'))\n",
      "\n",
      "col creates column objects\n",
      "\n",
      "prices.select(\n",
      "\n",
      "\tcol('store'),\n",
      "\tcol('brand').alias('brandname')\n",
      ").distinct()\n",
      "\n",
      "  > grouping and aggregating\n",
      "\n",
      "(prices\n",
      "\t.groupBy(col('brand'))\n",
      "\t.mean('price')\n",
      ").show()\n",
      "\n",
      "(prices\n",
      "\t.groupBy(col('brand'))\n",
      "\t.agg(\n",
      "\t\tavg('price').alias('average_price')\n",
      "\t\tcount('brand').alias('number_of_times')\n",
      "\t)\n",
      ")\n",
      "\n",
      "\n",
      "  > joining data\n",
      "\n",
      "ratings_with_prices=ratings.join(prices,['brand','model'])\n",
      "\n",
      "     sample    select distinct columns\n",
      "\n",
      "from pyspark.sql.functions import col\n",
      "\n",
      "# Select the columns and rename the \"absorption_rate\" column\n",
      "result = ratings.select([col(\"brand\"),\n",
      "                       col(\"model\"),\n",
      "                       col(\"absorption_rate\").alias('absorbency')])\n",
      "\n",
      "# Show only unique values\n",
      "result.distinct().show()\n",
      "\n",
      "\n",
      "    sample  > agg\n",
      "\n",
      "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
      "\n",
      "aggregated = (purchased\n",
      "              # Group rows by 'Country'\n",
      "              .groupBy(col('Country'))\n",
      "              .agg(\n",
      "                # Calculate the average salary per group and rename\n",
      "                avg('Salary').alias('average_salary'),\n",
      "                # Calculate the standard deviation per group\n",
      "                stddev_samp('Salary'),\n",
      "                # Retain the highest salary per group and rename\n",
      "                sfmax('Salary').alias('highest_salary')\n",
      "              )\n",
      "             )\n",
      "\n",
      "aggregated.show()\n",
      "\n",
      "\n",
      "+-------+--------------+-------------------+--------------+\n",
      "    |Country|average_salary|stddev_samp(Salary)|highest_salary|\n",
      "    +-------+--------------+-------------------+--------------+\n",
      "    |Germany|       63000.0|                NaN|         63000|\n",
      "    | France|       48000.0|                NaN|         48000|\n",
      "    |  Spain|       62000.0| 12727.922061357855|         71000|\n",
      "    +-------+--------------+-------------------+--------------+\n",
      "    \n",
      "\n",
      "Note that the standard deviation column has returned NaN in a few cases. Thats because there werent enough data points for these countries (only one record, so you cant compute a meaningful sample standard deviation), as were only loading a small file in this exercise\n",
      "\n",
      "\n",
      "    >Packaging your application\n",
      "\n",
      "\n",
      "python my_pyspark_data_pipeline.py\n",
      "\n",
      "spark-submit\n",
      "\n",
      "1. sets up launch environment for use with the cluster manager and the selected deploy mode\n",
      "\n",
      "spark-submit\n",
      "\n",
      "\t--master \"local[*]\" \\\n",
      "\t--py-files PY_FILES \\\n",
      "\tMAIN_PYTHON_FILE \\\n",
      "\tapp_arguments\n",
      "\n",
      "zip files\n",
      "\tzip \\\n",
      "\t\t--recurse-path\\\n",
      "\t\tdependencies.zip\n",
      "\t\tpydiaper\n",
      "\n",
      "spark-submit \\\n",
      "\t--py-files dependencies.zip \\\n",
      "\tpydiaper/cleaning/clean_prices.py\n",
      "\n",
      "\n",
      "        >importance of tests\n",
      "\n",
      "1. new functionality desired\n",
      "2. bugs need to get squashed\n",
      "\n",
      "written expectations of the code\n",
      "\n",
      "raises confidence that the code is correct now\n",
      "\n",
      "tests are the most up-to-date form of documentation\n",
      "\n",
      "testing takes time\n",
      "testing have a high return on investment\n",
      "\n",
      "unit tests\n",
      "service tests\n",
      "ui test (end to end tests)\n",
      "\n",
      "   writing unit tests\n",
      "\n",
      "1. Extract\n",
      "2. Transform\n",
      "3. Load\n",
      "\n",
      "transformation is where we add the business logic\n",
      "\n",
      "\n",
      "prices_with_ratings=spark.read.csv()\n",
      "exchange_rates=spark.read.csv()\n",
      "\n",
      "unit_prices_with_ratings = (prices_with_ratings.join() #transform\n",
      ".withColumn())\n",
      "\n",
      "transformations operate on dataframes\n",
      "\n",
      "     >dataframes in memory\n",
      "\n",
      "from pyspark.sql import Row\n",
      "\n",
      "purchase=Row(\"price\",\n",
      "\t\"quantity\",\n",
      "\t\"product\")\n",
      "\n",
      "record=purchase(12.99,1,\"cake\")\n",
      "\n",
      "df=spark.createDataFrame((record,))\n",
      "\n",
      "unit_prices_with_ratings=(prices_with_ratings\n",
      "\t.join(exchange_rates,['currency','date'])\n",
      "\t.withColumn('unit_price_in_euro',\n",
      "\tcol('price')/col('quantity')\n",
      "\t*col('exchange_rate_to_euro'))\n",
      "\n",
      "\n",
      "     create reusable well name functions\n",
      "\n",
      "def link_with_exchange_rates(prices,rates):\n",
      "\treturn prices.join(rates,['currency','date'])\n",
      "\n",
      "def calculate_unit_price_in_euro(df):\n",
      "\treturn\n",
      "\tdf.withColumn('unit_price_in_euro',\n",
      "\tcol('price')/col('quantity')\n",
      "\t*col('exchange_rate_to_euro'))\n",
      "\n",
      "\n",
      "unit_price_with_ratings=(\n",
      "\tcalculate_unit_price_in_euro(\n",
      "\tlink_with_exchange_rates(prices,exchange_rates)\n",
      "\t)\n",
      ")\n",
      "\n",
      "***each transformation can be tested and reduced\n",
      "\n",
      "def test_calculate_unit_price_in_euro():\n",
      "\trecord=dict(price=10,\n",
      "\t\tquantity=5,\n",
      "\t\texchange_rate_to_euro=2.)\n",
      "\n",
      "\tdf=spark.createDataFrame([Row(**record)])\n",
      "\n",
      "\tresult=calculate_unit_price_in_euro(df)\n",
      "\n",
      "\texpected_record=Row(**record, unit_price_in_euro=4.)\n",
      "\texpected=spark.createDateFrame([expected_record])\n",
      "\tassertDataFrameEqual(result,expected)\n",
      "\n",
      "testing framework: pytest\n",
      "\n",
      "create in-memory dataframes makes testing easier because the data is in plain sight\n",
      "focus is on a small number of examples\n",
      "\n",
      "    sample  > in memory dataframe\n",
      "\n",
      "\n",
      "from datetime import date\n",
      "from pyspark.sql import Row\n",
      "\n",
      "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
      "\n",
      "# Create a tuple of records\n",
      "data = (\n",
      "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
      "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
      "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
      ")\n",
      "\n",
      "# Create a DataFrame from these records\n",
      "frame = spark.createDataFrame(data)\n",
      "frame.show()\n",
      "\n",
      "\n",
      "\n",
      "script.py\n",
      "\n",
      "\n",
      "\n",
      "Light mode\n",
      "\n",
      "\n",
      "\n",
      "from datetime import date\n",
      "from pyspark.sql import Row\n",
      " \n",
      "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
      " \n",
      "# Create a tuple of records\n",
      "data = (\n",
      "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
      "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
      "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
      ")\n",
      " \n",
      "# Create a DataFrame from these records\n",
      "frame = spark.createDataFrame(data)\n",
      "frame.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Run Code\n",
      "Submit Answer\n",
      "\n",
      "\n",
      "IPython Shell\n",
      "\n",
      "\n",
      "Slides\n",
      "\n",
      "\n",
      "\n",
      "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
      "\n",
      "aggregated = (purchased\n",
      "              # Group rows by 'Country'\n",
      "              .groupBy(col('Country'))\n",
      "              .agg(\n",
      "                # Calculate the average salary per group and rename\n",
      "                avg('Salary').alias('average_salary'),\n",
      "                # Calculate the standard deviation per group\n",
      "                stddev_samp('Salary'),\n",
      "                # Retain the highest salary per group and rename\n",
      "                sfmax('Salary').alias('highest_salary')\n",
      "              )\n",
      "             )\n",
      "\n",
      "aggregated.show()\n",
      "from datetime import date\n",
      "from pyspark.sql import Row\n",
      "\n",
      "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
      "\n",
      "# Create a tuple of records\n",
      "data = (\n",
      "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
      "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
      "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
      ")\n",
      "\n",
      "# Create a DataFrame from these records\n",
      "frame = spark.createDataFrame(data)\n",
      "frame.show()\n",
      "+-------+------------+------------------+----------+----------+\n",
      "|country|utm_campaign|airtime_in_minutes|start_date|  end_date|\n",
      "+-------+------------+------------------+----------+----------+\n",
      "|    USA|DiapersFirst|                28|2017-01-20|2017-01-27|\n",
      "|Germany|  WindelKind|                31|2017-01-25|      null|\n",
      "|  India|CloseToCloth|                32|2017-01-25|2017-02-02|\n",
      "+-------+------------+------------------+----------+----------+\n",
      "\n",
      "    sample\t\n",
      "\n",
      "pipenv run pytest\n",
      "\n",
      "\n",
      "from .chinese_provinces_improved import \\\n",
      "    aggregate_inhabitants_by_province\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.types import StructType, \\\n",
      "    StructField, StringType, LongType, BooleanType\n",
      "\n",
      "\n",
      "def test_aggregate_inhabitants_by_province():\n",
      "    \"\"\"The number of inhabitants per province should be aggregated,\n",
      "    regardless of their distinctive features.\n",
      "    \"\"\"\n",
      "\n",
      "    spark = SparkSession.builder.getOrCreate()\n",
      "\n",
      "    fields = [\n",
      "        StructField(\"country\", StringType(), True),\n",
      "        StructField(\"province\", StringType(), True),\n",
      "        StructField(\"inhabitants\", LongType(), True),\n",
      "        StructField(\"foo\", BooleanType(), True),  # distinctive features\n",
      "    ]\n",
      "\n",
      "    frame = spark.createDataFrame({\n",
      "        (\"China\", \"A\", 3, False),\n",
      "        (\"China\", \"A\", 2, True),\n",
      "        (\"China\", \"B\", 14, False),\n",
      "        (\"US\", \"A\", 4, False)},\n",
      "        schema=StructType(fields)\n",
      "    )\n",
      "    actual = aggregate_inhabitants_by_province(frame).cache()\n",
      "\n",
      "    # In the older implementation, the data was first filtered for a specific\n",
      "    # country, after which you'd aggregate by province. The same province\n",
      "    # name could occur in multiple countries though.\n",
      "    # This test is expecting the data to be grouped by country,\n",
      "    # then province from aggregate_inhabitants_by_province()\n",
      "    expected = spark.createDataFrame(\n",
      "        {(\"China\", \"A\", 5), (\"China\", \"B\", 14), (\"US\", \"A\", 4)},\n",
      "        schema=StructType(fields[:3])\n",
      "    ).cache()\n",
      "\n",
      "    assert actual.schema == expected.schema, \"schemas don't match up\"\n",
      "    assert sorted(actual.collect()) == sorted(expected.collect()),\\\n",
      "        \"data isn't equal\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " > improvements\n",
      "\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import col, lower, sum\n",
      "\n",
      "from .catalog import catalog\n",
      "\n",
      "\n",
      "def extract_demographics(sparksession, catalog):\n",
      "    return sparksession.read.parquet(catalog[\"clean/demographics\"])\n",
      "\n",
      "\n",
      "def store_chinese_demographics(frame, catalog):\n",
      "    frame.write.parquet(catalog[\"business/chinese_demographics\"])\n",
      "\n",
      "\n",
      "# Improved aggregation function, grouped by country and province\n",
      "def aggregate_inhabitants_by_province(frame):\n",
      "    return (frame\n",
      "            .groupBy(\"province\")\n",
      "            .agg(sum(col(\"inhabitants\")).alias(\"inhabitants\"))\n",
      "            )\n",
      "\n",
      "\n",
      "def main():\n",
      "    spark = SparkSession.builder.getOrCreate()\n",
      "    frame = extract_demographics(spark, catalog)\n",
      "    chinese_demographics = frame.filter(lower(col(\"country\")) == \"china\")\n",
      "    aggregated_demographics = aggregate_inhabitants_by_province(\n",
      "        chinese_demographics\n",
      "    )\n",
      "    store_chinese_demographics(aggregated_demographics, catalog)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "\n",
      "      continuous testing     \n",
      "\n",
      "\n",
      "unittest\n",
      "pytest\n",
      "doctest\n",
      "nose\n",
      "\n",
      "assert or raise\n",
      "\n",
      "assert compute == expected\n",
      "\n",
      "report which test passed and which ones failed\n",
      "\n",
      "automation is one of the objectives of a data engineer\n",
      "\n",
      "ci/cd pipeline\n",
      "\n",
      "continuous integration\n",
      "1. get code changes integrated with the master branch regularly\n",
      "\n",
      "continuous delivery\n",
      "1. Create artifacts (deliverables like documentation, but also programs) that can be deployed into production without breaking things\n",
      "\n",
      "cicleci - run tests automatically for you\n",
      "\n",
      "circleci looks for .circleci/config.yml\n",
      "1. has a section called jobs\n",
      "\n",
      "jobs:\n",
      "\ttest:\n",
      "\t\tdocker:\n",
      "\t\t\t-image:circleci/python:3.6.4\n",
      "\t\tsteps:\n",
      "\t\t\t-checkout\n",
      "\t\t\t-run: pip install -r requirements.txt\n",
      "\t\t\t-run: pytest\n",
      "\n",
      "\n",
      "cicleci\n",
      "1. checkout code\n",
      "2. install test & build requirements\n",
      "3. run tests\n",
      "\n",
      "\n",
      "order\n",
      "1. check out your application from version control\n",
      "2. install your python application dependencies\n",
      "3. run the test suite of your application\n",
      "4. create artifacts\n",
      "5. save the artifacts to location accessible by your company's compute infrastructure\n",
      "\n",
      "\n",
      "\n",
      "Add flake8 to the development section in the Pipfile, which is in the projects root folder. This file serves a similar purpose as the requirements.txt files you might have seen in other Python projects. It solves some problems with those though. To add flake8 correctly, look at the line that mentions pytest.\n",
      "\n",
      "\n",
      "      Modern day workflow management\n",
      "\n",
      "\n",
      "sequence of tasks scheduled to be run\n",
      "a task can be trigger by a sequence of event\n",
      "\n",
      "schedule or triggered\n",
      "\n",
      "scheduled with cron\n",
      "\n",
      "reads crontab files\n",
      "\n",
      "#Minutes hours Days Months Day of the week Command\n",
      "\n",
      "*/15 9-17 * * 1-3,5 log_my_activity\n",
      "1. one task per line\n",
      "2. launch my process, log my activity at a specific time\n",
      "3. every fifteen minutes between normal office hours, ever day of the month, for every month, \n",
      "Mon, tues, wednesday, and fridays\n",
      "\n",
      "your can add comments\n",
      "\n",
      "other tools\n",
      "1. luigi\n",
      "2. azkaban\n",
      "3. airflow\n",
      "\n",
      "apache airflow fulfills modern engineering needs\n",
      "1. create and visualize complex workflows\n",
      "2. monitor and log workflows\n",
      "3. scales horizontally (work with other machines)\n",
      "\n",
      "      >The directed Acyclic Graph (DAG)\n",
      "\n",
      "1. nodes are connected by edges\n",
      "2. the edge denote a sense of direction on the nodes\n",
      "3. Acyclic means there is no way to circle back to the same node\n",
      "4. The nodes are operators\n",
      "\n",
      "from airflow import DAG\n",
      "\n",
      "my_dag = DAG(\n",
      "\tdag_id=\"publish_logs\",\n",
      "\tschedule_interval=\"* * * * *\",\n",
      "\tstate_date=datetime(2010,1,1)\n",
      "\n",
      ")\n",
      "\n",
      "BashOperator (bash script)\n",
      "Pythonoperator (python script)\n",
      "SparkSubmitOperator\n",
      "\n",
      "    > defining dependencies between task is established using set_downstream and set_upstream operators\n",
      "\n",
      "task1.set_downstream(task2)\n",
      "task3.set_upstream(task2)\n",
      "\n",
      "\n",
      "\n",
      "    Dag schedule job\n",
      "\n",
      "schedule interval: * default\n",
      "\n",
      "minute\n",
      "hour\n",
      "day of the month\n",
      "day of the week\n",
      "\n",
      "\n",
      "from datetime import datetime\n",
      "from airflow import DAG\n",
      "\n",
      "reporting_dag = DAG(\n",
      "    dag_id=\"publish_EMEA_sales_report\", \n",
      "    # Insert the cron expression\n",
      "    schedule_interval=\"0 7 * * 1\",\n",
      "    start_date=datetime(2019, 11, 24),\n",
      "    default_args={\"owner\": \"sales\"}\n",
      ")\n",
      "\n",
      "# Specify direction using verbose method\n",
      "prepare_crust.set_downstream(apply_tomato_sauce)\n",
      "\n",
      "tasks_with_tomato_sauce_parent = [add_cheese, add_ham, add_olives, add_mushroom]\n",
      "\n",
      "for task in tasks_with_tomato_sauce_parent:\n",
      "    # Specify direction using verbose method on relevant task\n",
      "    apply_tomato_sauce.set_downstream(task)\n",
      "\n",
      "# Specify direction using bitshift operator\n",
      "tasks_with_tomato_sauce_parent   bake_pizza\n",
      "\n",
      "# Specify direction using verbose method\n",
      "bake_pizza.set_upstream(prepare_oven)\n",
      "\n",
      "a.set_downstream(b) means b must be executed after a.\n",
      "a   b also means b must be executed after a.\n",
      "b.set_upstream(a) means a must be executed before b.\n",
      "b << a also means a must be executed before b.\n",
      "\n",
      "Set prepare_crust to precede apply_tomato_sauce using the appropriate method.\n",
      "\n",
      "Set apply_tomato_sauceto precede each of tasks in tasks_with_tomato_sauce_parent using the appropriate method.\n",
      "Set the tasks_with_tomato_sauce_parent list to precede bake_pizza using either the bitshift operator   or <<.\n",
      "Set bake_pizza to succeed prepare_oven using the appropriate method.\n",
      "\n",
      "   >sample\n",
      "\n",
      "# Specify direction using verbose method\n",
      "prepare_crust.set_downstream(apply_tomato_sauce)\n",
      "\n",
      "tasks_with_tomato_sauce_parent = [add_cheese, add_ham, add_olives, add_mushroom]\n",
      "for task in tasks_with_tomato_sauce_parent:\n",
      "    # Specify direction using verbose method on relevant task\n",
      "    apply_tomato_sauce.set_downstream(task)\n",
      "\n",
      "# Specify direction using bitshift operator\n",
      "tasks_with_tomato_sauce_parent   bake_pizza\n",
      "\n",
      "# Specify direction using verbose method\n",
      "bake_pizza.set_upstream(prepare_oven)\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['datetime']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
