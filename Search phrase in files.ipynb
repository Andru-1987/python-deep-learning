{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\dimensions reduction techniques.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\dimensions reduction techniques.txt\n",
      "df.shape\n",
      "each row should be an observation\n",
      "\n",
      "remove columns with vary little variance\n",
      "user pd.describe()\n",
      "\n",
      "the column generation had a std of 0 and min and max values that were the same.  you can drop the generation column.\n",
      "\n",
      "pd.describe(exclude='number')\n",
      "\n",
      "describes only non numeric columns\n",
      "\n",
      "  sample    combine list of column names\n",
      "\n",
      "# Remove the feature without variance from this list\n",
      "number_cols = ['HP', 'Attack', 'Defense']\n",
      "\n",
      "# Leave this list as is for now\n",
      "non_number_cols = ['Name', 'Type', 'Legendary']\n",
      "\n",
      "print(pokemon_df.columns)\n",
      "# Sub-select by combining the lists with chosen features\n",
      "df_selected = pokemon_df[number_cols + non_number_cols]\n",
      "\n",
      "# Prints the first 5 lines of the new dataframe\n",
      "print(df_selected.head())\n",
      "\n",
      "print(df_selected.describe(exclude='number'))\n",
      "\n",
      "#remove the columns with almost all similarities\n",
      "\n",
      "\n",
      "       >Reducing dimensionality\n",
      "\n",
      "your dataset will be less complex\n",
      "your dataset will require less disk space\n",
      "\n",
      "training will require less computation time\n",
      "you will have less of chance of overfitting.\n",
      "\n",
      "decide on which features are important\n",
      "\n",
      "dropping a column\n",
      "insurance_df.drop('favorite color', axis=1)\n",
      "\n",
      "       Exploring the dataset\n",
      "\n",
      "sns.pairplot(ansur_df, hue='gender', diag_kind='hist')\n",
      "\n",
      "it provides an one by one comparison of all numeric columns in the dataframe as a scatter plot\n",
      "\n",
      "removing features with very little information prevents information loss.\n",
      "\n",
      "Extract new features from the existing features\n",
      "\n",
      "pca\n",
      "\n",
      "\n",
      "   >sample using pairplot\n",
      "\n",
      "# Create a pairplot and color the points using the 'Gender' feature\n",
      "sns.pairplot(ansur_df_1,hue='Gender', kind='reg', diag_kind='hist')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   sample  > remove stature_m\n",
      "#US Army ANSUR body measurement dataset\n",
      "\n",
      "print(ansur_df_1.columns)\n",
      "# Remove one of the redundant features\n",
      "reduced_df = ansur_df_1.drop('stature_m', axis=1)\n",
      "\n",
      "# Create a pairplot and color the points using the 'Gender' feature\n",
      "sns.pairplot(reduced_df, hue='Gender')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "Index(['Gender', 'footlength', 'headlength', 'n_legs'], dtype='object')\n",
      "\n",
      "  sample remove n_legs which has low variance\n",
      "\n",
      "# Remove the redundant feature\n",
      "reduced_df = ansur_df_2.drop('n_legs', axis=1)\n",
      "\n",
      "# Create a pairplot and color the points using the 'Gender' feature\n",
      "sns.pairplot(reduced_df, hue='Gender', diag_kind='hist')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "         >t-SNE visualization\n",
      "\n",
      "\n",
      "t-SNE is a way to visual high dimensional data using feature extraction\n",
      "\n",
      "t-SNE maximize distance in 2 dimensional space that are different in high dimensional space\n",
      "\n",
      "items that are close to each other may cluster\n",
      "\n",
      "non_numericnon_numeric=['BMI_class','Height_class','Gender','Component','Branch']\n",
      "\n",
      "df_numeric=df.drop(non_numeric,axis=1)\n",
      "\n",
      "df_numeric.shape\n",
      "\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "\n",
      "m=TSNE(learning_rate=50)\n",
      "\n",
      "learning rates 10 to 1000 range\n",
      "\n",
      "tnse_features = m.fit_transform(df_numeric)\n",
      "tsne_features[1:4,:]\n",
      "\n",
      "sns.scatterplot(x='x',y='y', hue='BMI_class', data=df)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "bmi_class: Overweight, normal, underweight\n",
      "\n",
      "\n",
      "Over weight 25 to 29.9\n",
      "Normal weight 18.5 to 24.9\n",
      "Under weight 18.5 or less\n",
      "\n",
      "weight in lbs * 703/ heightin**2\n",
      "\n",
      "\n",
      "Tall >5'9\n",
      "Normal >5'4 to <5'9\n",
      "short <5'4\n",
      "\n",
      "  > Sample tsne  higher dimensional view of the data\n",
      "\n",
      "# Non-numerical columns in the dataset\n",
      "non_numeric = ['Branch', 'Gender', 'Component']\n",
      "\n",
      "# Drop the non-numerical columns from df\n",
      "df_numeric = df.drop(non_numeric, axis=1)\n",
      "\n",
      "# Create a t-SNE model with learning rate 50\n",
      "m = TSNE(learning_rate=50)\n",
      "\n",
      "# Fit and transform the t-SNE model on the numeric dataset\n",
      "tsne_features = m.fit_transform(df_numeric)\n",
      "print(tsne_features.shape)\n",
      "\n",
      "# Color the points according to Army Component\n",
      "sns.scatterplot(x=\"x\", y=\"y\", hue=\"Component\", data=df)\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "    The curse of dimensionality\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc= SVC()\n",
      "\n",
      "svc.fit(X_train, y_train)\n",
      "print(accuracy_score(y_test,svc.predict(X_test))\n",
      "\n",
      "print(accuracy_score(y_train, svc.predict(X_train))\n",
      "\n",
      "\n",
      "features: city, price, n_floors, n_bathrooms, surface_m2\n",
      "\n",
      "increase the number of observations to ensure generalization.  otherwise the model memorize the smaller training set overfitting and it does not generalize well.\n",
      "\n",
      "observations should increase exponentially with the number of features\n",
      "\n",
      "this is called the curse of dimensionality\n",
      "\n",
      "\n",
      "  Sample load and split train and test\n",
      "\n",
      "# Import train_test_split()\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Select the Gender column as the feature to be predicted (y)\n",
      "y = ansur_df['Gender']\n",
      "\n",
      "# Remove the Gender column to create the training data\n",
      "X = ansur_df.drop('Gender', axis=1)c\n",
      "\n",
      "# Perform a 70% train and 30% test data split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
      "\n",
      "print(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))\n",
      "\n",
      "\n",
      "   sample  > fit and predict using svc\n",
      "\n",
      "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Create an instance of the Support Vector Classification class\n",
      "svc = SVC()\n",
      "\n",
      "# Fit the model to the training data\n",
      "svc.fit(X_train, y_train)\n",
      "\n",
      "# Calculate accuracy scores on both train and test data\n",
      "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
      "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
      "\n",
      "print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
      "\n",
      "output: 49.7% accuracy\n",
      "\n",
      "\n",
      "        >features with missing values or little variance\n",
      "\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "\n",
      "sel = VarianceThreshold(threshold=1)\n",
      "sel.fit(ansur_df)\n",
      "\n",
      "mask=sel.get_support()\n",
      "print(mask)\n",
      "\n",
      "reduced_df=ansur_df.loc[:,mask]\n",
      "\n",
      "print(reduced_df.shape)\n",
      "\n",
      "\n",
      "   >normalize the variance\n",
      "\n",
      "sel=VarianceThreshold(threshold=0.005)\n",
      "set.fit(ansur_df / ansur_df.mean())\n",
      "\n",
      "\n",
      "   missing values     >.repairing\n",
      "\n",
      "df.isna().sum()\n",
      "\n",
      "df.isna().sum()/len(df)\n",
      "\n",
      "mask=df.isna().sum()/len(df)<0.3\n",
      "\n",
      "reduced_df=df.loc[:,mask]\n",
      "\n",
      "reduced_df.head()\n",
      "\n",
      "\n",
      "  >sample   > create boxplot\n",
      "\n",
      "# Create the boxplot\n",
      "head_df.boxplot()\n",
      "\n",
      "\n",
      "   sample  > boxplot   normalize   print the variance\n",
      "\n",
      "# Normalize the data\n",
      "normalized_df = head_df / head_df.mean()\n",
      "\n",
      "# Print the variances of the normalized data\n",
      "print(normalized_df.var())\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  >sample  > remove columns with low variance\n",
      "\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "\n",
      "# Create a VarianceThreshold feature selector\n",
      "sel = VarianceThreshold(threshold=0.001)\n",
      "\n",
      "# Fit the selector to normalized head_df\n",
      "sel.fit(head_df / head_df.mean())\n",
      "\n",
      "# Create a boolean mask\n",
      "mask = sel.get_support()\n",
      "\n",
      "# Apply the mask to create a reduced dataframe\n",
      "reduced_df = head_df.loc[:, mask]\n",
      "\n",
      "print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))\n",
      "\n",
      "  sample remove the missing values using a mask\n",
      "\n",
      "# Create a boolean mask on whether each feature less than 50% missing values.\n",
      "mask = school_df.isna().sum() / len(school_df) < 0.5\n",
      "\n",
      "# Create a reduced dataset by applying the mask\n",
      "reduced_df = school_df.loc[:,mask]\n",
      "\n",
      "print(school_df.shape)\n",
      "print(reduced_df.shape)\n",
      "\n",
      "\n",
      "            >Pairwise correlation\n",
      "\n",
      "sns.pairplot(ansur, hue=gender)\n",
      "\n",
      "strength of correlation coefficient\n",
      "\n",
      "r=-1 and r=0  and r=1\n",
      "\n",
      "\n",
      "-1 is perfectly negative correlation\n",
      "1 is perfectly postive correlation\n",
      "0 is no correlation\n",
      "\n",
      "weights_df_corr()\n",
      "\n",
      "the dialog tells us that each feature is perfectly correlated to itself\n",
      "\n",
      "visual the correlation using the seaborn heatmap\n",
      "\n",
      "cmap=sns.diverging_palette(h_neg=10, h_pos=240, as_cmap=True)\n",
      "\n",
      "sns.heatmap(weights_df.corr(), center=0, cmap=cmap, linewidths=1,\n",
      "annot=True, fmt=\".2f\")\n",
      "\n",
      " > remove the diagonal feature referencing itself\n",
      "\n",
      "corr= weights_df.corr()\n",
      "\n",
      "mask=np.triu(np.ones_like(corr,dtype=bool))\n",
      "\n",
      "remove\n",
      "SubjectNumericRace\n",
      "DODRace\n",
      "\n",
      "\n",
      "  sample  > create a heatmap of the correlation\n",
      "\n",
      "# Create the correlation matrix\n",
      "corr = ansur_df.corr()\n",
      "\n",
      "# Draw the heatmap\n",
      "sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  sample  > add a mask\n",
      "\n",
      "# Create the correlation matrix\n",
      "corr = ansur_df.corr()\n",
      "\n",
      "# Generate a mask for the upper triangle\n",
      "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
      "\n",
      "\n",
      "sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\",mask=mask)\n",
      "plt.show()\n",
      "\n",
      "  > removing highly correlated features\n",
      "\n",
      "\n",
      "-1 and 1 and 0\n",
      "\n",
      "drop features that are close to 1 or -1\n",
      "\n",
      "cervical height and suprastermale height\n",
      "chest height and suprastermale height\n",
      "chest height and cericale height\n",
      "\n",
      "\n",
      "corr_df=chest_df.corr().abs()\n",
      "mask=np.triu(np.ones_like(corr_df,dtype=bool))\n",
      "\n",
      "\n",
      "tri_df=corr_matrix.mask(mask)\n",
      "\n",
      "to_drop=[c for c in tri_df.columns if any(tri_df[c]>0.95)]\n",
      "\n",
      "print(to_drop)\n",
      "\n",
      "reduced_df=chest_df.drop(to_drop,axis=1)\n",
      "\n",
      "\n",
      "  > sample  > dropping highly correlated features from the dataframe\n",
      "\n",
      "# Calculate the correlation matrix and take the absolute value\n",
      "corr_matrix = ansur_df.corr().abs()\n",
      "\n",
      "# Create a True/False mask and apply it\n",
      "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
      "tri_df = corr_matrix.mask(mask)\n",
      "\n",
      "# List column names of highly correlated features (r > 0.95)\n",
      "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
      "\n",
      "# Drop the features in the to_drop list\n",
      "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
      "\n",
      "print(\"The reduced dataframe has {} columns.\".format(reduced_df.shape[1]))\n",
      "\n",
      "\n",
      "  >predict gender\n",
      "\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler = StandardScaler()\n",
      "\n",
      "X_train_std= scaler.fit_transform(X_train)\n",
      "\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "lr=LogisticRegression()\n",
      "lr.fit(X_train_std, y_train)\n",
      "\n",
      "X_test_std= scaler.transform(X_test)\n",
      "\n",
      "y_pred=lr.predict(X_test_std)\n",
      "print(accurancy_score(y_test, y_pred))\n",
      "\n",
      "print(lr.coef_)\n",
      "\n",
      "output: array[[-3, 0.14, 7.46, 1.22, 0.87]])\n",
      "\n",
      "coefficients close to zero will contribute little to the end result\n",
      "\n",
      "print(dict(zip(X.column, abs(lr.coef_[0]))))\n",
      "\n",
      "{'chestdepth': 3.0,\n",
      "'handlength':0.14,\n",
      "'neckcircumference':7.46,\n",
      "'shoulderlength':1.22,\n",
      "'earlength':0.87\n",
      "}\n",
      "\n",
      "remove handlength\n",
      "\n",
      "         >Recursive Feature Elimination\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "\n",
      "rfe=RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n",
      "\n",
      "scaler = StandardScaler()\n",
      "X_train_std= scaler.fit_transform(X_train)\n",
      "\n",
      "rfe.fit(X_train_std, y_train)\n",
      "\n",
      "X.columns[rfe.support_]\n",
      "\n",
      "print(dict(zip(X.columns,rfe.ranking_)))\n",
      "\n",
      "high values mean the feature was dropped early on\n",
      "\n",
      "\n",
      "  > Sample  > test features contribution using logistic regression\n",
      "#Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression\n",
      "\n",
      "# Fit the scaler on the training features and transform these in one go\n",
      "X_train_std = scaler.fit_transform(X_train)\n",
      "\n",
      "# Fit the logistic regression model on the scaled training data\n",
      "lr=LogisticRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "\n",
      "# Scale the test features\n",
      "X_test_std = scaler.transform(X_test)\n",
      "\n",
      "# Predict diabetes presence on the scaled test set\n",
      "y_pred = lr.predict(X_test_std)\n",
      "\n",
      "# Prints accuracy metrics and feature coefficients\n",
      "print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \n",
      "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
      "\n",
      "79.6% accuracy on test set.\n",
      "{'pregnant': 0.04, 'glucose': 1.23, 'diastolic': 0.03, 'triceps': 0.24, 'insulin': 0.19, 'bmi': 0.38, 'family': 0.34, 'age': 0.34}\n",
      "\n",
      "\n",
      " >sample  > remove diastolic\n",
      "\n",
      "# Remove the feature with the lowest model coefficient\n",
      "X = diabetes_df[['pregnant', 'glucose',  'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
      "\n",
      "# Performs a 25-75% train test split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
      "\n",
      "# Scales features and fits the logistic regression model\n",
      "lr.fit(scaler.fit_transform(X_train), y_train)\n",
      "\n",
      "# Calculates the accuracy on the test set and prints coefficients\n",
      "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
      "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
      "\n",
      "\n",
      "\n",
      "  Sample  > RFE   > dropping feature columns\n",
      "\n",
      "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
      "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n",
      "\n",
      "# Fits the eliminator to the data\n",
      "rfe.fit(X_train, y_train)\n",
      "\n",
      "# Print the features and their ranking (high = dropped early on)\n",
      "print(dict(zip(X.columns, rfe.ranking_)))\n",
      "\n",
      "# Print the features that are not eliminated\n",
      "print(X.columns[rfe.support_])\n",
      "\n",
      "# Calculates the test set accuracy\n",
      "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
      "\n",
      "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
      "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n",
      "\n",
      "# Fits the eliminator to the data\n",
      "rfe.fit(X_train, y_train)\n",
      "\n",
      "# Print the features and their ranking (high = dropped early on)\n",
      "print(dict(zip(X.columns, rfe.ranking_)))\n",
      "\n",
      "# Print the features that are not eliminated\n",
      "print(X.columns[rfe.support_])\n",
      "\n",
      "# Calculates the test set accuracy\n",
      "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
      " \n",
      "\n",
      "{'pregnant': 5, 'glucose': 1, 'diastolic': 6, 'triceps': 3, 'insulin': 4, 'bmi': 1, 'family': 2, 'age': 1}\n",
      "Index(['glucose', 'bmi', 'age'], dtype='object')\n",
      "80.6% accuracy on test set.\n",
      "\n",
      "\n",
      "diastolic and pregnant dropped early\n",
      "\n",
      "tricept and bmi\n",
      "insulin and glucose\n",
      "\n",
      "        >Random forest classifer\n",
      "\n",
      "ensemble of multiple decision trees \n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "rf=RandomForestClassifier()\n",
      "rf.fit(X_train, y_train)\n",
      "\n",
      "print(rf.feature_importances_)\n",
      "print(sum(rf.feature_importances_))\n",
      "\n",
      "#always sum to 1\n",
      "\n",
      "\n",
      "mask=rf.feature_importances_ > 0.1\n",
      "\n",
      "print(mask)\n",
      "\n",
      "X_reduced=X.loc[:,mask]\n",
      "print(X_reduced.columns)\n",
      "\n",
      "  >drop the least 10 important features at a cycle\n",
      "\n",
      "rfe=RFE(esimator=RandomForestClassifier(),\n",
      "n_features_to_select=6, step=10, verbose=1)\n",
      "\n",
      "#drop the least 10 important features at a cycle\n",
      "\n",
      "print(X.columns[rfe.support_])\n",
      "\n",
      "#contains the remaining features in the model\n",
      "\n",
      "  >sample  > use a randomforestclassifier to determine feature importance\n",
      "\n",
      "# Perform a 75% training and 25% test data split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
      "\n",
      "# Fit the random forest model to the training data\n",
      "rf = RandomForestClassifier(random_state=0)\n",
      "rf.fit(X_train, y_train)\n",
      "\n",
      "# Calculate the accuracy\n",
      "acc = accuracy_score(y_test, rf.predict(X_test))\n",
      "\n",
      "# Print the importances per feature\n",
      "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
      "\n",
      "# Print accuracy\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
      "\n",
      "\n",
      "{'pregnant': 0.09, 'glucose': 0.21, 'diastolic': 0.08, 'triceps': 0.11, 'insulin': 0.13, 'bmi': 0.09, 'family': 0.12, 'age': 0.16}\n",
      "77.6% accuracy on test set.\n",
      "\n",
      "\n",
      "   >sample   > measure feature importances\n",
      "\n",
      "# Create a mask for features importances above the threshold\n",
      "mask = rf.feature_importances_>0.15\n",
      "\n",
      "# Prints out the mask\n",
      "print(mask)\n",
      "\n",
      "mask = rf.feature_importances_ > 0.15\n",
      "\n",
      "# Apply the mask to the feature dataset X\n",
      "reduced_X = X.loc[:,mask]\n",
      "\n",
      "# prints out the selected column names\n",
      "print(reduced_X.columns)\n",
      "\n",
      "output:  'glucose', 'age'\n",
      "\n",
      "   >sample  > RFE\n",
      "\n",
      "# Wrap the feature eliminator around the random forest model\n",
      "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
      "\n",
      "# Fit the model to the training data\n",
      "rfe.fit(X_train, y_train)\n",
      "\n",
      "# Create a mask using an attribute of rfe\n",
      "mask = rfe.support_\n",
      "\n",
      "# Apply the mask to the feature dataset X and print the result\n",
      "reduced_X = X.loc[:, mask]\n",
      "print(reduced_X.columns)\n",
      "\n",
      "output: Index(['glucose', 'insulin'], dtype='object')\n",
      "\n",
      "\n",
      "   Linear regressor\n",
      "linear moe\n",
      "x1,x2,x3 target y \n",
      "where y is a contineous value\n",
      "\n",
      "normal distribution\n",
      "\n",
      "the coefficients determine the affect the feature has on the target\n",
      "\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lr=LinearRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "\n",
      "print(lr.coef_)\n",
      "\n",
      "print(lr.intercept_)\n",
      "\n",
      "r2 tells us the variance of prediction and whether the data is linear or non linear\n",
      "\n",
      "the model tries to fit through the data by minimizing the loss function  (MSE)\n",
      "\n",
      "mse or mean square error creates the linear line through your data.  r2 tells you if the linear regressor is linear or non linear.  regularization helps reduce overfit of the data by smoothing your distribution to look more guassian.\n",
      "\n",
      "regularization will try to keep the model simple by keeping the coefficients low\n",
      "\n",
      "if the model is too low it might overfit, if the model is too high it might become inaccurate\n",
      "\n",
      "\n",
      "la = Lasso()\n",
      "la.fit(X_train, y_train)\n",
      "\n",
      "print(la.coef_)\n",
      "\n",
      "change the alpha\n",
      "la=Lasso(alpha=0.05)\n",
      "\n",
      "output: [4.91 1.76 0]\n",
      "\n",
      "\n",
      "  > sample  > regularize and lasso\n",
      "\n",
      "\n",
      "# Set the test size to 30% to get a 70-30% train test split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=0)\n",
      "\n",
      "# Fit the scaler on the training features and transform these in one go\n",
      "X_train_std = scaler.fit_transform(X_train,y_train)\n",
      "\n",
      "# Create the Lasso model\n",
      "la = Lasso()\n",
      "\n",
      "# Fit it to the standardized training data\n",
      "la.fit(X_train_std,y_train)\n",
      "\n",
      "\n",
      "print(la.coef_)\n",
      "\n",
      " > sample  > using R2 to determine the number of ignored features\n",
      "\n",
      "# Transform the test set with the pre-fitted scaler\n",
      "X_test_std = scaler.transform(X_test)\n",
      "\n",
      "# Calculate the coefficient of determination (R squared) on X_test_std\n",
      "r_squared = la.score(X_test_std, y_test)\n",
      "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
      "\n",
      "# Create a list that has True values when coefficients equal 0\n",
      "zero_coef = la.coef_ == 0\n",
      "\n",
      "# Calculate how many features have a zero coefficient\n",
      "n_ignored = sum(zero_coef)\n",
      "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))\n",
      "\n",
      "    combining features\n",
      "\n",
      "\n",
      "from sklearn.linear_model import Lasso\n",
      "\n",
      "la=Lasso(alpha=0.05)\n",
      "la.fit(X_train, y_train)\n",
      "\n",
      "print(la.coef_)\n",
      "\n",
      "print(la.score(X_test,y_test))\n",
      "\n",
      "  >lassoCV\n",
      "\n",
      "from sklearn.linear_model import LassoCV\n",
      "\n",
      "lcv=LassoCV()\n",
      "\n",
      "lcv.fit(X_train, y_train)\n",
      "print(lcv.alpha_)\n",
      "\n",
      "mask= lcv.coef_ !=0\n",
      "print(mask)\n",
      "\n",
      "reduced_X=X.loc[:,mask]\n",
      "\n",
      "\n",
      "  Combining feature selectors\n",
      "\n",
      "Random forest is a combination of decision trees\n",
      "It is based on the idea that a combination of models can combine to form a strong one\n",
      "\n",
      "\n",
      "from sklearn.linear_model import LassoCV\n",
      "\n",
      "lcv=LassoCV()\n",
      "\n",
      "lcv.fit(X_train, y_train)\n",
      "lcv.score(X_test, y_test)\n",
      "\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "rfe_rf= RFE(estimator=RandomForestRegressor(),\n",
      "\tn_features_to_select =66, step =5, verbose=1)\n",
      "\n",
      "rfe_rf.fit(X_train, y_train)\n",
      "\n",
      "rf_mask=rfe_rf.support_\n",
      "\n",
      "\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "rfe_gb= RFE(estimator=GradientBoostingRegressor(),\n",
      "\tn_features_to_select =66, step =5, verbose=1)\n",
      "\n",
      "rfe_gb.fit(X_train, y_train)\n",
      "\n",
      "gb_mask=rfe_rg.support_\n",
      "\n",
      "votes=np.sum([lcv_mask, rf_mask, gb_mask],axis=0)\n",
      "print(votes)\n",
      "\n",
      "mask=votes>=2\n",
      "\n",
      "reduced_X = X.loc[:,mask]\n",
      "\n",
      "\n",
      "  >Sample    lassoCV   >\n",
      "\n",
      "from sklearn.linear_model import LassoCV\n",
      "\n",
      "# Create and fit the LassoCV model on the training set\n",
      "lcv = LassoCV()\n",
      "lcv.fit(X_train,y_train)\n",
      "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
      "\n",
      "# Calculate R squared on the test set\n",
      "r_squared = lcv.score(X_test,y_test)\n",
      "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
      "\n",
      "# Create a mask for coefficients not equal to zero\n",
      "lcv_mask = lcv.coef_!=0\n",
      "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))\n",
      "\n",
      "X.loc[:,lcv_mask].columns\n",
      "\n",
      "\n",
      "Output: Optimal alpha = 0.089\n",
      "The model explains 88.2% of the test set variance\n",
      "26 features out of 32 selected\n",
      "\n",
      "['acromialheight', 'bideltoidbreadth', 'buttockcircumference', 'buttockpopliteallength', 'chestcircumference', 'chestheight', 'earprotrusion', 'footbreadthhorizontal',\n",
      "       'forearmcircumferenceflexed', 'handlength', 'headbreadth', 'heelbreadth', 'hipbreadth', 'interscyeii', 'lateralfemoralepicondyleheight', 'lateralmalleolusheight', 'radialestylionlength',\n",
      "       'shouldercircumference', 'shoulderelbowlength', 'thighcircumference', 'thighclearance', 'verticaltrunkcircumferenceusa', 'waistcircumference', 'waistdepth', 'wristheight', 'BMI'],\n",
      "      dtype='object')\n",
      "\n",
      "  >sample    RFE   > GradientBoostRegressor\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
      "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
      "             n_features_to_select=10, step=3, verbose=1)\n",
      "rfe_gb.fit(X_train, y_train)\n",
      "\n",
      "# Calculate the R squared on the test set\n",
      "r_squared = rfe_gb.score(X_test,y_test)\n",
      "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
      "\n",
      "gb_mask = rfe_gb.support_!=0\n",
      "print(X.loc[:,gb_mask].columns) \n",
      "\n",
      "\n",
      "Index(['bideltoidbreadth', 'buttockcircumference', 'chestcircumference', 'forearmcircumferenceflexed', 'hipbreadth', 'lateralmalleolusheight', 'shouldercircumference', 'thighcircumference',\n",
      "       'waistcircumference', 'BMI'],\n",
      "      dtype='object')\n",
      "\n",
      "   sample    rfe with RandomForestRegressor\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
      "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
      "             n_features_to_select=10, step=3, verbose=1)\n",
      "rfe_rf.fit(X_train, y_train)\n",
      "\n",
      "# Calculate the R squared on the test set\n",
      "r_squared = rfe_rf.score(X_test, y_test)\n",
      "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
      "\n",
      "# Assign the support array to gb_mask\n",
      "rf_mask = rfe_rf.support_\n",
      "\n",
      "  sample sum the masks\n",
      "\n",
      "# Sum the votes of the three models\n",
      "votes = np.sum([lcv_mask,rf_mask,gb_mask],axis=0)\n",
      "print(votes)\n",
      "\n",
      "meta_mask = votes>=2\n",
      "print(meta_mask)\n",
      "\n",
      "X_reduced = X.loc[:,meta_mask]\n",
      "print(X_reduced.columns)\n",
      "\n",
      "# Plug the reduced dataset into a linear regression pipeline\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
      "lm.fit(scaler.fit_transform(X_train), y_train)\n",
      "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
      "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))\n",
      "\n",
      "\n",
      "Index(['chestcircumference', 'forearmcircumferenceflexed', 'hipbreadth', 'thighcircumference', 'waistcircumference', 'wristheight', 'BMI'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "In [1]:\n",
      "\n",
      "\n",
      "    Feature Extraction\n",
      "\n",
      "feature extraction are new features resulting from the combinations of existing features.\n",
      "\n",
      "df_body['BMI']=df['Weight kg']/df_body['Height m']**2\n",
      "\n",
      "weight and height are obsolete\n",
      "\n",
      "leg_df['leg mm']=leg_df[['right leg mm','left leg mm']].mean(axis=1)\n",
      "\n",
      "    pca\n",
      "\n",
      "scaler = StandardScaler()\n",
      "\n",
      "df_std=pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
      "\n",
      "footlength and handlength\n",
      "\n",
      "people with big feet tend to have big hands\n",
      "\n",
      "principal components\n",
      "\n",
      "   sample  > combine quantity and revenue into price and drop the columns\n",
      "\n",
      "# Calculate the price from the quantity sold and revenue\n",
      "sales_df['price'] = sales_df['revenue']/sales_df['quantity']\n",
      "\n",
      "# Drop the quantity and revenue features\n",
      "reduced_df = sales_df.drop(['quantity','revenue'], axis=1)\n",
      "\n",
      "print(reduced_df.head())\n",
      "\n",
      "   sample  > add three columns into a new column and drop them\n",
      "\n",
      "# Calculate the mean height\n",
      "height_df['height'] = height_df[['height_1','height_2','height_3']].mean(axis=1)\n",
      "\n",
      "print(height_df.columns)\n",
      "# Drop the 3 original height features\n",
      "reduced_df = height_df.drop(['height_1','height_2','height_3'], axis=1)\n",
      "\n",
      "print(reduced_df.head())\n",
      "\n",
      "\n",
      "      >Principal component analysis\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler = StandardScaler()\n",
      "std_df = scaler.fit_transform(df)\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca=PCA()\n",
      "print(pca.fit_transform(std_df))\n",
      "\n",
      "pca.fit(std_df)\n",
      "\n",
      "print(pca.explained_variance_ratio_)\n",
      "\n",
      "\n",
      "\n",
      "   sample   > standard scaler\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Create the scaler and standardize the data\n",
      "scaler = StandardScaler()\n",
      "ansur_std = scaler.fit_transform(ansur_df)\n",
      "\n",
      "\n",
      "   sample  > pca fit transform\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# Create the scaler and standardize the data\n",
      "scaler = StandardScaler()\n",
      "ansur_std = scaler.fit_transform(ansur_df)\n",
      "\n",
      "# Create the PCA instance and fit and transform the data with pca\n",
      "pca = PCA()\n",
      "pc = pca.fit_transform(ansur_std)\n",
      "\n",
      "# This changes the numpy array output back to a dataframe\n",
      "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
      "\n",
      "\n",
      "sns.pairplot(data=pc_df)\n",
      "plt.show()\n",
      "\n",
      "  >sample  > pca component\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# Scale the data\n",
      "scaler = StandardScaler()\n",
      "ansur_std = scaler.fit_transform(ansur_df)\n",
      "\n",
      "# Apply PCA\n",
      "pca = PCA()\n",
      "pca.fit(ansur_std)\n",
      "\n",
      "# Inspect the explained variance ratio per component\n",
      "print(pca.explained_variance_ratio_)\n",
      "\n",
      "print(pca.explained_variance_ratio_.cumsum())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0.61449404 0.19893965 0.06803095 0.03770499 0.03031502 0.0171759\n",
      " 0.01072762 0.00656681 0.00634743 0.00436015 0.0026586  0.00202617\n",
      " 0.00065268]\n",
      "\n",
      "\n",
      "        PCA applications\n",
      "\n",
      "one downside to pca is the remaining components can be hard to intrept.\n",
      "\n",
      "print(pca.components_)\n",
      "\n",
      "this tells to what extent the component is affected by a feature\n",
      "\n",
      "PC 1 = 0.71x hand length + 0.71 foot length\n",
      "PC 2 = -071 x hand length + 0.71 x foot length\n",
      "\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "pipe=Pipeline([\n",
      "\t('scaler',StandardScaler()),\n",
      "\t('reducer',PCA())])\n",
      "\n",
      "pc=pipe.fit_transform(ansur_df)\n",
      "print(pc[:,2])\n",
      "\n",
      "ansur_categories['PC 1'] = pc[:,0]\n",
      "ansur_categories['PC 2'] = pc[:,1]\n",
      "\n",
      "\n",
      "sns.scatterplot(data=ansur_categories,\n",
      "x='PC 1', y='PC 2', hue='Height_class', alpha=0.4)\n",
      "\n",
      "\n",
      "   Add a classifier to the pipeline\n",
      "pipe=Pipeline([\n",
      "\t('scaler',StandardScaler()),\n",
      "\t('reducer',PCA(n_components=3)),\n",
      "\t('classifier', RandomForestClassifier())\n",
      "])\n",
      "\n",
      "\n",
      "pipe.fit(X_train,y_train)\n",
      "print(pipe.steps[1])\n",
      "\n",
      "print(pipe.steps[1][1].explained_variance_ratio_.cumsum())\n",
      "\n",
      "\n",
      "print(pipe.score(X_test,y_test))\n",
      "\n",
      "\n",
      "   sample  > build the pca pipeline\n",
      "\n",
      "# Build the pipeline\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "        \t\t ('reducer', PCA(n_components=2))])\n",
      "\n",
      "# Fit it to the dataset and extract the component vectors\n",
      "pipe.fit(poke_df)\n",
      "vectors = pipe.steps[1][1].components_.round(2)\n",
      "\n",
      "# Print feature effects\n",
      "print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
      "print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))\n",
      "\n",
      "  sample pca pipeline\n",
      "\n",
      "# Build the pipeline\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "                 ('reducer', PCA(n_components=2))])\n",
      "\n",
      "# Fit the pipeline to poke_df and transform the data\n",
      "pc = pipe.fit_transform(poke_df)\n",
      "\n",
      "print(pc)\n",
      "\n",
      "[[-1.5563747  -0.02148212]\n",
      " [-0.36286656 -0.05026854]\n",
      " [ 1.28015158 -0.06272022]\n",
      " ...\n",
      " [ 2.45821626 -0.51588158]\n",
      " [ 3.5303971  -0.95106516]\n",
      " [ 2.23378629  0.53762985]]\n",
      "\n",
      "Index(['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'], dtype='object')\n",
      "\n",
      "poke_cat_df['PC 1'] = pc[:, 0]\n",
      "poke_cat_df['PC 2'] = pc[:, 1]\n",
      "\n",
      "print(poke_cat_df.head())\n",
      "\n",
      "# Use the Type feature to color the PC 1 vs PC 2 scatterplot\n",
      "sns.scatterplot(data=poke_cat_df, \n",
      "                x='PC 1', y='PC 2', hue='Type')\n",
      "plt.show()\n",
      "\n",
      "  sample  > pipeline with pca and randomforest classifier\n",
      "\n",
      "# Build the pipeline\n",
      "pipe = Pipeline([\n",
      "        ('scaler', StandardScaler()),\n",
      "        ('reducer', PCA(n_components=2)),\n",
      "        ('classifier',  RandomForestClassifier(random_state=0))])\n",
      "\n",
      "\n",
      "# Fit the pipeline to the training data\n",
      "pipe.fit(X_train,y_train)\n",
      "\n",
      "# Prints the explained variance ratio\n",
      "print(pipe.steps[1][1].explained_variance_ratio_)\n",
      "\n",
      "# Score the accuracy on the test set\n",
      "accuracy = pipe.score(X_test,y_test)\n",
      "\n",
      "# Prints the model accuracy\n",
      "print('{0:.1%} test set accuracy'.format(accuracy))\n",
      "\n",
      "\n",
      "[0.45624044 0.17767414 0.12858833]\n",
      "95.0% test set accuracy\n",
      "\n",
      "     >Principal component selection\n",
      "\n",
      "pipe= Pipeline([\n",
      "('scaler', StandardScaler()),\n",
      "('reducer',PCA(n_components=0.9))])\n",
      "\n",
      "#explains 90% of the variance\n",
      "\n",
      "pipe.fit(poke_df)\n",
      "\n",
      "print(len(pipe.steps[1][1].components_))\n",
      "\n",
      "There is no right answer to the number of components i should keep. It depends on how much information you are willing to lose to reduce complexity\n",
      "\n",
      "var=pipe.steps[1][1].explain_variance_ratio_\n",
      "\n",
      "plt.plot(var)\n",
      "\n",
      "plt.xlabel('Principal component index')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.show()\n",
      "\n",
      "X=pca.inverse_transform(pc)\n",
      "\n",
      "moves from principal component space back to feature space.\n",
      "\n",
      "2914 grayscale values\n",
      "62x47 pixels=2914 grayscale values\n",
      "\n",
      "test\n",
      "(15,2914)\n",
      "15 pictures\n",
      "training\n",
      "(1333,2914)\n",
      "1333 images\n",
      "\n",
      "pipe= Pipeline([\n",
      "('scaler', StandardScaler()),\n",
      "('reducer',PCA(n_components=290))])\n",
      "\n",
      "pipe.fit(X_train)\n",
      "\n",
      "pc=pipe.fit_transform(X_test)\n",
      "\n",
      "print(pc.shape)\n",
      "15,290\n",
      "\n",
      "10 fold number reduction in features\n",
      "\n",
      "X_rebuilt=pipe.inverse_transform(pc)\n",
      "print(X_rebuilt.shape)\n",
      "\n",
      "img_plotter(X_rebuilt)\n",
      "\n",
      "\n",
      "   sample  > pipeline\n",
      "\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "        \t\t ('reducer', PCA(n_components=0.8))])\n",
      "\n",
      "# Fit the pipe to the data\n",
      "pipe.fit(ansur_df)\n",
      "\n",
      "print('{} components selected'.format(len(pipe.steps[1][1].components_)))\n",
      "\n",
      "11 components selected\n",
      "\n",
      ".9 n_components requires 23 components selected\n",
      "\n",
      " > sample pipeline     variance elbow\n",
      "\n",
      "# Pipeline a scaler and pca selecting 10 components\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "        \t\t ('reducer', PCA(n_components=10))])\n",
      "\n",
      "# Fit the pipe to the data\n",
      "pipe.fit(ansur_df)\n",
      "\n",
      "\n",
      "# Plot the explained variance ratio\n",
      "plt.plot(pipe.steps[1][1].explained_variance_ratio_)\n",
      "\n",
      "plt.xlabel('Principal component index')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.show()\n",
      "\n",
      "   sample  > hand written numbers\n",
      "\n",
      "plot_digits(X_test)\n",
      "\n",
      "print(X_test.shape)\n",
      "(16,784)\n",
      "\n",
      "\n",
      "    sample  > pc transform\n",
      "\n",
      "# Transform the input data to principal components\n",
      "pc = pipe.transform(X_test)\n",
      "\n",
      "\n",
      "# Prints the number of features per dataset\n",
      "print(\"X_test has {} features\".format(X_test.shape[1]))\n",
      "print(\"pc has {} features\".format(pc.shape[1]))\n",
      "\n",
      "X_test has 784 features\n",
      "pc has 78 features\n",
      "\n",
      "# Inverse transform the components to original feature space\n",
      "X_rebuilt = pipe.inverse_transform(pc)\n",
      "\n",
      "# Prints the number of features\n",
      "print(\"X_rebuilt has {} features\".format(X_rebuilt.shape[1]))\n",
      "\n",
      "X_rebuilt has 784 features\n",
      "\n",
      "# Plot the reconstructed data\n",
      "plot_digits(X_rebuilt)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\kpi.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\kpi.txt\n",
      "import pandas as pd\n",
      "\n",
      "customer_demographics=pd.read_csv('customer_demographics.csv')\n",
      "\n",
      "uid\n",
      "reg_date\n",
      "device\n",
      "gender\n",
      "country\n",
      "age\n",
      "\n",
      "\n",
      "#customer actions\n",
      "customer_subscriptions=pd.read_csv('customer_subscriptions.csv')\n",
      "\n",
      "print(customer_subscriptions.head())\n",
      "\n",
      "uid\n",
      "lapse_date\n",
      "subscription_date\n",
      "price\n",
      "\n",
      "KPI : conversion rate\n",
      "\n",
      "importance across different user groups\n",
      "\n",
      "sub_data_demo=customer_demographics.merge(\n",
      "\tcustomer_subscriptions,\n",
      "\thow='inner',\n",
      "\ton=['uid']\n",
      "\t)\n",
      "\n",
      "\n",
      "\n",
      "   > sample\n",
      "\n",
      "# Import pandas \n",
      "import pandas as pd\n",
      "\n",
      "# Load the customer_data\n",
      "customer_data = pd.read_csv('customer_data.csv')\n",
      "\n",
      "# Load the app_purchases\n",
      "app_purchases = pd.read_csv('inapp_purchases.csv')\n",
      "\n",
      "# Print the columns of customer data\n",
      "print(customer_data.columns)\n",
      "\n",
      "# Print the columns of app_purchases\n",
      "print(app_purchases.columns)\n",
      "\n",
      "\n",
      "Index(['uid', 'reg_date', 'device', 'gender', 'country', 'age'], dtype='object')\n",
      "\n",
      "Index(['date', 'uid', 'sku', 'price'], dtype='object')\n",
      "\n",
      "# Merge on the 'uid' field\n",
      "uid_combined_data = app_purchases.merge(customer_data, on=['uid'], how='inner')\n",
      "\n",
      "# Examine the results \n",
      "print(uid_combined_data.head())\n",
      "print(len(uid_combined_data))\n",
      "\n",
      "\n",
      "date_x       uid            sku  price      date_y device gender country  age\n",
      "0  2017-07-10  41195147  sku_three_499    499  2017-06-26    and      M     BRA   17\n",
      "1  2017-07-15  41195147  sku_three_499    499  2017-06-26    and      M     BRA   17\n",
      "2  2017-11-12  41195147   sku_four_599    599  2017-06-26    and      M     BRA   17\n",
      "3  2017-09-26  91591874    sku_two_299    299  2017-01-05    and      M     TUR   17\n",
      "4  2017-12-01  91591874   sku_four_599    599  2017-01-05    and      M     TUR   17\n",
      "9006\n",
      "In [1]:\n",
      "\n",
      "\n",
      "# Merge on the 'uid' and 'date' field\n",
      "uid_date_combined_data = app_purchases.merge(customer_data, on=['uid', 'date'], how='inner')\n",
      "\n",
      "# Examine the results \n",
      "print(uid_date_combined_data.head())\n",
      "print(len(uid_date_combined_data))\n",
      "\n",
      "\n",
      " uid             sku  price device gender country  age\n",
      "0  2016-03-30  94055095    sku_four_599    599    iOS      F     BRA   16\n",
      "1  2015-10-28  69627745     sku_one_199    199    and      F     BRA   18\n",
      "2  2017-02-02  11604973  sku_seven_1499    499    and      F     USA   16\n",
      "3  2016-06-05  22495315    sku_four_599    599    and      F     USA   19\n",
      "4  2018-02-17  51365662     sku_two_299    299    iOS      M     TUR   16\n",
      "\n",
      "      . exploratory analysis of kpi\n",
      "\n",
      "1. most companies will have many kpis\n",
      "2. each serves a different purpose\n",
      "\n",
      "#axis=0 is columns\n",
      "#as_index will use group labels as index\n",
      "\n",
      "sub_data_grp=sub_data_deep.groupby(by=['country','device'], axis=0, as_index=False)\n",
      "\n",
      "sub_data_grp.mean()\n",
      "or\n",
      "sub_data_grp.agg('mean')\n",
      "or\n",
      "sub_data_grp.agg(['mean','median'])\n",
      "or\n",
      "sub_data_grp.agg({'price':['mean','median','max'],\n",
      "\t'age':['mean','median','max']\n",
      "\t})\n",
      "\n",
      "def truncate_mean(data):\n",
      "\ttop_val=data.quantile(.9)\n",
      "\tbot_val=data.quantile(.1)\n",
      "\ttrunc_data=data[(data<=top_val) & (data>=bot_val)]\n",
      "\tmean=trunc_data.mean()\n",
      "\treturn (mean)\n",
      "\n",
      "\n",
      "sub_data_grp.agg({'age':[truncated_mean]})\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "# Calculate the mean and median purchase price \n",
      "purchase_price_summary = purchase_data.price.agg(['mean', 'median'])\n",
      "\n",
      "# Examine the output \n",
      "print(purchase_price_summary)\n",
      "\n",
      "mean      406.772596\n",
      "median    299.000000\n",
      "\n",
      "# Calculate the mean and median of price and age\n",
      "purchase_summary = purchase_data.agg({'price': ['mean', 'median'], 'age': ['mean', 'median']})\n",
      "\n",
      "# Examine the output \n",
      "print(purchase_summary)\n",
      "\n",
      "             price        age\n",
      "mean    406.772596  23.922274\n",
      "median  299.000000  21.000000\n",
      "\n",
      "\n",
      "Notice how the mean is higher than the median? This suggests that we have some users who are making a lot of purchases!\n",
      "\n",
      "# Group the data \n",
      "grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])\n",
      "\n",
      "# Aggregate the data\n",
      "purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})\n",
      "\n",
      "# Examine the results\n",
      "print(purchase_summary)\n",
      "\n",
      "\n",
      "price                   \n",
      "                     mean median         std\n",
      "device gender                               \n",
      "and    F       400.747504    299  179.984378\n",
      "       M       416.237308    499  195.001520\n",
      "iOS    F       404.435330    299  181.524952\n",
      "       M       405.272401    299  196.843197\n",
      "\n",
      "       calculating a conversion rate\n",
      "\n",
      "import pandas as pd\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "current_date=pd.to_datetime('2018-03-17')\n",
      "\n",
      "#what is the maximum lapse date in our dataset\n",
      "\n",
      "max_lapse_date = current_date - timedelta(days=7)\n",
      "\n",
      "conv_sub_data=sub_data_demo[(sub_data_demo.lapse_date<max_lapse_date)]\n",
      "\n",
      "\n",
      "total_users_count=conv_sub_data.price.count()\n",
      "print(total_users_count)\n",
      "\n",
      "max_sub_date=conv_sub_data.lapse_date+timedelta(days=7)\n",
      "\n",
      "total_subs=conv_sub_data[\n",
      "(conv_sub_data.price>0) &\n",
      "(conv_sub_data.subscription_data<=max_sub_data)\n",
      "]\n",
      "\n",
      "total_sub_count=total_sub.price.count()\n",
      "print(total_subs_count)\n",
      "\n",
      "conversion rate = Total subscribers/potential subscribers\n",
      "\n",
      "conversion_rate = total_subs_count / total_users_count\n",
      "print(conversion_rate)\n",
      "\n",
      "\n",
      "      cohort conversion rate\n",
      "\n",
      "conv_sub_data = conv_sub_data.copy()\n",
      "\n",
      "#keep users who lapsed prior to the last 14 days\n",
      "\n",
      "max_lapse_date = current_date - timedelta(days=14)\n",
      "\n",
      "conv_sub_data = sub_data_demo[\n",
      " (sub_data_demo.lapse_date <=max_lapse_date)\n",
      "]\n",
      "\n",
      "sub time is the number of days been the lapse date and the subscription date\n",
      "\n",
      "np.where receives a number to return a true and one to return a false\n",
      "\n",
      "sub_time = np. where(\n",
      "\tconv_sub_data.subscription_date.notnull(),\n",
      "\t#then find how many days since their lapse\n",
      "\t(conv_sub_data.scription_date - conv_sub_data.lapse_date).dt.days,\n",
      "\t#else set the value to pd.NaT\n",
      "\tpd.NaT)\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "\n",
      "find the conversion rate gcr7() and gcr14()\n",
      "\n",
      "purchase_cohorts=conv_sub_data.groupby(by=['gender','device'],as_index=False)\n",
      "\n",
      "#find the conversion rate for each cohort using gcr7 and gcr14\n",
      "\n",
      "purchase_cohorts.agg({sub_time:[gcr7,gcr14]})\n",
      "\n",
      "     How to choose KPI metrics\n",
      "\n",
      "how long does it take to gain insight on a metric\n",
      "\n",
      "what is an actionable time scale\n",
      "\n",
      "monthly conversion rate = 1 month wait time\n",
      "\n",
      "leverage exploratory data analysis\n",
      "* reveals relationships between metrics and key results\n",
      "\n",
      "KPI should measure strong growth\n",
      "* potential early warning sign of problems\n",
      "* senstive to changes in the overall ecosystem\n",
      "\n",
      "       sample\n",
      "\n",
      "# Compute max_purchase_date \n",
      "max_purchase_date = current_date - timedelta(days=28)\n",
      "\n",
      "# Filter to only include users who registered before our max date\n",
      "purchase_data_filt = purchase_data[purchase_data.reg_date < max_purchase_date]\n",
      "\n",
      "# Filter to contain only purchases within the first 28 days of registration\n",
      "purchase_data_filt = purchase_data_filt[(purchase_data_filt.date <=\n",
      "                                         purchase_data_filt.reg_date + \n",
      "                                         timedelta(days=28))]\n",
      "\n",
      "# Output the mean price paid per purchase\n",
      "print(purchase_data_filt.price.mean())\n",
      "\n",
      "414.4237288135593\n",
      "\n",
      "\n",
      "      find a 1 month of data\n",
      "\n",
      "# Set the max registration date to be one month before today\n",
      "max_reg_date = current_date - timedelta(days=28)\n",
      "\n",
      "# Find the month 1 values:\n",
      "month1 = np.where((purchase_data.reg_date < max_reg_date) &\n",
      "                    (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),\n",
      "                  purchase_data.price, \n",
      "                  np.NaN)\n",
      "                 \n",
      "# Update the value in the DataFrame \n",
      "purchase_data['month1'] = month1\n",
      "\n",
      "print(month1)\n",
      "\n",
      "# Group the data by gender and device \n",
      "purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False)\n",
      "\n",
      "# Aggregate the month1 and price data \n",
      "purchase_summary = purchase_data_upd.agg(\n",
      "                        {'month1': ['mean', 'median'],\n",
      "                        'price': ['mean', 'median']})\n",
      "\n",
      "# Examine the results \n",
      "print(purchase_summary)\n",
      "\n",
      "gender device      month1              price       \n",
      "                       mean median        mean median\n",
      "0      F    and  388.204545  299.0  400.747504    299\n",
      "1      F    iOS  432.587786  499.0  404.435330    299\n",
      "2      M    and  413.705882  399.0  416.237308    499\n",
      "3      M    iOS  433.313725  499.0  405.272401    299\n",
      "\n",
      "\n",
      "\n",
      "      >.Working with time series\n",
      "\n",
      "exploratory data analysis\n",
      "\n",
      "2nd week subscribers\n",
      "\n",
      "exclude customers who have not been on the platform for two weeks\n",
      "\n",
      "\n",
      "current_date=pd.to_datetime('2018-03-17')\n",
      "max_lapse_date=current_date - timedelta(days=14)\n",
      "\n",
      "#find all uses who have not been on the platform in the last two weeks\n",
      "\n",
      "conv_sub_data= sub_data_demo[\n",
      "  sub_data_demo.lapse_date < max_lapse_date\n",
      "]\n",
      "\n",
      "#how many days before the user subscribed\n",
      "\n",
      "sub_time = conv_sub_data.subscription_date -\n",
      "conv_sub_data.lapse_date\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "or\n",
      "conv_sub_data['sub_time']= conv_sub_data.sub_time.dt.days\n",
      "\n",
      "#find the users who have not subscribed in week one but have been on the platform for two or more weeks\n",
      "\n",
      "conv_base=conv_sub_data[\n",
      " (conv_sub_data.sub_time.notnull()) |\n",
      "\t(conv_sub_data.sub_time > 7)]\n",
      "\n",
      "total_users=len(conv_base)\n",
      "\n",
      "total_sub=np.where(conv_sub_data.sub_time.notnull()) &\n",
      "\t(conv_sub_data.sub_time <= 14,1,0)\n",
      "\n",
      "total_subs=sum(total_subs)\n",
      "\n",
      "conversion_rate = total_subs/total_users\n",
      "\n",
      "output\n",
      "0.009\n",
      "\n",
      "    pandas date parser on read_csv\n",
      "\n",
      "pandas.read_csv(\n",
      "\n",
      "\tparse_dates=False\n",
      "\tinfer_datetime_format=False\n",
      "\tkeep_date_col=False\n",
      "\tdate_parser=None\n",
      "\tdayFirst=False\n",
      "\t)\n",
      "\n",
      "strftime\n",
      "\"%Y-%m-%d\"\n",
      "\"%H:%M:%S\"\n",
      "\n",
      "\"%B %d, %Y\"\n",
      "\n",
      "to_datetime\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_one = pd.to_datetime(date_data_one, format=\"%A %B %d, %Y\")\n",
      "print(date_data_one)\n",
      "\n",
      "output:\n",
      "DatetimeIndex(['2017-01-27', '2017-12-02'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_two = pd.to_datetime(date_data_two, format=\"%Y-%m-%d\")\n",
      "print(date_data_two)\n",
      "\n",
      "output:\n",
      "'2017-01-01', '2016-05-03']\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_three = pd.to_datetime(date_data_three, format=\"%m/%d/%Y\")\n",
      "print(date_data_three)\n",
      "\n",
      "output:\n",
      "'1978-08-17', '1976-01-07'\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_four = pd.to_datetime(date_data_four, format=\"%Y %B %d %H:%M\")\n",
      "print(date_data_four)\n",
      "\n",
      "output:\n",
      "2016-03-01 01:56:00', '2016-01-04 02:16:00'\n",
      "\n",
      "    Creating time series graphs with matplotlib\n",
      "\n",
      "#find all uses who have not been on the platform in the last two weeks\n",
      "\n",
      "conv_sub_data= sub_data_demo[\n",
      "  sub_data_demo.lapse_date < max_lapse_date\n",
      "]\n",
      "\n",
      "#how many days before the user subscribed\n",
      "\n",
      "sub_time = conv_sub_data.subscription_date -\n",
      "conv_sub_data.lapse_date\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "or\n",
      "conv_sub_data['sub_time']= conv_sub_data.sub_time.dt.days\n",
      "\n",
      "#find the users who have not subscribed in week one but have been on the platform for two or more weeks\n",
      "\n",
      "conv_base=conv_sub_data[\n",
      " (conv_sub_data.sub_time.notnull()) |\n",
      "\t(conv_sub_data.sub_time > 7)]\n",
      "\n",
      "total_users=len(conv_base)\n",
      "\n",
      "total_sub=np.where(conv_sub_data.sub_time.notnull()) &\n",
      "\t(conv_sub_data.sub_time <= 14,1,0)\n",
      "\n",
      "total_subs=sum(total_subs)\n",
      "\n",
      "conversion_rate = total_subs/total_users\n",
      "\n",
      "   new stuff\n",
      "conversion_data = conv_sub_data.groupby(\n",
      "\tby=['lapse_date'], as_index=False\n",
      ").agg('sub_time': [gc7]})\n",
      "\n",
      "#produces the week one conversion rate by conversion date.\n",
      "\n",
      "\n",
      "conversion_data.plot(x='lapse_date',y='sub_time')\n",
      "\n",
      "* compare users of different genders\n",
      "* evaluate the impact of a change across regions\n",
      "* see the impact for different devices\n",
      "\n",
      "reformatted_cntry_data=pd.pivot_table(\n",
      "\tconversion_data,\n",
      "\tvalues=['sub_time'],\n",
      "\tcolumns=['country'],\n",
      "\tindex=['reg_data'],\n",
      "\tfill_value=0\n",
      ")\n",
      "\n",
      "reformat_cntry_data.plot(\n",
      "\tx='reg_date',\n",
      "\ty=['BRA','FRA','DEU','TUR','USA','CAN']\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >sample   graph reg_date by first_week_purchases\n",
      "\n",
      "# Group the data and aggregate first_week_purchases\n",
      "\n",
      "user_purchases columns: 'reg_date', 'first_week_purchases'\n",
      "\n",
      "user_purchases = user_purchases.groupby(by=['reg_date', 'uid']).agg({'first_week_purchases': ['sum']})\n",
      "\n",
      "# Reset the indexes\n",
      "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
      "user_purchases.reset_index(inplace=True)\n",
      "\n",
      "# Find the average number of purchases per day by first-week users\n",
      "user_purchases = user_purchases.groupby(by=['reg_date']).agg({'first_week_purchases': ['mean']})\n",
      "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
      "user_purchases.reset_index(inplace=True)\n",
      "\n",
      "# Plot the results\n",
      "user_purchases.plot(x='reg_date', y='first_week_purchases')\n",
      "plt.show()\n",
      "\t\n",
      "\n",
      "   sample pivot table on the first_week_purchases by country\n",
      "\n",
      "# Pivot the data\n",
      "country_pivot = pd.pivot_table(user_purchases_country, values=['first_week_purchases'], columns=['country'], index=['reg_date'])\n",
      "print(country_pivot.head())\n",
      "\n",
      "\n",
      "# Pivot the data\n",
      "device_pivot = pd.pivot_table(user_purchases_device, values=['first_week_purchases'], columns=['device'], index=['reg_date'])\n",
      "print(device_pivot.head())\n",
      "\n",
      "\n",
      "          first_week_purchases          \n",
      "device                      and       iOS\n",
      "reg_date                                 \n",
      "2017-06-01             0.714286  1.000000\n",
      "2017-06-02             1.400000  1.285714\n",
      "2017-06-03             1.545455  1.000000\n",
      "2017-06-04             1.600000  1.833333\n",
      "2017-06-05             1.625000  2.000000\n",
      "\n",
      "\n",
      "# Plot the average first week purchases for each country by registration date\n",
      "country_pivot.plot(x='reg_date', y=['USA', 'CAN', 'FRA', 'BRA', 'TUR', 'DEU'])\n",
      "plt.show()\n",
      "\n",
      "# Plot the average first week purchases for each device by registration date\n",
      "device_pivot.plot(x='reg_date', y=['and', 'iOS'])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     understanding and visualizing trends in customer data\n",
      "\n",
      "usa_subscriptions['sub_day']=(usa_subscriptions.sub_date - usa_subscriptions.lapse_date).dt.days\n",
      "\n",
      "\n",
      "usa_subscriptions = usa_subscriptions[usa_subscriptions.sub_day <=7]\n",
      "\n",
      "usa_subscriptions = usa_subscriptions.groupby(\n",
      "\tby=['sub_date'],as_index=False\n",
      ").agg({'subs':['sum']})\n",
      "\n",
      "\n",
      "     >looking for seasonal change in buying movement\n",
      "\n",
      "Trailing average smoothing technique that averages over a lagging window\n",
      "1. reveal hidden trends by smoothing out seasonality\n",
      "2. average across the period of seasonality\n",
      "3. 7-day window to smooth weekly seasonality\n",
      "4. average out day level effects to produce the average week effect\n",
      "\n",
      "calculate the rolling average over the usa subscribers data with .rolling()\n",
      "\n",
      "rolling_subs = usa_subscriptions.subs.rolling(\n",
      "\twindow=7,\n",
      "\t#specify to average backwards\n",
      "\tcenter=False\n",
      ")\n",
      "\n",
      "usa_subscriptions['rolling_subs']\n",
      "\t=rolling_subs.mean()\n",
      "usa_subscriptions.tail()\n",
      "\n",
      "high_sku_purchases = pd.read_csv(\n",
      "\t'high_sku_purchases.csv',\n",
      "\tparse_dates=True,\n",
      "\tinfer_datetime_format=True\n",
      ")\n",
      "\n",
      "high_sku_purchases.plot(x='date', y='purchases')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "       exponential moving average\n",
      "\n",
      "1. weighted moving (rolling) average\n",
      "\n",
      "* weights more recent items in the window more\n",
      "* applies weights according to an exponential distribution\n",
      "* average back to a central trend without masking any recent movements\n",
      "\n",
      ".ewm() : exponential weighting function\n",
      "\n",
      "\n",
      "window to apply weights over\n",
      "\n",
      "exp_mean=high_sku_purchases.purchases.ewm(span=30)\n",
      "\n",
      "high_sku_purchases['exp_mean'] = exp_mean.mean()\n",
      "\n",
      "\n",
      "   >  sample  > rolling window 7, 28, 365\n",
      "\n",
      "# Compute 7_day_rev\n",
      "daily_revenue['7_day_rev'] = daily_revenue.revenue.rolling(window=7,center=False).mean()\n",
      "\n",
      "# Compute 28_day_rev\n",
      "daily_revenue['28_day_rev'] = daily_revenue.revenue.rolling(window=28,center=False).mean()\n",
      "    \n",
      "# Compute 365_day_rev\n",
      "daily_revenue['365_day_rev'] = daily_revenue.revenue.rolling(window=365,center=False).mean()\n",
      "    \n",
      "# Plot date, and revenue, along with the 3 rolling functions (in order)    \n",
      "daily_revenue.plot(x='date', y=['revenue', '7_day_rev', '28_day_rev', '365_day_rev', ])\n",
      "plt.show()\n",
      "\n",
      "   > sample ewm\n",
      "\n",
      "# Calculate 'small_scale'\n",
      "daily_revenue['small_scale'] = daily_revenue.revenue.ewm(span=10).mean()\n",
      "\n",
      "# Calculate 'medium_scale'\n",
      "daily_revenue['medium_scale'] = daily_revenue.revenue.ewm(span=100).mean()\n",
      "\n",
      "# Calculate 'large_scale'\n",
      "daily_revenue['large_scale'] = daily_revenue.revenue.ewm(span=500).mean()\n",
      "\n",
      "# Plot 'date' on the x-axis and, our three averages and 'revenue'\n",
      "# on the y-axis\n",
      "daily_revenue.plot(x = 'date', y =['revenue', 'small_scale', 'medium_scale', 'large_scale'])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >Events and releases\n",
      "\n",
      "discover the cause of an issue\n",
      "\n",
      "visualizing the drop in conversion rate (3 years)\n",
      "\n",
      "we notice a dip in new user retention\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "conv_sub_data = sub_data_demo(\n",
      "\tsub_data_demo.lapse_date <= max_lapse_date]\n",
      "\n",
      "sub_time = (conv_sub_data.subscription_date -\n",
      "\tconv_sub_data.lapse_date).dt.days\n",
      "\n",
      "conv_sub_date['sub_time']=sub_time\n",
      "\n",
      "conversion_data = conv_sub_data.groupby(\n",
      "\tby=['lapse_data'], as_index=False)\n",
      ".agg({sub_time':[gc7]})\n",
      "\n",
      "conversion_data.plot()\n",
      "plt.show()\n",
      "\n",
      "    >look at the recent six months\n",
      "\n",
      "current_date = pd.to_date('2018-03-17')\n",
      "\n",
      "start_date=current_date - timedelta(days=(6*28))\n",
      "\n",
      "conv_filter=(\n",
      "\tconversion_data.lapse_date >= start_date)\n",
      "\t& (conversion_data.lapse_date <= current_date)\n",
      ")\n",
      "\n",
      "con_data_filt=conversion_data[conv_filter]\n",
      "\n",
      "conv_data_filt.plot(x='lapse_date', y='sub_time')\n",
      "plt.show()\n",
      "\n",
      "* is this drop impacting all users or just specific cohort\n",
      "\n",
      "* this could provide clues on what the issue may be\n",
      "\n",
      "* ecosystems within our data\n",
      "1. distinct countries\n",
      "2. specific device (android or ios)\n",
      "\n",
      "\n",
      "\n",
      "#pivot the results to have one column per country\n",
      "\n",
      "conv_data_cntry = pd.pivot_table(\n",
      "\tconv_data_cntry, values=['sub_time'],\n",
      "\tcolumns=['country'], index=['lapse_date'], fill_value=0\n",
      ")\n",
      "\n",
      "#pivot the results to have one column per device\n",
      "\n",
      "\n",
      "conv_data_device = pd.pivot_table(\n",
      "\tconv_data_cntry, values=['sub_time'],\n",
      "\tcolumns=['device'], index=['lapse_date'], fill_value=0\n",
      ")\n",
      "\n",
      "* all countries experience the drop\n",
      "\n",
      "* most pronounced in Brazil & Turkey\n",
      "\n",
      "* breaking out by device\n",
      "1 the drop only appears on android devices\n",
      "\n",
      "events: holidays and events impacting user behavior\n",
      "\n",
      "events=pd.read_csv('events.csv')\n",
      "1. Date\n",
      "2. Event\n",
      "\n",
      "releases: ios and android software releases\n",
      "\n",
      "releases = pd.read_csv('releases.csv')\n",
      "\n",
      "     >Plot the conversion rate trend per device\n",
      "\n",
      "conv_data_dev.plot(\n",
      "\tx=['lapse_date'], y=['iOS','and']\n",
      ")\n",
      "\n",
      "events.Date = pd.to_datetime(events.Date)\n",
      "\n",
      "#iterate through events and plot each one\n",
      "\n",
      "for row in events.iterrows():\n",
      "\ttmp=row[1]\n",
      "\tplt.axvline(\n",
      "\tx=tmp.Date, color='k', linestyle='---'\n",
      ")\n",
      "\n",
      "\n",
      "#iterate through the releases and plot each one\n",
      "\n",
      "releases.Date = pd.to_datetime(releases.Date)\n",
      "\n",
      "\n",
      "for row in releases.iterrows():\n",
      "\ttmp=row[1]\n",
      "\tif tmp.Event== 'iOS Release':\n",
      "\n",
      "\t\tplt.axvline(\n",
      "\t\tx=tmp.Date, color='b', linestyle='---'\n",
      ")\n",
      "\telse:\n",
      "\t\tplt.axvline(\n",
      "\t\tx=tmp.Date, color='r', linestyle='---'\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "There was an android release in feb/mar aligns with our dip in conversion rate\n",
      "\n",
      "\n",
      "visualizing data over time to uncover hidden trends\n",
      "\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "user_revenue:\n",
      "1. device\n",
      "2. gender\n",
      "3. country\n",
      "4. date \n",
      "5. revenue\n",
      "6. month\n",
      "\n",
      "\n",
      "# Pivot user_revenue\n",
      "pivoted_data = pd.pivot_table(user_revenue, values ='revenue', columns=['device', 'gender'], index='month')\n",
      "pivoted_data = pivoted_data[1:(len(pivoted_data) -1 )]\n",
      "\n",
      "# Create and show the plot\n",
      "pivoted_data.plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "more female bought ios devices\n",
      "\n",
      "      Introduction to A/B testing\n",
      "\n",
      "discoverying causal relationships\n",
      "\n",
      "test two or more variants against each other\n",
      "\n",
      "to evaluate which one performs best\n",
      "\n",
      "in context of a randomized experiment\n",
      "\n",
      "testing two more ideas against each other\n",
      "\n",
      "control: the current state of your product\n",
      "\n",
      "treatment: the variant that you want to test\n",
      "\n",
      "current paywall: I hope you enjoyed your free-trial please consider subscribing\n",
      "\n",
      "proposed paywall: your free-trial has ended, don't miss out, subscribe today\n",
      "\n",
      "randomly select a subset of users and show one set the control and on e the treatment\n",
      "\n",
      "monitor the conversion rates of each group to see which is better\n",
      "\n",
      "by randomly assigning the user we isolate the impact of the change and reduce the potential impact of confounding variables\n",
      "\n",
      "using an assignment criteria may introduce confounders\n",
      "\n",
      "A/B testing can be used to \n",
      "1. improve sales within a mobile application\n",
      "2. increase user interactions with a website\n",
      "3. identify the impact of a medical treatment\n",
      "4. optimize an assembly lines efficiency\n",
      "\n",
      "good problems for ab testing\n",
      "1. where users are being impacted individually\n",
      "2. testing changes that can directly impact their behavior\n",
      "\n",
      "bad problems for ab testing\n",
      "1. challenging to segment the users into groups\n",
      "2. difficult to untangle the impact of the test\n",
      "\n",
      "\n",
      "     >initial ab test design\n",
      "\n",
      "increasing our apps revenue with a/b testing\n",
      "\n",
      "1. test change to our consumable purchase paywall\n",
      "2. increase revenue by increasing the purchase rate\n",
      "\n",
      "general concepts\n",
      "1. a/b testing techniques transfer across a variety of context\n",
      "2. keep in mind how you would apply these techniques\n",
      "\n",
      "    paywall views & demographics data\n",
      "\n",
      "demographics_data = pd.read_csv('user_demographics.csv')\n",
      "demographics_data.head(n=2)\n",
      "\n",
      "1.uid\n",
      "2.reg_date\n",
      "3.device\n",
      "4.gender\n",
      "5.country\n",
      "6.age\n",
      "\n",
      "\n",
      "paywall_views = pd.read_csv('paywall_views.csv')\n",
      "\n",
      "1.uid\n",
      "2.date\n",
      "3.purchase\n",
      "4.sku\n",
      "5.price\n",
      "\n",
      "\n",
      "   >Response variable\n",
      "1. A response variable is used to measure the impact of your change\n",
      "2. should either be a kpi or directly related to a kpi\n",
      "3. something that is easy to measure\n",
      "\n",
      "factors:\n",
      "1. the paywall color\n",
      "\n",
      "variants:\n",
      "1. particular changes you are testing\n",
      "\n",
      "Experimental unit of our test\n",
      "1. the smallest unit you are measuring the change over\n",
      "2. Individual users make a convenient experimental unit\n",
      "\n",
      "purchase_data = demographics_data.merge(\n",
      "\tpaywall_views, how='left', on=['uid'])\n",
      "\n",
      "#find the total purchases for each user\n",
      "\n",
      "total_purchases = purchase_data.groupby(\n",
      "\tby=['uid'], as_index=False).purchase.sum()\n",
      "\n",
      "#find the mean number of purchase per user\n",
      "total_purchases.purchase.mean()\n",
      "\n",
      "print('total purchases average does not make alot of sense, instead try min and max')\n",
      "\n",
      "\n",
      "#find the min and max number of purchases per users in the time period\n",
      "\n",
      "total_purchases.purchase.min()\n",
      "total_purchases.purchase.max()\n",
      "\n",
      "    user days\n",
      "\n",
      "user interactions on a given day\n",
      "1. more convenient than users by itself\n",
      "2. not required to track users actions across time\n",
      "3. can treat simpler actions as responses to the test\n",
      "\n",
      "\n",
      "total_purchases = purchase_data.groupby(\n",
      "\tby=['uid','date'], as_index=False).purchase.sum()\n",
      "\n",
      "total_purchases.purchase.mean()\n",
      "users in the time period\n",
      "total_purchases.purchase.min()\n",
      "total_purchases.purchase.max()\n",
      "\n",
      "\n",
      "   Randomize by user\n",
      "1. best to randomize by individuals regardless of our experimental unit\n",
      "2. otherwise users can have inconsistent experience\n",
      "\n",
      "important to build intuition about your users and data overall\n",
      "\n",
      "\n",
      "   sample  > calculate the user average purchase per day\n",
      "\n",
      "# Extract the 'day'; value from the timestamp\n",
      "purchase_data.date = purchase_data.date.dt.floor('d')\n",
      "\n",
      "# Replace the NaN price values with 0 \n",
      "purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)\n",
      "\n",
      "# Aggregate the data by 'uid' & 'date'\n",
      "purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)\n",
      "revenue_user_day = purchase_data_agg.sum()\n",
      "\n",
      "# Calculate the final average\n",
      "revenue_user_day = revenue_user_day.price.mean()\n",
      "print(revenue_user_day) \n",
      "\n",
      "\n",
      "output:\n",
      "407.33800579385104\n",
      "\n",
      "\n",
      "    Preparing to run an ab test\n",
      "\n",
      "current paywall: \"I hope you are enjoying the relaxing benefits of our app.  Consider making a purchase\"\n",
      "\n",
      "proposed Paywall: \"don't miss out! try one of our new products!\"\n",
      "\n",
      "Questions:\n",
      "Will updating the paywall text impact our revenue\n",
      "How do our three different consumable prices impact this?\n",
      "\n",
      "Considerations in test design\n",
      "1. can our test be run well in practice\n",
      "2. will we be able to derive meaningful results from it\n",
      "\n",
      "Test sensitivity\n",
      "1. What size of impact is meaningful to detect\n",
      "\n",
      "smaller changes are more difficult to detect and can be hidden by randomness\n",
      "\n",
      "Sensitivity is the minimum level of change we want to be able to detect in our tests\n",
      "\n",
      "     Calculating the revenue per user\n",
      "\n",
      "purchase_data = demographics_data.merge(\n",
      "\tpaywall_views, how='left', on=['uid'])\n",
      "\n",
      "total_revenue = purchase_data.groupby(by=['uid'], as_index=False).price.sum()\n",
      "\n",
      "total_revenue.price = np.where(\n",
      "\tnp.isnan(total_revenue.price),0, total_revenue.price)\n",
      "\n",
      "#calculate the average revenue per user\n",
      "\n",
      "avg_revenue = total_revenue.price.mean()\n",
      "\n",
      "print(avg_revenue)\n",
      "16\n",
      "\n",
      "#find the 1% 10% and 20% change in revenue\n",
      "\n",
      "avg_revenue *1 1.01\n",
      "16.32\n",
      "avg_revenue *1 1.10\n",
      "17.77\n",
      "avg_revenue *1 1.20\n",
      "19.39\n",
      "\n",
      "    Data variability\n",
      "1. important to understand the variability in the data\n",
      "2. does the amount spent vary alot among users\n",
      "a. if it does not then it will be easier to detect a change\n",
      "\n",
      "\n",
      "#calculate the standard deviation of revenue per user\n",
      "\n",
      "revenue_variation = total_revenue.price.std()\n",
      "\n",
      "print(revenue_variation)\n",
      "\n",
      "17.520\n",
      "\n",
      "notice the standard deviation is roughly 100% of what the mean average of 16 is.\n",
      "\n",
      "revenue_variation/avg_revenue\n",
      "1.084\n",
      "\n",
      "\n",
      "#find the average number of purchases per user\n",
      "avg_purchases = total_purchases.purchase.mean()\n",
      "3.15\n",
      "\n",
      "purchase_variation = total_purchases.purchase.std()\n",
      "2.68\n",
      "\n",
      "purchase_variation/avg_purchases\n",
      "0.850\n",
      "\n",
      "Primary goal is the increase revenue\n",
      "1. paywall view to purchase conversion rate\n",
      "a. more granular than overall revenue\n",
      "b. directly related to our test\n",
      "\n",
      "Experimental unit: paywall views\n",
      "1. simplest to work with\n",
      "2. assuming these interactions are independent\n",
      "\n",
      "\n",
      "     finding the baseline conversion rate\n",
      "\n",
      "purchase_data = demographic_data.merge(\n",
      "\tpaywall_views, how='inner', on=['uid']\n",
      ")\n",
      "\n",
      "conversion_rate = (sum(purchase_data.purchase) /\n",
      "\tpurchase_data.purchase.count())\n",
      "\n",
      "print(conversion_rate)\n",
      "\n",
      "0.347\n",
      "\n",
      "      sample get the sum and count\n",
      "\n",
      "# Merge and group the datasets\n",
      "purchase_data = demographics_data.merge(paywall_views,  how='left', on=['uid'])\n",
      "purchase_data.date = purchase_data.date.dt.floor('d')\n",
      "\n",
      "# Group and aggregate our combined dataset \n",
      "daily_purchase_data = purchase_data.groupby(by=['uid'], as_index=False)\n",
      "daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})\n",
      "\n",
      "# Find the mean of each field and then multiply by 1000 to scale the result\n",
      "daily_purchases = daily_purchase_data.purchase['sum'].mean()\n",
      "daily_paywall_views = daily_purchase_data.purchase['count'].mean()\n",
      "daily_purchases = daily_purchases * 1000\n",
      "daily_paywall_views = daily_paywall_views * 1000\n",
      "\n",
      "print(daily_purchases)\n",
      "print(daily_paywall_views)\n",
      "\n",
      "3150.0 (purchases)\n",
      "90814.54545454546 (number of views)\n",
      "\n",
      "\n",
      "        calculating lift dependent upon sensitivity\n",
      "\n",
      "small_sensitivity = 0.1 \n",
      "\n",
      "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
      "small_conversion_rate = conversion_rate * (1 + small_sensitivity) \n",
      "\n",
      "# Apply the new conversion rate to find how many more users per day that translates to\n",
      "small_purchasers = daily_paywall_views * small_conversion_rate\n",
      "\n",
      "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
      "purchaser_lift = small_purchasers - daily_purchases\n",
      "\n",
      "print(small_conversion_rate)\n",
      "print(small_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "\n",
      "0.03814800000000001 (small conversion rate)\n",
      "3499.384706400001 (small purchasers)\n",
      "317.58470640000087 (lift)\n",
      "\n",
      "  > medium sensitivity\n",
      "\n",
      "medium_sensitivity = 0.2\n",
      "\n",
      "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
      "medium_conversion_rate = conversion_rate * (1 + medium_sensitivity) \n",
      "\n",
      "# Apply the new conversion rate to find how many more users per day that translates to\n",
      "medium_purchasers = daily_paywall_views * medium_conversion_rate\n",
      "\n",
      "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
      "purchaser_lift = medium_purchasers - daily_purchases\n",
      "\n",
      "print(medium_conversion_rate)\n",
      "print(medium_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "0.041616 (4% conversion rate)\n",
      "3817.5105888000003 (purchasers)\n",
      "635.7105888000001 (lift)\n",
      "\n",
      "     large sensitivity\n",
      "\n",
      "large_sensitivity = 0.5\n",
      "\n",
      "# Find the conversion rate lift with the sensitivity above\n",
      "large_conversion_rate = conversion_rate * (1 + large_sensitivity)\n",
      "\n",
      "# Find how many more users per day that translates to\n",
      "large_purchasers = daily_paywall_views * large_conversion_rate\n",
      "purchaser_lift = large_purchasers - daily_purchases\n",
      "\n",
      "print(large_conversion_rate)\n",
      "print(large_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "0.052020000000000004\n",
      "4771.888236000001\n",
      "1590.0882360000005\n",
      "\n",
      "Awesome! While it seems that a 50% increase may be too drastic and unreasonable to expect, the small and medium sensitivities both seem very reasonable.\n",
      "\n",
      "\n",
      "       standard error\n",
      "\n",
      "\n",
      "# Find the n & v quantities\n",
      "n = purchase_data.purchase.count()\n",
      "\n",
      "# Calculate the quantity \"v\"\n",
      "v = conversion_rate * (1 - conversion_rate) \n",
      "\n",
      "# Calculate the variance and standard error of the estimate\n",
      "var = v / n \n",
      "se = var**0.5\n",
      "\n",
      "print(var)\n",
      "print(se)\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "3.351780834114284e-07\n",
      "0.0005789456653360731\n",
      "\n",
      "     >calculating sample size\n",
      "\n",
      "what is the null hypothesis\n",
      "\n",
      "1. hypothesis that control and treatment have the same impact on response\n",
      "a. updated paywall does not improve conversion rate\n",
      "b. any observed difference is due to randomness\n",
      "\n",
      "rejecting the null hypothesis\n",
      "a. determine their is a difference between the treatment and control\n",
      "b. we say the test has statistical significances\n",
      "\n",
      "\n",
      "\n",
      "Null hypothesis\n",
      "\n",
      "     \ttrue   \t\tfalse\n",
      "accept\tcorrect\t\ttype II error\n",
      "reject\ttype I error\tcorrect\n",
      "\n",
      "types of error & confidence level\n",
      "1. probablilty of not making type 1 error\n",
      "2. higher this value, larger the test sample needed\n",
      "\n",
      "common values is 0.95\n",
      "\n",
      "     >Statistical power\n",
      "\n",
      "statistical power is the probability of finding a statistically siginificant result when the null hypothesis is false\n",
      "\n",
      "confidence level\n",
      "standard error\n",
      "statistical power\n",
      "test sensitivity\n",
      "\n",
      "\n",
      "as the sample size increases so does our power increase\n",
      "\n",
      "\n",
      "    calculating our needed sample size\n",
      "\n",
      "baseline conversion rate 0.3468\n",
      "confidence level: 0.95\n",
      "desired power: 0.80\n",
      "sensitivity=0.1\n",
      "\n",
      "sample_size_group=get_sample(size(0.8, conversion_rate *1.1, 0.95)\n",
      "\n",
      "print(sample_size_per_group)\n",
      "\n",
      "output:\n",
      "45788\n",
      "\n",
      "\n",
      "      >generality of this function\n",
      "\n",
      "function shown specific to conversion rate calculations\n",
      "\n",
      "different response variables have different buy analogous formulas\n",
      "\n",
      "\n",
      "  > decreasing the need sample size\n",
      "\n",
      "* choose a unit of observation with lower variability\n",
      "\n",
      "* excluding users irrelevant to the process/change\n",
      "\n",
      "* think through how different factors relate to the sample size\n",
      "\n",
      "\n",
      "\n",
      "       increase the confidence level\n",
      "\n",
      "# Look at the impact of sample size increase on power\n",
      "n_param_one = get_power(n=1000, p1=p1, p2=p2, cl=cl)\n",
      "n_param_two = get_power(n=2000, p1=p1, p2=p2, cl=cl)\n",
      "\n",
      "# Look at the impact of confidence level increase on power\n",
      "alpha_param_one = get_power(n=n1, p1=p1, p2=p2, cl=0.8)\n",
      "alpha_param_two = get_power(n=n1, p1=p1, p2=p2, cl=0.95)\n",
      "    \n",
      "# Compare the ratios\n",
      "print(n_param_two / n_param_one)\n",
      "print(alpha_param_one / alpha_param_two)\n",
      "\n",
      "\n",
      "1.7596440001351992  (change sample size)\n",
      "1.8857367092232278  (change confidence levels)\n",
      "\n",
      "\n",
      "\n",
      "With these particular values it looks like decreasing our confidence level has a slightly larger impact on the power than increasing our sample size\n",
      "\n",
      "      calculate the conversion rate\n",
      "\n",
      "# Merge the demographics and purchase data to only include paywall views\n",
      "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
      "                            \n",
      "# Find the conversion rate\n",
      "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
      "            \n",
      "print(conversion_rate)\n",
      "\n",
      "0.03468607351645712\n",
      "\n",
      "    > calculate sample size\n",
      "\n",
      "# Merge the demographics and purchase data to only include paywall views\n",
      "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
      "                            \n",
      "# Find the conversion rate\n",
      "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
      "\n",
      "# Desired Power: 0.8\n",
      "# CL: 0.90\n",
      "# Percent Lift: 0.1\n",
      "p2 = conversion_rate * (1 + 0.1)\n",
      "sample_size = get_sample_size(0.8, conversion_rate, p2, 0.90)\n",
      "print(sample_size)\n",
      "\n",
      "36101\n",
      "\n",
      "\n",
      "# Desired Power: 0.95\n",
      "# CL 0.90\n",
      "# Percent Lift: 0.1\n",
      "p2 = conversion_rate * (1 + 0.1)\n",
      "sample_size = get_sample_size(0.95, conversion_rate, p2, 0.90)\n",
      "print(sample_size)\n",
      "\n",
      "63201\n",
      "\n",
      "\n",
      "      analyzing the ab test results\n",
      "\n",
      "compare the two groups purchase rates\n",
      "\n",
      "test_demographics = pd.read_csv('test_demographics.csv')\n",
      "\n",
      "#results for our ab test\n",
      "#group column c for control | v for variant\n",
      "\n",
      "test_results=pd.read_csv('ab_test_results.csv')\n",
      "test_results.head()\n",
      "\n",
      "uid\n",
      "date\n",
      "purchase\n",
      "sku\n",
      "price\n",
      "group\n",
      "\n",
      "\n",
      "    confirming our test results\n",
      "\n",
      "does the data look reasonable\n",
      "\n",
      "\n",
      "test_results_grpd = test_results.groupby(\n",
      "\tby=['group'], as_index=False)\n",
      "\n",
      "test_results_grpd.uid.count()\n",
      "\n",
      "48236\n",
      "49867\n",
      "\n",
      "\n",
      "test_results_demo = test_results.merge(\n",
      "\ttest_demo, how='inner', on='uid')\n",
      "\n",
      "test_results_grpd = test_results_demo.groupby(\n",
      "\tby=['country','gender','device','group'],\n",
      "as_index=False)\n",
      "\n",
      "test_results_grd.uid.count()\n",
      "\n",
      "\n",
      "    > find the mean conversion\n",
      "\n",
      "test_results_summary= test_results_demo.groupby(\n",
      "\tby=['group'], as_index=False\n",
      ").agg({'purchase':['count','sum']})\n",
      "\n",
      "test_results_summary['conv'] = (test_results_summary.purchase['sum']/\n",
      "\ttest_results_summary.purchase['count'])\n",
      "\n",
      "test_results_summary\n",
      "\n",
      "grp  sum   count   conversion\n",
      "c    48236 1657    0.034351\n",
      "v    49867 2094    0.041984\n",
      "\n",
      "Is the result statistically significant\n",
      "1. are the conversion rates different enough\n",
      "2. if yes then reject the null hypothesis\n",
      "3. conclude that the paywalls have different effects\n",
      "4. if no then it may just be randomness\n",
      "\n",
      "    p -value\n",
      "\n",
      "probability if the null hypothesis is true\n",
      "\n",
      "of observing a value as or more extreme\n",
      " \n",
      "what does a low p-value mean\n",
      "1. the power is low\n",
      "2. the observation is unlikely to happen due to randomness\n",
      "\n",
      "\n",
      "<0.01 very strong evidence against the null hypothesis\n",
      "\n",
      "0.01-0.5 strong evidence against the null hypothesis\n",
      "0.05-1. very weak evidence against the null hypothesis\n",
      ">0.1 small or no evidence against the null hypothesis\n",
      "\n",
      "\n",
      "?     sample test the null hypothesis\n",
      "\n",
      "# Compute and print the results\n",
      "results = ab_test_results.groupby('group').agg({'uid':pd.Series.nunique}) \n",
      "print(results)\n",
      "\n",
      "\n",
      "   uid\n",
      "group        \n",
      "C      2825.0\n",
      "V      2834.0\n",
      "\n",
      "\n",
      "# Find the unique users in each group \n",
      "results = ab_test_results.groupby('group').agg({'uid': pd.Series.nunique}) \n",
      "\n",
      "# Find the overall number of unique users using \"len\" and \"unique\"\n",
      "unique_users = len(ab_test_results.uid.unique()) \n",
      "\n",
      "# Find the percentage in each group\n",
      "results = results / unique_users * 100\n",
      "print(results)\n",
      "\n",
      "     uid\n",
      "group           \n",
      "C      49.920481\n",
      "V      50.079519\n",
      "\n",
      "\n",
      "   find the number of users in group device and gender\n",
      "\n",
      "# Find the unique users in each group, by device and gender\n",
      "results = ab_test_results.groupby(by=['group', 'device', 'gender']).agg({'uid': pd.Series.nunique}) \n",
      "\n",
      "# Find the overall number of unique users using \"len\" and \"unique\"\n",
      "unique_users = len(ab_test_results.uid.unique())\n",
      "\n",
      "# Find the percentage in each group\n",
      "results = results / unique_users * 100\n",
      "print(results)\n",
      "\n",
      "uid\n",
      "group device gender           \n",
      "C     and    F       14.896625\n",
      "             M       13.518289\n",
      "      iOS    F       11.309419\n",
      "             M       10.196148\n",
      "V     and    F       14.861283\n",
      "             M       13.659657\n",
      "      iOS    F       10.920657\n",
      "             M       10.637922\n",
      "\n",
      "\n",
      "     understanding statistical significance\n",
      "\n",
      "distribution of expected difference between control and test groups _if_ the null hypothesis is true\n",
      "\n",
      "The red line is the observed difference in the conversion rates from our tests\n",
      "\n",
      "p-value: probability of being as or more extreme than the red line on either side of the distribution.\n",
      "\n",
      "\n",
      "def get_pvalue ( con_conv, test_conv, con_size, test_size):\n",
      "\n",
      "\tlift= - abs(test_conv - con_conv)\n",
      "\tscale_one = con_conv * (1-con_conv) * (1/con_size)\n",
      "\tscale_two= test_conv * (1-test_conv) * (1/test_size)\n",
      "\tscale_val = (scale_one + scale_two) **0.5\n",
      "\tp_value=2*stats.norm.cdf(lift, loc=0, scale=scale_val)\n",
      "\treturn p_value\n",
      "\n",
      "\n",
      "con_conv=0.034351\n",
      "test_conv=0.041984\n",
      "con_size=48236\n",
      "test_size=49867\n",
      "\n",
      "p_value=get_pvalue(con_conv,test_conv,con_size,test_size)\n",
      "print(p_value)\n",
      "\n",
      "4.2572974 e-10  (extremely small p-value)\n",
      "\n",
      "accept the null hypothesis\n",
      "\n",
      "\n",
      "    find the power of the test\n",
      "\n",
      "def get_power(n, p1, p2, cl):\n",
      "    alpha = 1 - cl\n",
      "    qu = stats.norm.ppf(1 - alpha/2)\n",
      "    diff = abs(p2-p1)\n",
      "    bp = (p1+p2) / 2\n",
      "    \n",
      "    v1 = p1 * (1-p1)\n",
      "    v2 = p2 * (1-p2)\n",
      "    bv = bp * (1-bp)\n",
      "    \n",
      "    power_part_one = stats.norm.cdf((n**0.5 * diff - qu * (2 * bv)**0.5) / (v1+v2) ** 0.5)\n",
      "    power_part_two = 1 - stats.norm.cdf((n**0.5 * diff + qu * (2 * bv)**0.5) / (v1+v2) ** 0.5)\n",
      "    \n",
      "    power = power_part_one + power_part_two\n",
      "    \n",
      "    return (power)\n",
      "\n",
      "\n",
      "power= get_power (test_size, con_conv, test_conv, 0.95)\n",
      "print(power)\n",
      "0.9999925941372282\n",
      "\n",
      "\n",
      "small p-value and nearly perfect power\n",
      "\n",
      "        confidence interval\n",
      "\n",
      "ranges of values for our estimation rather than a single number\n",
      "\n",
      "provides context for our estimation process\n",
      "\n",
      "series of repeated experiments\n",
      "1. the calculated intervals will contain the true parameter x% of the time\n",
      "2. the true conversion rate is fixed quantity, it is the interval that is random not the conversion rate.\n",
      "\n",
      "\n",
      "The estimated parameter or difference in conversion rate follows a normal distribution\n",
      "\n",
      "1. we can estimate the standard deviation\n",
      "2. the mean of this distribution\n",
      "\n",
      "alpha is the desired confidence interval width\n",
      "\n",
      "bounds containing X% of hte probabilty around the mean (95%) of that distribution\n",
      "\n",
      "\n",
      "from scipy import stats\n",
      "\n",
      "def get_ci(test_conv, con_conv, test_size, con_size, ci):\n",
      "\n",
      "\tsd=((test_conv * (1-test_conv))/test_size+\n",
      "\t(con_conv * (1-con_conv)) / con_size)**0.5\n",
      "\n",
      "\tlift=test_conv - con_conv\n",
      "\n",
      "\tval=stats.norm.isf((1-ci)/2)\n",
      "\tlwr_bnd=lift - val *sd\n",
      "\tupr_bnd=lift+ val*sd\n",
      "\treturn ((lwr_bnd,upr_bnd))\n",
      "\n",
      "\n",
      "    get p-value\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=0.1, test_conv=0.17, con_size=1000, test_size=1000)\n",
      "print(p_value)\n",
      "\n",
      "4.131297741047306e-06\n",
      "\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=.1, test_conv=.15, con_size=100, test_size=100)\n",
      "print(p_value) \n",
      "\n",
      "0.28366948940702086\n",
      "\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=.48, test_conv=.5, con_size=1000, test_size=1000)\n",
      "print(p_value)\n",
      "\n",
      "0.370901935824383\n",
      "\n",
      "\n",
      "To recap we observed that a large lift makes us confident in our observed result, while a small sample size makes us less so, and ultimately high variance can lead to a high p-value!\n",
      "\n",
      "\n",
      "    check for statistically signficant\n",
      "\n",
      "\n",
      "cont_conv=0.09096495570387314 \n",
      "test_conv=0.1020053238686779 \n",
      "con_size=5329 \n",
      "test_size=5748\n",
      "\n",
      "# Compute the p-value\n",
      "p_value = get_pvalue(con_conv=cont_conv, test_conv=test_conv, con_size=cont_size, test_size=test_size)\n",
      "print(p_value)\n",
      "\n",
      "# Check for statistical significance\n",
      "if p_value >= 0.05:\n",
      "    print(\"Not Significant\")\n",
      "else:\n",
      "    print(\"Significant Result\")\n",
      "\n",
      "\n",
      "  > confidence interval\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, 0.975, 0.5)\n",
      "print(confidence_interval)\n",
      "\n",
      "(0.9755040421682947, 1.0244959578317054)\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, .95, 2)\n",
      "print(confidence_interval)\n",
      "\n",
      "2 standard deviations\n",
      "\n",
      "(0.6690506448818785, 1.3309493551181215)\n",
      "\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, 0.95, .001)\n",
      "print(confidence_interval)\n",
      "\n",
      "(1.0, 1.0)\n",
      "\n",
      "\n",
      "As our standard deviation decreases so too does the width of our confidence interval. Great work!\n",
      "\n",
      "\n",
      "con_conv=0.034351\n",
      "test_conv=0.041984\n",
      "con_size=48236\n",
      "test_size=49867\n",
      "ci=.95\n",
      "\n",
      "# Calculate the mean of our lift distribution \n",
      "lift_mean = test_conv -cont_conv\n",
      "\n",
      "# Calculate variance and standard deviation \n",
      "lift_variance = (1 - test_conv) * test_conv /test_size + (1 - cont_conv) * cont_conv / cont_size\n",
      "lift_sd = lift_variance**0.5\n",
      "\n",
      "# Find the confidence intervals with cl = 0.95\n",
      "confidence_interval = get_ci(lift_mean, 0.95,lift_sd)\n",
      "print(confidence_interval)\n",
      "\n",
      "confidence interval:\n",
      "(0.011039999822042502, 0.011040000177957487)\n",
      "\n",
      "Notice that our interval is very narrow thanks to our substantial lift and large sample size.\n",
      "\n",
      "      interpreting your results\n",
      "\n",
      "report \n",
      "\t\tTest Group\tControl Group\n",
      "1. Sample size  \t7030\t6970\n",
      "2. run time\t2 weeks\t\t2weeks\n",
      "3. mean\t\t3.12\t\t2.69\n",
      "4. variance\t3.20\t\t2.64\n",
      "5. est lift\t0.56\n",
      "6. conf level\t0.56 += 0.4\n",
      "\n",
      "* significant at the 0.05 level\n",
      "\n",
      "visualization\n",
      "\n",
      "histograms: bucketed counts of observations across values\n",
      "\n",
      "user data rolled up to group and user level\n",
      "uid\n",
      "group\n",
      "purchase\n",
      "\n",
      "var=results[results.group=='V']\n",
      "con=results[results.group=='C']\n",
      "\n",
      "plt.hist(var['purchase'],color='yellow',\n",
      "\talpha=0.8, bins=50, label='Test')\n",
      "plt.hist(con['purchase'], color='blue',\n",
      "\talpha=0.8, bins=50, label='Control')\n",
      "plt.legend(loc='upper right')\n",
      "\n",
      "\n",
      "plt.axvline(x= np.mean(results.purchase),\n",
      "\tcolor='red')\n",
      "plt.axvline(x=np.mean(results.purchase),\n",
      "\tcolor='green')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >plotting a distribution\n",
      "\n",
      "mean_con=0.090965\n",
      "mean_test=0.102005\n",
      "var_con=(mean_con * (1-mean_con))/58583\n",
      "var_test=(mean-test *(1-mean_test))/56350\n",
      "\n",
      "con_line = np.linspace(-3*var_con**0.5+mean_con,\n",
      "\t3*vr_con**0.5+mean_con, 100)\n",
      "\n",
      "test_linenp.linspace(-3*var_test**0.5+mean_test,\n",
      "\t3*vr_test**0.5+mean_test, 100)\n",
      "\n",
      "\n",
      "\n",
      "#plot the probabilities across the distribution of conversion rates\n",
      "\n",
      "plt.plot(con_line, mlab.normpdf(\n",
      "\tcon_line, mean_con, var_con**0.5)\n",
      ")\n",
      "\n",
      "plt.plot(test_line, mlab.normpdf(\n",
      "\ttest_line, mean_test, var_test**0.5)\n",
      ")\n",
      "plt.show()\n",
      "\n",
      "mlab.normpdf(): converts values to probablities from Normal Distribution\n",
      "\n",
      "   plotting the difference of conversion rates\n",
      "\n",
      "lift= mean_test - mean_control\n",
      "var = var_test + var_control\n",
      "\n",
      "variance is the sum of variances\n",
      "\n",
      "diff_line = np.linspace(-3*var**0.5 + lift,\n",
      "3*var**0.5 + lift, 100)\n",
      "\n",
      "plt.plot(diff_line, mlab.normpdf(\n",
      "\tdiff_line, lift, var**0.5)\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pandas time series.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pandas time series.txt\n",
      "How to plot data without a date\n",
      "\n",
      "data={'key':[0,1,2,3],'data_values':[100,150,400,200]}\n",
      "data=pd.DataFrame(data)\n",
      "data2={'key':[0,1,2,3],'data_values':[45,98,200,300]}\n",
      "data2=pd.DataFrame(data2)\n",
      "data.set_index('key')\n",
      "data2.set_index('key')\n",
      "print(data)\n",
      "\n",
      "fig, axs = plt.subplots(2,1, figsize=(5,10))\n",
      "data.iloc[:,1].plot(y='data_values',ax=axs[0])\n",
      "data2.iloc[:,1].plot(y='data_values',ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "\n",
      " adding an x time stamp\n",
      "\n",
      "data={'time':['2020-04-01','2020-04-02','2020-04-03','2020-04-04'],'data_values':[100,150,400,200]}\n",
      "data=pd.DataFrame(data)\n",
      "data2={'time':['2020-04-01','2020-04-02','2020-04-03','2020-04-04'],'data_values':[45,98,200,300]}\n",
      "\n",
      "data2=pd.DataFrame(data2)\n",
      "data.set_index('time')\n",
      "data2.set_index('time')\n",
      "print(data)\n",
      "\n",
      "fig, axs = plt.subplots(2,1, figsize=(5,10))\n",
      "data.plot(x='time',y='data_values',ax=axs[0])\n",
      "data2.plot(x='time', y='data_values',ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "\n",
      " linear regression\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "# Prepare input and output DataFrames\n",
      "X = boston[['AGE']]\n",
      "y = boston[['RM']]\n",
      "\n",
      "# Fit the model\n",
      "model = linear_model.LinearRegression()\n",
      "model.fit(X, y)\n",
      "\n",
      "print(new_inputs.reshape(-1,1))\n",
      "predictions = model.predict(new_inputs.reshape(-1,1))\n",
      "\n",
      "# Visualize the inputs and predicted values\n",
      "plt.scatter(new_inputs, predictions, color='r', s=3)\n",
      "plt.xlabel('inputs')\n",
      "plt.ylabel('predictions')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " generating time\n",
      "\n",
      "generates 11 numbers \n",
      "\n",
      "indices=np.arange(0,10)  \n",
      "print(indices)\n",
      "\n",
      "creates 10 evenly spaced numbers starting with 1 and ending with 10\n",
      "print(np.linspace(1,10,10))\n",
      "\n",
      "\n",
      "import librosa as lr\n",
      "from glob import glob\n",
      "\n",
      "# List all the wav files in the folder\n",
      "audio_files = glob(data_dir + '/*.wav')\n",
      "\n",
      "# Read in the first audio file, create the time array\n",
      "audio, sfreq = lr.load(audio_files[0])\n",
      "time = np.arange(0, len(audio)) / sfreq\n",
      "\n",
      "\n",
      "# Plot audio over time\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(time, audio)\n",
      "ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      " >read in stock prices per year\n",
      "\n",
      "# Read in the data\n",
      "data = pd.read_csv('prices.csv', index_col=0)\n",
      "\n",
      "# Convert the index of the DataFrame to datetime\n",
      "data.index = pd.to_datetime(data.index)\n",
      "print(data.head())\n",
      "\n",
      "# Loop through each column, plot its values over time\n",
      "fig, ax = plt.subplots()\n",
      "for column in data:\n",
      "    data[column].plot(ax=ax, label=column)\n",
      "ax.legend()\n",
      "plt.show()\n",
      "\n",
      " classification and feature engineering\n",
      "\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "X=np.column_stack([means,maxs,stds])\n",
      "y=labels.reshape([-1,1])\n",
      "model=LinearSVC()\n",
      "model.fit(X,y)\n",
      "\n",
      "predictions=model.predict(X_test)\n",
      "\n",
      "percent_score= sum(predictions==labels_test)/len(labels_test)\n",
      "percent_score= accuracy_score(labels_test,predictions)\n",
      "\n",
      "\n",
      " heartbeat analysis\n",
      "\n",
      "fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True)\n",
      "\n",
      "# Calculate the time array\n",
      "time = np.arange(normal.shape[0]) / sfreq\n",
      "\n",
      "# Stack the normal/abnormal audio so you can loop and plot\n",
      "stacked_audio = np.hstack([normal, abnormal]).T\n",
      "\n",
      "# Loop through each audio file / ax object and plot\n",
      "# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
      "for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
      "    ax.plot(time, iaudio)\n",
      "show_plot_and_make_titles()\n",
      "\n",
      " using the mean to smooth the noise\n",
      "\n",
      "mean_normal = np.mean(normal, axis=1)\n",
      "mean_abnormal = np.mean(abnormal, axis=1)\n",
      "\n",
      "# Plot each average over time\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
      "ax1.plot(time, mean_normal)\n",
      "ax1.set(title=\"Normal Data\")\n",
      "ax2.plot(time, mean_abnormal)\n",
      "ax2.set(title=\"Abnormal Data\")\n",
      "plt.show()\n",
      "\n",
      " Test the data\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "# Initialize and fit the model\n",
      "model = LinearSVC()\n",
      "model.fit(X_train,y_train)\n",
      "\n",
      "# Generate predictions and score them manually\n",
      "predictions = model.predict(X_test)\n",
      "print(sum(predictions == y_test.squeeze()) / len(y_test))\n",
      "\n",
      " Smoothing signal\n",
      "\n",
      "# Rectify the audio signal\n",
      "audio_rectified = audio.apply(np.abs)\n",
      "\n",
      "# Plot the result\n",
      "# figsize parameter 1 is the width and parameter 2 is the height\n",
      "audio_rectified.plot(figsize=(10, 5))\n",
      "plt.show() \n",
      "\n",
      " Rolling and mean\n",
      "\n",
      "# Smooth by applying a rolling mean\n",
      "audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
      "\n",
      "# Plot the result\n",
      "audio_rectified_smooth.plot(figsize=(10, 5))\n",
      "plt.show()\n",
      "\n",
      "\n",
      " cross_val_score\n",
      "\n",
      "means = np.mean(audio_rectified_smooth, axis=0)\n",
      "stds = np.std(audio_rectified_smooth, axis=0)\n",
      "maxs = np.max(audio_rectified_smooth, axis=0)\n",
      "\n",
      "# Create the X and y arrays\n",
      "X = np.column_stack([means, stds, maxs])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "from sklearn.model_selection import cross_val_score\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "print(percent_score)\n",
      "\n",
      " tempo\n",
      "\n",
      "tempos = []\n",
      "for col, i_audio in audio.items():\n",
      "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
      "\n",
      "# Convert the list to an array so you can manipulate it more easily\n",
      "tempos = np.array(tempos)\n",
      "\n",
      "# Calculate statistics of each tempo\n",
      "tempos_mean = tempos.mean(axis=-1)\n",
      "tempos_std = tempos.std(axis=-1)\n",
      "tempos_max = tempos.max(axis=-1)\n",
      "\n",
      "  column_stack\n",
      "\n",
      "# Create the X and y arrays\n",
      "X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "\n",
      "\n",
      "\n",
      "  converting date index to a datetime\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "  grid=True\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "# Plot the entire time series diet and show gridlines\n",
      "diet.plot(grid=True)\n",
      "plt.show()\n",
      "\n",
      " filter on index\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "# Slice the dataset to keep only 2012\n",
      "diet2012 = diet['2012']\n",
      "\n",
      "# Plot 2012 data\n",
      "diet2012.plot(grid=True)\n",
      "plt.show()\n",
      "\n",
      " Differences between two sets of dates\n",
      "\n",
      "set_stock_dates = set(stocks.index)\n",
      "set_bond_dates = set(bonds.index)\n",
      "\n",
      "\n",
      "differences=set_stock_dates - set_bond_dates\n",
      "# Take the difference between the sets and print\n",
      "print(differences)\n",
      "\n",
      "# Merge stocks and bonds DataFrames using join()\n",
      "stocks_and_bonds = stocks.join(bonds, how='inner')\n",
      "\n",
      "\n",
      " Correlation between two variables\n",
      "\n",
      "# Compute percent change using pct_change()\n",
      "returns = stocks_and_bonds.pct_change()\n",
      "\n",
      "# Compute correlation using corr()\n",
      "correlation = returns['SP500'].corr(returns['US10Y'])\n",
      "print(\"Correlation of stocks and interest rates: \", correlation)\n",
      "\n",
      "# Make scatter plot\n",
      "plt.scatter(returns['SP500'],returns['US10Y'])\n",
      "plt.show()\n",
      "\n",
      "        Spectrogram\n",
      "\n",
      "fourier transforms\n",
      "fast or slow moving waves\n",
      "fft show a series of fast and slow wave osciliations in a time series.\n",
      "\n",
      "short-time fourier transform is calculating a fft over a time frame then sliding the window over by one\n",
      "\n",
      "The spectrogram is the square of each sfft\n",
      "\n",
      "we can calculate the stft with librosa\n",
      "\n",
      "the sound frequencies are converted in to decibels which normalizes the average values of all the frequencies\n",
      "\n",
      "we can visualize it with specshow() function\n",
      "\n",
      "from librosa.core import stft, amplitude_to_db\n",
      "from librosa.display import specshow\n",
      "\n",
      "HOP_LENGTH = 2**4\n",
      "SIZE_WINDOW= 2**7\n",
      "\n",
      "#calculate the short fast fourier transform\n",
      "\n",
      "audio_spec = stft(audio, hop_length=HOP_LENGTH, n_fft=SIZE_WINDOW)\n",
      "\n",
      "#convert into decibels\n",
      "spec_db=amplitude_to_db(audio_spec)\n",
      "\n",
      "#visualize\n",
      "specshow(spec_db, sr=sfreq, x_axis='time',\n",
      "\ty_axis='hz', hop_length=HOP_LENGTH)\n",
      "\n",
      "bandwidths=lr.feature.spectral_bandwidth(S=spec)[0]\n",
      "centroids=lr.feature.spectral_centroid(S=spec)[0]\n",
      "\n",
      "ax= spectshow(spec, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "ax.plot(times_spec, centroids)\n",
      "ax.fill_between(times_spec, centroids-bandwidths/2, centroids+bandwidths/2, alpha=0.5)\n",
      "\n",
      "\n",
      "each spectral has different patterns\n",
      "we can use these patterns to distinquish spectrals from one another\n",
      "\n",
      "for example spectral bandwidth and spectral centroids describe where most of the energy is at each moment in time.\n",
      "\n",
      "centroids_all=[]\n",
      "bandwidths_all=[]\n",
      "\n",
      "for spec in spectrograms:\n",
      "\tbandwidths=lr.feature.spectral_bandwidth(S=lr.db_to_amplitude(spec))\n",
      "\tcentroids=lr.feature.spectral_centroid(S=lr.db_to_amplitude(spec))\n",
      "\tbandwidths_all.append(np.mean(bandwidths))\n",
      "\tcentroids_all.appen(np.mean(centroids))\n",
      "\n",
      "#input matrix\n",
      "\n",
      "X= np.column_stack([means,stds,maxs,tempo_mean,tempo_max,tempo_std, bandwidths_all, centroids_all])\n",
      "\n",
      "\n",
      "\n",
      "   sample   >  short term fourier transform heart beat audio\n",
      "#Spectral engineering is one of the most common techniques in machine learning for time series data\n",
      "\n",
      "# Import the stft function\n",
      "from librosa.core import stft\n",
      "\n",
      "# Prepare the STFT\n",
      "HOP_LENGTH = 2**4\n",
      "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n",
      "\n",
      " > sample  > convert the spectral to decibals\n",
      "\n",
      "# Convert into decibels\n",
      "spec_db = amplitude_to_db(spec)\n",
      "\n",
      "# Compare the raw audio to the spectrogram of the audio\n",
      "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
      "axs[0].plot(time, audio)\n",
      "\n",
      "specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "plt.show()\n",
      "\n",
      "#the heartbeats come in pairs as seen by the vertical lines in the spectrogram\n",
      "\n",
      "\n",
      "   >sample  > calculate the bandwidths and centroids\n",
      "\n",
      "# By computing the spectral features, you have a much better idea of what's going on. \t\n",
      "\n",
      "import librosa as lr\n",
      "from librosa.core import amplitude_to_db\n",
      "from librosa.display import specshow\n",
      "\n",
      "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
      "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
      "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
      "\n",
      "\n",
      "# Convert spectrogram to decibels for visualization\n",
      "spec_db = amplitude_to_db(spec)\n",
      "\n",
      "# Display these features on top of the spectrogram\n",
      "fig, ax = plt.subplots(figsize=(10, 5))\n",
      "ax = specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "ax.plot(times_spec, centroids)\n",
      "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
      "ax.set(ylim=[None, 6000])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#You've spent this lesson engineering many features from the audio data - some contain information about how the audio changes in time\n",
      "\n",
      "#Combine all of them into an array that can be fed into the classifier, and see how it does.\n",
      "\n",
      " > sample build the final array for the classifier\n",
      "\n",
      "# Loop through each spectrogram\n",
      "bandwidths = []\n",
      "centroids = []\n",
      "\n",
      "for spec in spectrograms:\n",
      "    # Calculate the mean spectral bandwidth\n",
      "    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
      "    # Calculate the mean spectral centroid\n",
      "    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
      "    # Collect the values\n",
      "    bandwidths.append(this_mean_bandwidth)  \n",
      "    centroids.append(this_mean_centroid)\n",
      "\n",
      "# Create X and y arrays\n",
      "X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "\n",
      "output .48\n",
      "\n",
      "#To improve the accuracy, you want to find the right features that provide relevant information and also build models on much larger data\n",
      "\n",
      "   >Regression\n",
      "\n",
      "regression model predict continueous models\n",
      "\n",
      "regression: a process that results in a formal model of the data\n",
      "correlation: a statistic that describe the data.  how two features correlate between each other.\n",
      "\n",
      "down or up together or an inverse relationship\n",
      "\n",
      "timeseries often have patterns that change over time\n",
      "\n",
      "two timeseries that seem correlated at one moment may not remain so over time\n",
      "\n",
      "fig, axs=plt.subplots(1,2)\n",
      "\n",
      "axs[0].plot(x,c='k',lw=3,alpha=.2)\n",
      "axs[0].plot(y)\n",
      "axs[0].set(xlabel='time',title='X values=time')\n",
      "\n",
      "#encode time as acolor in a scatterplot\n",
      "\n",
      "axs[1].scatter(x_long,y_long, c=np.arange(len(x_long)),cmap='viridis')\n",
      "axs[1].set(xlabel='x',ylabel='y',title='Color=time')\n",
      "\n",
      "  >regression models\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "model=LinearRegression()\n",
      "model.fit(X,y)\n",
      "model.predict(X)\n",
      "\n",
      "  >Ridge (see introductory course on skilearn)\n",
      "\n",
      "alphas=[.1,1e2,1e3]\n",
      "\n",
      "ax.plot(y_test,color='k', alpha=.3,lw=3)\n",
      "\n",
      "for ii, alpha in enumerate(alphas):\n",
      "\ty_predicted=Ridge(alpha=alpha).fit(X_train,y_train).predict(X_test)\n",
      "\tax.plot(y_predict, c=cmap(ii/len(alphas)))\n",
      "\n",
      "ax.legend(['True values','Model 1', 'Model 2', 'Model 3'])\n",
      "ax.set(xlabel='Time')\n",
      "\n",
      "  >Scoring a regression model\n",
      "\n",
      "Correlation (r)\n",
      "Coefficient of Determination(R2)\n",
      "\n",
      "\n",
      "Coefficient of Determination R2\n",
      "1- error(model)/variance(testdata)\n",
      "\n",
      "Error is actual - predicted of the model\n",
      "\n",
      "Variance is the mean squared distance of the data from their mean\n",
      "(x-x_mean) ** 2 / n\n",
      "\n",
      "or\n",
      "\n",
      "np.var(versicolor_petal_length)\n",
      "\n",
      "\n",
      "deviations = np.mean(y_data) - y_data\n",
      "\n",
      "VAR = np.sum(np.square(deviations))\n",
      "\n",
      "R-Squared : what fraction of variation is linear\n",
      "\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "print(r2_score(y_predicted, y_test))\n",
      "\n",
      "\n",
      "   Sample  > plot the prices for Ebay and Yhoo over time\n",
      "\n",
      "# Plot the raw values over time\n",
      "print(prices.columns)\n",
      "prices.plot()\n",
      "plt.show()\n",
      "\n",
      "   SAMPLE  > plot a scatter plot for ebay and yahoo prices\n",
      "\n",
      "# Scatterplot with one company per axis\n",
      "prices.plot.scatter('EBAY', 'YHOO')\n",
      "plt.show()\n",
      "\n",
      "   sample  > plot the scatter plot with color showing the price index\n",
      "# Scatterplot with color relating to time\n",
      "prices.plot.scatter('EBAY', 'YHOO', c=prices.index, \n",
      "                    cmap=plt.cm.viridis, colorbar=False)\n",
      "plt.show()\n",
      "\n",
      "The prices.index are dates.   The color changes over time.\n",
      "\n",
      "<<<<<sample  > add a ridge regressor and do 3 fold cross validation to check the accuracy.\n",
      "\n",
      "\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "# Use stock symbols to extract training data\n",
      "X = all_prices[['EBAY','NVDA','YHOO']]\n",
      "y = all_prices[['AAPL']]\n",
      "\n",
      "# Fit and score the model with cross-validation\n",
      "scores = cross_val_score(Ridge(), X, y, cv=3)\n",
      "print(scores)\n",
      "\n",
      "If Measure of fit of the model is a small value that means model is well fit to the data.\n",
      "\n",
      "If Measure of magnitude of coefficient is a small value that means model is not overfit.\n",
      "\n",
      "  >sample  > calculating the R2 factor\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "# Split our data into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, \n",
      "                                                    train_size=.8, shuffle=False, random_state=1)\n",
      "\n",
      "# Fit our model and generate predictions\n",
      "model = Ridge()\n",
      "model.fit(X_train,y_train)\n",
      "predictions = model.predict(X_test)\n",
      "score = r2_score(y_test, predictions)\n",
      "print(score)\n",
      "\n",
      "output: -5.70939901949\n",
      "\n",
      "ebay, nvda, yhoo are not linear predicters for apple prices\n",
      "\n",
      "  >sample    y_test is falling then predicted apple price is climbing\n",
      "\n",
      "# Visualize our predictions along with the \"true\" values, and print the score\n",
      "fig, ax = plt.subplots(figsize=(15, 5))\n",
      "ax.plot(y_test, color='k', lw=3)\n",
      "ax.plot(predictions, color='r', lw=2)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "The poor r2 score reflects a deviation between the predicted and true time series values.\n",
      "\n",
      "\n",
      "       cleaning and improving the data\n",
      "\n",
      "time data errors often happens because of human error, machine sensor malfunctions, and database failures.\n",
      "\n",
      "\n",
      "interpolation is using time to fill in missing data\n",
      "\n",
      "you can use time to assist in interpolation\n",
      "\n",
      "interpolation means using known values on either side of a gap in the data to make assumptions about what missing\n",
      "\n",
      "#create a boolean mask to find where the missing values are\n",
      "\n",
      "missing= prices.isna()\n",
      "\n",
      "#create a list of interpolated missing values\n",
      "prices_interp = prices.interpolate('linear')\n",
      "\n",
      "ax=prices_interp.plot(c='r')\n",
      "prices.plot(c='k',ax=ax, lw=2)\n",
      "\n",
      "    another way to fix missing data is to transform it so it more better behaved\n",
      "\n",
      "1. smooth the data\n",
      "2. use more complex transformations\n",
      "\n",
      "a common transformation is to standardize the mean and variance over time\n",
      "\n",
      "1. convert the dataset so each point represents the % change over a previous window\n",
      "2. this makes timepoints more comparable to one another if absolute values of data change a lot\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def percent_change(values):\n",
      "\t\"\"\"Calculates the % change between the last value and the mean of previous values\"\"\"\n",
      "\n",
      "\tprevious_values=values[:-1]\n",
      "\tlast_value=values[-1]\n",
      "\tpercent_change=(last_value-np.mean(previous_values))/np.mean(previous_values)\n",
      "\treturn percent_change\n",
      "\n",
      "\n",
      " > pass the percent_change function as a input to rolling prices\n",
      "\n",
      "fig,axs=plt.subplots(1,2,figsize=(10,5))\n",
      "ax=prices.plot(ax=axs[0])\n",
      "\n",
      "ax=prices.rolling(window=20).aggregate(percent_change).plot(ax=axs[1])\n",
      "ax.legend_.set_visible(False)\n",
      "\n",
      "#periods of high or low changes are easier to spot.\n",
      "\n",
      "    Outliers\n",
      "\n",
      "outliers are datapoints that are significantly statistically different from the dataset\n",
      "\n",
      "they have negative effects on the predictive power of your model, biasing it away from its true value\n",
      "\n",
      "One solution: remove or replace outliers with a more representative vlaue\n",
      "\n",
      "** there can be legitimate extreme values\n",
      "\n",
      "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
      "\n",
      "for data, ax in zip([prices,prices_perc_change],axs):\n",
      "\tthis_mean=data.mean()\n",
      "\tthis.std=data.std()\n",
      "\n",
      "\tdata.plot(ax=ax)\n",
      "\tax.axhline(this_mean+this_std*3, ls='--',c='r')\n",
      "\tax.axhline(this_mean-this_std*3, ls='--',c='r')\n",
      "\n",
      "\n",
      "find price outside of 3 std range from the mean\n",
      "\n",
      "#replace the outliers with the median\n",
      "\n",
      "prices_outlier_centered=prices_outlier_perc - prices_outlier_perc.mean()\n",
      "\n",
      "std=prices_outlier_perc.std()\n",
      "\n",
      "outliers=np.abs(prices_outlier_centered)>(std*3)\n",
      "\n",
      "prices_outlier_fixed=prices_outlier_centered.copy()\n",
      "prices_outlier_fixed[outliers]=np.nanmedian(prices_outlier_fixed)\n",
      "\n",
      "fig,axs = plt.subplots(1,2,figsize=(10,5))\n",
      "prices_outlier_centered.plot(ax=axs[0])\n",
      "prices_outlier_fixed.plot(ax=axs[1])\n",
      "\n",
      "\n",
      "   >sample      visualize missing values in continueous data\n",
      "\n",
      "# Visualize the dataset\n",
      "prices.plot(legend=False)\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "# Count the missing values of each time series\n",
      "missing_values = prices.isna().sum()\n",
      "print(missing_values)\n",
      "\n",
      "   sample  > interpolate\n",
      "\n",
      "# Create a function we'll use to interpolate and plot\n",
      "def interpolate_and_plot(prices, interpolation_type):\n",
      "\n",
      "    # Create a boolean mask for missing values\n",
      "    missing_values = prices.isna()\n",
      "\n",
      "    # Interpolate the missing values\n",
      "    prices_interp = prices.interpolate(interpolation_type)\n",
      "\n",
      "    # Plot the results, highlighting the interpolated values in black\n",
      "    fig, ax = plt.subplots(figsize=(10, 5))\n",
      "    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
      "    \n",
      "    # Now plot the interpolated values on top in red\n",
      "    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "# Interpolate using the latest non-missing value\n",
      "interpolation_type = 'zero' #'linear' or 'quadratic'\n",
      "interpolate_and_plot(prices, interpolation_type)\n",
      "\n",
      "\n",
      "  > sample  > rolling percent change\n",
      "\n",
      "# Your custom function\n",
      "def percent_change(series):\n",
      "    # Collect all *but* the last value of this window, then the final value\n",
      "    previous_values = series[:-1]\n",
      "    last_value = series[-1]\n",
      "\n",
      "    # Calculate the % difference between the last value and the mean of earlier values\n",
      "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
      "    return percent_change\n",
      "\n",
      "# Apply your custom function and plot\n",
      "prices_perc = prices.rolling(20).aggregate(percent_change)\n",
      "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
      "plt.show()\n",
      "\n",
      "    sample  > apply  replace_outliers\n",
      "\n",
      "def replace_outliers(series):\n",
      "    # Calculate the absolute difference of each timepoint from the series mean\n",
      "    absolute_differences_from_mean = np.abs(series - np.mean(series))\n",
      "\n",
      "    # Calculate a mask for the differences that are > 3 standard deviations from the mean\n",
      "    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n",
      "    \n",
      "    # Replace these values with the median accross the data\n",
      "    series[this_mask] = np.nanmedian(series)\n",
      "    return series\n",
      "\n",
      "# Apply your preprocessing function to the timeseries and plot the results\n",
      "prices_perc = prices_perc.apply(replace_outliers)\n",
      "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "          creating features over time\n",
      "\n",
      "extract features as they change over time\n",
      "\n",
      "feats=prices.rolling(20).aggregate([np.std,np.max]).dropna()\n",
      "print(feats.head())\n",
      "\n",
      "\n",
      "\n",
      "rolling is a rolling window\n",
      "\n",
      "AIG: std, amax\n",
      "ABt: std, amax\n",
      "\n",
      "always plot properties of your features\n",
      "it will help you spot noise data and outliers\n",
      "\n",
      "    >partial function\n",
      "\n",
      "lets you define a new function with parts of the old one\n",
      "\n",
      "\n",
      "from functools import partial\n",
      "\n",
      "mean_over_first_axis = partial(np.mean, axis=0)\n",
      "\n",
      "print(mean_over_first_axis(a))\n",
      "\n",
      "#the mean function always operates on the first axis\n",
      "\n",
      "percentiles give fine grained summaries of your data\n",
      "\n",
      "print(np.percentile(np.linespace(0,200),q=20))\n",
      "\n",
      "percentiles first input is an array\n",
      "q, the second input is an integer between 0 and 100\n",
      "\n",
      "returns the values of the first input as a percentile of the second input\n",
      "\n",
      "\n",
      "data = np.linspace(0,100)\n",
      "\n",
      "percentile_funcs= [partial(np.percentile, q=ii) for ii in [20,40,60]]\n",
      "\n",
      "percentiles = [i_func(data) for i_func in percentile_funcs]\n",
      "print(percentiles)\n",
      "\n",
      "output: [20,40,60]\n",
      "\n",
      "data.rolling(20).aggregrate(percentiles)\n",
      "\n",
      "         Calculating date-based features      \n",
      "\n",
      "statistical features: are numerical features like mean and standard deviation.\n",
      "human features like days of the week, holidays\n",
      "these features can span multiple years.\n",
      "\n",
      "\n",
      "#ensure index is datetime\n",
      "prices.index=pd.to_datetime(prices.index)\n",
      "\n",
      "#extract datetime features\n",
      "\n",
      "day_of_week_num = prices.index.weekday\n",
      "print(day_of_week_num[:10])\n",
      "output:[0 1 2 3 4 0 1 2 3 4]\n",
      "\n",
      "day_of_week = prices.index.weekday_name\n",
      "print(day_of_week[:10])\n",
      "output:['Monday','Tuesday'...'Friday']\n",
      "\n",
      "\n",
      "   >sample  > rolling window   > visualize min, max,mean, std for ebay\n",
      "\n",
      "# Define a rolling window with Pandas, excluding the right-most datapoint of the window\n",
      "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
      "\n",
      "# Define the features you'll calculate for each window\n",
      "features_to_calculate = [np.min, np.max, np.mean, np.std]\n",
      "\n",
      "# Calculate these features for your rolling window object\n",
      "features = prices_perc_rolling.aggregate(features_to_calculate)\n",
      "\n",
      "# Plot the results\n",
      "ax = features.loc[:\"2011-01\"].plot()\n",
      "prices_perc.loc[:\"2011-01\"].plot(ax=ax, color='k', alpha=.2, lw=3)\n",
      "ax.legend(loc=(1.01, .6))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   percentiles and partial functions\n",
      "\n",
      "# Import partial from functools\n",
      "from functools import partial\n",
      "percentiles = [1, 10, 25, 50, 75, 90, 99]\n",
      "\n",
      "# Use a list comprehension to create a partial function for each quantile\n",
      "percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n",
      "\n",
      "# Calculate each of these quantiles on the data using a rolling window\n",
      "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
      "features_percentiles = prices_perc_rolling.aggregate(percentile_functions)\n",
      "\n",
      "# Plot a subset of the result\n",
      "ax = features_percentiles.loc[:\"2011-01\"].plot(cmap=plt.cm.viridis)\n",
      "ax.legend(percentiles, loc=(1.01, .5))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Extract date features from the data, add them as columns\n",
      "prices_perc['day_of_week'] = prices_perc.index.dayofweek\n",
      "prices_perc['week_of_year'] = prices_perc.index.weekofyear\n",
      "prices_perc['month_of_year'] = prices_perc.index.month\n",
      "\n",
      "# Print prices_perc\n",
      "print(prices_perc)\n",
      "\n",
      "\n",
      "       >Feature extraction\n",
      "\n",
      "time series has a linear flow with relationships between the data\n",
      "\n",
      "information in the past can help predict what happens in the future\n",
      "\n",
      "often the features best-suited to predict a timeseries are previous values of the same timeseries\n",
      "\n",
      "the smoothness of the data help determine how correlated a timepoint is with its neighboring timepoints\n",
      "\n",
      "the amount of auto-correlation in data will impact your models\n",
      "\n",
      "data= pd.Series()\n",
      "\n",
      "shifts=[0,1,2,3,4,5,6,7]\n",
      "\n",
      "many_shifts={'lag_{}'.format(ii): data.shift(ii) for ii in shifts}\n",
      "\n",
      "many_shifts=pd.DataFrame(many_shifts)\n",
      "\n",
      "model=Ridge()\n",
      "model.fit(many_shifts,data)\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots()\n",
      "ax.bar(many_shifts.columns, model_coef_)\n",
      "ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
      "\n",
      "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "\n",
      "    Sample  > time shifted features\n",
      "In machine learning for time series, it's common to use information about previous time points to predict a subsequent time point.\n",
      "\n",
      "# These are the \"time lags\"\n",
      "shifts = np.arange(1, 11).astype(int)\n",
      "\n",
      "print(prices_perc)\n",
      "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
      "shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
      "\n",
      "# Convert into a DataFrame for subsequent use\n",
      "prices_perc_shifted = pd.DataFrame(shifted_data)\n",
      "\n",
      "# Plot the first 100 samples of each\n",
      "ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis)\n",
      "prices_perc.iloc[:100].plot(color='r', lw=2)\n",
      "ax.legend(loc='best')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Replace missing values with the median for each column\n",
      "X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
      "y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
      "\n",
      "# Fit the model\n",
      "model = Ridge()\n",
      "model.fit(X, y)\n",
      "\n",
      "def visualize_coefficients(coefs, names, ax):\n",
      "    # Make a bar plot for the coefficients, including their names on the x-axis\n",
      "    ax.bar(names, coefs)\n",
      "    ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
      "    \n",
      "    # Set formatting so it looks nice\n",
      "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "    return ax\n",
      "\n",
      "# Visualize the output data up to \"2011-01\"\n",
      "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
      "y.loc[:'2011-01'].plot(ax=axs[0])\n",
      "\n",
      "# Run the function to visualize model's coefficients\n",
      "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "Increase the data window from 20 to 40\n",
      "\n",
      "As you can see here, by transforming your data with a larger window, you've also changed the relationship between each timepoint and the ones that come just before it. This model's coefficients gradually go down to zero, which means that the signal itself is smoother over time.\n",
      "\n",
      "\n",
      "     >Cross validating time series data\n",
      "\n",
      "KFold is the most common cross validation\n",
      "\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "cv=KFold(n_splits=5)\n",
      "for tr,tt in cv.split(X,y):\n",
      "\n",
      "always visualize your models behavior during cross validation\n",
      "\n",
      "fig, axs = plt.subplots(2,1)\n",
      "\n",
      "axs[0].scatter(tt,[0]*len(tt),marker='_',s=2,lw=40)\n",
      "axs[0].set(ylim=[-.1,.1],title='Test set indices (color=CV loop)', xlabel='Index of raw data')\n",
      "\n",
      "axs[1].plot(model.predict(X[tt]))\n",
      "axs[1].set(title='Test set predictions on each CV loop', xlabel('Prediction index')\n",
      "\n",
      "\n",
      "from sklearn.model_selection import ShuffleSplit\n",
      "\n",
      "cv=ShuffleSplit(n_splits=3)\n",
      "for tr,tt in cv.split(X,y):\n",
      "\n",
      "\n",
      "    >time series cv iterator  (use only the past to validate)\n",
      "\n",
      "1. generally you should not use datapoints in the future to predict data in the past\n",
      "\n",
      "2. Always use training data from the past to predict the future\n",
      "\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "cv=TimeSeriesSplit(n_splits=10)\n",
      "\n",
      "fig,ax=plt.subplots(figsize=(10,5))\n",
      "\n",
      "for ii,(tr,tt) in enumerate(cv.split(X,y)):\n",
      "\tl1=ax.scatter(tr,[ii]*len(tr), c=[plt.cm.coolwarm(.1)],marker='_',lw=6)\n",
      "\n",
      "\tl2=ax.scatter(tt,[ii]*len(tt), c=[plt.cm.coolwarm(.9)],marker='_',lw=6)\n",
      "\n",
      "\tax.set(ylim[10,-1],title='TimeSeriesSplit behavior', xlabel='data index', ylabel='CV iteration')\n",
      "\tax.legend([l1,l2],['Training','Validation'])\n",
      "\n",
      "\n",
      "only the past is use to validate the data\n",
      "\n",
      "def myfunction(estimator, X,y):\n",
      "\ty_pred=estimator.predict(X)\n",
      "\tmy_custom_score=my_custom_function(y_pred,y)\n",
      "\treturn my_custom_score\n",
      "\n",
      "def my_pearsonr(est,X,y):\n",
      "\ty_pred=est.predict(X).squeeze()\n",
      "\tmy_corrcoef_matrix=np.corrcoef(y_pred,y.squeeze())\n",
      "\tmy_corrcoef = my_corrcoef[1,0]\n",
      "\treturn my_corrcoef\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Sample     Shufflesplit    > visualization by time\n",
      "\n",
      "# Import ShuffleSplit and create the cross-validation object\n",
      "from sklearn.model_selection import ShuffleSplit\n",
      "cv = ShuffleSplit(n_splits=3, random_state=1)\n",
      "\n",
      "# Iterate through CV splits\n",
      "results = []\n",
      "\n",
      "for tr, tt in cv.split(X, y):\n",
      "    # Fit the model on training data\n",
      "    model.fit(X[tr], y[tr])\n",
      "    \n",
      "    # Generate predictions on the test data, score the predictions, and collect\n",
      "    prediction = model.predict(X[tt])\n",
      "    score = r2_score(y[tt], prediction)\n",
      "    results.append((prediction, score, tt))\n",
      "\n",
      "# Custom function to quickly visualize predictions\n",
      "visualize_predictions(results)\n",
      "\n",
      "https://goodboychan.github.io/chans_jupyter/python/datacamp/time_series_analysis/machine_learning/2020/06/18/02-Validating-and-Inspecting-Time-Series-Models.html\n",
      "\n",
      "def visualize_predictions(results):\n",
      "    fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
      "\n",
      "    # Loop through our model results to visualize them\n",
      "    for ii, (prediction, score, indices) in enumerate(results):\n",
      "        # Plot the predictions of the model in the order they were generated\n",
      "        offset = len(prediction) * ii\n",
      "        axs[0].scatter(np.arange(len(prediction)) + offset, prediction, \n",
      "                       label='Iteration {}'.format(ii))\n",
      "\n",
      "        # Plot the predictions of the model according to how time was ordered\n",
      "        axs[1].scatter(indices, prediction)\n",
      "    axs[0].legend(loc=\"best\")\n",
      "    axs[0].set(xlabel=\"Test prediction number\", title=\"Predictions ordered by test prediction number\")\n",
      "    axs[1].set(xlabel=\"Time\", title=\"Predictions ordered by time\")\n",
      "\n",
      "\n",
      "            Sample    using KFold\n",
      "\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "# Create KFold cross-validation object\n",
      "from sklearn.model_selection import KFold\n",
      "cv = KFold(n_splits=10, shuffle=False, random_state=1)\n",
      "\n",
      "# Iterate through CV splits\n",
      "results = []\n",
      "for tr, tt in cv.split(X, y):\n",
      "    # Fit the model on training data\n",
      "    model.fit(X[tr],y[tr])\n",
      "    \n",
      "    # Generate predictions on the test data and collect\n",
      "    prediction = model.predict(X[tt])\n",
      "    results.append((prediction, tt))\n",
      "    \n",
      "# Custom function to quickly visualize predictions\n",
      "visualize_predictions(results)\n",
      "\n",
      "#This time, the predictions generated within each CV loop look 'smoother' than they were before - they look more like a real time series because you didn't shuffle the data\n",
      "\n",
      "\n",
      "     Sample   > Timeseries Split\n",
      "\n",
      "# Import TimeSeriesSplit\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "\n",
      "# Create time-series cross-validation object\n",
      "cv = TimeSeriesSplit(n_splits=10)\n",
      "\n",
      "# Iterate through CV splits\n",
      "fig, ax = plt.subplots()\n",
      "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
      "    # Plot the training data on each iteration, to see the behavior of the CV\n",
      "    ax.plot(tr, ii + y[tr])\n",
      "\n",
      "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
      "plt.show()\n",
      "\n",
      "#Note that the size of the training set grew each time when you used the time series cross-validation object\n",
      "\n",
      "\n",
      "      >Stationary and stability\n",
      "\n",
      "a stationary time series is one that does not change their statistical properties over time.\n",
      "\n",
      "most time series are non-stationary to some extent\n",
      "\n",
      "it has the same mean, standard deviation, and trends\n",
      "\n",
      "cross validation to quantify parameter stability\n",
      "\n",
      "calculate model parameter on each iteration\n",
      "\n",
      "assess parameter stability across all cv split\n",
      "\n",
      "bootstrapping is a way to estimate the confidence using the mean of a group of numbers\n",
      "\n",
      "1. take a random sample of data with replacement\n",
      "2. calculate the mean of the sample\n",
      "3. repeat the process many times\n",
      "4. caclulate the percentiles of the result\n",
      "\n",
      "the result is a 95% confidence interval of the mean of each coefficent\n",
      "\n",
      "from sklearn.utils import resample\n",
      "\n",
      "n_boots=100\n",
      "\n",
      "bootstrap_means=np.zeros(n_boots, n_coefficients)\n",
      "for ii in range(n_boots):\n",
      "\trandom_sample=resample(cv_coefficients)\n",
      "\tbootstrap_means[ii]=random_sample.mean(axis=0)\n",
      "\n",
      "percentiles=np.percentiles(bootstrap_means,(2.5,97.5),axis=0)\n",
      "\n",
      "fig,ax=plt.subplots()\n",
      "\n",
      "ax.scatter(many_shifts.columns,percentiles[0], marker='_',s=200)\n",
      "ax.scatter(many_shifts.columns,percentiles[1], marker='_',s=200)\n",
      "\n",
      "this gives an idea of the variability of the mean across all cross validation iterations\n",
      "\n",
      "      Assessing model performance stability\n",
      "\n",
      "if your using the TimeSeriesSplit, you can plot the models score over time.\n",
      "\n",
      "This is helpful to find certain regions of time that hurt the score\n",
      "\n",
      "it is also import to find non-stationary signals\n",
      "\n",
      "\n",
      "def my_corrcoef(est,X,y):\n",
      "\t\"\"\"return the correlation coefficient between model predictions and a validation set\"\"\"\n",
      "\treturn np.corrcoef(y,est,predict(X))[1,0]\n",
      "\n",
      "first_indices=[data.index[tt[0]] for tr,tt in cv.split(X,y)]\n",
      "\n",
      "cv_scores=cross_val_score(model, X,y,cv=cv, scoring=my_corrcoef)\n",
      "cv_scores=pd.Series(cv_scores, index=first_indices)\n",
      "\n",
      "\n",
      "cv.split rturns a ndarray train set indices and test ndarray set indices\n",
      "\n",
      "find the beginning of each validation block using a list comprehension\n",
      "\n",
      "collect the score and convert them into a pandas series\n",
      "\n",
      "visualize the results as a timeseries\n",
      "\n",
      "fig, axs=plt.subplots(2,1, figsize=(10,5), sharex=True)\n",
      "\n",
      "cv_scores_mean=cv_scores.rolling(10, min_periods=1).mean()\n",
      "\n",
      "cv_scores.plot(ax=axs[0])\n",
      "axs[0].set(title='Validation scores (correlation)', ylim=[0,1])\n",
      "\n",
      "data.plot(ax=axs[1])\n",
      "axs[1].set(title='Validation data')\n",
      "\n",
      "   >restrict to the latest time points to be used in training\n",
      "\n",
      "window=100\n",
      "\n",
      "cv=TimeSeries(n_splits=10, max_train_size=window)\n",
      "\n",
      "\n",
      "      >Sample  > boot strap the data    build the function\n",
      "\n",
      "from sklearn.utils import resample\n",
      "\n",
      "def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
      "    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
      "    # Create our empty array to fill the results\n",
      "    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
      "    for ii in range(n_boots):\n",
      "        # Generate random indices for our data *with* replacement, then take the sample mean\n",
      "        random_sample = resample(data)\n",
      "        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
      "        \n",
      "    # Compute the percentiles of choice for the bootstrapped means\n",
      "    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
      "    return percentiles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['smoothing']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
