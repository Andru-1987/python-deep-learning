{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\importing data from the web.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\importing data from the web.txt\n",
      "uci machine learning repository\n",
      "\n",
      "http get requests\n",
      "\n",
      "BeautifulSoup\n",
      "\n",
      "urllib to fetch data from the web\n",
      "\n",
      "urlopen()\n",
      "\n",
      "from urllib.request import urlretrieve\n",
      "\n",
      "url=\"http://https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
      "urlretrieve(url,\"winequalitiy-white.csv\")\n",
      "\n",
      "\n",
      "   Sample      load wine color red from the repository and load it into a dataframe\n",
      "\n",
      "# Import package\n",
      "from urllib.request import urlretrieve\n",
      "import pandas as pd\n",
      "\n",
      "url=\"https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv\"\n",
      "\n",
      "urlretrieve(url,\"winequality-red.csv\")\n",
      "\n",
      "df = pd.read_csv('winequality-red.csv', sep=';')\n",
      "print(df.head())\n",
      "\n",
      "#plot the acidity\n",
      "\n",
      "# Plot first column of df\n",
      "pd.DataFrame.hist(df.ix[:, 0:1])\n",
      "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
      "plt.ylabel('count')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "????????Sample     read_excel  > print keys and head\n",
      "\n",
      "# Import package\n",
      "import pandas as pd\n",
      "\n",
      "# Assign url of file: url\n",
      "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
      "\n",
      "# Read in all sheets of Excel file: xls\n",
      "xls = pd.read_excel(url, sheet_name=None)\n",
      "\n",
      "# Print the sheetnames to the shell\n",
      "print(xls.keys())\n",
      "\n",
      "# Print the head of the first sheet (using its name, NOT its index)\n",
      "print(xls['1700'].head())\n",
      "\n",
      "      httprequest get\n",
      "\n",
      "from urllib.request import urlopen, Request\n",
      "\n",
      "url=\"https://www.wikipedia.org\"\n",
      "\n",
      "request=Request(url)\n",
      "response= urlopen(request)\n",
      "html=response.read()\n",
      "response.close\n",
      "\n",
      " >Import requests\n",
      "\n",
      "url=\"https://www.wikipedia.org/\"\n",
      "\n",
      "r=requests.get(url)\n",
      "\n",
      "text=r.text\n",
      "\n",
      "\n",
      "  > sample  > scrap a page using request and urlopen\n",
      "\n",
      "# Import packages\n",
      "\n",
      "from urllib.request import urlopen, Request\n",
      "\n",
      "# Specify the url\n",
      "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
      "\n",
      "# This packages the request: request\n",
      "request=Request(url)\n",
      "response= urlopen(request)\n",
      "html=response.read()\n",
      "\n",
      "# Sends the request and catches the response: response\n",
      "\n",
      "\n",
      "# Print the datatype of response\n",
      "print(type(response))\n",
      "\n",
      "# Be polite and close the response!\n",
      "response.close()\n",
      "print(html)\n",
      "\n",
      "\n",
      "    sample    requests get\n",
      "\n",
      "import requests\n",
      "\n",
      "url=\"http://www.datacamp.com/teach/documentation\"\n",
      "\n",
      "r=requests.get(url)\n",
      "\n",
      "text=r.text\n",
      "\n",
      "print(text)\n",
      "\n",
      "        Scrapping the web with python\n",
      "\n",
      "beautifulsoup\n",
      "\n",
      "from urllib.request import urlretrieve,Request,urlopen\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url=\"https://www.woodysmithhyundai.com/\"\n",
      "\n",
      "r=requests.get(url)\n",
      "\n",
      "text=r.text\n",
      "\n",
      "soup=BeautifulSoup(text)\n",
      "\n",
      "print(soup.prettify())\n",
      "\n",
      "print(soup.title)\n",
      "print(soup.get_text())\n",
      "\n",
      "for link in soup.find_all(\"a\"):\n",
      "\tprint(link.get(\"href\"))\n",
      "\n",
      "\n",
      "   Sample  > parse html   > prettify()\n",
      "\n",
      "# Import packages\n",
      "import requests\n",
      "from urllib.request import urlretrieve,Request,urlopen\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Specify url: url\n",
      "url=\"https://www.python.org/~guido/\"\n",
      "\n",
      "# Package the request, send the request and catch the response: r\n",
      "\n",
      "r=requests.get(url)\n",
      "# Extracts the response as html: html_doc\n",
      "html=r.text\n",
      "\n",
      "# Create a BeautifulSoup object from the HTML: soup\n",
      "\n",
      "soup=BeautifulSoup(html)\n",
      "# Prettify the BeautifulSoup object: pretty_soup\n",
      "\n",
      "pretty_soup=soup.prettify()\n",
      "# Print the response\n",
      "print(pretty_soup)\n",
      "\n",
      "  > sample  > scrape for text and title\n",
      "\n",
      "# Import packages\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Specify url: url\n",
      "url = 'https://www.python.org/~guido/'\n",
      "\n",
      "# Package the request, send the request and catch the response: r\n",
      "r = requests.get(url)\n",
      "\n",
      "# Extract the response as html: html_doc\n",
      "html_doc = r.text\n",
      "\n",
      "# Create a BeautifulSoup object from the HTML: soup\n",
      "soup=BeautifulSoup(html_doc)\n",
      "\n",
      "# Get the title of Guido's webpage: guido_title\n",
      "\n",
      "title=soup.title\n",
      "# Print the title of Guido's webpage to the shell\n",
      "\n",
      "print(title)\n",
      "\n",
      "# Get Guido's text: guido_text\n",
      "\n",
      "guido_text=soup.get_text()\n",
      "\n",
      "  > sample  > print all href links in the page\n",
      "\n",
      "# Import packages\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Specify url\n",
      "url = 'https://www.python.org/~guido/'\n",
      "\n",
      "# Package the request, send the request and catch the response: r\n",
      "r = requests.get(url)\n",
      "\n",
      "# Extracts the response as html: html_doc\n",
      "html_doc = r.text\n",
      "\n",
      "# create a BeautifulSoup object from the HTML: soup\n",
      "soup = BeautifulSoup(html_doc)\n",
      "\n",
      "# Print the title of Guido's webpage\n",
      "print(soup.title)\n",
      "\n",
      "# Find all 'a' tags (which define hyperlinks): a_tags\n",
      "\n",
      "a_tags=soup.find_all(\"a\")\n",
      "\n",
      "\n",
      "for link in a_tags:\n",
      "    value=str(link.get(\"href\"))\n",
      "    #print(type(value))\n",
      "    if \"http://\" in value:\n",
      "        print(link.get(\"href\"))\n",
      "\n",
      "\n",
      "         >APIS and JSON\n",
      "application programming interfaces\n",
      "\n",
      "open movie database\n",
      "\n",
      "json : javascript object notation\n",
      "real-time server-to-browser communication\n",
      "human readable\n",
      "\n",
      "json are stored in a dictionary in python\n",
      "\n",
      "objects: string, value, dictionaries, or arrays\n",
      "\n",
      "import json\n",
      "\n",
      "with open(\"snakes.json\", \"r\") as json_file:\n",
      "\tjson_data=json.load(json_file)\n",
      "\n",
      "\n",
      "print (type(json_data))\n",
      "\n",
      "for key,value in json_data.items():\n",
      "\tprint(key+':'+,value)\n",
      "\n",
      "\n",
      " > Sample  > open a json file and read the key value pairs\n",
      "\n",
      "# Load JSON: json_data\n",
      "with open(\"a_movie.json\") as json_file:\n",
      "    json_data=json.load(json_file)\n",
      "\n",
      "# Print each key-value pair in json_data\n",
      "for k in json_data.keys():\n",
      "    print(k + ': ', json_data[k])\n",
      "\n",
      "\n",
      "     Apis and interacting with the world wide web\n",
      "\n",
      "Much of your data will be avaialable through apis\n",
      "\n",
      "import requests\n",
      "\n",
      "url=\"http://www.omdbapi.com/?t=hackers'\n",
      "\n",
      "r=requests.get(url)\n",
      "json_data=r.json()\n",
      "\n",
      "for key,value in json_data.items():\n",
      "\tprint(key+':',value)\n",
      "\n",
      "  > sample  > query title hackers\n",
      "\n",
      "# Import requests package\n",
      "import requests\n",
      "\n",
      "# Assign URL to variable: url\n",
      "url='http://www.omdbapi.com/?t=hackers&apikey=72bc447a'\n",
      "\n",
      "# Package the request, send the request and catch the response: r\n",
      "r = requests.get(url)\n",
      "json_data=r.json()\n",
      "\n",
      "# Print the text of the response\n",
      "for key,value in json_data.items():\n",
      "\tprint(key+':',value)\n",
      "\n",
      "\n",
      "   sample  > print a part of the json tree\n",
      "\n",
      "# Import package\n",
      "import requests\n",
      "\n",
      "# Assign URL to variable: url\n",
      "url=\"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza\"\n",
      "\n",
      "# Package the request, send the request and catch the response: r\n",
      "r = requests.get(url)\n",
      "json_data=r.json()\n",
      "\n",
      "print(json_data)\n",
      "# Print the text of the response\n",
      "for key,value in json_data.items():\n",
      "\tprint(key+':',value)\n",
      "# Decode the JSON data into a dictionary: json_data\n",
      "\n",
      "\n",
      "# Print the Wikipedia page extract\n",
      "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
      "\n",
      "print(pizza_extract)\n",
      "\n",
      "          Interacting with twitter\n",
      "\n",
      "stream data from twitter\n",
      "1. use filters for incoming tweets\n",
      "2. api authentication and oauth\n",
      "3. tweepy python package\n",
      "\n",
      "\n",
      "Application Settings\n",
      "1. Consumer key\n",
      "2. consumer secret\n",
      "3. access level\n",
      "4. owner\n",
      "5. ower id\n",
      "\n",
      "Your access token\n",
      "Access Token\n",
      "Access Token Secret\n",
      "Access level\n",
      "Owner\n",
      "Owner ID\n",
      "\n",
      "tweets are returned as  jsons\n",
      "\n",
      "conda install -c conda-forge tweepy\n",
      "\n",
      "import tweepy, json\n",
      "\n",
      "\n",
      "access_token=\"...\"\n",
      "access_token_secret=\"...\"\n",
      "consumer_key=\"...\"\n",
      "consumer_secret=\"...\"\n",
      "\n",
      "auth=tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token, access_token_secret)\n",
      "\n",
      "\n",
      "class MyStreamListener(tweepy.StreamListener)\n",
      "\tdef __init__(self, api=None):\n",
      "\t\tsuper(MyStreamListener, self).__init__()\n",
      "\t\tself.num_tweets=0\n",
      "\t\tself.file=open(\"tweets.txt\",\"w\")\n",
      "\tdef on_status(self,status):\n",
      "\t\ttweet=status._json\n",
      "\t\tself.file.write(json.dumps(tweet)+'\\\\n')\n",
      "\t\ttweet_list.append(status)\n",
      "\t\tself.num_tweets+=1\n",
      "\t\tif self.num_tweets<100:\n",
      "\t\t\treturn True\n",
      "\t\telse\n",
      "\t\t\treturn False\n",
      "\t\tself.file.close()\n",
      "\n",
      "l=MyStreamListener()\n",
      "stream=tweep.Stream(auth,l)\n",
      "\n",
      "stream.filter(track=['apples','oranges'])\n",
      "\n",
      "  >Sample  > oauth tweet stream\n",
      "\n",
      "# Import package\n",
      "import tweepy, json\n",
      "\n",
      "# Store OAuth authentication credentials in relevant variables\n",
      "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
      "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
      "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
      "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
      "\n",
      "# Pass OAuth details to tweepy's OAuth handler\n",
      "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token, access_token_secret)\n",
      "\n",
      "# Initialize Stream listener\n",
      "l = MyStreamListener()\n",
      "\n",
      "# Create your Stream object with authentication\n",
      "stream = tweepy.Stream(auth, l)\n",
      "\n",
      "# Filter Twitter Streams to capture data by the keywords:\n",
      "stream.filter(['clinton', 'trump', 'sanders', 'cruz'])\n",
      "\n",
      "   sample  > tweets.txt  > load the json \n",
      "\n",
      "# Import package\n",
      "import json\n",
      "\n",
      "# String of path to file: tweets_data_path\n",
      "\n",
      "tweets_data_path=\"tweets.txt\"\n",
      "# Initialize empty list to store tweets: tweets_data\n",
      "tweets_data=[]\n",
      "\n",
      "\n",
      "# Open connection to file\n",
      "tweets_file = open(tweets_data_path, \"r\")\n",
      "\n",
      "# Read in tweets and store in list: tweets_data\n",
      "for line in tweets_file:\n",
      "    tweet=json.loads(line)\n",
      "    tweets_data.append(tweet)\n",
      "    \n",
      "\n",
      "# Close connection to file\n",
      "tweets_file.close()\n",
      "\n",
      "# Print the keys of the first tweet dict\n",
      "print(tweets_data[0].keys())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "dict_keys(['in_reply_to_user_id', 'created_at', 'filter_level', 'truncated', 'possibly_sensitive', 'timestamp_ms', 'user', 'text', 'extended_entities', 'in_reply_to_status_id', 'entities', 'favorited', 'retweeted', 'is_quote_status', 'id', 'favorite_count', 'retweeted_status', 'in_reply_to_status_id_str', 'in_reply_to_user_id_str', 'id_str', 'in_reply_to_screen_name', 'coordinates', 'lang', 'place', 'contributors', 'geo', 'retweet_count', 'source'])\n",
      "\n",
      "\n",
      "# Import package\n",
      "import pandas as pd\n",
      "\n",
      "# Build DataFrame of tweet texts and languages\n",
      "df = pd.DataFrame(tweets_data, columns=[\"user\",\"text\"])\n",
      "\n",
      "# Print head of DataFrame\n",
      "print(df.head())\n",
      "\n",
      "\n",
      "  > Sample   > count by Word in text\n",
      "\n",
      "# Initialize list to store tweet counts\n",
      "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
      "\n",
      "# Iterate through df, counting the number of tweets in which\n",
      "# each candidate is mentioned\n",
      "for index, row in df.iterrows():\n",
      "    clinton += word_in_text('clinton', row['text'])\n",
      "    trump += word_in_text('trump',row['text'])\n",
      "    sanders += word_in_text('sanders', row['text'])\n",
      "    cruz += word_in_text('cruz', row['text'])\n",
      "\n",
      "\n",
      "   sample  > plot the results\n",
      "\n",
      "# Import packages\n",
      "\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Set seaborn style\n",
      "sns.set(color_codes=True)\n",
      "\n",
      "# Create a list of labels:cd\n",
      "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
      "\n",
      "# Plot the bar chart\n",
      "ax = sns.barplot(x=cd,y=[clinton, trump,sanders,cruz])\n",
      "ax.set(ylabel=\"count\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\machine learning for business.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\machine learning for business.txt\n",
      "\n",
      "1. draw causal insight and answer the why questions (what is causing)\n",
      "2. predict future events (which customers)\n",
      "3. understanding pattern in data (are there groups of customers)\n",
      "a. groups that behave in a certain way\n",
      "\n",
      "Data hierarchy of needs\n",
      "1. collection (extract data from source systems)\n",
      "2. storage (reliable storage)\n",
      "3. preparation (organize and clean data to make it usable)\n",
      "4. analysis (understanding trends, distributions and segments) (key indicators, dashboards)\n",
      "5. machine learning, customer satisification (simple models) (prototyping and testing ml)\n",
      "6. ml in production (crm, mobile, research, and automation)\n",
      "\n",
      "unsupervised learning uses data points and assign similar groups into segments.\n",
      "\n",
      "1. predict which customers are likely to purchase next month\n",
      "2. predict each customer's expected lifetime value\n",
      "a. determining who are the high value customers\n",
      "b. use unsupervised learning\n",
      "3. predict customer behaviors like default\n",
      "\n",
      "unsupervised learning is used to group transactions into segments based on their attributes to understand which segments are the most profitable.\n",
      "\n",
      "supervised learning\n",
      "predict which items in production are likely faulty and should be manually inspected.\n",
      "predict which machines are likely to break and need maintenance.\n",
      "\n",
      "unsupervised learning group readings from machine sensors and identify anomolies for potential manufacturing malfunctions.\n",
      "\n",
      "machine learning in transportation\n",
      "a. predicted the expected delivery of the parcel\n",
      "b. identify the fastest route for driving\n",
      "c. predict production demand to prepare enough stock, rent/buy vehicles and hire workers\n",
      "\n",
      "\n",
      "Job roles, tools and technologies\n",
      "\n",
      "1. collection : (software engineers)\n",
      "2. storage : (data engineers and software engineers - focus on data pipelines)\n",
      "3. preparation: (data engineers, data analyst - usable data sets)\n",
      "4. analysis (data analysts, data scientist - dashboards and score cards and analyze trends and machine learning methods to find data signal)\n",
      "5. ml in production : (ml engineers - mobile and crm)\n",
      "\n",
      "team structure\n",
      "1. centralized (all data functions are in one functional team.  works well for small companies)\n",
      "2. decentralized (each business unit has their own data functions. results in siols, redundancies and adds complexity)\n",
      "3. hybrid (infrastructure)\n",
      "\n",
      "\n",
      "prediction vs inference dilemma\n",
      "\n",
      "inference or causal models: the goal is to understand the drivers of a business outcome\n",
      "a) what are the main indicators of churn\n",
      "b) what are the drivers of fraud\n",
      "c) what features are the stickest\n",
      "d) provide interpretable insights (interpretable means that its is easy to explain what the model does)\n",
      "e) less accurate than prediction models.\n",
      "\n",
      "\n",
      "prediction\n",
      "a) get the best prediction no matter how complex the model is.\n",
      "b) work more like a black box.\n",
      "c) more accurate than inference\n",
      "\n",
      "what are the main drivers of fraud (inferences)\n",
      "\n",
      "how much conditions x impact heart attack (inferences)\n",
      "which transactions are likely of fraud (prediction)\n",
      "is the patient at risk of having a heart attack (prediction)\n",
      "\n",
      "\n",
      "inference are interested in how much the attributes affect the outcome probability.\n",
      "\n",
      "\n",
      "inference models\n",
      "\n",
      "what is causality\n",
      "\n",
      "identify causal relationship of how much certain actions affect an outcome of interest\n",
      "\n",
      "answer the why questions\n",
      "optimizes for model interpretability ver performance\n",
      "\n",
      "experiments are designed and causal conclusions are guaranteed\n",
      "\n",
      "experiments are not possible, observation data must be used to determine what data is causual associated.\n",
      " Comparision between healthy class and unhealthy class of people.\n",
      "a.) use periodic experiment results to make decisions\n",
      "b.) build an inference model.\n",
      "\n",
      "\n",
      "Inference\n",
      "regression coefficients tell us how much the feature input affect the outcome\n",
      "\n",
      "\n",
      "supervised model\n",
      "\n",
      "a. predicting class/type of an outcome (classification)\n",
      "\n",
      "b. predicting quantity of outcome (dollars spent, hours played) - regression\n",
      "\n",
      "unsupervised models\n",
      "\n",
      "clustering - grouping observations into similar groups or clusters (customer segmentation and market segmentation)\n",
      "\n",
      "supervised learning types: classification (categorical) and regression (contineous amounts)\n",
      "\n",
      "\n",
      "machine learning teams collect data to predict the maximum outcome possible.\n",
      "a) customer information\n",
      "b) purchase history\n",
      "c) job cancelations\n",
      "d) order amounts\n",
      "e) change order amounts\n",
      "\n",
      "\n",
      "learn the rules to predict on unseen data, predict probability of the result.\n",
      "\n",
      "in regression training, predict an amount for an outcome\n",
      "\n",
      "unsupervised learning models\n",
      "\n",
      "1. Clustering grouping observations into similar groups or clusters \n",
      "a. customer segmentation\n",
      "b. market segmentation\n",
      "\n",
      "Anomaly detection - detecting which observations fall out of the discovered \"regular patterns\" and use it as an input in supervisored learning or business input\n",
      "\n",
      "recommendation \n",
      "\n",
      "example\n",
      "a. Monetary Value, Recency, Frequency\n",
      "\n",
      "The algorithm identifies similar segments or clusters\n",
      "\n",
      "\n",
      "gathering business requirements\n",
      "\n",
      "a. what is the business situation\n",
      "b. assess the business opportunity by identifying the right markets with the biggest demand\n",
      "c. what are the business actions to take. Prioritize and invest more in markets with higher predicted demand\n",
      "- fraud detection (situation, opportunity, and action)\n",
      "- business churn\n",
      "\n",
      "situation\n",
      "1. Ask the right questions\n",
      "2. Start with inference questions ( why or which or how questions)\n",
      "3. Build on the inference question to define the prediction question (identify or measure)\n",
      "\n",
      "opportunity\n",
      "1. don't spend more than you make (rule 1)\n",
      "2. Size up the opportunity\n",
      "a. once you know the drivers of outcome, how much will it cost changing them and what will be the value of doing that\n",
      "b. how do you know if you can affect the predicted outcome\n",
      "\n",
      "action\n",
      "a. even if the predictions are very accurate they may not be actionable\n",
      "b. look at the historical levels\n",
      "c. run experiments and if the experiments have an impact on the targeted group.  If yes, use that to calculate opportunity and make decisions if its a worthwhile investment.\n",
      "if no then collect more data and do qualitative research and narrow down the business question.\n",
      "\n",
      "\n",
      " >model training\n",
      "\n",
      "take the input features and target and train the model to predict on future data.\n",
      "\n",
      "ways to measure classification performance\n",
      "1. accuracy\n",
      "2. recall\n",
      "3. precision\n",
      "\n",
      "churn example\n",
      "a. purchases this year\n",
      "b. number of customer complaints\n",
      "\n",
      "accuracy is all correct predictions / all observations ( true positives)\n",
      "\n",
      "precision is the correct predictions/ all observations predicted of that class\n",
      "\n",
      "recall is the correct predictions/ all actual observations  (true positive/ (true positive + false negatives))\n",
      "\n",
      "there is always tension between precision and recall\n",
      "\n",
      " machine learning risks\n",
      "1. low precision (only a small percentage of true positives are captured by the model)\n",
      "2. low recall (small portion of the observations are captured by the model)\n",
      "3. large error (regression you might get a large error when comparing actual to the predicted)\n",
      "what is the cost to a mistake\n",
      "\n",
      "How to test the model correctly\n",
      "\n",
      "how to a/b test a model\n",
      "\n",
      "a. at risk, run retention campaing\n",
      "b. business as usual\n",
      "\n",
      "measure the experiment against usual\n",
      "\n",
      "What if test don't work?\n",
      "1. get more data\n",
      "2. business has experience of what is predictive\n",
      "3. build causal models to understand drivers\n",
      "4. change the scope of the problem\n",
      "\n",
      "Mistakes\n",
      "\n",
      "1. Machine learning first\n",
      "2. not enough data\n",
      "3. target variable definition\n",
      "4. late testing, no impact\n",
      "5. feature selection\n",
      "\n",
      "Target variable definition:\n",
      "a. what are you predicting.\n",
      "b. Can we observe it (in depth analysis, business involved to define definition)\n",
      "\n",
      "feature selection\n",
      "a. inference - what affects the target variable (latency, price, delivery, customer service)\n",
      "\n",
      "\n",
      "late testing, no impact (determine if any significant difference occurs)\n",
      "\n",
      "Communication\n",
      "1. Schedule recurring meetings to track progress and define the following:\n",
      "a. Define the business requirements\n",
      "b. Review machine learning model and business products\n",
      "c. Define inference versus prediction use cases\n",
      "d. Baseline model results & outline model updates\n",
      "e. Market testing\n",
      "\n",
      "Business Requirements\n",
      "a. What is the business situation\n",
      "b. What is the opportunity size? (expected quantity)\n",
      "c. What business actions will we take (what are the targets)\n",
      "\n",
      "1. inference into drivers of churn updated quarterly\n",
      "a. daily customer classification into lost customers, customers at risk, no risk\n",
      "\n",
      "2. real time list of risky transactions\n",
      "\n",
      "\n",
      "All models are wrong but some are useful.  They will make mistakes.  The business will need to decide the tolerance for model mistakes.\n",
      "\n",
      "identify what is the tolerance for model mistakes\n",
      "a. classification - which class is more expenisve to mis-classify\n",
      "\n",
      "regression\n",
      "\n",
      "\n",
      "the business will have to decide how much more they will spend on waste resource because of the error.\n",
      "\n",
      "what is the error tolerance for prediction\n",
      "\n",
      "machine learning in production\n",
      "a. are test results delivering consistent positive improvements?\n",
      "b. is the model stable enough\n",
      "c. Do we have systems and tools where the model should be integrated to\n",
      "\n",
      "Steps to a target variable >\n",
      "Define the target variable\n",
      "Build a machine learning model predicting the outcome\n",
      "Select customers with the predicted values and split into 2 groups a/b\n",
      "Run certain actions eg marketing campaign, for group A and no actions for group B\n",
      "Measure group A performance vs group B and conclude if there was an impact on the desired outcome\n",
      "\n",
      "\n",
      "  Production systems\n",
      "\n",
      "1. Customer Relationship Management (crm)\n",
      "2. Fraud detection system\n",
      "3. Online banking platform\n",
      "4. Autonomous cars\n",
      "\n",
      "Customer relationships (campaigns)\n",
      "\n",
      "Fraud detections automatically triggers transaction black and requests a manual review\n",
      "\n",
      "Online banking platform\n",
      "a.) discover customer behaviors\n",
      "b.) help discover new products, a recommendor engine, customizing the web experience by only showing relevant products\n",
      "\n",
      "staffing\n",
      "a.) prototyping ml is done by data scientist and ml engineers\n",
      "b.) ml in production is done by software engineers and data engineers and infrastructure owners\n",
      "\n",
      "launch\n",
      "a.) murphs law\n",
      "b.) launch to a small subset of customers\n",
      "c.) track results for consistency\n",
      "d.) track performance, stability, customer feedback\n",
      "e.) scale up\n",
      "f.) repeat 3,4,5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\requirements.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\requirements.txt\n",
      "absl-py                  \n",
      "agate                    \n",
      "agate-dbf                \n",
      "agate-excel              \n",
      "agate-sql                \n",
      "aiohttp                  \n",
      "anytree                  \n",
      "argon2-cffi              \n",
      "astor                    \n",
      "astunparse               \n",
      "async-generator          \n",
      "async-timeout            \n",
      "attrs                    \n",
      "Babel                    \n",
      "backcall                 \n",
      "beautifulsoup4           \n",
      "bleach                   \n",
      "blinker                  \n",
      "blis                     \n",
      "boto                     \n",
      "boto3                    \n",
      "botocore                 \n",
      "Bottleneck               \n",
      "brotlipy                 \n",
      "bz2file                  \n",
      "cachetools               \n",
      "catalogue                \n",
      "certifi                  \n",
      "cffi                     \n",
      "chardet                  \n",
      "click                    \n",
      "cloudpickle              \n",
      "colorama                 \n",
      "coverage                 \n",
      "cryptography             \n",
      "csvkit                   \n",
      "cycler                   \n",
      "cymem                    \n",
      "Cython                   \n",
      "dbfread                  \n",
      "dc-stat-think            \n",
      "deap                     \n",
      "decorator                \n",
      "defusedxml               \n",
      "dill                     \n",
      "entrypoints              \n",
      "et-xmlfile               \n",
      "fonttools                \n",
      "future                   \n",
      "fuzzywuzzy               \n",
      "gast                     \n",
      "gensim                   \n",
      "google-auth              \n",
      "google-auth-oauthlib     \n",
      "google-pasta             \n",
      "googleapis-common-protos \n",
      "graphviz                 \n",
      "greenlet                 \n",
      "grpcio                   \n",
      "gym                      \n",
      "h5py                     \n",
      "hyperopt                 \n",
      "idna                     \n",
      "imageio                  \n",
      "importlib-metadata       \n",
      "ipykernel                \n",
      "ipython                  \n",
      "ipython-genutils         \n",
      "isodate                  \n",
      "jdcal                    \n",
      "jedi                     \n",
      "Jinja2                   \n",
      "jmespath                 \n",
      "joblib                   \n",
      "jsonschema               \n",
      "jupyter-client           \n",
      "jupyter-core             \n",
      "jupyterlab-pygments      \n",
      "Keras                    \n",
      "Keras-Applications       \n",
      "Keras-Preprocessing      \n",
      "kiwisolver               \n",
      "leather                  \n",
      "llvmlite                 \n",
      "lxml                     \n",
      "Markdown                 \n",
      "MarkupSafe               \n",
      "matplotlib               \n",
      "matplotlib-inline        \n",
      "mistune                  \n",
      "mkl-fft                  \n",
      "mkl-random               \n",
      "mkl-service              \n",
      "multidict                \n",
      "munkres                  \n",
      "murmurhash               \n",
      "nbclient                 \n",
      "nbconvert                \n",
      "nbformat                 \n",
      "nest-asyncio             \n",
      "networkx                 \n",
      "nltk                     \n",
      "notebook                 \n",
      "numba                    \n",
      "numexpr                  \n",
      "numpy                    \n",
      "oauthlib                 \n",
      "olefile                  \n",
      "openpyxl                 \n",
      "opt-einsum               \n",
      "packaging                \n",
      "pandas                   \n",
      "pandas-datareader        \n",
      "pandocfilters            \n",
      "parsedatetime            \n",
      "parso                    \n",
      "patsy                    \n",
      "pickleshare              \n",
      "Pillow                   \n",
      "pip                      \n",
      "plac                     \n",
      "plotly                   \n",
      "preshed                  \n",
      "prometheus-client        \n",
      "promise                  \n",
      "prompt-toolkit           \n",
      "protobuf                 \n",
      "psutil                   \n",
      "psycopg2-binary          \n",
      "pyasn1                   \n",
      "pyasn1-modules           \n",
      "pycparser                \n",
      "pyglet                   \n",
      "Pygments                 \n",
      "PyJWT                    \n",
      "pymongo                  \n",
      "PyMySQL                  \n",
      "pyodbc                   \n",
      "pyOpenSSL                \n",
      "pyparsing                \n",
      "pyreadline               \n",
      "pyrsistent               \n",
      "PySocks                  \n",
      "python-dateutil          \n",
      "python-Levenshtein       \n",
      "python-slugify           \n",
      "pytimeparse              \n",
      "pytz                     \n",
      "PyWavelets               \n",
      "pywin32                  \n",
      "pywinpty                 \n",
      "PyYAML                   \n",
      "pyzmq                    \n",
      "regex                    \n",
      "requests                 \n",
      "requests-oauthlib        \n",
      "rsa                      \n",
      "s3transfer               \n",
      "sas7bdat                 \n",
      "scikit-image             \n",
      "scikit-learn             \n",
      "scipy                    \n",
      "seaborn                  \n",
      "Send2Trash               \n",
      "setuptools             \n",
      "sip                      \n",
      "six                      \n",
      "sklearn-pandas           \n",
      "smart-open               \n",
      "soupsieve                \n",
      "spacy                    \n",
      "sql2csv                  \n",
      "SQLAlchemy               \n",
      "srsly                    \n",
      "statsmodels              \n",
      "stopit                   \n",
      "tenacity                 \n",
      "tensorboard              \n",
      "tensorboard-plugin-wit   \n",
      "tensorflow               \n",
      "tensorflow-datasets      \n",
      "tensorflow-estimator     \n",
      "tensorflow-metadata      \n",
      "termcolor                \n",
      "terminado                \n",
      "testpath                 \n",
      "text-unidecode           \n",
      "textblob                 \n",
      "thinc                    \n",
      "threadpoolctl            \n",
      "tifffile                 \n",
      "torch                    \n",
      "torchvision              \n",
      "tornado                  \n",
      "TPOT                     \n",
      "tqdm                     \n",
      "traitlets                \n",
      "typing-extensions    \n",
      "Unidecode                \n",
      "update-checker           \n",
      "urllib3                  \n",
      "wasabi                   \n",
      "wcwidth                  \n",
      "webencodings             \n",
      "Werkzeug                 \n",
      "wheel                    \n",
      "win-inet-pton            \n",
      "wincertstore             \n",
      "wrapt                    \n",
      "xlrd                     \n",
      "yarl                     \n",
      "zipp     \n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\unit testing.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\unit testing.txt\n",
      "how can we test if the implementation is correct?\n",
      "\n",
      "implemention -> test -> if pass we accept it\n",
      "feature requests require code refractoring\n",
      "\n",
      "unit testing saves time\n",
      "\n",
      "data\n",
      "\n",
      "\n",
      "src/\n",
      "\tdata/\n",
      "\tfeatures/\n",
      "\tmodels/\n",
      "\tvisualization/\n",
      "\n",
      "\n",
      "tests/\n",
      "\tdata/\n",
      "\tfeatures/\n",
      "\tmodels/\n",
      "\tvisualization/\n",
      "\n",
      "\n",
      "   > write a simple unit test using pytest\n",
      "\n",
      "1. pytest\n",
      "2. unittest\n",
      "3. nosetests\n",
      "4. doctest\n",
      "\n",
      "pytest is the most popular testing library in python\n",
      "\n",
      "\n",
      "def test_for_clean_row():\n",
      "\tassert row_to_list(\"2,081\\t314,942\\n\") ==[\"2,081\",\"314,942\"]\n",
      "\n",
      "\n",
      "def test_for_missing_area():\n",
      "\tassert row_to_list(\"\\t293,410\\n\") is None\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "# Import the pytest package\n",
      "import pytest\n",
      "\n",
      "# Import convert_to_int() from the module preprocessing_helpers.py\n",
      "from preprocessing_helpers import convert_to_int\n",
      "\n",
      "# Complete the unit test name by adding a prefix\n",
      "def test_on_string_with_one_comma():\n",
      "  # Complete the assert statement\n",
      "  assert convert_to_int(\"2,081\")==2081\n",
      "\n",
      "  > understanding test result report\n",
      "\n",
      "test fails when the assertion raises and assertionError\n",
      "\n",
      "F means test failure\n",
      ". means test passed\n",
      "\n",
      "final lines is a test summary\n",
      "\n",
      "  >\n",
      "\n",
      "   def test_on_string_with_one_comma():\n",
      ">     assert convert_to_int(\"2,081\") == 2081\n",
      "E     AssertionError: assert '2081' == 2081\n",
      "E      +  where '2081' = convert_to_int('2,081')\n",
      "\n",
      "test_convert_to_int.py:7: AssertionError\n",
      "\n",
      "\n",
      "    more benefits and test types\n",
      "\n",
      "!cat test_row_to_list.py\n",
      "\n",
      "to see the test code\n",
      "\n",
      "user tests can reduce downtime for a system\n",
      "\n",
      "contineous integration ci server runs all unit tests.  if any unit test fails the code does not go to production.\n",
      "\n",
      "an unit is a small, independent piece of code\n",
      "\n",
      "integration test if multiple units work together when connected\n",
      "\n",
      "end to end test check the whole software at once.\n",
      "\n",
      "assert boolean_expression, message\n",
      "\n",
      "assert 1==2, \"One is not equal to two\"\n",
      "\n",
      "\n",
      "\n",
      "def test_for_missing_area_with_message():\n",
      "\tactual = row_to_list(\"\\t293,410\\n\")\n",
      "\texpected=None\n",
      "\tmessage=(\"row_to_list('\\t293,410\\n') \"\n",
      "\t\t\"returned {0} instead\"\n",
      "\t\t\"of {1}\".format(actual,expected)\n",
      ")\n",
      "\n",
      "\tassert actual is expected, message\n",
      "\n",
      "\n",
      "recommended to include a message with assert statements because it is much easier to understand\n",
      "\n",
      "\n",
      "  > comparisons of floats\n",
      "\n",
      "0.1 +0.1 + 0.1 ==0.3\n",
      "\n",
      "False (python has an unusual way of dealing with floats)\n",
      "\n",
      "print(0.1+0.1+0.1)\n",
      "0.30000000000000004\n",
      "\n",
      "you should not use the usual way of comparing floats\n",
      "\n",
      "use pytest.approx() to wrap expected return values\n",
      "\n",
      "assert([0.1+0.1,0.1+0.1+0.1]) ==pytest.approx(np.array([0.2,0.3]))\n",
      "\n",
      "\n",
      "  > multiple tests\n",
      "\n",
      "def test_on_string_with_one_comma():\n",
      "\treturn_value=convert_to_int(\"2,081\")\n",
      "\n",
      "\tassert isinstance(return_value,int)\n",
      "\tassert return_value == 2081\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "import pytest\n",
      "from preprocessing_helpers import convert_to_int\n",
      "\n",
      "def test_on_string_with_one_comma():\n",
      "    test_argument = \"2,081\"\n",
      "    expected = 2081\n",
      "    actual = convert_to_int(test_argument)\n",
      "    # Format the string with the actual return value\n",
      "    message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
      "\n",
      "    assert actual is expected, message\n",
      "\n",
      "\n",
      "    >\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import pytest\n",
      "from as_numpy import get_data_as_numpy_array\n",
      "\n",
      "def test_on_clean_file():\n",
      "  expected = np.array([[2081.0, 314942.0],\n",
      "                       [1059.0, 186606.0],\n",
      "  \t\t\t\t\t   [1148.0, 206186.0]\n",
      "                       ]\n",
      "                      )\n",
      "  actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
      "  message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
      "  # Complete the assert statement\n",
      "  assert actual == pytest.approx(expected), message\n",
      "\n",
      "\n",
      "   > pytest.approx\n",
      "\n",
      "def test_on_six_rows():\n",
      "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
      "                                 [1148.0, 206186.0], [1506.0, 248419.0],\n",
      "                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n",
      "                                )\n",
      "    # Fill in with training array's expected number of rows\n",
      "    expected_training_array_num_rows = pytest.approx(example_argument)\n",
      "\n",
      "\n",
      "     \n",
      "\n",
      "\n",
      "def test_on_six_rows():\n",
      "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
      "                                 [1148.0, 206186.0], [1506.0, 248419.0],\n",
      "                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n",
      "                                )\n",
      "    # Fill in with training array's expected number of rows\n",
      "    expected_training_array_num_rows = 4\n",
      "    # Fill in with testing array's expected number of rows\n",
      "    expected_testing_array_num_rows = 2\n",
      "    actual = split_into_training_and_testing_sets(example_argument)\n",
      "    # Write the assert statement checking training array's number of rows\n",
      "    assert actual[0].shape[0] == expected_training_array_num_rows, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n",
      "\n",
      "    assert actual[1].shape[0] == expected_testing_array_num_rows, \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)\n",
      "\n",
      "\n",
      "\n",
      "with statement code is known as the context\n",
      "\n",
      "def test_valueerror_on_one_dimensional_argument():\n",
      "\texample_argument=np.array([2081,314942,1059,186606,1148,206186])\n",
      "\twith pytest.raises(ValueError):\n",
      "\t\traise ValueError\n",
      "\n",
      "\t\t\n",
      "\twith pytest.raises(ValueError):\n",
      "\t\tpass\n",
      "\n",
      "\n",
      "\n",
      "def test_valueerror_on_one_dimensional_argument():\n",
      "\texample_argument=np.array([2081,314942,1059,186606,1148,206186])\n",
      "\twith pytest.raises(ValueError):\n",
      "\t\tsplit_into_training_and_test_set(example_argument)\n",
      "\n",
      "\n",
      "def test_valueerror_on_one_dimensional_argument():\n",
      "\texample_argument=np.array([2081,314942,1059,186606,1148,206186])\n",
      "\twith pytest.raises(ValueError) as exception_info:\n",
      "\t\tsplit_into_training_and_test_set(example_argument)\n",
      "\n",
      "\n",
      "\tassert exception_info.match(\"Argument data array must be two dimensional.\" \"Got 1 dimensional array instead!\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "exception_info stores the ValueError\n",
      "\n",
      "\n",
      "    >\n",
      "\n",
      "import pytest\n",
      "\n",
      "# Fill in with a context manager that will silence the ValueError\n",
      "with pytest.raises(ValueError):\n",
      "    raise ValueError\n",
      "\n",
      "\n",
      "    >\n",
      "\n",
      "import pytest\n",
      "\n",
      "try:\n",
      "    # Fill in with a context manager that raises Failed if no OSError is raised\n",
      "    with pytest.raises(OSError):\n",
      "        raise ValueError\n",
      "except:\n",
      "    print(\"pytest raised an exception because no OSError was raised in the context.\")\n",
      "\n",
      "\n",
      "     \n",
      "\n",
      "\n",
      "import pytest\n",
      "\n",
      "# Store the raised ValueError in the variable exc_info\n",
      "with pytest.raises(ValueError) as exc_info:\n",
      "    raise ValueError(\"Silence me!\")\n",
      "\n",
      "\n",
      "    >\n",
      "\n",
      "import pytest\n",
      "\n",
      "with pytest.raises(ValueError) as exc_info:\n",
      "    raise ValueError(\"Silence me!\")\n",
      "# Check if the raised ValueError contains the correct message\n",
      "assert exc_info.match(\"Silence me!\")\n",
      "\n",
      "     \n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import pytest\n",
      "from train import split_into_training_and_testing_sets\n",
      "\n",
      "def test_on_one_row():\n",
      "    test_argument = np.array([[1382.0, 390167.0]])\n",
      "    # Fill in with a context manager for checking ValueError\n",
      "    with pytest.raises(ValueError) as exc_info:\n",
      "      split_into_training_and_testing_sets(test_argument)\n",
      "\n",
      "\n",
      "     \n",
      "\n",
      "import numpy as np\n",
      "import pytest\n",
      "from train import split_into_training_and_testing_sets\n",
      "\n",
      "def test_on_one_row():\n",
      "    test_argument = np.array([[1382.0, 390167.0]])\n",
      "    # Store information about raised ValueError in exc_info\n",
      "    with pytest.raises(ValueError) as exc_info:\n",
      "      split_into_training_and_testing_sets(test_argument)\n",
      "    expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
      "    # Check if the raised ValueError contains the correct message\n",
      "    assert exc_info.match(expected_error_msg)\n",
      "\n",
      "\n",
      "         The well tested function\n",
      "\n",
      "\n",
      "#number of rows 0.75 * example_argument_value.shape[0]) ->rows for training\n",
      "\n",
      "Bad arguments\n",
      "1.  The function raises and exception instead of returning a value\n",
      "Special arguments\n",
      "1.  Boundary values  (a training set of 1 row is invalid, the boundary value would be 2 rows)\n",
      "2.  Some argument values where the function uses special logic\n",
      "Normal arguments\n",
      "1.  Not a bad or special argument\n",
      "\n",
      "\n",
      "    >  boundary test\n",
      "import pytest\n",
      "from preprocessing_helpers import row_to_list\n",
      "\n",
      "def test_on_no_tab_no_missing_value():    # (0, 0) boundary value\n",
      "    # Assign actual to the return value for the argument \"123\\n\"\n",
      "    actual = row_to_list(\"123\\n\")\n",
      "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "    \n",
      "def test_on_two_tabs_no_missing_value():    # (2, 0) boundary value\n",
      "    actual = row_to_list(\"123\\t4,567\\t89\\n\")\n",
      "    # Complete the assert statement\n",
      "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "\n",
      "def test_on_one_tab_with_missing_value():    # (1, 1) boundary value\n",
      "    actual = row_to_list(\"\\t4,567\\n\")\n",
      "    # Format the failure message\n",
      "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "   >\n",
      "\n",
      "import pytest\n",
      "from preprocessing_helpers import row_to_list\n",
      "\n",
      "def test_on_no_tab_with_missing_value():    # (0, 1) case\n",
      "    # Assign to the actual return value for the argument \"\\n\"\n",
      "    actual = row_to_list(\"\\n\")\n",
      "    # Write the assert statement with a failure message\n",
      "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "    \n",
      "def test_on_two_tabs_with_missing_value():    # (2, 1) case\n",
      "    # Assign to the actual return value for the argument \"123\\t\\t89\\n\"\n",
      "    actual = row_to_list(\"123\\t\\t89\\n\")\n",
      "    # Write the assert statement with a failure message\n",
      "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "\n",
      "<<<<<<< normal test\n",
      "\n",
      "import pytest\n",
      "from preprocessing_helpers import row_to_list\n",
      "\n",
      "def test_on_normal_argument_1():\n",
      "    actual = row_to_list(\"123\\t4,567\\n\")\n",
      "    # Fill in with the expected return value for the argument \"123\\t4,567\\n\"\n",
      "    expected = ['123','4,567']\n",
      "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
      "\n",
      "def test_on_normal_argument_2():\n",
      "    actual = row_to_list(\"1,059\\t186,606\\n\")\n",
      "    expected = [\"1,059\", \"186,606\"]\n",
      "    # Write the assert statement along with a failure message\n",
      "    assert actual==expected, \"Expected: {0}, Actual: {1}\".format(expected,actual)\n",
      "\n",
      "\n",
      "     test driven development\n",
      "\n",
      "feature development\n",
      "\n",
      "tdd ensures unit test are written\n",
      "\n",
      "write the unit test before implementation\n",
      "\n",
      "unit tests cannot be deprioritized\n",
      "\n",
      "time for writing unit tests factored in implementation time\n",
      "\n",
      "when requirements are clear and precise it make code easier\n",
      "\n",
      "  >\n",
      "\n",
      "def test_with_no_comma():\n",
      "    actual = convert_to_int(\"756\")\n",
      "    # Complete the assert statement\n",
      "    assert actual==756, \"Expected: 756, Actual: {0}\".format(actual)\n",
      "    \n",
      "def test_with_one_comma():\n",
      "    actual = convert_to_int(\"2,081\")\n",
      "    # Complete the assert statement\n",
      "    assert actual==2081, \"Expected: 2081, Actual: {0}\".format(actual)\n",
      "    \n",
      "def test_with_two_commas():\n",
      "    actual = convert_to_int(\"1,034,891\")\n",
      "    # Complete the assert statement\n",
      "    assert actual==1034891, \"Expected: 1034891, Actual: {0}\".format(actual)\n",
      "\n",
      "    \n",
      "\n",
      "# Give a name to the test for an argument with missing comma\n",
      "def test_on_string_with_missing_comma():\n",
      "    actual = convert_to_int(\"178100,301\")\n",
      "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "    \n",
      "def test_on_string_with_incorrectly_placed_comma():\n",
      "    # Assign to the actual return value for the argument \"12,72,891\"\n",
      "    actual = convert_to_int(\"12,72,891\")\n",
      "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "    \n",
      "def test_on_float_valued_string():\n",
      "    actual = convert_to_int(\"23,816.92\")\n",
      "    # Complete the assert statement\n",
      "    assert actual==None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "def convert_to_int(integer_string_with_commas):\n",
      "    comma_separated_parts = integer_string_with_commas.split(\",\")\n",
      "    for i in range(len(comma_separated_parts)):\n",
      "        # Write an if statement for checking missing commas\n",
      "        if len(comma_separated_parts[i]) > 3:\n",
      "            return None\n",
      "        # Write the if statement for incorrectly placed commas\n",
      "        if i != 0 and len(comma_separated_parts[i]) != 3:\n",
      "            return None\n",
      "    integer_string_without_commas = \"\".join(comma_separated_parts)\n",
      "    try:\n",
      "        return int(integer_string_without_commas)\n",
      "    # Fill in with a ValueError\n",
      "    except ValueError:\n",
      "        return None\n",
      "\n",
      "\n",
      "\n",
      "    > how to organize a growing set of tests\n",
      "\n",
      "\n",
      "need a strategy to organize tests\n",
      "\n",
      "src/  #all application code\n",
      "    \tdata/  #package for data preprocessing\n",
      "\tfeatures/ \n",
      "   \tmodels/\n",
      "\n",
      "\n",
      "tests/   #test suite\n",
      "   \tdata/\n",
      "\tfeatures/\n",
      "\tmodels/\n",
      "\n",
      "\n",
      "a test class is a module for a specific function\n",
      "\n",
      "import pytest\n",
      "from data.preprocessing_helpers import row_to_list, convert_to_int\n",
      "\n",
      "class TestRowToList(object):\n",
      "\tdef test_on_no_tab_no_missing_value(self):\n",
      "\n",
      "\tdef test_on_two_tabs_no_missing_value(self):\n",
      "\n",
      "\n",
      "\n",
      "class TestConvertToInt(object):\n",
      "\tdef test_with_no_comma(self):\n",
      "\n",
      "\tdef test_with_one_comma(self):\n",
      "\n",
      "\n",
      "     \n",
      "\n",
      "import pytest\n",
      "import numpy as np\n",
      "\n",
      "from models.train import split_into_training_and_testing_sets\n",
      "\n",
      "# Declare the test class\n",
      "class TestSplitIntoTrainingAndTestingSets(object):\n",
      "    # Fill in with the correct mandatory argument\n",
      "    def test_on_one_row(self):\n",
      "        test_argument = np.array([[1382.0, 390167.0]])\n",
      "        with pytest.raises(ValueError) as exc_info:\n",
      "            split_into_training_and_testing_sets(test_argument)\n",
      "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
      "        assert exc_info.match(expected_error_msg)\n",
      "\n",
      "\n",
      "     > test organization\n",
      "\n",
      "\n",
      "data, features, models\n",
      "\n",
      "#runs all the tests in the folder\n",
      "cd tests\n",
      "pytests -x   # stop after first failure\n",
      "\n",
      "pytest data/test_preprocessing_helpers.py\n",
      "\n",
      "Node ID\n",
      "\n",
      "Node ID of a test class\n",
      "\n",
      "<Path to test module>::<test class name>\n",
      "\n",
      "Node ID of a unit test\n",
      "\n",
      "<Path to test module>::<test class name>::<unit test name>\n",
      "\n",
      "pytest data/test_preprocessing_helpers.py::TestRowToList   #run the test class\n",
      "\n",
      "pytest data/test_preprocessing_helpers.py::TestRowToList::test_on_one_tab_with_missing_value   #run the test class method\n",
      "\n",
      "\n",
      "  > key word expression\n",
      "\n",
      "pytest -k \"pattern\"\n",
      "\n",
      "\n",
      "pytest -k \"TestSplit and not test_on_one_row\"\n",
      "\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def split_into_training_and_testing_sets(data_array):\n",
      "    dim = data_array.ndim\n",
      "    if dim != 2:\n",
      "        raise ValueError(\"Argument data_array must be two dimensional. Got {0} dimensional array instead!\".format(dim))\n",
      "    num_rows = data_array.shape[0]\n",
      "    if num_rows < 2:\n",
      "        raise ValueError(\"Argument data_array must have at least 2 rows, it actually has just {0}\".format(num_rows))\n",
      "    # Fill in with the correct float\n",
      "    num_training = int(3/4 * data_array.shape[0])\n",
      "    permuted_indices = np.random.permutation(data_array.shape[0])\n",
      "    return data_array[permuted_indices[:num_training], :], data_array[permuted_indices[num_training:], :]\n",
      "\n",
      "\n",
      "!pytest /tests/models/test_train.py::TestSplitIntoTrainingAndTestingSets\n",
      "\n",
      "!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets::test_on_six_rows\n",
      "\n",
      "\n",
      "    Expected failures and conditional skipping\n",
      "\n",
      "\n",
      "train_model: returns best fit line given training data\n",
      "\n",
      "  > marking tests as \"expected to fail\"  (@pytest.mark.xfail)\n",
      "\n",
      "import pytest\n",
      "\n",
      "class TestTrainModel(object):\n",
      "\t@pytest.mark.xfail(reason=\"Using TDD, train_model() is not implemented\")\n",
      "\tdef test_on_linear_data(self):\n",
      "\n",
      "\n",
      "     > expected failures, but conditionally   (@pytest.mark.skipif)\n",
      "\n",
      "import sys\n",
      "\n",
      "class TestConvertToInt(object):\n",
      "\t@pytest.mark.skipif(sys.version_info > (2,7), reason=\"requires Python 2.7\")\n",
      "\tdef test_with_no_comma(self):\n",
      "\t\t\"\"\"Only runs on Python 2.7 or lower\"\"\"\n",
      "\t\ttest_argument=\"756\"\n",
      "\t\texpected=756\n",
      "\t\tactual=convert_to_int(test_argument)\n",
      "\t\tmessage = unicode(\"Expected: 2081, actual: {0}\".format(actual))\n",
      "\t\tassert actual == expected, message\n",
      "\n",
      "pytest -r #displays the reason why the test was skipped\n",
      "\n",
      "pytest -rs #show tests that were skipped\n",
      "\n",
      "pytest -rx #show tests that are xfailed in the summary info\n",
      "\n",
      "pytest -rsx #show both skipped and xfailed\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "!pytest models/test_train.py::TestModelTest\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "______________________ TestModelTest.test_on_linear_data _______________________\n",
      "self = <tmpd50x_i40.models.test_train.TestModelTest object at 0x7ff215930940>\n",
      "\n",
      "    def test_on_one_dimensional_array(self):\n",
      "        test_input = np.array([1.0, 2.0, 3.0, 4.0])\n",
      "        with pytest.raises(ValueError) as exc_info:\n",
      ">           model_test(test_input, 1.0, 1.0)\n",
      "E           NameError: name 'model_test' is not defined\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "# Mark the whole test class as \"expected to fail\"\n",
      "@pytest.mark.xfail(reason=\"Using TDD, model_test() has not yet been implemented\")\n",
      "class TestModelTest(object):\n",
      "    def test_on_linear_data(self):\n",
      "        test_input = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
      "        expected = 1.0\n",
      "        actual = model_test(test_input, 2.0, 1.0)\n",
      "        message = \"model_test({0}) should return {1}, but it actually returned {2}\".format(test_input, expected, actual)\n",
      "        assert actual == pytest.approx(expected), message\n",
      "        \n",
      "    def test_on_one_dimensional_array(self):\n",
      "        test_input = np.array([1.0, 2.0, 3.0, 4.0])\n",
      "        with pytest.raises(ValueError) as exc_info:\n",
      "            model_test(test_input, 1.0, 1.0)\n",
      "\n",
      "  >\n",
      "!pytest features/test_as_numpy.py::TestGetDataAsNumpyArray::test_on_clean_file\n",
      "\n",
      "E           NameError: name 'xrange' is not defined\n",
      "\n",
      "\n",
      "     \n",
      "\n",
      "# Import the sys module\n",
      "import sys\n",
      "\n",
      "class TestGetDataAsNumpyArray(object):\n",
      "    # Mark as skipped if Python version is greater than 2.7\n",
      "    @pytest.mark.skipif(sys.version_info > (2,7), reason=\"requires Python 2.7\")\n",
      "    def test_on_clean_file(self):\n",
      "        expected = np.array([[2081.0, 314942.0],\n",
      "                             [1059.0, 186606.0],\n",
      "                             [1148.0, 206186.0]\n",
      "                             ]\n",
      "                            )\n",
      "        actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
      "        message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
      "        assert actual == pytest.approx(expected), message\n",
      "\n",
      "\n",
      "   > contineous integration and code coverage\n",
      "\n",
      "contineous integration server runs all unit test every time a commit occurs.\n",
      "\t\t\n",
      "travis ci\n",
      "\n",
      "|--src\n",
      "|--test\n",
      "|--travis.yml\n",
      "\n",
      "travis.yml\n",
      "\n",
      "language:python\n",
      "python: \n",
      "  - \"3.6\"\n",
      "install:\n",
      "  - pip install -e .\n",
      "  - pip install pytest-cov codecov #install packages for code coverage report\n",
      "\n",
      "script:\n",
      "  - pytest --cov=src tests\n",
      "\n",
      "after_success:\n",
      "  - codecov\n",
      "\n",
      "markdown to the readme\n",
      "\n",
      "\n",
      "code coverage = num line of application code that ran during testing/ total num lines of application code * 100\n",
      "\n",
      "install codecov from the github market place\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<< beyond assertion (setup and teardown)\n",
      "\n",
      "def preprocess(raw_data_file_path, clean_data_file_path):\n",
      "\n",
      "preprocess has a pre-condition to work properly. it needs a raw data file in its environment.  it modifies the environment by creating a clean datafile.  setup is creating the raw data file.\n",
      "\n",
      "def test_on_raw_data():\n",
      "\tpreprocess(raw_data_file_path,\n",
      "\t\tclean_data_file_path)\n",
      "\twith open(clean_data_file_path) as f:\n",
      "\t\tlines=f.readlines()\n",
      "\tfirst_line=lines[0]\n",
      "\tassert first_line \"1801\\t201411\\n\"\n",
      "\tsecond_line=lines[1]\n",
      "\tassert second_line \"2002\\t333209\\n\"\n",
      "\n",
      "teardown brings environment to initial state.\n",
      "\t\t\n",
      "workflow\n",
      "setup -> assert -> teardown\n",
      "\n",
      "\n",
      "       @pytest.fixture\n",
      "\n",
      "@pytest.fixture\n",
      "def raw_and_clean_data_file():\n",
      "\t#do setup \n",
      "\traw_data_file_path=\"raw.txt\"\n",
      "\tclean_data_file_path=\"clean.txt\"\t\n",
      "\n",
      "\twith open(raw_data_file_path, \"w\") as f:\n",
      "\t\tf.write(\"1,801\\t201,411\\n\"\n",
      "\t\t\t\"1,767565,112\\n\"\n",
      "\t\t\t\"2,002\\t333,209\\n\"\n",
      "\t\t\t\"1990\\t782,911\\n\"\n",
      "\t\t\t\"1,285\\t389129\\n\")\n",
      "\n",
      "\tyield raw_data_file_path, clean_data_file_path\n",
      "\n",
      "\t#do teardown\n",
      "\n",
      "\tos.remove(raw_data_file_path)\n",
      "\tos.remove(clean_data_file_path)\n",
      "\n",
      "def test_on_raw_data(raw_and_clean_data_file):\n",
      "\n",
      "\n",
      "\traw_path,clean_path = raw_and_clean_data_file\n",
      "\n",
      "\tpreprocess(raw_path, clean_path)\n",
      "\n",
      "        with open(clean_data_file_path) as f:\n",
      "\t\tlines=f.readlines()\n",
      "\tfirst_line=lines[0]\n",
      "\tassert first_line \"1801\\t201411\\n\"\n",
      "\tsecond_line=lines[1]\n",
      "\tassert second_line \"2002\\t333209\\n\"\n",
      "\n",
      "\n",
      "\n",
      "tmpdir fixture\n",
      "\n",
      "1. setup: create a temporary directory\n",
      "2. teardown: delete the temporary directory along with contents\n",
      "\n",
      "\n",
      "\n",
      "         >@pytest.fixture\n",
      "\n",
      "1. setup of tmpdir() --> Setup of raw_and_clean_data_file() -> test->teardown of raw_and_clean_data_file -> teardown of tmpdir()\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def raw_and_clean_data_file(tmpdir):\n",
      "\traw_data_file_path = tmpdir.join(\"raw.txt\")\n",
      "\tclean_data_file_path = tmpdir.join(\"clean.txt\")\n",
      "\n",
      "\twith open(raw_data_file_path, \"w\") as f:\n",
      "\t\tf.write(\"1,801\\t201,411\\n\"\n",
      "\t\t\t\"1,767565,112\\n\"\n",
      "\t\t\t\"2,002\\t333,209\\n\"\n",
      "\t\t\t\"1990\\t782,911\\n\"\n",
      "\t\t\t\"1,285\\t389129\\n\")\n",
      "\n",
      "\tyield raw_data_file_path, clean_data_file_path\n",
      "\n",
      "\t#no teardown code necessary\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "# Add a decorator to make this function a fixture\n",
      "@pytest.fixture\n",
      "def clean_data_file():\n",
      "    file_path = \"clean_data_file.txt\"\n",
      "    with open(file_path, \"w\") as f:\n",
      "        f.write(\"201\\t305671\\n7892\\t298140\\n501\\t738293\\n\")\n",
      "    yield file_path\n",
      "    os.remove(file_path)\n",
      "    \n",
      "# Pass the correct argument so that the test can use the fixture\n",
      "def test_on_clean_file(clean_data_file):\n",
      "    expected = np.array([[201.0, 305671.0], [7892.0, 298140.0], [501.0, 738293.0]])\n",
      "    # Pass the clean data file path yielded by the fixture as the first argument\n",
      "    actual = get_data_as_numpy_array(clean_data_file, 2)\n",
      "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual) \n",
      "\n",
      "\n",
      "pytest keeps the fixtures separate from the tests as this encourages reusing fixtures for tests that need the same/similar setup and teardown code.\t\n",
      "\n",
      "   \n",
      "\n",
      "@pytest.fixture\n",
      "def empty_file():\n",
      "    # Assign the file path \"empty.txt\" to the variable\n",
      "    file_path = \"empty.txt\"\n",
      "    open(file_path, \"w\").close()\n",
      "    # Yield the variable file_path\n",
      "    yield file_path\n",
      "    # Remove the file in the teardown\n",
      "    os.remove(file_path)\n",
      "    \n",
      "def test_on_empty_file(self, empty_file):\n",
      "    expected = np.empty((0, 2))\n",
      "    actual = get_data_as_numpy_array(empty_file, 2)\n",
      "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
      "\n",
      "pytest fixtures are functions attached to the tests which run before the test function is execute\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "import pytest\n",
      "\n",
      "@pytest.fixture\n",
      "# Add the correct argument so that this fixture can chain with the tmpdir fixture\n",
      "def empty_file(tmpdir):\n",
      "    # Use the appropriate method to create an empty file in the temporary directory\n",
      "    file_path = tmpdir.join(\"empty.txt\")\n",
      "    open(file_path, \"w\").close()\n",
      "    yield file_path\n",
      "\n",
      "\n",
      "setup of tmpdir  setup of empty_file()  teardown of empty_file()  teardown of tmpdir.\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<Mocking\n",
      "\n",
      "Mocking:: testing functions independently of dependencies\n",
      "\n",
      "pytest-mock\n",
      "\n",
      "conda install -c conda-forge pytest-mock\n",
      "\n",
      "unittest.mock\n",
      "conda install -c conda-forge backports.unittest_mock\n",
      "\n",
      "            MagicMock and mocker.patch()\n",
      "\n",
      "\n",
      "from unittest.mock import call\n",
      "\n",
      "def row_to_list_bug_free(row):\n",
      "\treturn_values={\n",
      "\t\t\t\"1,801\\t201,411\\n\":[\"1,801\",\"201,411\"],\n",
      "\t\t\t\"1,767565,112\\n\":None,\n",
      "\t\t\t\"2,002\\t333,209\\n\":[\"2,002\",\"333,209\"],\n",
      "\t\t\t\"1990\\t782,911\\n\":[\"1990\",\"782,911\"],\n",
      "\t\t\t\"1,285\\t389129\\n\":[\"1,285\",\"389129\"],\n",
      "\t}\n",
      "\treturn return_values[row]\n",
      "\n",
      "def test_on_raw_data(raw_and_clean_data_file,\n",
      "\tmocker,\n",
      "\t):\n",
      "\n",
      "\t\traw_path, clean_path = raw_and_clean_data_file\n",
      "\t\trow_to_list_mock = mocker.patch(\"data.preprocessing_helpers.row_to_list\"\n",
      "\t\t\t,side_effect=row_to_list_bug_free\n",
      "\t\t)\n",
      "\n",
      "\t\tpreprocess(raw_path, clean_path)\n",
      "\t\tassert row_to_list_mock.call_args_list ==[\n",
      "\t\t\tcall(\"1,801\\t201,411\\n\"),\n",
      "\t\t\tcall(\"1,767565,112\\n\"),\n",
      "\t\t\tcall(\"2,002\\t333,209\\n\"),\n",
      "\t\t\tcall(\"1990\\t782,911\\n\"),\n",
      "\t\t\tcall(\"1,285\\t389129\\n\"),\n",
      "\t\t]\n",
      "\n",
      "\n",
      "\n",
      "mocker.patch(\"data.preprocessing_helpers.row_to_list\")\n",
      "\n",
      "\n",
      "row_to_list_mock.call_args_list\n",
      "\n",
      "\n",
      "call_args_list attribute returns a list of arguments that the mock was called with\n",
      "\n",
      "\n",
      "\n",
      "        >\n",
      "\n",
      "# Define a function convert_to_int_bug_free\n",
      "def convert_to_int_bug_free(comma_separated_integer_string):\n",
      "    # Assign to the dictionary holding the correct return values \n",
      "    return_values = {\"1,801\": 1801, \"201,411\": 201411, \"2,002\": 2002, \"333,209\": 333209, \"1990\": None, \"782,911\": 782911, \"1,285\": 1285, \"389129\": None}\n",
      "    # Return the correct result using the dictionary return_values\n",
      "    return return_values[comma_separated_integer_string]\n",
      "\n",
      "\n",
      "# Add the correct argument to use the mocking fixture in this test\n",
      "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
      "    raw_path, clean_path = raw_and_clean_data_file\n",
      "\n",
      "# Add the correct argument to use the mocking fixture in this test\n",
      "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
      "    raw_path, clean_path = raw_and_clean_data_file\n",
      "    # Replace the dependency with the bug-free mock\n",
      "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
      "                                    side_effect=convert_to_int_bug_free)\n",
      "\n",
      "\n",
      "# Add the correct argument to use the mocking fixture in this test\n",
      "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
      "    raw_path, clean_path = raw_and_clean_data_file\n",
      "    # Replace the dependency with the bug-free mock\n",
      "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
      "                                       side_effect=convert_to_int_bug_free)\n",
      "    preprocess(raw_path, clean_path)\n",
      "    # Check if preprocess() called the dependency correctly\n",
      "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
      "    with open(clean_path, \"r\") as f:\n",
      "        lines = f.readlines()\n",
      "    first_line = lines[0]\n",
      "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
      "    second_line = lines[1]\n",
      "    assert second_line == \"2002\\\\t333209\\\\n\" \n",
      "\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "from unittest.mock import call\n",
      "\n",
      "import pytest\n",
      "\n",
      "from data.preprocessing_helpers import convert_to_int, row_to_list, preprocess\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def raw_and_clean_data_file(tmpdir):\n",
      "    raw_path = tmpdir.join(\"raw.txt\")\n",
      "    clean_path = tmpdir.join(\"clean.txt\")\n",
      "    with open(raw_path, \"w\") as f:\n",
      "        f.write(\"1,801\\t201,411\\n\"\n",
      "                \"1,767565,112\\n\"\n",
      "                \"2,002\\t333,209\\n\"\n",
      "                \"1990\\t782,911\\n\"\n",
      "                \"1,285\\t389129\\n\"\n",
      "                )\n",
      "    return raw_path, clean_path\n",
      "    \n",
      "\n",
      "def convert_to_int_bug_free(comma_separated_integer_string):\n",
      "    return_values = {\"1,801\": 1801,\n",
      "                     \"201,411\": 201411,\n",
      "                     \"2,002\": 2002,\n",
      "                     \"333,209\": 333209,\n",
      "                     \"1990\": None,\n",
      "                     \"782,911\": 782911,\n",
      "                     \"1,285\": 1285,\n",
      "                     \"389129\": None,\n",
      "                     }\n",
      "    return return_values[comma_separated_integer_string]\n",
      "\n",
      "\n",
      "class TestConvertToInt(object):\n",
      "    def test_with_no_comma(self):\n",
      "        test_argument = \"756\"\n",
      "        expected = 756\n",
      "        actual = convert_to_int(test_argument)\n",
      "        message = \"Expected: 756, Actual: {0}\".format(actual)\n",
      "        assert actual == expected, message\n",
      "\n",
      "    def test_with_one_comma(self):\n",
      "        test_argument = \"2,081\"\n",
      "        expected = 2081\n",
      "        actual = convert_to_int(test_argument)\n",
      "        assert actual == expected, \"Expected: 2081, Actual: {0}\".format(actual)\n",
      "\n",
      "    def test_with_two_commas(self):\n",
      "        test_argument = \"1,034,891\"\n",
      "        expected = 1034891\n",
      "        actual = convert_to_int(test_argument)\n",
      "        assert actual == expected, \"Expected: 1034891, Actual: {0}\".format(actual)\n",
      "\n",
      "    def test_on_string_with_incorrectly_placed_comma(self):\n",
      "        test_argument = \"12,72,891\"\n",
      "        expected = None\n",
      "        actual = convert_to_int(test_argument)\n",
      "        assert actual == expected, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "    def test_on_string_with_missing_comma(self):\n",
      "        test_argument = \"178100,301\"\n",
      "        expected = None\n",
      "        actual = convert_to_int(test_argument)\n",
      "        assert actual == expected, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "    def test_on_float_valued_string(self):\n",
      "        test_argument = \"23,816.92\"\n",
      "        expected = None\n",
      "        actual = convert_to_int(test_argument)\n",
      "        assert actual == expected, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "\n",
      "class TestRowToList(object):\n",
      "    def test_on_no_tab_no_missing_value(self):    # (0, 0) boundary value\n",
      "        actual = row_to_list(\"123\\n\")\n",
      "        assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "    def test_on_two_tabs_no_missing_value(self):    # (2, 0) boundary value\n",
      "        actual = row_to_list(\"123\\t4,567\\t89\\n\")\n",
      "        assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "    def test_on_one_tab_with_missing_value(self):    # (1, 1) boundary value\n",
      "        actual = row_to_list(\"\\t4,567\\n\")\n",
      "        assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "    def test_on_no_tab_with_missing_value(self):    # (0, 1) case\n",
      "        actual = row_to_list(\"\\n\")\n",
      "        assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "    def test_on_two_tabs_with_missing_value(self):    # (0, 1) case\n",
      "        actual = row_to_list(\"123\\t\\t89\\n\")\n",
      "        assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
      "\n",
      "    def test_on_normal_argument_1(self):\n",
      "        actual = row_to_list(\"123\\t4,567\\n\")\n",
      "        expected = [\"123\", \"4,567\"]\n",
      "        assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
      "\n",
      "    def test_on_normal_argument_2(self):\n",
      "        actual = row_to_list(\"1,059\\t186,606\\n\")\n",
      "        expected = [\"1,059\", \"186,606\"]\n",
      "        assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
      "        \n",
      "        \n",
      "class TestPreprocess(object):\n",
      "    def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
      "        raw_path, clean_path = raw_and_clean_data_file\n",
      "        convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
      "                                           side_effect=convert_to_int_bug_free\n",
      "                                           )\n",
      "        preprocess(raw_path, clean_path)\n",
      "        assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"),\n",
      "                                                      call(\"1990\"),  call(\"782,911\"), call(\"1,285\"), call(\"389129\")\n",
      "                                                      ]\n",
      "        with open(clean_path, \"r\") as f:\n",
      "            lines = f.readlines()\n",
      "        first_line = lines[0]\n",
      "        assert first_line == \"1801\\t201411\\n\"\n",
      "        second_line = lines[1]\n",
      "        assert second_line == \"2002\\t333209\\n\"   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Testing models \n",
      "\n",
      "\n",
      "data-> raw-> housing_data.txt\n",
      "data->clean\n",
      "\n",
      "\n",
      "\n",
      "from data.preprocessing_helpers import preprocess\n",
      "from features.as_numpy import get_data_as_numpy_array\n",
      "from models.train import (split_into_training_and_testing_sets)\n",
      "\n",
      "preprocess(\"data/raw/housing_data.txt\",\n",
      "\t\t\"data/clean/clean_housing_data.txt\"\n",
      "\t)\n",
      "\n",
      "data = get_data_as_numpy_array(\n",
      "\t\"data/clean/clean_housing_data.txt\",2\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "training_set, testing_set = (\n",
      "\tsplit_into_training_and_testing_sets(data)\n",
      ")\n",
      "\n",
      "\n",
      "from scipy.stats import linregress\n",
      "\n",
      "def train_model(training_set):\n",
      "\tsource=training_set[:0]\n",
      "\ttarget=training_set[:,1]\n",
      "\tslope, intercept, r_value, p_value, std_err = linregress(source,target)\n",
      "\n",
      "\tpredicted_y = slope * source + intercept\n",
      "\n",
      "\t# Plot the dependent variable (y) against the independent variable (x)\n",
      "\tfig = plt.figure()\n",
      "\tplt.scatter(source,target, alpha=0.5)\n",
      "\tplt.plot(source, predicted_y, 'k--')\n",
      "\tplt.title(\"Linear Regression\")\n",
      "\tplt.show()\n",
      "\n",
      "def split_into_training_and_testing_sets(data_array):\n",
      "    \n",
      "    num_training = int(3/4 * data_array.shape[0])\n",
      "    permuted_indices = np.random.permutation(data_array.shape[0])\n",
      "    training_set=housing_array[permuted_indices[:num_training], :]\n",
      "    testing_set=housing_array[permuted_indices[num_training:], :]\n",
      "    return training_set, testing_set\n",
      "\n",
      "training_set,testing_set=split_into_training_and_testing_sets(housing_array)\n",
      "train_model(training_set)   \n",
      "\n",
      "if we do not know the expected return value we can not test the function\n",
      "\n",
      "use a dataset where the return value is known\n",
      "\n",
      "do not leave models untested just because they are complex\n",
      "\n",
      "perform many sanity checks as possible\n",
      "\n",
      "\n",
      "def model_test( testing_set, slope, intercept):\n",
      "\t\"\"\"\"\n",
      "\t\tr2 equals 1 if perfect fit\n",
      "\t\tr2 equals 0 if no fit\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import pytest\n",
      "from models.train import model_test\n",
      "\n",
      "def test_on_perfect_fit():\n",
      "    # Assign to a NumPy array containing a linear testing set\n",
      "    test_argument = np.array([[1, 3], [2, 5], [3, 7]])\n",
      "    # Fill in with the expected value of r^2 in the case of perfect fit\n",
      "    expected = 1.0\n",
      "    # Fill in with the slope and intercept of the model\n",
      "    actual = model_test(test_argument, slope=2.0, intercept=1.0)\n",
      "    # Complete the assert statement\n",
      "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
      "\n",
      "\n",
      "\n",
      "def test_on_circular_data(self):\n",
      "    theta = pi/4.0\n",
      "    # Complete the NumPy array holding the circular testing data\n",
      "    test_argument = np.array([[1.0, 0.0], \n",
      "                              [cos(theta), sin(theta)],\n",
      "                              [0.0, 1.0],\n",
      "                              [cos(3 * theta), sin(3 * theta)],\n",
      "                              [-1.0, 0.0],\n",
      "                              [cos(5*theta), sin(5*theta)],\n",
      "                              [0.0,-1.0],\n",
      "                              [cos(7*theta), sin(7*theta)]]\n",
      "                            )\n",
      "\n",
      "    actual = model_test(test_argument, slope=0.0, intercept=0.0)\n",
      "    # Complete the assert statement\n",
      "    assert actual == pytest.approx(0.0)\n",
      "\n",
      "\n",
      "       data plots\n",
      "\n",
      "src->visualization\n",
      "\n",
      "\n",
      "def get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title):\n",
      "\t\n",
      "\n",
      "\n",
      "slope, intercept = train_model(training_set)\n",
      "\n",
      "get_plot_for_best_fit_line(slope, intercept, training_set[:,0], training_set[:,1],\"training\")\n",
      "\n",
      "\n",
      "use pytest-mpl for image comparisons\n",
      "\n",
      "\n",
      "\n",
      "><      >\n",
      "\n",
      "import pytest\n",
      "import numpy as np\n",
      "from visualization import get_plot_for_best_fit_line\n",
      "\n",
      "@pytest.mark.mpl_image_compare\n",
      "def test_plot_for_linear_data():\n",
      "\tslope=2.0\n",
      "\tintercept=1.0\n",
      "\tx_array=np.array([1.0,2.0,3.0])\n",
      "\ty_array=np.array([3.0,5.0,7.0])\n",
      "\ttitle=\"Test plot for linear data\"\n",
      "\treturn get_plot_for_best_fit(slope, intercept, x_array, y_array, title)\n",
      "\n",
      "images are stored in\n",
      "\n",
      "tests->visualization->baseline\n",
      "\n",
      "!pytest -k \"test_plot_for_linear_data\"\n",
      "\t--mpl-generate-path\n",
      "\tvisualization/baseline\n",
      "\n",
      "\n",
      "to test the image\n",
      "\n",
      "!pytest -k \"test_plot_for_linear_data\" --mpl\n",
      "\n",
      "compares the baseline image with the actual image\n",
      "\n",
      "failures will result in : saved base line image, actual image, and difference pixel image\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "import pytest\n",
      "import numpy as np\n",
      "\n",
      "from visualization.plots import get_plot_for_best_fit_line\n",
      "\n",
      "class TestGetPlotForBestFitLine(object):\n",
      "    # Add the pytest marker which generates baselines and compares images\n",
      "    @pytest.mark.mpl_image_compare\n",
      "    def test_plot_for_almost_linear_data(self):\n",
      "        slope = 5.0\n",
      "        intercept = -2.0\n",
      "        x_array = np.array([1.0, 2.0, 3.0])\n",
      "        y_array = np.array([3.0, 8.0, 11.0])\n",
      "        title = \"Test plot for almost linear data\"\n",
      "        # Return the matplotlib figure returned by the function under test\n",
      "        return get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title)\n",
      "\n",
      "\n",
      "pytest --mpl-generate-path /home/repl/workspace/project/tests/visualization/baseline -k \"test_plot_for_almost_linear_data\"\n",
      "\n",
      "\n",
      "pytest -k \"TestGetPlotForBestFitLine\" --mpl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "import codecs\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['requests']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python_files\\\\python_notes')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pattern=\"(\\s{1}airlines.csv\\s{1})\"\n",
    "pattern=\"(auc)\"\n",
    "path= 'C:\\\\Users\\\\dnishimoto\\\\python_files'  \n",
    "for filename in [item for item in os.listdir(path) if item.endswith(\".txt\")  ]:\n",
    "    if os.access(path + \"\\\\\" + filename, os.R_OK):\n",
    "        with open(path + \"\\\\\" + filename,\"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if re.search(pattern,line):\n",
    "                    print(filename)\n",
    "                    print(\"\\t{}\".format(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9. Stackoverflow.ipynb\n",
      "\n",
      "\timport requests\n",
      "\n",
      "Importing Data from the Web.ipynb\n",
      "\n",
      "\timport requests\n",
      "\n",
      "\tr=requests.get(url)\n",
      "\n",
      "\tr=requests.get(url)\n",
      "\n",
      "\tr=requests.get(url)\n",
      "\n",
      "Search phrase in files.ipynb\n",
      "\n",
      "\tsearch=['requests']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def readJupyterNotesFnc(path,phrase):\n",
    "    pySource=\"\"\n",
    "    count=0\n",
    "    path=os.path.expanduser(path)\n",
    "    for filename in [item for item in os.listdir(path) if item.endswith(\".ipynb\")  ]:\n",
    "        if os.access(path + \"\\\\\" + filename, os.R_OK):\n",
    "            with open(path + \"\\\\\" + filename,\"r\", encoding=\"utf8\") as f:\n",
    "                source = f.read()\n",
    "                y = json.loads(source)\n",
    "                #print(y)\n",
    "                doc=[]\n",
    "                found=False\n",
    "                for x in y['cells']:\n",
    "                    for line in x['source']:\n",
    "                    #print(line)\n",
    "                        if phrase in line:\n",
    "                            doc.append(line)\n",
    "                            found=True\n",
    "                if found==True:\n",
    "                    print(\"{}\\n\".format(filename))\n",
    "                    for item in doc:\n",
    "                        print(\"\\t{}\".format(item))\n",
    "                count+=1\n",
    "\n",
    "path= 'C:\\\\Users\\\\dnishimoto\\\\python_files\\\\python-deep-learning-master'               \n",
    "readJupyterNotesFnc(path,\"requests\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
