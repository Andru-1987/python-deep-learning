{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\analysis for traffic stops by police officers.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\analysis for traffic stops by police officers.txt\n",
      "the dataset for stops by police officers in the state of rhode island.\n",
      "\n",
      "1. State\n",
      "2. stop_date\n",
      "3. stop_time\n",
      "4. county_name (contains nan values)\n",
      "5. driver_gender\n",
      "6. driver_race\n",
      "\n",
      "\n",
      "ri=pd.read_csv('police.csv')\n",
      "ri.isnull()\n",
      "\n",
      "ri.isnull().sum()\n",
      "county_name=91741\n",
      "\n",
      "ri.shape()\n",
      "output: 91741,15\n",
      "\n",
      "drop county_name column\n",
      "\n",
      "ri.drop('county_name',axis='columns', inplace=True)\n",
      "\n",
      ".dropna() : drops rows based on the presence of missing values.\n",
      "\n",
      "\n",
      "\n",
      "   sample  > dropping a column\n",
      "\n",
      "# Examine the shape of the DataFrame\n",
      "print(ri.shape)\n",
      "\n",
      "# Drop the 'county_name' and 'state' columns\n",
      "ri.drop(['county_name', 'state'], axis='columns', inplace=True)\n",
      "\n",
      "# Examine the shape of the DataFrame (again)\n",
      "print(ri.shape)\n",
      "\n",
      "   sample  > drop na subset\n",
      "\n",
      "# Count the number of missing values in each column\n",
      "print(ri.isnull().sum())\n",
      "\n",
      "# Drop all rows that are missing 'driver_gender'\n",
      "ri.dropna(subset=['driver_gender'], inplace=True)\n",
      "\n",
      "# Count the number of missing values in each column (again)\n",
      "print(ri.isnull().sum())\n",
      "\n",
      "# Examine the shape of the DataFrame\n",
      "print(ri.shape)\n",
      "\n",
      "\n",
      "removing columns and rows that will not be useful.\n",
      "\n",
      "\n",
      "   Examining the data types\n",
      "read_csv creates an inferred datatype\n",
      "\n",
      "print(ri.dtypes)\n",
      "\n",
      "dtype:\n",
      "1.object\n",
      "2.bool\n",
      "3.int\n",
      "4.float\n",
      "5.datetime\n",
      "6.category\n",
      "\n",
      "datatype affect opeations you can perform\n",
      "\n",
      "math operations can be performed on int and floats\n",
      "\n",
      "datetime \n",
      "category uses less memory and runs faster\n",
      "bool enables logical and mathematical operations\n",
      "\n",
      "\n",
      "apple\n",
      "1. date\n",
      "2. time\n",
      "3. price\n",
      "\n",
      "apple.price.dtype\n",
      "output dtype('O') means object\n",
      "\n",
      "apple['price']= apple.price.astype('float')\n",
      "\n",
      "\n",
      "  >Sample  > convert object dtype to bool\n",
      "\n",
      "# Examine the head of the 'is_arrested' column\n",
      "print(ri.is_arrested.dtype)\n",
      "\n",
      "# Change the data type of 'is_arrested' to 'bool'\n",
      "ri['is_arrested'] = ri.is_arrested.astype(bool)\n",
      "\n",
      "# Check the data type of 'is_arrested' \n",
      "print(ri.is_arrested.dtype)\n",
      "\n",
      "\n",
      "   sample  > value_counts and unique\n",
      "\n",
      "# Count the unique values in 'violation'\n",
      "print(ri['violation'].unique())\n",
      "\n",
      "# Express the counts as proportions\n",
      "print(ri['violation'].value_counts(normalize=True))\n",
      "\n",
      "['Equipment' 'Speeding' 'Other' 'Moving violation' 'Registration/plates'\n",
      " 'Seat belt']\n",
      "Speeding               48423\n",
      "Moving violation       16224\n",
      "Equipment              10921\n",
      "Other                   4409\n",
      "Registration/plates     3703\n",
      "Seat belt               2856\n",
      "Name: violation, dtype: int64\n",
      "\n",
      "  >normalized=True    output\n",
      "\n",
      "Speeding               0.559571\n",
      "Moving violation       0.187483\n",
      "Equipment              0.126202\n",
      "Other                  0.050950\n",
      "Registration/plates    0.042791\n",
      "Seat belt              0.033004\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "\n",
      "  >sample    women have more speeding violations\n",
      "\n",
      "# Create a DataFrame of female drivers\n",
      "female = ri[ri['driver_gender']=='F']\n",
      "\n",
      "# Create a DataFrame of male drivers\n",
      "male = ri[ri['driver_gender']=='M']\n",
      "\n",
      "print(female.violation.value_counts(normalize=True))\n",
      "\n",
      "# Compute the violations by male drivers (as proportions)\n",
      "print(male.violation.value_counts(normalize=True))\n",
      "\n",
      "output:\n",
      "\n",
      "Speeding               0.658114\n",
      "Moving violation       0.138218\n",
      "Equipment              0.105199\n",
      "Registration/plates    0.044418\n",
      "Other                  0.029738\n",
      "Seat belt              0.024312\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "Speeding               0.522243\n",
      "Moving violation       0.206144\n",
      "Equipment              0.134158\n",
      "Other                  0.058985\n",
      "Registration/plates    0.042175\n",
      "Seat belt              0.036296\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "\n",
      "Filtering a dataframe using multiple conditions\n",
      "\n",
      "female = ri[ri.driver_gender=='F']\n",
      "female.shape\n",
      "\n",
      "or\n",
      "\n",
      "female = ri[\n",
      "(ri.driver_gender=='F') &\n",
      "(ri.is_arrested==True)\n",
      "]\n",
      "female.shape\n",
      "\n",
      "\n",
      "each condition is surround by parentheses and the & separates the conditions\n",
      "\n",
      "only female drivers who were arrested\n",
      "\n",
      "| represents the or condition\n",
      "\n",
      "|| represents the and condition\n",
      "\n",
      "\n",
      " sample  > filtering\n",
      "\n",
      "ri[(ri.driver_gender=='F') & (ri.violation=='Speeding')]\n",
      "\n",
      "\n",
      " > Sample  > Stop outcomes\n",
      "\n",
      "\n",
      "# Create a DataFrame of female drivers stopped for speeding\n",
      "female_and_speeding = ri[(ri.driver_gender=='F') & (ri.violation=='Speeding')]\n",
      "\n",
      "# Create a DataFrame of male drivers stopped for speeding\n",
      "male_and_speeding = ri[(ri.driver_gender=='M') & (ri.violation=='Speeding')]\n",
      "\n",
      "print(\"male\")\n",
      "# Compute the stop outcomes for female drivers (as proportions)\n",
      "print(female_and_speeding.stop_outcome.value_counts(normalize=True))\n",
      "print(\"female\")\n",
      "# Compute the stop outcomes for male drivers (as proportions)\n",
      "print(male_and_speeding.stop_outcome.value_counts(normalize=True))\n",
      "\n",
      "Output::  (95% of stops resulting in a ticket)\n",
      "\n",
      "male\n",
      "Citation            0.952192\n",
      "Warning             0.040074\n",
      "Arrest Driver       0.005752\n",
      "N/D                 0.000959\n",
      "Arrest Passenger    0.000639\n",
      "No Action           0.000383\n",
      "Name: stop_outcome, dtype: float64\n",
      "\n",
      "female\n",
      "Citation            0.944595\n",
      "Warning             0.036184\n",
      "Arrest Driver       0.015895\n",
      "Arrest Passenger    0.001281\n",
      "No Action           0.001068\n",
      "N/D                 0.000976\n",
      "Name: stop_outcome, dtype: float64\n",
      "\n",
      "\n",
      "   >Does gender affect the vehicles that are searched?\n",
      "\n",
      "\n",
      "ri.isnull().sum()\n",
      "\n",
      "true is 1\n",
      "false is 0\n",
      "then sum the rows\n",
      "\n",
      "the mean of a boolean series represents the percentage of True values\n",
      "\n",
      "ri.is_arrested.value_counts(normalized=True)\n",
      ".03\n",
      "ri.is_arrested.mean()\n",
      ".03\n",
      "\n",
      "\n",
      "find the unique districts\n",
      "\n",
      "ri.district.unique()\n",
      "\n",
      "print(df_sas[df_sas['District'].isin(districts)]['ArrestInt'].mean())\n",
      "\n",
      "\n",
      "print(df_sas.groupby('District')['ArrestInt'].mean())\n",
      "\n",
      "print(df_sas.groupby(['District','Ward'])['ArrestInt'].mean())\n",
      "\n",
      "  >Sample    search_conducted\n",
      "\n",
      "# Check the data type of 'search_conducted'\n",
      "print(ri['search_conducted'].dtype)\n",
      "\n",
      "# Calculate the search rate by counting the values\n",
      "print(ri['search_conducted'].value_counts(normalize=True))\n",
      "\n",
      "# Calculate the search rate by taking the mean\n",
      "print(ri.search_conducted.mean())\n",
      "\n",
      "\n",
      "output\n",
      "bool\n",
      "False    0.961785\n",
      "True     0.038215\n",
      "Name: search_conducted, dtype: float64\n",
      "0.0382153092354627\n",
      "\n",
      "\n",
      "  Sample  > female\n",
      "\n",
      "# Calculate the search rate for female drivers\n",
      "print(ri[ri.driver_gender=='F'].search_conducted.mean())\n",
      "\n",
      "output: 0.019180617481282074 (female)\n",
      "output: 0.04542557598546892 (male)\n",
      "\n",
      " >Sample  > groupby\n",
      "\n",
      "# Calculate the search rate for both groups simultaneously\n",
      "print(ri.groupby('driver_gender').search_conducted.mean())\n",
      "\n",
      " >Sample  > groupby multiple column\n",
      "\n",
      "print(ri.groupby(['driver_gender','violation']).search_conducted.mean())\n",
      "\n",
      "driver_gender  violation          \n",
      "F              Equipment              0.039984\n",
      "               Moving violation       0.039257\n",
      "               Other                  0.041018\n",
      "               Registration/plates    0.054924\n",
      "               Seat belt              0.017301\n",
      "               Speeding               0.008309\n",
      "\n",
      "M              Equipment              0.071496\n",
      "               Moving violation       0.061524\n",
      "               Other                  0.046191\n",
      "               Registration/plates    0.108802\n",
      "               Seat belt              0.035119\n",
      "               Speeding               0.027885\n",
      "Name: search_conducted, dtype: float64\n",
      "\n",
      "\n",
      "       >Gender affect frisking\n",
      "\n",
      "ri.search_type.value_counts(dropna=False)\n",
      "1. Incident to Arrest\n",
      "2. Probable cause\n",
      "3. Inventory\n",
      "4. Reasonable Suspicion\n",
      "5. Protective Frisk\n",
      "6. Incident to Arrest, Inventory\n",
      "7. Incident to Arrest, Probable Cause\n",
      "\n",
      "\n",
      "ri['inventory']=ri.search_type.str.contains('Inventory',na=False)\n",
      "\n",
      "na=False means return a false when it finds a missing value\n",
      "ri.inventory.sum()\n",
      "\n",
      "\n",
      "search=ri[ri.searched_conducted==True]\n",
      "searched.inventory.mean()\n",
      "\n",
      "\n",
      " >Sample    > search type count, frisk in the search_type\n",
      "\n",
      "# Count the 'search_type' values\n",
      "print(len(ri.search_type.unique()))\n",
      "\n",
      "# Check if 'search_type' contains the string 'Protective Frisk'\n",
      "ri['frisk'] = ri.search_type.str.contains('Protective Frisk', na=False)\n",
      "\n",
      "# Check the data type of 'frisk'\n",
      "print(ri['frisk'].dtype)\n",
      "\n",
      "# Take the sum of 'frisk'\n",
      "print(ri['frisk'].sum())\n",
      "\n",
      "\n",
      " >Sample  > search conduction    frisk average per gender\n",
      "\n",
      "# Create a DataFrame of stops in which a search was conducted\n",
      "searched = ri[ri.search_conducted == True]\n",
      "\n",
      "# Calculate the overall frisk rate by taking the mean of 'frisk'\n",
      "print(searched.frisk.mean())\n",
      "\n",
      "# Calculate the frisk rate for each gender\n",
      "print(searched.groupby(\"driver_gender\").frisk.mean())\n",
      "\n",
      "      Does the time of day affect arrest rate\n",
      "\n",
      "analyzing datetime data\n",
      "\n",
      "apple\n",
      "1. price\n",
      "2. volume (shares traded)\n",
      "3. date_and_time\n",
      "\n",
      "\n",
      "dt.month\n",
      "dt.week\n",
      "dt.dayofweek\n",
      "dt.hour\n",
      "\n",
      "apple.set_index('date_and_time', inplace=True)\n",
      "apple.index.month\n",
      "apple.price.mean()\n",
      "\n",
      "month_price=apple.groupby(apple.index.month).price.mean()\n",
      "\n",
      "monthly_price.plot()\n",
      "plt.xlabel('Month')\n",
      "plt.ylabel('Price')\n",
      "\n",
      "df_sas['Year']=pd.DatetimeIndex(df_sas['date']).year\n",
      "arrest_year=df_sas.groupby(['Year'])['ArrestInt'].sum()\n",
      "\n",
      "\n",
      "  >Sample   > arrest rate as a time of day\n",
      "\n",
      "# Calculate the overall arrest rate\n",
      "print(ri.is_arrested.mean())\n",
      "\n",
      "# Calculate the hourly arrest rate\n",
      "print(ri.groupby(ri.index.hour).is_arrested.mean())\n",
      "\n",
      "# Save the hourly arrest rate\n",
      "hourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean()\n",
      "\n",
      "\n",
      "  Sample  > plot arrest time\n",
      "\n",
      "# Import matplotlib.pyplot as plt\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Create a line plot of 'hourly_arrest_rate'\n",
      "hourly_arrest_rate.plot()\n",
      "\n",
      "# Add the xlabel, ylabel, and title\n",
      "plt.xlabel('Hour')\n",
      "plt.ylabel('Arrest Rate')\n",
      "plt.title('Arrest Rate by Time of Day')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "         Are drug related stops on the rise\n",
      "1. We will use a subplot to see how two variables change over time\n",
      "\n",
      "2. Resampling is when you change the frequency of the time series\n",
      "\n",
      "monthly_price=apple.price.resample('M').mean()\n",
      "\n",
      "resample by month\n",
      "\n",
      "the output is the last day of the month rather than a number\n",
      "\n",
      "monthly_volume=apple.volume.resample('M').mean()\n",
      "\n",
      "\n",
      "pd.concat([monthly_price,monthly_volume],axis='columns')\n",
      "\n",
      "concatenates along a specified axis\n",
      "\n",
      "monthly.plot(subplots=True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  Sample    drug related stops   resampling\n",
      "\n",
      "# Calculate the annual rate of drug-related stops\n",
      "print(ri.drugs_related_stop.resample('A').mean())\n",
      "\n",
      "# Save the annual rate of drug-related stops\n",
      "annual_drug_rate = ri.drugs_related_stop.resample('A').mean()\n",
      "\n",
      "# Create a line plot of 'annual_drug_rate'\n",
      "annual_drug_rate.plot(subplots=True)\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "  Sample  > concatenate the two columns\n",
      "\n",
      "# Calculate and save the annual search rate\n",
      "annual_search_rate = ri.search_conducted.resample('A').mean()\n",
      "\n",
      "# Concatenate 'annual_drug_rate' and 'annual_search_rate'\n",
      "annual = pd.concat([annual_drug_rate,annual_search_rate], axis='columns')\n",
      "\n",
      "# Create subplots from 'annual'\n",
      "annual.plot(subplots=True)\n",
      "\n",
      "# Display the subplots\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    What violations are caught in each district\n",
      "\n",
      "result=df_sas.groupby(['Year','Month','fbi_code'])['ArrestInt'].sum().reset_index()\n",
      "#print(top20.columns)\n",
      "mask=result['ArrestInt']>30\n",
      "fbi_codes=result[mask]['fbi_code'].unique()\n",
      "\n",
      "filter=df_sas['fbi_code'].isin(fbi_codes) \n",
      "fbi_codes=df_sas['fbi_code'].unique()\n",
      "\n",
      "arrest_breakdown=df_sas[filter].groupby(['Year','Month','fbi_code'])['ArrestInt'].sum().reset_index()\n",
      "keys=arrest_breakdown.keys()\n",
      "#print(arrest_breakdown)\n",
      "\n",
      "g = sns.factorplot(data=arrest_breakdown, x='Year', y='ArrestInt', \n",
      "                  hue='fbi_code',  kind='point',size=8,aspect=2)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "         cross tab\n",
      "\n",
      "table=pd.crosstab(ri.driver_race, ri_driver_gender)\n",
      "\n",
      "creates a pivot table building a frequency table\n",
      "\n",
      "ri[(ri.driver_race=='Asian') & (ri.driver_gender=='F')].shape\n",
      "\n",
      "\n",
      "range=table.loc['Asian':'Hispanic']\n",
      "\n",
      "range.plot(kind='bar')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " > stack bar plot\n",
      "\n",
      "range.plot(kind='bar', stacked=True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    Sample\n",
      "\n",
      "# Create a frequency table of districts and violations\n",
      "print(pd.crosstab(ri.district,ri.violation))\n",
      "\n",
      "# Save the frequency table as 'all_zones'\n",
      "all_zones = pd.crosstab(ri.district,ri.violation)\n",
      "\n",
      "# Select rows 'Zone K1' through 'Zone K3'\n",
      "print(all_zones.loc['Zone K1':'Zone K3'])\n",
      "\n",
      "# Save the smaller table as 'k_zones'\n",
      "k_zones = all_zones.loc['Zone K1':'Zone K3']\n",
      "\n",
      "k_zone.plot(kind='bar', stacked=True)\n",
      "plt.show()\n",
      "\n",
      "     How long might you be stopped\n",
      "\n",
      "apple\n",
      "date_and_time\n",
      "price\n",
      "volume\n",
      "change\n",
      "\n",
      "change when  change\n",
      "\n",
      "True if the price went up\n",
      "\n",
      "calculate how often the price went up taking the column mean\n",
      "\n",
      "\n",
      "Stefan Jansen\n",
      "https://www.amazon.com/dp/B08D9SP6MB/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1\n",
      "\n",
      "mapping = {'up':True, 'down':False}\n",
      "apple['is_up']=apple.chage.map(mapping)\n",
      "\n",
      "apple.is_up.mean()\n",
      "\n",
      "  >how often searches occur after each violation\n",
      "\n",
      "search_rate=ri.groupby('violation').search_conducted.mean()\n",
      "\n",
      "search_rate.plot(kind='bar')\n",
      "plt.show()\n",
      "\n",
      "search rate is on the y axis\n",
      "the violation is on the x axis\n",
      "\n",
      "\n",
      "search_rate.plot(kind='barh')\n",
      "plt.show()\n",
      "\n",
      "  sample    mapping\n",
      "\n",
      "# Print the unique values in 'stop_duration'\n",
      "print(ri.stop_duration.unique())\n",
      "\n",
      "# Create a dictionary that maps strings to integers\n",
      "mapping = {\n",
      "    '0-15 Min': 8,\n",
      "    '16-30 Min': 23,\n",
      "    '30+ Min': 45\n",
      "    }\n",
      "\n",
      "# Convert the 'stop_duration' strings to integers using the 'mapping'\n",
      "ri['stop_minutes'] = ri.stop_duration.map(mapping)\n",
      "\n",
      "# Print the unique values in 'stop_minutes'\n",
      "print(ri['stop_minutes'].unique())\n",
      "\n",
      "\n",
      "   sample  >  groupby    average  > sort\n",
      "\n",
      "\n",
      "# Calculate the mean 'stop_minutes' for each value in 'violation_raw'\n",
      "print(ri.groupby('violation_raw')['stop_minutes'].mean())\n",
      "\n",
      "# Save the resulting Series as 'stop_length'\n",
      "stop_length = ri.groupby('violation_raw')['stop_minutes'].mean()\n",
      "\n",
      "# Sort 'stop_length' by its values and create a horizontal bar plot\n",
      "stop_length.sort_values().plot(kind='barh')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   Exploring the weather dataset\n",
      "\n",
      "do weather conditions affect police behavior\n",
      "\n",
      "noaa : national centers for environmental information\n",
      "\n",
      "single station in rhode islands to give weather information\n",
      "\n",
      "weather = pd.read_csv('weather.csv')\n",
      "\n",
      "weather.head(3)\n",
      "\n",
      "TAVG, TMIN, TMAX: Temperature\n",
      "AWND. WSF2: Wind Speed (average, fastest wind speed in a 2 minute interval)\n",
      "WT01, WT022: Bad Weather conditions\n",
      "\n",
      "https://mesonet.agron.iastate.edu/request/download.phtml?network=ID_ASOS\n",
      "\n",
      "increased convinced the data is trustworthy\n",
      "\n",
      "\n",
      "station:three or four character site identifier\n",
      "valid:timestamp of the observation\n",
      "tmpf:Air Temperature in Fahrenheit, typically @ 2 meters\n",
      "dwpf:Dew Point Temperature in Fahrenheit, typically @ 2 meters\n",
      "relh:Relative Humidity in %\n",
      "drct:Wind Direction in degrees from north\n",
      "sknt:Wind Speed in knots \n",
      "p01i:One hour precipitation for the period from the observation time to the time of the previous hourly precipitation reset. This varies slightly by site. Values are in inches. This value may or may not contain frozen precipitation melted by some device on the sensor or estimated by some other means. Unfortunately, we do not know of an authoritative database denoting which station has which sensor.\n",
      "alti:Pressure altimeter in inches\n",
      "mslp:Sea Level Pressure in millibar\n",
      "vsby:Visibility in miles\n",
      "gust:Wind Gust in knots\n",
      "skyc1:Sky Level 1 Coverage\n",
      "skyc2:Sky Level 2 Coverage\n",
      "skyc3:Sky Level 3 Coverage\n",
      "skyc4:Sky Level 4 Coverage\n",
      "skyl1:Sky Level 1 Altitude in feet\n",
      "skyl2:Sky Level 2 Altitude in feet\n",
      "skyl3:Sky Level 3 Altitude in feet\n",
      "skyl4:Sky Level 4 Altitude in feet\n",
      "wxcodes:Present Weather Codes (space seperated)\n",
      "feel:Apparent Temperature (Wind Chill or Heat Index) in Fahrenheit\n",
      "ice_accretion_1hr:Ice Accretion over 1 Hour (inches)\n",
      "ice_accretion_3hr:Ice Accretion over 3 Hours (inches)\n",
      "ice_accretion_6hr:Ice Accretion over 6 Hours (inches)\n",
      "peak_wind_gust:Peak Wind Gust (from PK WND METAR remark) (knots)\n",
      "peak_wind_drct:Peak Wind Gust Direction (from PK WND METAR remark) (deg)\n",
      "peak_wind_time:Peak Wind Gust Time (from PK WND METAR remark)\n",
      "metar:unprocessed reported observation in METAR format\n",
      "\n",
      "weather[['AWND','WSF2']].describe()\n",
      "\n",
      "create a box plot\n",
      "weather[['AWND','WSF2']].plot(kind='box')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "take the fast wind speed minus the average wind speed\n",
      "\n",
      "weather['WDIFF']= weather.WSF2-weather.AWND\n",
      "\n",
      "weather.WDIFF.plot(kind='hist', bins =20)\n",
      "plt.show()\n",
      "\n",
      "  Sample   > box plot temperatures\n",
      "\n",
      "# Read 'weather.csv' into a DataFrame named 'weather'\n",
      "df=pd.read_csv('weather.csv')\n",
      "\n",
      "# Describe the temperature columns\n",
      "print(df[['TMIN','TAVG','TMAX']].describe())\n",
      "\n",
      "# Create a box plot of the temperature columns\n",
      "df[['TMIN','TAVG','TMAX']].plot(kind='box')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "  >Sample   histogram to confirm temperature range distribution\n",
      "\n",
      "# Create a 'TDIFF' column that represents temperature difference\n",
      "weather['TDIFF']=weather.TMAX- weather.TMIN\n",
      "\n",
      "# Describe the 'TDIFF' column\n",
      "print(weather['TDIFF'].describe())\n",
      "\n",
      "# Create a histogram with 20 bins to visualize 'TDIFF'\n",
      "weather['TDIFF'].plot(kind='hist',bins=20)\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "            >Categorizing the weather\n",
      "\n",
      "\n",
      "slicing columns of the original dataframe\n",
      "\n",
      "temp=weather.loc[:,'TAVG':'TMAX']\n",
      "\n",
      "\n",
      "temp.sum(axis='columns').head()  \n",
      "\n",
      "this sums all the columns\n",
      "\n",
      "ri.stop_duration.unique()\n",
      "\n",
      "mapping = {\n",
      "    '0-15 Min': 'short',\n",
      "    '16-30 Min': 'medium',\n",
      "    '30+ Min': 'long'\n",
      "    }\n",
      "\n",
      "# Convert the 'stop_duration' strings to integers using the 'mapping'\n",
      "ri['stop_length'] = ri.stop_duration.map(mapping)\n",
      "\n",
      "ri.stop_length.dtype\n",
      "outputs object type because it contains string data\n",
      "\n",
      "ri.stop_length.unique()\n",
      "\n",
      "\n",
      "cats=['short','medium','long']\n",
      "ri.stop_length.astype('category',ordered=True,categories=cats)\n",
      "\n",
      "\n",
      "1. stores more efficiently\n",
      "2. allows a logical order\n",
      "\n",
      "ri.stop_length.memory_usage(deep=True)  #memory used to store the column\n",
      "\n",
      "cats=['short','medium','long']\n",
      "\n",
      "ri[ri.stop_length>'short']\n",
      "output data with categories of medium or long stop_length\n",
      "\n",
      "ri.groupby('stop_length').is_arrested.mean()\n",
      "\n",
      "\n",
      "   Sample  > bad weather conditions\n",
      "\n",
      "# Copy 'WT01' through 'WT22' to a new DataFrame\n",
      "WT = weather.loc[:,'WT01':'WT22']\n",
      "\n",
      "# Calculate the sum of each row in 'WT'\n",
      "weather['bad_conditions'] = WT.sum(axis='columns')\n",
      "\n",
      "# Replace missing values in 'bad_conditions' with '0'\n",
      "weather['bad_conditions'] = weather.bad_conditions.fillna(0).astype('int')\n",
      "\n",
      "# Create a histogram to visualize 'bad_conditions'\n",
      "\n",
      "\n",
      "weather['bad_conditions'].plot(kind='hist')\n",
      "plt.show()\n",
      "# Display the plot\n",
      "\n",
      "   sample   > bad_conditions by category\n",
      "\n",
      "# Count the unique values in 'bad_conditions' and sort the index\n",
      "print(weather.bad_conditions.value_counts().sort_index())\n",
      "\n",
      "# Create a dictionary that maps integers to strings\n",
      "mapping = {0:'good', 1:'bad', 2:'bad', 3:'bad', 4:'bad', 5:'worse', 6:'worse', 7:'worse', 8:'worse', 9:'worse'}\n",
      "\n",
      "# Convert the 'bad_conditions' integers to strings using the 'mapping'\n",
      "weather['rating'] = weather.bad_conditions.map(mapping)\n",
      "\n",
      "# Count the unique values in 'rating'\n",
      "print(weather['rating'].unique())\n",
      "\n",
      "print(weather.rating.value_counts())\n",
      "\n",
      "output:\n",
      "bad      1836\n",
      "good     1749\n",
      "worse     432\n",
      "Name: rating, dtype: int64\n",
      "\n",
      "  Sample   > create a column as a category\n",
      "\n",
      "# Create a list of weather ratings in logical order\n",
      "cats=['good','bad','worse']\n",
      "\n",
      "# Change the data type of 'rating' to category\n",
      "weather['rating'] = weather.rating.astype('category', ordered=True, categories=cats)\n",
      "\n",
      "# Examine the head of 'rating'\n",
      "print(weather['rating'].head())\n",
      "\n",
      "\n",
      "       Merging Datasets\n",
      "\n",
      "reset_index returns the index to an autonumber\n",
      "\n",
      "\n",
      "high=high_low[['DATE','HIGH']]\n",
      "\n",
      "we only need the high column\n",
      "\n",
      "apple_high=pd.merge(left=apple, right=high, left_on='date', right_on='DATE', how=left)\n",
      "\n",
      "apple_high.set_index('date_and_time', inplace=True)\n",
      "\n",
      "\n",
      "\n",
      "<<<<<<<Sample  > reset the index to the autonumber,  extract the DATE and rating columns from the weather dataframe\n",
      "\n",
      "# Reset the index of 'ri'\n",
      "ri.reset_index(inplace=True)\n",
      "\n",
      "# Examine the head of 'ri'\n",
      "print(ri.head())\n",
      "\n",
      "# Create a DataFrame from the 'DATE' and 'rating' columns\n",
      "weather_rating=weather[['DATE','rating']]\n",
      "\n",
      "# Examine the head of 'weather_rating'\n",
      "print(weather_rating.head())\n",
      "\n",
      "\n",
      "   Sample  > merge columns on stop_date and date\n",
      "\n",
      "# Examine the shape of 'ri'\n",
      "print(ri.shape)\n",
      "\n",
      "# Merge 'ri' and 'weather_rating' using a left join\n",
      "ri_weather = pd.merge(left=ri, right=weather_rating, left_on='stop_date', right_on='DATE', how='left')\n",
      "\n",
      "# Examine the shape of 'ri_weather'\n",
      "print(ri_weather.shape)\n",
      "\n",
      "# Set 'stop_datetime' as the index of 'ri_weather'\n",
      "ri_weather.set_index('stop_datetime', inplace=True)\n",
      "\n",
      "\n",
      "https://datatofish.com/multiple-linear-regression-python/\n",
      "\n",
      "\n",
      " > weather and behavior\n",
      "\n",
      "search_rate = ri.groupby(['violation','driver_gender']).search_conducted.mean()\n",
      "\n",
      "multi-index\n",
      "pandas.core.indexes.multi.multiindex (second dimension)\n",
      "\n",
      "level=0\n",
      "level=1\n",
      "\n",
      "search_rate.loc['Equipment'] #level 0\n",
      "search_rate.loc['Equipment','M'] #level 1\n",
      "\n",
      "search_rate.unstack()\n",
      "\n",
      "results in a dataframe\n",
      "\n",
      "ri.pivot_table(index='violation',\n",
      "\tcolumns='driver_gender',\n",
      "\tvalues='search_conducted')\n",
      "\n",
      "  >Sample\n",
      "print(ri_weather.is_arrested.mean())\n",
      "0.0355690117407784\n",
      "\n",
      "overall arrest rate\n",
      "\n",
      "\n",
      "# Calculate the arrest rate for each 'rating'\n",
      "print(ri_weather.groupby('rating').is_arrested.mean())\n",
      "\n",
      "stop_minutes  \n",
      "rating                \n",
      "good     0.033715\n",
      "bad      0.036261\n",
      "worse    0.041667\n",
      "\n",
      "  Sample   > create a multi index series    violation and rating for is_arrested mean\n",
      "\n",
      "# Calculate the arrest rate for each 'violation' and 'rating'\n",
      "print(ri_weather.groupby(['violation','rating']).is_arrested.mean())\n",
      "\n",
      "violation            rating\n",
      "*Equipment            good      0.059007\n",
      "                     bad       0.066311\n",
      "                     worse     0.097357\n",
      "*Moving violation     good      0.056227\n",
      "                     bad       0.058050\n",
      "                     worse     0.065860\n",
      "*Other                good      0.076966\n",
      "                     bad       0.087443\n",
      "                     worse     0.062893\n",
      "*Registration/plates  good      0.081574\n",
      "                     bad       0.098160\n",
      "                     worse     0.115625\n",
      "Seat belt            good      0.028587\n",
      "                     bad       0.022493\n",
      "                     worse     0.000000\n",
      "Speeding             good      0.013405\n",
      "                     bad       0.013314\n",
      "                     worse     0.016886\n",
      "\n",
      "  sample  > slicing the multi-index series\n",
      "\n",
      "# Save the output of the groupby operation from the last exercise\n",
      "arrest_rate = ri_weather.groupby(['violation', 'rating']).is_arrested.mean()\n",
      "\n",
      "# Print the 'arrest_rate' Series\n",
      "print(arrest_rate)\n",
      "\n",
      "# Print the arrest rate for moving violations in bad weather\n",
      "print(arrest_rate.loc['Moving violation','bad'])\n",
      "\n",
      "# Print the arrest rates for speeding violations in all three weather conditions\n",
      "print(arrest_rate.loc['Speeding'])\n",
      "\n",
      "\n",
      "  sample  > unstack and pivot\n",
      "\n",
      "# Unstack the 'arrest_rate' Series into a DataFrame\n",
      "print(arrest_rate.unstack())\n",
      "\n",
      "# Create the same DataFrame using a pivot table\n",
      "print(ri_weather.pivot_table(index='violation', columns='rating', values='is_arrested'))\n",
      "\n",
      "practice answering questions using data\n",
      "\n",
      "https://openpolicing.stanford.edu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\cohort analysis.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\cohort analysis.txt\n",
      "customer segmentation\n",
      "1. group customers by the month of their first purchase\n",
      "2. segment for frequency and monetary values\n",
      "3. k-means clustering to find similar customers\n",
      "4. experience in ecommerce, banking, consulting, and finance\n",
      "\n",
      "\n",
      "What is cohort analysis\n",
      "\n",
      "Dataframe fields: Invoice #, StockCode, Description, Quantity, Invoice Dt, UnitPrice, CustomerID, Country\n",
      "\n",
      "\n",
      "1. it groups customers into mutually exclusive segments - cohorts\n",
      "\n",
      "2. compare metrics across product lifecycle\n",
      "\n",
      "3. compare metrics across customer lifecycle\n",
      "\n",
      " Type of cohorts\n",
      "\n",
      "1. time cohorts (behavior - time interval)\n",
      "2. behavior cohorts (groups customers by product or service they have bought)\n",
      "3. size cohorts (different size of cohorts , amount of spending for a certain period of time)\n",
      "\n",
      "data is formatted as a pivot table\n",
      "\n",
      "cohortMonth is a column in a pivot table\n",
      "cohortIndex is months\n",
      "metrics are in the table\n",
      "\n",
      " Time cohorts\n",
      "\n",
      "Columns\n",
      "a. Time since first activity\n",
      "b. Months since acquistion\n",
      "\n",
      "Rows\n",
      "a. First activity\n",
      "b. Month of acquistion\n",
      "\n",
      "The index is the months since acquistion.  the metric on the row is the percent of retention after a certain number of months represented by the intersection fo the month and the index in the pivot table.\n",
      "\n",
      " Build the cohortDay\n",
      "\n",
      "# Define a function that will parse the date\n",
      "def get_day(x): return dt.datetime(x.year, x.month, x.day)  \n",
      "# Create InvoiceDay column\n",
      "online['InvoiceMonth'] = online['InvoiceDate'].apply(get_day) \n",
      "\n",
      "# Group by CustomerID and select the InvoiceDay value\n",
      "grouping = online.groupby('CustomerID')['InvoiceMonth'] \n",
      "\n",
      "# Assign a minimum InvoiceDay value to the dataset\n",
      "online['CohortDay'] = grouping.transform('min')\n",
      "\n",
      "# View the top 5 rows\n",
      "print(online.head())\n",
      "\n",
      "def get_date_int(df,column):\n",
      "\tyear=df[column].dt.year\n",
      "\tmonth=df[column].dt.month\n",
      "\tday=df[column].dt.day\n",
      "\treturn year,month,day\n",
      "\n",
      "# Get the integers for date parts from the `InvoiceDay` column\n",
      "invoice_year, invoice_month, invoice_day = get_date_int(online, 'InvoiceDay')\n",
      "\n",
      "# Get the integers for date parts from the `CohortDay` column\n",
      "cohort_year, cohort_month, cohort_day  = get_date_int(online, 'CohortDay')\n",
      "\n",
      "# Calculate difference in years\n",
      "years_diff= invoice_year - cohort_year\n",
      "\n",
      "# Calculate difference in months\n",
      "months_diff= invoice_month - cohort_month\n",
      "\n",
      "# Calculate difference in days\n",
      "days_diff = invoice_day - cohort_day\n",
      "\n",
      "# Extract the difference in days from all previous values\n",
      "online['CohortIndex'] = years_diff * 365 + months_diff * 30 + days_diff + 1\n",
      "print(online.head())\n",
      "\n",
      " online.head()\n",
      "\n",
      "  Count customers per metric\n",
      "\n",
      "grouping= online.groupby(['CohortMonth','CohortIndex'])\n",
      "\n",
      "cohort_data= grouping['CustomerID].apply(pd.Series.nunique)\n",
      "\n",
      "cohort_data= cohort_data.reset_index()\n",
      "\n",
      "cohort_counts= cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')\n",
      "\n",
      "\n",
      " > calculate metrics\n",
      "\n",
      "retention - how many customers have returned in the subsequence months\n",
      "\n",
      "\n",
      "cohort_sizes= cohort_counts.iloc[:,0]\n",
      "\n",
      "retention= cohort_counts.divide(chohort_sizes,axis=0)  #axis=0 divides along the row axis\n",
      "\n",
      "retention.round(3) * 100\n",
      "\n",
      " other metrics - Sum Quantity\n",
      "\n",
      "grouping= online.groupby(['CohortMonth','CohortIndex'])\n",
      "cohort_data=grouping['Quantity'].mean()\n",
      "cohort_data=cohort_data.reset_index()\n",
      "\n",
      "average_quantity= cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='Quantity')\n",
      "\n",
      "average_quantity.round(1)\n",
      "\n",
      "  Unit price\n",
      "\n",
      "# Create a groupby object and pass the monthly cohort and cohort index as a list\n",
      "grouping = online.groupby(['CohortMonth', 'CohortIndex']) \n",
      "\n",
      "# Calculate the average of the unit price column\n",
      "cohort_data = grouping['UnitPrice'].mean()\n",
      "\n",
      "# Reset the index of cohort_data\n",
      "cohort_data = cohort_data.reset_index()\n",
      "\n",
      "# Create a pivot \n",
      "average_price = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='UnitPrice')\n",
      "print(average_price.round(1))\n",
      "\n",
      "\n",
      "Analyze a cohort map using a heat map\n",
      "\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.title('Retention rates')\n",
      "sns.heatmap(data=retention,\n",
      "annot=True,\n",
      "fmt='.0%',\n",
      "vmin=0.0,\n",
      "vmax=0.5\n",
      "cmap='BuGn'\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\customer churn.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\customer churn.txt\n",
      "churn is when a customer ends a relationship with the company\n",
      "\n",
      "non-contractual churn (consumer loyalty)\n",
      "involuntary churn (expiration or non payment)\n",
      "\n",
      "Customer\n",
      "1. Lack of usage\n",
      "2. Poor service\n",
      "3. Better price\n",
      "\n",
      "Domain/industry knowledge\n",
      "\n",
      "Telco churn dataset.\n",
      "\n",
      "telecom features\n",
      "1. voice mail\n",
      "2. international calling\n",
      "3. cost for the service\n",
      "4. customer usage\n",
      "5. customer churn indicator\n",
      "\n",
      "churn is defined as the customer cancelling their cellular plan at a given point in time.\n",
      "\n",
      "print(telco['Churn'].value_counts())\n",
      "\n",
      " > Sample using groupby\n",
      "print(telco.groupby(['Churn']).count())\n",
      "print(telco.groupby(['Churn']).std())\n",
      "print(telco.groupby('State')['Churn'].value_counts())\n",
      "\n",
      " >seaborn\n",
      "understand how your variables are distributed\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "sns.distplot(telco['Account_Length')\n",
      "\n",
      "sns.boxplot(x='Churn', y='Account_Length', data=telco,sym=\"\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#The bell curve means that the data is normally distributed. This means that the data\n",
      "can be simulated by random sampling to increase the accurracy of the prediction\n",
      "\n",
      "sns.boxplot(x=\"Churn\", y='Account_length',data=telco)\n",
      "plt.show()\n",
      "\n",
      "#The line in the middle represents the median\n",
      "#The colored boxes represent the middle 50% of each group\n",
      "\n",
      "#The floating points represent outliers\n",
      "sym=\"\" removes the outliers\n",
      "\n",
      "\n",
      "sns.boxplot(x='Churn', y='Account_Length', data=telco,sym=\"\", hue='StreamingMovies')\n",
      "plt.show()\n",
      "\n",
      " >Sample\n",
      "Day_Mins\n",
      "Eve_Mins\n",
      "Night_Mins\n",
      "Intl_Mins\n",
      "\n",
      "# Import matplotlib and seaborn\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Visualize the distribution of 'Day_Mins'\n",
      "\n",
      "sns.distplot(telco['Day_Mins'])\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#If the data was not normal distributed, you would apply a feature transformation\n",
      "\n",
      "#In such cases, the extreme values could be identified and removed in order to make the distribution more Gaussian. These extreme values are often called outliers\n",
      "\n",
      "#Taking the square root and the logarithm of the observation in order to make the distribution normal belongs to a class of transforms called power transforms.\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Import matplotlib and seaborn\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Create the box plot\n",
      "sns.boxplot(x = 'Churn',\n",
      "          y = 'CustServ_Calls',\n",
      "          data = telco)\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "        >Churn Prediction Fundamentals\n",
      "\n",
      "test decision trees and logistic regression models (compare the models)\n",
      "\n",
      "churn definition depends on company\n",
      "1. churn happens when a customer stops buying or engaging with the company\n",
      "2. The business context could be contractual or non-contractual\n",
      "3. Failing to update subscription can cause involuntary churn\n",
      "4. Contractual churn happens explicitly when customers decide to terminate the relationship\n",
      "5. Non contractual churn happens on online shopping or when the customer stops shopping\n",
      "\n",
      "\n",
      "Encoding churn\n",
      "1=Churn\n",
      "0=No churn\n",
      "Or it could be a string churn and no churn\n",
      "\n",
      "Increase accuracy with under sampling or over sampling techniques\n",
      "\n",
      "train,test = train_test_split(telcom, test_size=.25)\n",
      "\n",
      "separate the independant features and the target variable\n",
      "\n",
      "target==['Churn']\n",
      "custid=['CustomerId']\n",
      "\n",
      "cols=[col for col in telcom.columns if col not in custid+target]\n",
      "\n",
      "train_X = train[cols]\n",
      "train_Y = train[target]\n",
      "test_X = test[cols]\n",
      "test_Y = test[target]\n",
      "\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Print the unique Churn values\n",
      "print(set(telcom['Churn']))\n",
      "\n",
      "# Calculate the ratio size of each churn group\n",
      "telcom.groupby(['Churn']).size() / telcom.shape[0] * 100\n",
      "\n",
      "# Import the function for splitting data to train and test\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Split the data into train and test\n",
      "train, test = train_test_split(telcom, test_size = .25)\n",
      "\n",
      "\n",
      "# Store column names from `telcom` excluding target variable and customer ID\n",
      "cols = [col for col in telcom.columns if col not in custid + target]\n",
      "\n",
      "# Extract training features\n",
      "train_X = train[cols]\n",
      "\n",
      "# Extract training target\n",
      "train_Y = train[target]\n",
      "\n",
      "# Extract testing features\n",
      "test_X = test[cols]\n",
      "\n",
      "# Extract testing target\n",
      "test_Y = test[target]\n",
      "\n",
      "\n",
      "      Predicting with Logistic Regression\n",
      "\n",
      "1. Statistical classification model for binary responses\n",
      "2. Models log-odds of the probabilty of the target\n",
      "\n",
      "odds= is the probability of the odd occurring divided by the probabiity of the event not occurring\n",
      "\n",
      "p/1-p\n",
      "\n",
      "helps to find the decision boundary between the two coeffiencts but keeping the variables linearly relatived.\n",
      "\n",
      "Accuracy - the % of correctly predicted labels (both churn and non-churn)\n",
      "Precision - the % of total models positive class predictions (here - predicted as Churn) that wee correctly classified\n",
      "Recall - The % of total positive class samples (all churned customers) that were correctly classified\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy\n",
      "\n",
      "pred_train_Y=logreg.predict(train_X)\n",
      "pred_test_Y= logreg.predict(test_X)\n",
      "\n",
      "train_accuracy = accuracy_score(train_Y, pred_train_Y)\n",
      "test_accuracy=accuracy_score(test_Y, pred_test_Y)\n",
      "\n",
      "from sklearn.metrics import precision_score, recall_score\n",
      "\n",
      "train_precision = round(precision_score(train_Y. pred_train_Y,4)\n",
      "test_precision=round(precision_score(test_Y,pred_test_Y),4)\n",
      "\n",
      "\n",
      "  Regularization\n",
      "\n",
      "* Introduces penalty coefficient in the model building phase\n",
      "* Addresses over-fitting (when patterns are memorized by the model)\n",
      "\n",
      "-- the classifier does well at recalling the predictions on the training data but does not do well on the testing data\n",
      "\n",
      "L1 Regularization and feature selection\n",
      "-- reduces the number of features and makes the model more predictable\n",
      "\n",
      "L1 regularization called LASSO can be called explicitly, and this approach performs\n",
      "feature selection by shrinking some of the model coefficients to zero\n",
      "\n",
      "logreg = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
      "logreg.fit(train_X,train_Y)\n",
      "\n",
      "C=0 to 1\n",
      "\n",
      "C=[1,.5,.25,.1,.05,.25,.01,.005,.0025]\n",
      "\n",
      "l1_metrics=np.zeros(len(C),5))\n",
      "l1_metrics[:,0]=C\n",
      "\n",
      "for index in range(0, len(C)):\n",
      "\tlogreg = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
      "\tlogreg.fit(train_X,train_Y)\n",
      "\tpred_test_Y= logreg.predict(test_X)\n",
      "\n",
      "\tl1_metrics[index,1]=np.count_nonzero(logreg.coef_)\n",
      "\tl1_metrics[index,1]=accuracy_score(test_Y, pred_test_Y)\n",
      "\tl1_metrics[index,1]=precision_score(test_Y,pred_test_Y)\n",
      "\tl1_metrics[index,1]=recall_score(test_Y,pred_test_Y)\n",
      "col_names=['C','non-zero coeffs','accuracy','precision','recall']\n",
      "print(pd.DataFrame(l1_metrics, columns=col_names)\n",
      "\n",
      "we want a model that has reduced complexity but similar performance metrics\n",
      "\n",
      "Non-Zero coeffs are feature count\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Fit logistic regression on training data\n",
      "logreg.fit(train_X, train_Y)\n",
      "\n",
      "# Predict churn labels on testing data\n",
      "pred_test_Y = logreg.predict(test_X)\n",
      "\n",
      "# Calculate accuracy score on testing data\n",
      "test_accuracy = accuracy_score(test_Y, pred_test_Y)\n",
      "\n",
      "# Print test accuracy score rounded to 4 decimals\n",
      "print('Test accuracy:', round(test_accuracy, 4))\n",
      "\n",
      "  >Sample\n",
      "\n",
      "# Initialize logistic regression instance \n",
      "logreg = LogisticRegression(penalty='l1', C=0.025, solver='liblinear')\n",
      "\n",
      "# Fit the model on training data\n",
      "logreg.fit(train_X, train_Y)\n",
      "\n",
      "# Predict churn values on test data\n",
      "pred_test_Y = logreg.predict(test_X)\n",
      "\n",
      "# Print the accuracy score on test data\n",
      "print('Test accuracy:', round(accuracy_score(test_Y, pred_test_Y), 4))\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Run a for loop over the range of C list length\n",
      "for index in range(0, len(C)):\n",
      "  # Initialize and fit Logistic Regression with the C candidate\n",
      "  logreg = LogisticRegression(penalty='l1', C=C[index], solver='liblinear')\n",
      "  logreg.fit(train_X, train_Y)\n",
      "  # Predict churn on the testing data\n",
      "  pred_test_Y = logreg.predict(test_X)\n",
      "  # Create non-zero count and recall score columns\n",
      "  l1_metrics[index,1] = np.count_nonzero(logreg.coef_)\n",
      "  l1_metrics[index,2] = recall_score(test_Y, pred_test_Y)\n",
      "\n",
      "\n",
      "         >Decision Tree\n",
      "\n",
      "if else rules\n",
      "\n",
      "dt= DecisionTreeClassifier(max_depth=2, random_state=1)\n",
      "\n",
      "dt.fit(X_train, y_train)\n",
      "\n",
      "pred_test= dt.predict(X_test)\n",
      "pred_train= dt.predict(X_train)\n",
      "\n",
      "\n",
      "buffer=pd.Series(pred_test)\n",
      "buffer.value_counts().plot(kind='pie')\n",
      "plt.show()\n",
      "\n",
      "print(\"0 none churn 1 churn\")\n",
      "\n",
      "print(\"Training accuracy:\",round(accuracy_score(y_train,pred_train),4))\n",
      "print(\"Testing accuracy:\", round(accuracy_score(y_test, pred_test),4))\n",
      "\n",
      "\n",
      "depth_list=list(range(2,15))\n",
      "depth_tuning = np.zeros((len(depth_list),4))\n",
      "depth_tuning[:,0]=depth_list\n",
      "\n",
      "for index in range(len(depth_list)):\n",
      "    mytree=DecisionTreeClassifier(max_depth=depth_list[index])\n",
      "    mytree.fit(X_train,y_train)\n",
      "    pred_test_Y= mytree.predict(X_test)\n",
      "\n",
      "    depth_tuning[index,1]=accuracy_score(y_test,pred_test_Y)\n",
      "    depth_tuning[index,2]=precision_score(y_test,pred_test_Y)\n",
      "    depth_tuning[index,3]=recall_score(y_test,pred_test_Y)\n",
      "    \n",
      "col_names=['Max_Depth','Accuracy','Precision','Recall']\n",
      "print(pd.DataFrame(depth_tuning, columns=col_names))\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Initialize decision tree classifier\n",
      "mytree = tree.DecisionTreeClassifier()\n",
      "\n",
      "# Fit the decision tree on training data\n",
      "mytree.fit(train_X, train_Y)\n",
      "\n",
      "# Predict churn labels on testing data\n",
      "pred_test_Y = mytree.predict(test_X)\n",
      "\n",
      "# Calculate accuracy score on testing data\n",
      "test_accuracy = accuracy_score(test_Y, pred_test_Y)\n",
      "\n",
      "# Print test accuracy\n",
      "print('Test accuracy:', round(test_accuracy, 4))\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Run a for loop over the range of depth list length\n",
      "for index in range(0, len(depth_list)):\n",
      "  # Initialize and fit decision tree with the `max_depth` candidate\n",
      "  mytree = DecisionTreeClassifier(max_depth=depth_list[index])\n",
      "  mytree.fit(train_X, train_Y)\n",
      "  # Predict churn on the testing data\n",
      "  pred_test_Y = mytree.predict(test_X)\n",
      "  # Calculate the recall score \n",
      "  depth_tuning[index,1] = recall_score(test_Y, pred_test_Y)\n",
      "\n",
      "\n",
      " >Identifying insights into churn\n",
      "\n",
      "from sklearn import tree\n",
      "import graphviz\n",
      "\n",
      "\n",
      "exported=tree.export_graphviz(\n",
      "\tdecision_tree=mytree,\n",
      "\tout_file=None,\n",
      "\tfeature_names=cols,\n",
      "\tprecision=1,\n",
      "\tclass_names=['Not churn','Churn'],\n",
      "\tfilled=True)\n",
      "\n",
      "graph=graphviz.Source(exported)\n",
      "display(graph)\n",
      "\n",
      "\n",
      "<<<<<Logistic regression coefficients\n",
      "\n",
      "1. Logistic regression returns beta coefficients\n",
      "2. The coeffients can to be intrepretated as the log-odds of churn associated with 1 unit increase in the feature\n",
      "\n",
      "logb p/(1-p)\n",
      "\n",
      "log of odds is hard to intrepret\n",
      "\n",
      "logreg.coef_\n",
      "\n",
      "* calculate the exponent of the coefficients\n",
      "* This gives us the change in odds associated with 1 unit increase in the feature\n",
      "\n",
      "\n",
      "coefficients = pd.concat([pd.DataFrame(train_X.columns),\n",
      "pd.DataFrame(np.transpose(logit.coef_))],\n",
      "axis=1)\n",
      "\n",
      "coefficients.columns=['Feature','Coefficient']\n",
      "\n",
      "coefficients['Exp_Coefficients']=np.exp(coefficients['Coefficient'])\n",
      "coefficients=cefficients[coefficients['Coefficients]!=0]\n",
      "print(coefficients.sort_value(by=['Coefficient']))\n",
      "\n",
      "\n",
      "*values less than 1 decrease the odds\n",
      "*values greater than 1 increase the odds\n",
      "\n",
      "One additional year of tenure decrease churn odds by 60%\n",
      "\n",
      "\n",
      "   >Customer Lifetime Value basics (CLV)\n",
      "\n",
      "*CLV is the amount of money a company expect to earn in a lifetime\n",
      "\n",
      "Historical CLV = (revenues)*Profit Margin\n",
      "\n",
      "* Does not account for tenure, retention and churn rates\n",
      "\n",
      "* Does not account for new customers and their future revenue\n",
      "\n",
      "\n",
      "CLV = Average Revenue (for a certain period of time) * Profit Margin * Average Lifespan\n",
      "\n",
      "* lifespan is knowledge about its customers or the average lifespan of the customer churn.\n",
      "\n",
      "CLV (avg.revenue per purchase * avg.frequency* profit margin) * average lifespan\n",
      "\n",
      "* does not account for customer retention rates\n",
      "\n",
      "CLV = (Average Revenue * Profit Margin) * Retention Rate/Churn Rate\n",
      "\n",
      "churn= 1- retention\n",
      "\n",
      "cohort_sizes=cohort_counts.iloc[:,0]\n",
      "retention=cohorts_counts.divide(cohort_sizes,axis=0)\n",
      "churn=1-retention\n",
      "\n",
      "sns.heatmap(retention, annot=True, vmin=0, vmax=0.5, map=\"Y1Gn\")\n",
      "\n",
      "\n",
      "  Sample (calculate retention and churn)\n",
      "\n",
      "# Extract cohort sizes from the first column of cohort_counts\n",
      "cohort_sizes = cohort_counts.iloc[:,0]\n",
      "\n",
      "# Calculate retention by dividing the counts with the cohort sizes\n",
      "retention = cohort_counts.divide(cohort_sizes, axis=0)\n",
      "\n",
      "# Calculate churn\n",
      "churn = 1 - retention\n",
      "\n",
      "# Print the retention table\n",
      "print(churn)\n",
      "print(cohort_counts.shape)\n",
      "\n",
      "\n",
      "  Sample (calculate retention rate and churn rate)\n",
      "\n",
      "Now that you have calculated the monthly retention and churn metrics for monthly customer cohorts, you can calculate the overall mean retention and churn rates. You will use the .mean() method twice in a row (this is called \"chaining\") to calculate the overall mean\n",
      "\n",
      "# Calculate the mean retention rate\n",
      "retention_rate = retention.iloc[:,1:].mean().mean()\n",
      "\n",
      "# Calculate the mean churn rate\n",
      "churn_rate = churn.iloc[:,1:].mean().mean()\n",
      "\n",
      "# Print rounded retention and churn rates\n",
      "print('Retention rate: {:.2f}; Churn rate: {:.2f}'.format(retention_rate, churn_rate))\n",
      "\n",
      "    CLV\n",
      "1. goal clv measure customers in terms of revenue or profit\n",
      "2. benchmark customers\n",
      "3. identify maximum investment to gain customer acquistion\n",
      "\n",
      "CLV = Average Revenue * Retention Rate/churn rate\n",
      "\n",
      "\n",
      "      >Basic CLV\n",
      "\n",
      "1, Calculate monthly spent by the customer\n",
      "\n",
      "monthly_revenue = online.groupby(['CustomerID','InvoiceMonth'])\n",
      "['TotalSum'].sum().mean()\n",
      "\n",
      "monthly_revenue=np.mean(month_revenue)\n",
      "\n",
      "lifespan_months=36\n",
      "\n",
      "clv_basic=monthly_revenue * lifespn_months\n",
      "\n",
      "print('Average basic CLV is (:1f) USD'.format(clv_basic))\n",
      "\n",
      "       Granular CLV calculation\n",
      "\n",
      "revenue_per_purchase= online.groupby(['InvoiceNo']).['TotalSum'].mean().mean()\n",
      "\n",
      "##overall revenue for a purchase\n",
      "\n",
      "freq=online.groupby(['CustomerId','InvoicedMonth'])['InvoiceMonth'].nunique().mean()\n",
      "\n",
      "##calculate the average number of unique invoices per customer per month\n",
      "\n",
      "lifespan_months=36\n",
      "\n",
      "clv_granular= revenue_per_purchase * freq * lifespan_months\n",
      "\n",
      "print('Average granular CLV is (:,1f) USD'.format(clv_granular))\n",
      "\n",
      "print('Revenue per purchase is (:,1f) USD'.format(revenue_per_purchase)\n",
      "\n",
      "print('Frequency per month is (:,1f) USD'.format(freq)\n",
      "\n",
      "\n",
      "       Traditional CLV calculation\n",
      "\n",
      "monthly_revenue= online.groupby(['CustomerID','InvoiceMonth'])['TotalSum'].sum().mean()\n",
      "\n",
      "retention_rate=retention.iloc[:,1].mean().mean()\n",
      "\n",
      "churn_rate=1-retention_rate\n",
      "\n",
      "clv_traditional=month_revenue * (retention_rate/churn_rate)\n",
      "\n",
      "print('Average traditional clv is (:.1f) % retention_rate'.format(clv_traditional, retention_rate*100))\n",
      "\n",
      "   Which method to use\n",
      "\n",
      "1. depends on business model\n",
      "2.traditional clv model - assumes churn is definitive - customer dies.  The customer is assumed to not come back if they have churned once.\n",
      "3. traditional model is not robust at low retention values\n",
      "4. hardest thing to predict - frequency in the future\n",
      "\n",
      "\n",
      "  Sample (basic clv of 36 months)\n",
      "\n",
      "\n",
      "# Calculate monthly spend per customer\n",
      "monthly_revenue = online.groupby(['CustomerID','InvoiceMonth'])['TotalSum'].sum().mean()\n",
      "\n",
      "# Calculate average monthly spend\n",
      "monthly_revenue = np.mean(monthly_revenue)\n",
      "\n",
      "# Define lifespan to 36 months\n",
      "lifespan_months = 36\n",
      "\n",
      "# Calculate basic CLV\n",
      "clv_basic = monthly_revenue * lifespan_months\n",
      "\n",
      "# Print the basic CLV value\n",
      "print('Average basic CLV is {:.1f} USD'.format(clv_basic))\n",
      "\n",
      "\n",
      " >Sample (granular)\n",
      "\n",
      "# Calculate average revenue per invoice\n",
      "revenue_per_purchase = online.groupby(['InvoiceNo'])['TotalSum'].mean().mean()\n",
      "\n",
      "# Calculate average number of unique invoices per customer per month\n",
      "frequency_per_month = online.groupby(['CustomerID','InvoiceMonth'])['InvoiceNo'].nunique().mean()\n",
      "\n",
      "# Define lifespan to 36 months\n",
      "lifespan_months = 36\n",
      "\n",
      "# Calculate granular CLV\n",
      "clv_granular = revenue_per_purchase * frequency_per_month * lifespan_months\n",
      "\n",
      "# Print granular CLV value\n",
      "print('Average granular CLV is {:.1f} USD'.format(clv_granular))\n",
      "\n",
      " >Sample (traditional)\n",
      "\n",
      "#Now you will calculate one of the most popular descriptive CLV models that accounts for the retention and churn rates. This gives a more robust estimate, but comes with certain assumptions that have to be validated\n",
      "\n",
      "# Calculate monthly spend per customer\n",
      "monthly_revenue = online.groupby(['CustomerID','InvoiceMonth'])['TotalSum'].sum().mean()\n",
      "\n",
      "# Calculate average monthly retention rate\n",
      "retention_rate = retention.iloc[:,1:].mean().mean()\n",
      "\n",
      "# Calculate average monthly churn rate\n",
      "churn_rate = 1 - retention_rate\n",
      "\n",
      "# Calculate traditional CLV \n",
      "clv_traditional = monthly_revenue * (retention_rate / churn_rate)\n",
      "\n",
      "# Print traditional CLV and the retention rate values\n",
      "print('Average traditional CLV is {:.1f} USD at {:.1f} % retention_rate'.format(clv_traditional, retention_rate*100))\n",
      "\n",
      "#As you can see, the traditional CLV formula yields a much lower estimate as it accounts for monthly retention which is quite low for this company.\n",
      "\n",
      "\n",
      "         Data preparation for purchase prediction\n",
      "\n",
      "* regression to predict purchasing\n",
      "* simplest model is linear regression\n",
      "* target variable is either continous or count\n",
      "\n",
      "* count data (number of active days) work better with poisson or negative binomal regression\n",
      "\n",
      "RFM - recency, frequency, or monetary features\n",
      "\n",
      "explore the sales distribution by month\n",
      "\n",
      "online.groupby(['InvoiceMonth']).size()\n",
      "\n",
      "#prints out the number of observations per month\n",
      "\n",
      "online_X=online[online['InvoiceMonth']='2011-11']\n",
      "\n",
      "#calculate the recency\n",
      "\n",
      "NOW= dt.datetime(2011,11,1)\n",
      "\n",
      "features = online_X.groupby('CustomerID').agg({\n",
      "\t'InvoiceDate': lambda x(NOW-x.max())days,\n",
      "\t'InvoiceMo': pd.Series.nunique,\n",
      "\t'TotalSum': np.sum,\n",
      "\t'Quantity': ['mean','sum']\n",
      "\n",
      "}).reset_index()\n",
      "\n",
      "features.columns=['CustomerID','recency','frequency','monetary','quantity_avg','quantity_total']\n",
      "\n",
      "#recency is the now date - the lastest invoice date\n",
      "#frequency by counting the unique number of invoice\n",
      "#sum the revenue for that customer\n",
      "#calculate the quantity and sum of the quantities\n",
      "#reindex makes sure the columns are not stored as an index for use later\n",
      "\n",
      "  Calculate the target variable\n",
      "\n",
      "#build a pivot table\n",
      "\n",
      "cust_month_tx= pd.pivot_table(data=online, index=['CustomerID'],\n",
      "\tvalues='InvoiceNo',\n",
      "\tcolumns=['InvoiceMonth'],\n",
      "\taggfunc=pd.Series.nunique, fill_value=0)\n",
      "\n",
      "print(cust_month_tx.head())\n",
      "\n",
      "#the result is a matrix of unique invoices per month by customer ID\n",
      "\n",
      "#use the last month of data\n",
      "\n",
      "#store the identifier and the target variable as separate list\n",
      "\n",
      "custid=['CustomerID']\n",
      "target=['2011-11']\n",
      "\n",
      "Y=cust_month_tx[target]\n",
      "\n",
      "cols=[col for col in features.columns if col not in custid]\n",
      "\n",
      "X=featurs(cols)\n",
      "\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "train_X,test_X,train_Y,test_Y= train_test_split(X,Y,\n",
      "\ttest_size=0.25, random_state=99)\n",
      "\n",
      "\n",
      "print(train_X.shape, train_Y.shape, test_\n",
      "X.shape, test_Y.shape)\n",
      "\n",
      "  >Sample (building features of Recency, Frequency, and Monetary)\n",
      "\n",
      "\n",
      "# Define the snapshot date\n",
      "NOW = dt.datetime(2011,11,1)\n",
      "\n",
      "# Calculate recency by subtracting current date from the latest InvoiceDate\n",
      "features = online_X.groupby('CustomerID').agg({\n",
      "  'InvoiceDate': lambda x: (NOW - x.max()).days,\n",
      "  # Calculate frequency by counting unique number of invoices\n",
      "  'InvoiceNo': pd.Series.nunique,\n",
      "  # Calculate monetary value by summing all spend values\n",
      "  'TotalSum': np.sum,\n",
      "  # Calculate average and total quantity\n",
      "  'Quantity': ['mean', 'sum']}).reset_index()\n",
      "\n",
      "# Rename the columns\n",
      "features.columns = ['CustomerID', 'recency', 'frequency', 'monetary', 'quantity_avg', 'quantity_total']\n",
      "\n",
      "\n",
      "# Build a pivot table counting invoices for each customer monthly\n",
      "cust_month_tx = pd.pivot_table(data=online, values='InvoiceNo',\n",
      "                               index=['CustomerID'], columns=['InvoiceMonth'],\n",
      "                               aggfunc=pd.Series.nunique, fill_value=0)\n",
      "\n",
      "# Store November 2011 data column name as a list\n",
      "target = ['2011-11']\n",
      "\n",
      "# Store target value as `Y`\n",
      "Y = cust_month_tx[target]\n",
      "\n",
      "# Store customer identifier column name as a list\n",
      "custid = ['CustomerID']\n",
      "\n",
      "# Select feature column names excluding customer identifier\n",
      "cols = [col for col in features.columns if col not in custid]\n",
      "\n",
      "# Extract the features as `X`\n",
      "X = features[cols]\n",
      "\n",
      "# Split data to training and testing\n",
      "Train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.25, random_state=99)\n",
      "\n",
      "\n",
      "   Predicting next months transactions\n",
      "\n",
      "Use linear regression to predict next months transactions\n",
      "initializing the model\n",
      "fit and predict\n",
      "measure\n",
      "\n",
      "\n",
      "root mean squared error (RMSE) - Square root of the average squared differences between prediction and actuals\n",
      "a. subtract the predicted and actuals\n",
      "b. square the results\n",
      "c. calculate the average\n",
      "d. take the square root to get a normalized measurement\n",
      "\n",
      "\n",
      "    Mean absolute error (MAE)\n",
      "mean absolute error - Average absolute difference between the predicted and actuals\n",
      "\n",
      "    Mean absolute percentage error (MAPE)\n",
      "average percentage difference between prediction and actuals\n",
      "normalized between 0 and 100 percent (actuals can't be zero)\n",
      "\n",
      "R-squared: statistical measure that represents the percentage proportion of variance that is explained by the model.  \n",
      "applies only to regression\n",
      "(Higer is better)\n",
      "\n",
      "coefficient p-values - probability that the regression coefficient is observed due to chance.  (lower is better)\n",
      "threshholds are 5% to 10%  (measures the significance of the null hypothesis)\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "linreg=LinearRegression()\n",
      "\n",
      "linreq.fit(train_X,train_Y)\n",
      "\n",
      "train_pred_Y= linreq.predict(train_X)\n",
      "test_pred_Y = linreq.predict(test_X)\n",
      "\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "rmse_train=np.sqrt(mean_squared_error(train_Y,train_pred_Y))\n",
      "mae_train=mean_absolute_error(train_Y,train_pred_Y)\n",
      "\n",
      "rmse_test=np.sqrt(mean_squared_error(test_Y,test_pred_Y))\n",
      "mae_test=mean_absolute_error(test_Y,test_pred_Y)\n",
      "\n",
      "print('RMSE train: (:3f): RMSE test: (:3f)\\nMAE train :{:3f}, MAE test: {:3f}'.format(rmse_train,rmse_test, mae_train, mae_test))\n",
      "\n",
      "\n",
      " >Interpreting the coefficients\n",
      "\n",
      "1. statistical significance - standard statistical significant is 95%\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "train_Y=np.array(train_Y)\n",
      "\n",
      "#Ordinary Least Square Model (curve fitting algorithm)\n",
      "\n",
      "olsreg = sm.OLS(train_Y, train_X)\n",
      "olsreg=olsreg.fit()\n",
      "\n",
      "print(olsreg.summary())\n",
      "\n",
      "#R-squared is the percentage of explained variance.  What percentage does the model explain of the variation? (higher is better)\n",
      "\n",
      "check the P-value coefficients\n",
      "(change in the output variable if one unit changed in the feature).  Some of the coeffiencts are not statistically significant. 1-significance = 100-95% or 5%  (look features for p values less than 5%)\n",
      "\n",
      "\n",
      "Sample \n",
      "\n",
      "# Initialize linear regression instance\n",
      "linreg = LinearRegression()\n",
      "\n",
      "# Fit the model to training dataset\n",
      "linreg.fit(train_X, train_Y)\n",
      "\n",
      "# Predict the target variable for training data\n",
      "train_pred_Y = linreg.predict(train_X)\n",
      "\n",
      "# Predict the target variable for testing data\n",
      "test_pred_Y = linreg.predict(test_X)\n",
      "\n",
      "\n",
      "#This is a critical step where you are measuring how \"close\" are the model predictions compared to actual values.\n",
      "\n",
      "# Calculate root mean squared error on training data\n",
      "rmse_train = np.sqrt(mean_squared_error(train_Y, train_pred_Y))\n",
      "\n",
      "# Calculate mean absolute error on training data\n",
      "mae_train = mean_absolute_error(train_Y, train_pred_Y)\n",
      "\n",
      "# Calculate root mean squared error on testing data\n",
      "rmse_test = np.sqrt(mean_squared_error(test_Y, test_pred_Y))\n",
      "\n",
      "# Calculate mean absolute error on testing data\n",
      "mae_test = mean_absolute_error(test_Y, test_pred_Y)\n",
      "\n",
      "# Print the performance metrics\n",
      "print('RMSE train: {}; RMSE test: {}\\nMAE train: {}, MAE test: {}'.format(rmse_train, rmse_test, mae_train, mae_test))\n",
      "\n",
      "\n",
      " >Sample OLS \n",
      "\n",
      "OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.488\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.487\n",
      "Method:                 Least Squares   F-statistic:                              480.3\n",
      "Date:                Mon, 07 Sep 2020   Prob (F-statistic):                        0.00\n",
      "Time:                        22:03:37   Log-Likelihood:                         -2769.8\n",
      "No. Observations:                2529   AIC:                                      5550.\n",
      "Df Residuals:                    2524   BIC:                                      5579.\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "recency            0.0002      0.000      1.701      0.089   -2.92e-05       0.000\n",
      "frequency          0.1316      0.003     38.000      0.000       0.125       0.138\n",
      "monetary        1.001e-06   3.59e-05      0.028      0.978   -6.95e-05    7.15e-05\n",
      "quantity_avg       0.0001      0.000      0.803      0.422      -0.000       0.000\n",
      "quantity_total    -0.0001   5.74e-05     -2.562      0.010      -0.000   -3.45e-05\n",
      "==============================================================================\n",
      "Omnibus:                      987.494   Durbin-Watson:                   1.978\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5536.657\n",
      "Skew:                           1.762   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.334   Cond. No.                         249.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "In [2]: \n",
      "\n",
      "\n",
      "  Customer and product segmentation on basics\n",
      "\n",
      "wholesale.head()\n",
      "\n",
      "1. Fresh\n",
      "2. Milk\n",
      "3. Grocery\n",
      "4. Frozen\n",
      "5. Detergents_Paper\n",
      "6. Delicassens\n",
      "\n",
      "Unsupervised learning models\n",
      "\n",
      "* k-means\n",
      "* non-negative matrix factorization nmf\n",
      "\n",
      "1. initialize the model\n",
      "2. fit the model\n",
      "3. assign cluster values\n",
      "\n",
      "wholesale.agg(['mean','std']).round(0)\n",
      "\n",
      "averages= wholesale.mean()\n",
      "st_dev = wholesale.std()\n",
      "x_names=wholesale.columns\n",
      "x_ix= np.arange(wholesale.shape[1])\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.bar(x_ix-0.2, averages, color='grey', label='Average', width=0.4)\n",
      "plt.bar(x_ix+0.2, std_dev, color='orange',' label='Standard Deviation', width=0.4)\n",
      "plt.xticks(x_ix, x_names, rotation=90)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "sns.pairplot(wholesale,diag_kind='kde')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      " >Sample (pairplot)\n",
      "\n",
      "# Print the header of the `wholesale` dataset\n",
      "print(wholesale.head())\n",
      "\n",
      "# Plot the pairwise relationships between the variables\n",
      "sns.pairplot(wholesale, diag_kind='kde')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "  Sample (bar plot average and standard deviation)\n",
      "\n",
      "# Create column names list and same length integer list\n",
      "x_names = wholesale.columns\n",
      "x_ix = np.arange(wholesale.shape[1])\n",
      "\n",
      "# Plot the averages data in gray and standard deviations in orange \n",
      "plt.bar(x=x_ix-0.2, height=averages, color='grey', label='Average', width=0.4)\n",
      "plt.bar(x=x_ix+0.2, height=std_devs, color='orange', label='Standard Deviation', width=0.4)\n",
      "\n",
      "# Add x-axis labels and rotate\n",
      "plt.xticks(ticks=x_ix, labels=x_names, rotation=90)\n",
      "\n",
      "# Add the legend and display the chart\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  >Data preparation for segmentation\n",
      "\n",
      "1. start with k-means\n",
      "2. k-means works well when the data is normally distributed\n",
      "a. mean=0\n",
      "b. standard deviation=1\n",
      "\n",
      "Non-negative matrix factorization works well with on draw sparse matrices\n",
      "\n",
      "wholesale_log = np.log(wholesale)\n",
      "\n",
      "sns.pairplot(wholesale_log, diag_kind='kde')\n",
      "plt.show()\n",
      "\n",
      "#result in less skewed data\n",
      "\n",
      " >Box-cox transformation\n",
      "\n",
      "\n",
      "from scipy import stats\n",
      "\n",
      "def boxcox_df(x):\n",
      "\tx_boxcox, _ = stats.boxcox(x)\n",
      "\treturn x_boxcox\n",
      "\n",
      "wholesale_boxcox = wholesale.apply(boxcox_df,axis, 0)\n",
      "\n",
      "sns.pairplot(wholesale_boxcox, diag_kind='kde')\n",
      "plt.show()\n",
      "\n",
      "   Scale the data\n",
      "1. Subtract column average from each column value\n",
      "2. Divide each column value by column standard deviation\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler=StandardScaler()\n",
      "\n",
      "scaler.fit(wholesale_boxcox)\n",
      "\n",
      "#numpy array\n",
      "wholesale_scaled= scaler.transform(wholesale_box)\n",
      "\n",
      "wholesale_scaled_df=pd.DataFrame(data=whosale_scaled,\n",
      "\tindex=wholesale_boxcox.index,\n",
      "\tcolumns=wholesale_boxcox.columns)\n",
      "\n",
      "wholesale_scaled_df.agg(['mean','[std']).round()\n",
      "\n",
      " >Sample sns pairplot\n",
      "\n",
      "# Define custom Box Cox transformation function\n",
      "def boxcox_df(x):\n",
      "    x_boxcox, _ = stats.boxcox(x)\n",
      "    return x_boxcox\n",
      "\n",
      "# Apply the function to the `wholesale` dataset\n",
      "wholesale_boxcox = wholesale.apply(boxcox_df, axis=0)\n",
      "\n",
      "# Plot the pairwise relationships between the transformed variables \n",
      "sns.pairplot(wholesale_boxcox, diag_kind='kde')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample (Scaling)\n",
      "\n",
      "# Fit the initialized `scaler` instance on the Box-Cox transformed dataset\n",
      "scaler.fit(wholesale_boxcox)\n",
      "\n",
      "# Transform and store the scaled dataset as `wholesale_scaled`\n",
      "wholesale_scaled = scaler.transform(wholesale_boxcox)\n",
      "\n",
      "# Create a `pandas` DataFrame from the scaled dataset\n",
      "wholesale_scaled_df = pd.DataFrame(data=wholesale_scaled,\n",
      "                                       index=wholesale_boxcox.index,\n",
      "                                       columns=wholesale_boxcox.columns)\n",
      "\n",
      "# Print the mean and standard deviation for all columns\n",
      "print(wholesale_scaled_df.agg(['mean','std']).round())\n",
      "\n",
      "  >Kmeans\n",
      "\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans= KMeans(n_cluster=k)\n",
      "\n",
      "kmeans.fit(wholesale_scaled_df)\n",
      "\n",
      "\n",
      "#Use the original df not the scaled one\n",
      "wholesale_kmeans4 = wholesale.assign(segment=kmeans.labels_)\n",
      "\n",
      "\n",
      " >NMF\n",
      "\n",
      "from sklearn.decomposition import NMF\n",
      "\n",
      "nmf=NMF(k)\n",
      "nmf.fit(wholesale)\n",
      "\n",
      "components=pd.DataFrame(nmf.components_, columns=wholesale.columns)\n",
      "\n",
      "segment_weights= pd.DataFrame(nmf.transform(wholesale, columns=component.index)\n",
      "\n",
      "segment_weights.index=wholesale.index\n",
      "\n",
      "wholesale_nmf= wholesale.assign(segment=segment_weights.idxmax(axis=1))\n",
      "\n",
      "#new column - which cluster weight is largest for each customer\n",
      "\n",
      "  Defining k\n",
      "elbow criterion method to get the optimal number of k clusters\n",
      "a. iterate through a number of k values\n",
      "b. running cluster for each on the same data\n",
      "c. calculate sum of squared errors (se) for each\n",
      "d. plot the sse against k and identify the elbow of diminishing incremental improvements\n",
      "\n",
      "  Samples (NMF heatmap)\n",
      "\n",
      "# Create the W matrix\n",
      "W = pd.DataFrame(data=nmf.transform(wholesale), columns=components.index)\n",
      "W.index = wholesale.index\n",
      "\n",
      "# Assign the column name where the corresponding value is the largest\n",
      "wholesale_nmf3 = wholesale.assign(segment = W.idxmax(axis=1))\n",
      "\n",
      "# Calculate the average column values per each segment\n",
      "nmf3_averages = wholesale_nmf3.groupby('segment').mean().round(0)\n",
      "\n",
      "# Plot the average values as heatmap\n",
      "sns.heatmap(nmf3_averages.T, cmap='YlGnBu')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "sse={}\n",
      "\n",
      "for k in range(1,11):\n",
      "\tkmeans=KMeans(n_clusters=k, random_state=333)\n",
      "\tkmeans.fit(wholesale_scaled_df)\n",
      "\tsse(k)=kmeans.inertia_\n",
      "\n",
      "\n",
      "plt.title('Elbow criterion method chart')\n",
      "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
      "plt.show()\n",
      "\n",
      "build meanful segmentation\n",
      "can you give the segmentation a name given the clustering.\n",
      "\n",
      " >Sample (elbow)\n",
      "\n",
      "# Create empty sse dictionary\n",
      "sse = {}\n",
      "\n",
      "# Fit KMeans algorithm on k values between 1 and 11\n",
      "for k in range(1, 11):\n",
      "    kmeans = KMeans(n_clusters=k, random_state=333)\n",
      "    kmeans.fit(wholesale_scaled_df)\n",
      "    sse[k] = kmeans.inertia_\n",
      "\n",
      "# Add the title to the plot\n",
      "plt.title('Elbow criterion method chart')\n",
      "\n",
      "# Create and display a scatter plot\n",
      "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample (KMeans)\n",
      "\n",
      "# Import `KMeans` module\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Initialize `KMeans` with 4 clusters\n",
      "kmeans=KMeans(n_clusters=4, random_state=123)\n",
      "\n",
      "# Fit the model on the pre-processed dataset\n",
      "kmeans.fit(wholesale_scaled_df)\n",
      "\n",
      "# Assign the generated labels to a new column\n",
      "wholesale_kmeans4 = wholesale.assign(segment = kmeans.labels_)\n",
      "\n",
      "  Sample (NMF)\n",
      "\n",
      "# Import the non-negative matrix factorization module\n",
      "from sklearn.decomposition import NMF\n",
      "\n",
      "# Initialize NMF instance with 4 components\n",
      "nmf = NMF(4)\n",
      "\n",
      "# Fit the model on the wholesale sales data\n",
      "nmf.fit(wholesale)\n",
      "\n",
      "# Extract the components \n",
      "components = pd.DataFrame(data=nmf.components_, columns=wholesale.columns)\n",
      "\n",
      "       >Visualize and interpret segmentation solutions\n",
      "1. Calculate average/median/other percentile values for each variable by segment\n",
      "2. Calculate relative importance for each variable by segment\n",
      "3. Visualize using a heatmap\n",
      "\n",
      "\n",
      "\n",
      "kmeans4_averages= wholesale_kmeans4.groupby(['segment']).mean().round(0)\n",
      "\n",
      "print(kmeans4_averages)\n",
      "\n",
      "The four segments have different average values for fresh, milk, grocery, frozen, detergents_paper, delicassen\n",
      "\n",
      "sns.heatmap(kmeans4_averages.T, cmap='Y1GnBu')\n",
      "plt.show()\n",
      "\n",
      " >Plot average NMF segmentation attributes\n",
      "\n",
      "nmf4_averages=wholesale_nmf4.groupby('segment').mean().round(0)\n",
      "sns.heatmap(nmf4_averages.T, cmap='Y1GnBu')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample (heatmap kmeans clusters)\n",
      "# Group by the segment label and calculate average column values\n",
      "kmeans3_averages= wholesale_kmeans3.groupby(['segment']).mean().round(0)\n",
      "\n",
      "# Print the average column values per each segment\n",
      "print(kmeans3_averages)\n",
      "\n",
      "# Create a heatmap on the average column values per each segment\n",
      "sns.heatmap(kmeans3_averages.T, cmap='YlGnBu')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<<<<<<<<  Predicting Customer Churn\n",
      "    One hot encoding\n",
      "\n",
      "1. numeric one for a category in a column\n",
      "\n",
      "print(telco.dtypes) to find the objects to encode\n",
      "\n",
      "    Standardization\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "df=StandardScaler().fit_transform(df)\n",
      "\n",
      "  Sample (encoding)\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Vmail_Plan'\n",
      "telco['Vmail_Plan'] = telco['Vmail_Plan'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Churn'\n",
      "telco['Churn'] = telco['Churn'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Print the results to verify\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Perform one hot encoding on 'State'\n",
      "telco_state = pd.get_dummies(telco['State'])\n",
      "\n",
      "print(telco_state)\n",
      "\n",
      "\n",
      " >Sample (Scaler)\n",
      "\n",
      "# Import StandardScaler\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Scale telco using StandardScaler\n",
      "telco_scaled = StandardScaler().fit_transform(telco)\n",
      "\n",
      "# Add column names back for readability\n",
      "telco_scaled_df = pd.DataFrame(telco_scaled, columns=[\"Intl_Calls\", \"Night_Mins\"])\n",
      "\n",
      "# Print summary statistics\n",
      "print(telco_scaled_df.describe())\n",
      "\n",
      "\n",
      "       Feature selection\n",
      "\n",
      "Unique identifiers that need to be dropped\n",
      "1. phone numbers\n",
      "2. customerid\n",
      "3. account numbers\n",
      "\n",
      "telco.drop(['Soc_Sec'], axis=1)\n",
      "\n",
      "Features that are highly correlated to features can be dropped because they offer no additional information to the model.\n",
      "\n",
      "telco.corr()\n",
      "\n",
      "remove features that are highly correlated\n",
      "\n",
      "should consult with business and subject matter experts\n",
      "\n",
      "A new feature could be\n",
      "\n",
      "Total_Minutes = Day_Mins+Eve_Mins+Night_Mins+Intl_Mins\n",
      "\n",
      "understanding the ratio of minutes and charge\n",
      "\n",
      "telco['Day_Cost']=telco['Day_Mins']/telco['Day_Charge']\n",
      "\n",
      "\n",
      " >Sample dropping columns\n",
      "\n",
      "# Drop the unnecessary features\n",
      "telco = telco.drop(['Area_Code','Phone'],axis=1)\n",
      "\n",
      "  Sample (new features)\n",
      "\n",
      "# Create the new feature\n",
      "telco['Avg_Night_Calls'] = telco['Night_Mins']/telco['Night_Calls']\n",
      "\n",
      "# Print the first five rows of 'Avg_Night_Calls'\n",
      "print(telco['Avg_Night_Calls'])\n",
      "\n",
      "      >Making predictions\n",
      "\n",
      "Logistic Regression: Good baseline offering simplicity and interpretability.\n",
      "\n",
      "Can not capture more complex relationships in the dataset\n",
      "\n",
      "Random forest\n",
      "\n",
      "Support vector machines\n",
      "\n",
      "  >Suport Vector Machine\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "\n",
      "svc.fit(telco[features], telco['target'])\n",
      "\n",
      "1. features must be contineous values\n",
      "2. dataframes or numpy arrays\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "decision_function_shape='ovr', degree=3, gamma='auto',\n",
      "kernel='rbf', max_iter=-1, probability=False,\n",
      "random_state=None, shrinking=True, tol=0.001,\n",
      "verbose=False)\n",
      "\n",
      "prediction = svc.predict(new_customer)\n",
      "print(prediction)\n",
      "\n",
      "\n",
      " >Sample (Logistic Regression)\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "clf = LogisticRegression()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "New customer\n",
      "\n",
      "Account_Length  Vmail_Message  Day_Mins  Eve_Mins  Night_Mins  ...  Eve_Charge  Night_Calls  Night_Charge  Intl_Calls  Intl_Charge\n",
      "0              91             23     232.4     186.0       190.5  ...       15.81          128          8.57           3         3.32\n",
      "\n",
      "\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "  >Sample (Decision Tree Classifier)\n",
      "\n",
      "# Import DecisionTreeClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "\n",
      "clf=DecisionTreeClassifier()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "# Predict the label of new_customer\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "       evaluate model performance\n",
      "1. Compute its accuracy\n",
      "2. Accuracy = Correct Predictions/total number of data points\n",
      "training data may not represent actual data\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(telco['data'],\n",
      "\ttelco['target'],\n",
      "\ttest_size=0.2,\n",
      "\trandom_state=42)\n",
      "\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "svc.fit(X_train, y_train)\n",
      "svc.predict(X_test)\n",
      "\n",
      "svc.score(X_test,y_test)\n",
      "\n",
      "#overfitting means the model has become to sensitive to noise in the training data\n",
      "\n",
      "#underfitting is means not capturing trends in the training data\n",
      "\n",
      "  Sample - Train Test Split\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(X,\n",
      "\ty,\n",
      "\ttest_size=0.3,\n",
      "\trandom_state=42)\n",
      "\n",
      "print(X_train.shape, X_test.shape)\n",
      "\n",
      " >Sample - (Random Forest Classifier)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "# Compute accuracy\n",
      "print(clf.score(X_test, y_test))\n",
      "\n",
      "        >Model Metrics\n",
      "\n",
      "\n",
      "<<<<<<<<  Predicting Customer Churn\n",
      "    One hot encoding\n",
      "\n",
      "1. numeric one for a category in a column\n",
      "\n",
      "print(telco.dtypes) to find the objects to encode\n",
      "\n",
      "    Standardization\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "df=StandardScaler().fit_transform(df)\n",
      "\n",
      "  Sample (encoding)\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Vmail_Plan'\n",
      "telco['Vmail_Plan'] = telco['Vmail_Plan'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Churn'\n",
      "telco['Churn'] = telco['Churn'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Print the results to verify\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Perform one hot encoding on 'State'\n",
      "telco_state = pd.get_dummies(telco['State'])\n",
      "\n",
      "print(telco_state)\n",
      "\n",
      "\n",
      " >Sample (Scaler)\n",
      "\n",
      "# Import StandardScaler\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Scale telco using StandardScaler\n",
      "telco_scaled = StandardScaler().fit_transform(telco)\n",
      "\n",
      "# Add column names back for readability\n",
      "telco_scaled_df = pd.DataFrame(telco_scaled, columns=[\"Intl_Calls\", \"Night_Mins\"])\n",
      "\n",
      "# Print summary statistics\n",
      "print(telco_scaled_df.describe())\n",
      "\n",
      "\n",
      "       Feature selection\n",
      "\n",
      "Unique identifiers that need to be dropped\n",
      "1. phone numbers\n",
      "2. customerid\n",
      "3. account numbers\n",
      "\n",
      "telco.drop(['Soc_Sec'], axis=1)\n",
      "\n",
      "Features that are highly correlated to features can be dropped because they offer no additional information to the model.\n",
      "\n",
      "telco.corr()\n",
      "\n",
      "remove features that are highly correlated\n",
      "\n",
      "should consult with business and subject matter experts\n",
      "\n",
      "A new feature could be\n",
      "\n",
      "Total_Minutes = Day_Mins+Eve_Mins+Night_Mins+Intl_Mins\n",
      "\n",
      "understanding the ratio of minutes and charge\n",
      "\n",
      "telco['Day_Cost']=telco['Day_Mins']/telco['Day_Charge']\n",
      "\n",
      "\n",
      " >Sample dropping columns\n",
      "\n",
      "# Drop the unnecessary features\n",
      "telco = telco.drop(['Area_Code','Phone'],axis=1)\n",
      "\n",
      "  Sample (new features)\n",
      "\n",
      "# Create the new feature\n",
      "telco['Avg_Night_Calls'] = telco['Night_Mins']/telco['Night_Calls']\n",
      "\n",
      "# Print the first five rows of 'Avg_Night_Calls'\n",
      "print(telco['Avg_Night_Calls'])\n",
      "\n",
      "      >Making predictions\n",
      "\n",
      "Logistic Regression: Good baseline offering simplicity and interpretability.\n",
      "\n",
      "Can not capture more complex relationships in the dataset\n",
      "\n",
      "Random forest\n",
      "\n",
      "Support vector machines\n",
      "\n",
      "  >Suport Vector Machine\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "\n",
      "svc.fit(telco[features], telco['target'])\n",
      "\n",
      "1. features must be contineous values\n",
      "2. dataframes or numpy arrays\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "decision_function_shape='ovr', degree=3, gamma='auto',\n",
      "kernel='rbf', max_iter=-1, probability=False,\n",
      "random_state=None, shrinking=True, tol=0.001,\n",
      "verbose=False)\n",
      "\n",
      "prediction = svc.predict(new_customer)\n",
      "print(prediction)\n",
      "\n",
      "\n",
      " >Sample (Logistic Regression)\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "clf = LogisticRegression()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "New customer\n",
      "\n",
      "Account_Length  Vmail_Message  Day_Mins  Eve_Mins  Night_Mins  ...  Eve_Charge  Night_Calls  Night_Charge  Intl_Calls  Intl_Charge\n",
      "0              91             23     232.4     186.0       190.5  ...       15.81          128          8.57           3         3.32\n",
      "\n",
      "\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "  >Sample (Decision Tree Classifier)\n",
      "\n",
      "# Import DecisionTreeClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "\n",
      "clf=DecisionTreeClassifier()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "# Predict the label of new_customer\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "       evaluate model performance\n",
      "1. Compute its accuracy\n",
      "2. Accuracy = Correct Predictions/total number of data points\n",
      "training data may not represent actual data\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(telco['data'],\n",
      "\ttelco['target'],\n",
      "\ttest_size=0.2,\n",
      "\trandom_state=42)\n",
      "\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "svc.fit(X_train, y_train)\n",
      "svc.predict(X_test)\n",
      "\n",
      "svc.score(X_test,y_test)\n",
      "\n",
      "#overfitting means the model has become to sensitive to noise in the training data\n",
      "\n",
      "#underfitting is means not capturing trends in the training data\n",
      "\n",
      "  Sample - Train Test Split\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(X,\n",
      "\ty,\n",
      "\ttest_size=0.3,\n",
      "\trandom_state=42)\n",
      "\n",
      "print(X_train.shape, X_test.shape)\n",
      "\n",
      " >Sample - (Random Forest Classifier)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "# Compute accuracy\n",
      "print(clf.score(X_test, y_test))\n",
      "\n",
      "        >Model Metrics\n",
      "\n",
      "imbalanced classes\n",
      "\n",
      "telco['Churn'].value_counts()\n",
      "\n",
      "up balancing and down balancing\n",
      "\n",
      "confusion matrix\n",
      "\n",
      "\n",
      "          Churn   \t\tNo Churn\n",
      "\n",
      "Churn\tTrue positive\t\tFalse positive\n",
      "\n",
      "No Churn false Negatives\tTrue Negatives\n",
      "\n",
      "\n",
      "Precision = true positives/(true positives+false positives)\n",
      "\n",
      "* high precision means there are not many false positives\n",
      "\n",
      "Recall sensitivity = true positives/(true positives+false negatives)\n",
      "* a high recall means it correctly recalled most churners\n",
      "* minimizes false negatives\n",
      "\n",
      "high precision when the offer is expensive\n",
      "\n",
      "high recall if losing customers is expensive\n",
      "\n",
      "\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "cm=confusion_matrix(y_test,y_pred)\n",
      "\n",
      " >Sample (confusion Matrix)\n",
      "\n",
      "# Import confusion_matrix\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "# Print the confusion matrix\n",
      "print(confusion_matrix(y_test,y_pred)\n",
      "\n",
      ")\n",
      "\n",
      "\n",
      "  Sample (confusion Matrix)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2))\n",
      "\n",
      " >Sample (confusion matrix - RandomForestClassifier)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import confusion_matrix\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "# Print confusion matrix\n",
      "print(confusion_matrix(y_test,y_pred))\n",
      "\n",
      " >Sample (score_precision)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import precision_score\n",
      "from sklearn.metrics import precision_score\n",
      "\n",
      "print(precision_score(y_test,y_pred))\n",
      "\n",
      "  >Sample (Precision and recall)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import precision_score\n",
      "from sklearn.metrics import precision_score, recall_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "# Print the precision\n",
      "print(precision_score(y_test,y_pred))\n",
      "\n",
      "print(recall_score(y_test,y_pred))\n",
      "\n",
      "print(confusion_matrix(y_test,y_pred))\n",
      "\n",
      "<<<<<<Other model metrics\n",
      "\n",
      "receiving operating curve ROC \n",
      "\n",
      "every prediction your classifier makes has an associated probability.\n",
      "\n",
      "> 50% belongs to the positive class\n",
      "\n",
      "* default probability threshold in scikit-learn is 50%\n",
      "\n",
      "measuring the true positive rate against the false positive rate we get the roc curve\n",
      "\n",
      "Area under the curve \n",
      "* a large area would have a well performing model\n",
      "\n",
      "* AUC allows you to compare the performance of different classifiers\n",
      "\n",
      "y_pred_prob= logreg.predict_proba(X_test)[:,1]\n",
      "\n",
      "*0 column- the probablity the first column will be 0\n",
      "*1 column= the probability the second column will be 1\n",
      "\n",
      "from sklearn.metrics import roc_curve\n",
      "\n",
      "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
      "\n",
      "fpr=false positive rate\n",
      "tpr=true positive rate\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.plot(fpr,tpr)\n",
      "plt.xlabel(\"False Positive Rate\")\n",
      "plt.ylabel(\"True Positive Rate\")\n",
      "plt.plot([0,1],[0,1],\"k--\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "auc= roc_auc_score(y_test, y_pred)\n",
      "\n",
      "  Sample (Print probabilities - ROC curve)\n",
      "\n",
      "# Generate the probabilities\n",
      "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
      "print(y_pred_prob)\n",
      "\n",
      "from sklearn.metrics import roc_curve\n",
      "\n",
      "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
      "\n",
      "# Plot the ROC curve\n",
      "plt.plot(fpr,tpr)\n",
      "\n",
      "# Add labels and diagonal line\n",
      "plt.xlabel(\"False Positive Rate\")\n",
      "plt.ylabel(\"True Positive Rate\")\n",
      "plt.plot([0, 1], [0, 1], \"k--\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  Sample ( auc)\n",
      "\n",
      "# Import roc_auc_score\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "# Print the AUC\n",
      "print(roc_auc_score(y_test, y_pred_prob)\n",
      "\n",
      " >Precision - recall curve\n",
      "\n",
      "Another way to evaluate model performance is using a precision-recall curve, which shows the tradeoff between precision and recall for different thresholds.\n",
      "\n",
      "  Sample F1 score\n",
      "\n",
      "\n",
      "f1=2 * (precision * recall) / (precision + recall)\n",
      "\n",
      "F1 score is it incorporates both precision and recall into a single metric\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import f1_score\n",
      "from sklearn.metrics import f1_score\n",
      "\n",
      "# Print the F1 score\n",
      "print(f1_score(y_test, y_pred))\n",
      "\n",
      "\n",
      "#a high F1 score is a sign of a well-performing model\n",
      "\n",
      "    >Tuning your model          \n",
      "\n",
      "hyper parameters of the random forest model\n",
      "1. n_estimators = number of trees\n",
      "2. criterion = quality of split\n",
      "3. max_features= number of features for best split\n",
      "4. max_depth= max depth of tree\n",
      "5. min_sample_splits=minimum samples to spit node\n",
      "6. bootstrap = whether bootstrap samples are used\n",
      "\n",
      "grid search is brute force search\n",
      "a. returns the best model fit\n",
      "\n",
      "cross validation\n",
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "param_grid={'n_estimators':np.arange(10,51)}\n",
      "\n",
      "clf_cv= GridSearchCV(RandomForestClassifier(), param_grid)\n",
      "\n",
      "clf_cv.fit(X,y)\n",
      "\n",
      "print(clf_cv.best_params_)\n",
      "\n",
      "print(clf_cv.best_score_)\n",
      "\n",
      "   >Sample GridSearchCV\n",
      "\n",
      "# Import GridSearchCV\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Create the hyperparameter grid\n",
      "param_grid = {'max_features': ['auto', 'sqrt', 'log2']}\n",
      "\n",
      "# Call GridSearchCV\n",
      "grid_search = GridSearchCV(RandomForestClassifier(), param_grid)\n",
      "\n",
      "# Fit the model\n",
      "grid_search.fit(X, y)\n",
      "\n",
      "print(grid_search.best_params_)\n",
      "print(grid_search.best_score_)\n",
      "\n",
      "  Sample (complex hyper parameter)\n",
      "\n",
      "\n",
      "# Import GridSearchCV\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Create the hyperparameter grid\n",
      "param_grid = {\"max_depth\": [3, None],\n",
      "              \"max_features\": [1, 3, 10],\n",
      "              \"bootstrap\": [True, False],\n",
      "              \"criterion\": [\"gini\", \"entropy\"]}\n",
      "\n",
      "# Call GridSearchCV\n",
      "grid_search = GridSearchCV(RandomForestClassifier(),param_grid)\n",
      "# Fit the model\n",
      "grid_search.fit(X, y)\n",
      "\n",
      "print(grid_search.best_params_)\n",
      "print(grid_search.best_score_)\n",
      "\n",
      "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': None, 'max_features': 10}\n",
      "0.9534953495349535\n",
      "\n",
      "   Sample (Random Grid Search)\n",
      "\n",
      "we could randomly jump around the grid and try different combinations.\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "# Create the hyperparameter grid\n",
      "param_dist = {\"max_depth\": [3, None],\n",
      "              \"max_features\": randint(1, 11),\n",
      "              \"bootstrap\": [True, False],\n",
      "              \"criterion\": [\"gini\", \"entropy\"]}\n",
      "\n",
      "# Call RandomizedSearchCV\n",
      "random_search = RandomizedSearchCV(RandomForestClassifier(),param_dist)\n",
      "# Fit the model)\n",
      "random_search.fit(X, y)\n",
      "\n",
      "print(random_search.best_params_)\n",
      "print(random_search.best_score_)\n",
      "\n",
      "\n",
      "         >Feature importances\n",
      "\n",
      "* scoring represents how much each feature contributes to a prediction.\n",
      "* visualization is an effective way to communicate results to stakeholders\n",
      "1. which features are important drivers of churn\n",
      "2. which features can be removed from the model\n",
      "\n",
      "interpretability might be reasons you get buyin from stakeholders\n",
      "\n",
      "\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "clf.feature_importances_\n",
      "\n",
      "  Sample (calculating feature importances)\n",
      "\n",
      "# Calculate feature importances\n",
      "importances = clf.feature_importances_\n",
      "\n",
      "# Create plot\n",
      "plt.barh(range(X.shape[1]), importances)\n",
      "plt.show()\n",
      "\n",
      "In order to make the plot more readable, we need to do achieve two goals:\n",
      "\n",
      "Re-order the bars in ascending order.\n",
      "Add labels to the plot that correspond to the feature names.\n",
      "\n",
      " >Sort features by importance\n",
      "\n",
      "# Sort importances\n",
      "sorted_index = np.argsort(importances)\n",
      "\n",
      "# Create labels\n",
      "labels = X.columns[sorted_index]\n",
      "\n",
      "# Clear current plot\n",
      "plt.clf()\n",
      "\n",
      "# Create plot\n",
      "plt.barh(range(X.shape[1]), importances[sorted_index], tick_label=labels)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    Adding New Features\n",
      "\n",
      "additional data sources: customer service, web logs, email campaigns, network, transactions, and signal strength\n",
      "\n",
      "1. can improve model performances\n",
      "2. avoid underfitting\n",
      "\n",
      "churn features:\n",
      "Region Code,\n",
      "Total Charges,\n",
      "Total Minutes,\n",
      "Minutes per Call\n",
      "Cost per Call\n",
      "Total Calls\n",
      "\n",
      "\n",
      "Compare both ROC curves\n",
      "\n",
      "Discuss with business the benefits and costs of incorporating the additional features\n",
      "1. improved return on investment\n",
      "2. decreased cost\n",
      "3. increased performance\n",
      "\n",
      "Benefits must exceed costs\n",
      "\n",
      "\n",
      "  Sample Adding additional features\n",
      "\n",
      "# Import necessary modules\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the data\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "# Print the accuracy\n",
      "print(clf.score(X_test, y_test))\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Print the F1 score\n",
      "print(f1_score(y_test,y_pred))\n",
      "\n",
      "\n",
      "\n",
      " >Exploratory Data Analysis in python\n",
      " >Designing Machine Learning Workflows in python\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\dataframe group and categoricals.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\dataframe group and categoricals.txt\n",
      "sales=pd.DataFrame({\n",
      "    'weekday':['Sun','Sun','Mon','Mon'],\n",
      "    'city':['Austin','Dallas','Austin','Dallas'],\n",
      "    'bread':[139,237,326,456],\n",
      "    'butter':[20,45,70,98]\n",
      "    \n",
      "})\n",
      "print(sales)\n",
      "print(sales.groupby('weekday').count())\n",
      "\n",
      "count is an aggregation reduction function\n",
      "\n",
      "others are:\n",
      "1. mean()\n",
      "2. std()\n",
      "3. sum()\n",
      "4. first(), last()\n",
      "5. min(), max()\n",
      "\n",
      "\n",
      "print(sales)\n",
      "print(sales.groupby('weekday').count())\n",
      "print(sales.groupby('weekday')['bread'].sum())\n",
      "sales.groupby('weekday')['bread'].sum().plot.pie()\n",
      "print(sales.groupby('weekday')['bread','butter'].sum())\n",
      "print(sales.groupby(['city','weekday']).mean())\n",
      "\n",
      " weekday    city  bread  butter\n",
      "0     Sun  Austin    139      20\n",
      "1     Sun  Dallas    237      45\n",
      "2     Mon  Austin    326      70\n",
      "3     Mon  Dallas    456      98\n",
      "         city  bread  butter\n",
      "weekday                     \n",
      "Mon         2      2       2\n",
      "Sun         2      2       2\n",
      "weekday\n",
      "Mon    782\n",
      "Sun    376\n",
      "Name: bread, dtype: int64\n",
      "         bread  butter\n",
      "weekday               \n",
      "Mon        782     168\n",
      "Sun        376      65\n",
      "                bread  butter\n",
      "city   weekday               \n",
      "Austin Mon        326      70\n",
      "       Sun        139      20\n",
      "Dallas Mon        456      98\n",
      "       Sun        237      45\n",
      "\n",
      "\n",
      "       >unique\n",
      "\n",
      "sales['weekday'].unique()\n",
      "\n",
      "output:\n",
      "[Mon,Sun]\n",
      "   category\n",
      "\n",
      "sales['weekday']=sales['weekday'].astype('category')\n",
      "\n",
      "* uses less memory and it runs faster\n",
      "\n",
      "   > sample groupby\n",
      "\n",
      "# Group titanic by 'pclass'\n",
      "by_class = titanic.groupby('pclass')\n",
      "\n",
      "# Aggregate 'survived' column of by_class by count\n",
      "count_by_class =by_class['survived'].count()\n",
      "\n",
      "# Print count_by_class\n",
      "print(count_by_class)\n",
      "\n",
      "# Group titanic by 'embarked' and 'pclass'\n",
      "by_mult = titanic.groupby(['embarked','pclass'])\n",
      "\n",
      "# Aggregate 'survived' column of by_mult by count\n",
      "count_mult = by_mult['survived'].count()\n",
      "\n",
      "# Print count_mult\n",
      "print(count_mult)\n",
      "\n",
      "pclass  name  sex  age  sibsp  ...  cabin  embarked  boat  body  home.dest\n",
      "survived                                 ...                                        \n",
      "pclass  name  sex  age  sibsp  ...  cabin  embarked  boat  body  home.dest\n",
      "survived                                 ...                                        \n",
      "pclass\n",
      "1    323\n",
      "2    277\n",
      "3    709\n",
      "Name: survived, dtype: int64\n",
      "embarked  pclass\n",
      "C         1         141\n",
      "          2          28\n",
      "          3         101\n",
      "Q         1           3\n",
      "          2           7\n",
      "          3         113\n",
      "S         1         177\n",
      "          2         242\n",
      "          3         495\n",
      "Name: survived, dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "In [1]:\n",
      "\n",
      "\n",
      "    groupby\n",
      "\n",
      "# Read life_fname into a DataFrame: life\n",
      "life = pd.read_csv(life_fname, index_col='Country')\n",
      "\n",
      "# Read regions_fname into a DataFrame: regions\n",
      "regions = pd.read_csv(regions_fname, index_col='Country')\n",
      "\n",
      "# Group life by regions['region']: life_by_region\n",
      "life_by_region = life.groupby(regions['region'])\n",
      "\n",
      "# Print the mean over the '2010' column of life_by_region\n",
      "print(life_by_region['2010'].mean())\n",
      "\n",
      "region\n",
      "America                       74.037350\n",
      "East Asia & Pacific           73.405750\n",
      "Europe & Central Asia         75.656387\n",
      "Middle East & North Africa    72.805333\n",
      "South Asia                    68.189750\n",
      "Sub-Saharan Africa            57.575080\n",
      "\n",
      " \n",
      "       >Group by and aggregation\n",
      "\n",
      "\n",
      "print(sales.groupby('city')[['bread','butter']].max())\n",
      "\n",
      "bread  butter\n",
      "city                 \n",
      "Austin    326      70\n",
      "Dallas    456      98\n",
      "\n",
      "print(sales.groupby('city')[['bread','butter']].agg(['max','sum','mean','count'])\n",
      "\n",
      "\n",
      "def data_range(series):\n",
      "\treturn series.max() - series.min()\n",
      "\n",
      "\n",
      "print(sales.groupby('weekday')[['bread','butter']].agg([data_range])\n",
      "\n",
      "\n",
      "#for each weekday find the sum for bread and the data_range for butter\n",
      "\n",
      "print(\"\\n\\naggregate by column \", sales.groupby('weekday')[['bread','butter']].agg({'bread':'sum','butter':data_range}))\n",
      "\n",
      "\n",
      "\n",
      "    print the median fare for each class\n",
      "\n",
      "# Group titanic by 'pclass': by_class\n",
      "by_class = titanic.groupby('pclass')\n",
      "\n",
      "# Select 'age' and 'fare'\n",
      "by_class_sub = by_class[['age','fare']]\n",
      "\n",
      "# Aggregate by_class_sub by 'max' and 'median': aggregated\n",
      "aggregated = by_class_sub.agg(['max','median'])\n",
      "print(aggregated)\n",
      "\n",
      "# Print the maximum age in each class\n",
      "print(aggregated.loc[:, ('age','max')])\n",
      "\n",
      "# Print the median fare in each class\n",
      "print(aggregated.loc[:,('fare','median')])\n",
      "\n",
      "         age             fare         \n",
      "         max median       max   median\n",
      "pclass                                \n",
      "1       80.0   39.0  512.3292  60.0000\n",
      "2       70.0   29.0   73.5000  15.0458\n",
      "3       74.0   24.0   69.5500   8.0500\n",
      "\n",
      "pclass (max age per pclass)  \n",
      "1    80.0\n",
      "2    70.0\n",
      "3    74.0\n",
      "\n",
      "Name: (age, max), dtype: float64\n",
      "pclass  (median fare per pclass)\n",
      "\n",
      "1    60.0000\n",
      "2    15.0458\n",
      "3     8.0500\n",
      "Name: (fare, median), dtype: float64\n",
      "\n",
      "\n",
      "     >sample aggregation functions using a dictionary for the column mapping to function\n",
      "\n",
      "# Read the CSV file into a DataFrame and sort the index: gapminder\n",
      "gapminder = pd.read_csv('gapminder.csv',index_col=['Year','region','Country']).sort_index()\n",
      "\n",
      "# Group gapminder by 'Year' and 'region': by_year_region\n",
      "by_year_region = gapminder.groupby(level=['Year','region'])\n",
      "\n",
      "# Define the function to compute spread: spread\n",
      "def spread(series):\n",
      "    return series.max() - series.min()\n",
      "\n",
      "# Create the dictionary: aggregator\n",
      "aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}\n",
      "\n",
      "# Aggregate by_year_region using the dictionary: aggregated\n",
      "aggregated = by_year_region.agg(aggregator)\n",
      "\n",
      "# Print the last 6 entries of aggregated \n",
      "print(aggregated.tail(6))\n",
      "\n",
      "\n",
      "population  child_mortality       gdp\n",
      "Year region                                                             \n",
      "2013 America                     9.629087e+08        17.745833   49634.0\n",
      "     East Asia & Pacific         2.244209e+09        22.285714  134744.0\n",
      "     Europe & Central Asia       8.968788e+08         9.831875   86418.0\n",
      "     Middle East & North Africa  4.030504e+08        20.221500  128676.0\n",
      "     South Asia                  1.701241e+09        46.287500   11469.0\n",
      "     Sub-Saharan Africa          9.205996e+08        76.944490   32035.0\n",
      "\n",
      "\n",
      "      sample group sales units by day\n",
      "# Read file: sales\n",
      "'Company', 'Product', 'Units'\n",
      "\n",
      "\n",
      "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
      "\n",
      "#print(sales.index.strftime('%a'))\n",
      "# Create a groupby object: by_day\n",
      "by_day = sales.groupby(sales.index.strftime('%a'))\n",
      "print(*by_day)\n",
      "# Create sum: units_sum\n",
      "units_sum = by_day['Units'].sum()\n",
      "\n",
      "# Print units_sum\n",
      "print(units_sum)\n",
      "\n",
      "\n",
      "         Groupby and Transformation\n",
      "\n",
      "def zscore(series):\n",
      "\treturn (series - series.mean())/series.std()\n",
      "\n",
      "auto.groupby('yr')['mpg'].transform(zscore).head()\n",
      "\n",
      "\n",
      "def zscore_with_year_and_name(group):\n",
      "\tdf=pd.DataFrame(\n",
      "\t\t{'mpg': zscore(group['mpg']),\n",
      "\t\t'year':group['yr'],\n",
      "\t\t'name':group['name']})\n",
      "\treturn df\n",
      "\n",
      "auto.groupby('yr').apply(zscore_with_year_and_name).head()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Company   Product  Units\n",
      "Date                                           \n",
      "2015-02-02 08:30:00      Hooli  Software      3\n",
      "2015-02-02 21:00:00  Mediacore  Hardware      9\n",
      "2015-02-09 09:00:00  Streeplex   Service     19\n",
      "2015-02-09 13:00:00  Mediacore  Software      7\n",
      "2015-02-16 12:00:00      Hooli  Software     10) ('Sat',                              Company   Product  Units\n",
      "Date                                                 \n",
      "2015-02-07 23:00:00  Acme Coporation  Hardware      1\n",
      "2015-02-21 05:00:00        Mediacore  Software      3\n",
      "2015-02-21 20:30:00            Hooli  Hardware      3) ('Thu',                              Company   Product  Units\n",
      "Date                                                 \n",
      "2015-02-05 02:00:00  Acme Coporation  Software     19\n",
      "2015-02-05 22:00:00            Hooli   Service     10\n",
      "2015-02-19 11:00:00        Mediacore  Hardware     16\n",
      "2015-02-19 16:00:00        Mediacore   Service     10\n",
      "2015-02-26 09:00:00        Streeplex   Service      4) ('Tue',                      Company   Product  Units\n",
      "Date                                         \n",
      "2015-02-03 14:00:00  Initech  Software     13) ('Wed',                              Company   Product  Units\n",
      "Date                                                 \n",
      "2015-02-04 15:30:00        Streeplex  Software     13\n",
      "2015-02-04 22:00:00  Acme Coporation  Hardware     14\n",
      "2015-02-11 20:00:00          Initech  Software      7\n",
      "2015-02-11 23:00:00            Hooli  Software      4\n",
      "2015-02-25 00:30:00          Initech   Service     10)\n",
      "Mon    48\n",
      "Sat     7\n",
      "Thu    59\n",
      "Tue    13\n",
      "Wed    48\n",
      "Name: Units, dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "           >Detecting outliers with zscore\n",
      "\n",
      "# Import zscore\n",
      "from scipy.stats import zscore\n",
      "\n",
      "# Group gapminder_2010: standardized\n",
      "standardized = gapminder_2010.groupby('region')['life','fertility'].transform(zscore)\n",
      "\n",
      "# Construct a Boolean Series to identify outliers: outliers\n",
      "outliers = (standardized['life'] < -3) | (standardized['fertility'] > 3)\n",
      "\n",
      "# Filter gapminder_2010 by the outliers: gm_outliers\n",
      "gm_outliers = gapminder_2010.loc[outliers]\n",
      "\n",
      "# Print gm_outliers\n",
      "print(gm_outliers)\n",
      "\n",
      "\n",
      "                 fertility    life  population  child_mortality     gdp                 region\n",
      "Country                                                                                   \n",
      "Guatemala        3.974  71.100  14388929.0             34.5  6849.0                America\n",
      "Haiti            3.350  45.000   9993247.0            208.8  1518.0                America\n",
      "Tajikistan       3.780  66.830   6878637.0             52.6  2110.0  Europe & Central Asia\n",
      "Timor-Leste      6.237  65.952   1124355.0             63.8  1777.0    East Asia & Pacific\n",
      "\n",
      "Using z-scores like this is a great way to identify outliers in your data.\n",
      "\n",
      "          sample impute using the median for age   transform\n",
      "\n",
      "# Create a groupby object: by_sex_class\n",
      "by_sex_class = titanic.groupby(['sex','pclass'])\n",
      "\n",
      "# Write a function that imputes median\n",
      "def impute_median(series):\n",
      "    return series.fillna(series.median())\n",
      "\n",
      "# Impute age and assign to titanic['age']\n",
      "titanic.age = by_sex_class['age'].transform(impute_median)\n",
      "\n",
      "# Print the output of titanic.tail(10)\n",
      "print(titanic.tail(10))\n",
      "\n",
      "      pclass  survived                                     name     sex   age  ...  cabin  embarked boat   body home.dest\n",
      "1299       3         0                      Yasbeck, Mr. Antoni    male  27.0  ...    NaN         C    C    NaN       NaN\n",
      "1300       3         1  Yasbeck, Mrs. Antoni (Selini Alexander)  female  15.0  ...    NaN         C  NaN    NaN       NaN\n",
      "1301       3         0                     Youseff, Mr. Gerious    male  45.5  ...    NaN         C  NaN  312.0       NaN\n",
      "1302       3         0                        Yousif, Mr. Wazli    male  25.0  ...    NaN         C  NaN    NaN       NaN\n",
      "1303       3         0                    Yousseff, Mr. Gerious    male  25.0  ...    NaN         C  NaN    NaN       NaN\n",
      "1304       3         0                     Zabour, Miss. Hileni  female  14.5  ...    NaN         C  NaN  328.0       NaN\n",
      "1305       3         0                    Zabour, Miss. Thamine  female  22.0  ...    NaN         C  NaN    NaN       NaN\n",
      "1306       3         0                Zakarian, Mr. Mapriededer    male  26.5  ...    NaN         C  NaN  304.0       NaN\n",
      "1307       3         0                      Zakarian, Mr. Ortin    male  27.0  ...    NaN         C  NaN    NaN       NaN\n",
      "1308       3         0                       Zimmerman, Mr. Leo    male  29.0  ...    NaN         S  NaN    NaN       NaN\n",
      "\n",
      "\n",
      "def disparity(gr):\n",
      "    # Compute the spread of gr['gdp']: s\n",
      "    s = gr['gdp'].max() - gr['gdp'].min()\n",
      "    # Compute the z-score of gr['gdp'] as (gr['gdp']-gr['gdp'].mean())/gr['gdp'].std(): z\n",
      "    z = (gr['gdp'] - gr['gdp'].mean())/gr['gdp'].std()\n",
      "    # Return a DataFrame with the inputs {'z(gdp)':z, 'regional spread(gdp)':s}\n",
      "    return pd.DataFrame({'z(gdp)':z , 'regional spread(gdp)':s})\n",
      "\n",
      "\n",
      "# Group gapminder_2010 by 'region': regional\n",
      "regional = gapminder_2010.groupby('region')\n",
      "\n",
      "# Apply the disparity function on regional: reg_disp\n",
      "reg_disp = regional.apply(disparity)\n",
      "\n",
      "# Print the disparity of 'United States', 'United Kingdom', and 'China'\n",
      "print(reg_disp.loc[['United States','United Kingdom','China']])\n",
      "\n",
      "\n",
      " z(gdp)  regional spread(gdp)\n",
      "Country                                       \n",
      "United States   3.013374               47855.0\n",
      "United Kingdom  0.572873               89037.0\n",
      "China          -0.432756               96993.0\n",
      "\n",
      "\n",
      "            Groupby and filtering\n",
      "\n",
      "auto.groupby('yr')['mpg'].mean()\n",
      "\n",
      "\n",
      "splitting = auto.groupby('yr')\n",
      "\n",
      "print(splitting.groups.keys())\n",
      "\n",
      "the output are the years\n",
      "\n",
      "for group_name, group in splitting:\n",
      "\tavg = group['mpg'].mean()\n",
      "\tprint(group_name, avg)\n",
      "\n",
      "\n",
      "#filter only groups for chevrolet\n",
      "\n",
      "for group_name, group in splitting:\n",
      "\tavg = group.loc[group[name].str.contains('chevrolet'),'mpg'].mean()\n",
      "\tprint(group_name, avg)\n",
      "\n",
      "#create a dictionary comprehension\n",
      "\n",
      "chevy_means = {year: group.loc[group['name'].str.contains('chevrolet'),'mpg'].mean()\n",
      "\tfor year, group in splitting}\n",
      "\n",
      "\n",
      "chevy= auto['name'].str.contains('chevrolet')\n",
      "\n",
      "auto.groupby(['yr',chevy])['mpg'].mean()\n",
      "\n",
      "compares chevrolet mpg against all of its competitors\n",
      "\n",
      "\n",
      "   > sample gender survival rates\n",
      "\n",
      "# Create a groupby object using titanic over the 'sex' column: by_sex\n",
      "by_sex = titanic.groupby('sex')\n",
      "\n",
      "# Call by_sex.apply with the function c_deck_survival\n",
      "c_surv_by_sex = by_sex.apply(c_deck_survival)\n",
      "\n",
      "# Print the survival rates\n",
      "print(c_surv_by_sex)\n",
      "\n",
      "sex\n",
      "female    0.913043\n",
      "male      0.312500\n",
      "\n",
      "    filtering a groupby\n",
      "\n",
      "# Read the CSV file into a DataFrame: sales\n",
      "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
      "\n",
      "# Group sales by 'Company': by_company\n",
      "by_company = sales.groupby('Company')\n",
      "\n",
      "# Compute the sum of the 'Units' of by_company: by_com_sum\n",
      "by_com_sum = by_company['Units'].sum()\n",
      "print(by_com_sum)\n",
      "\n",
      "# Filter 'Units' where the sum is > 35: by_com_filt\n",
      "by_com_filt = by_company.filter(lambda g:g['Units'].sum()>35)\n",
      "print(by_com_filt)\n",
      "\n",
      "Company\n",
      "Acme Coporation    34\n",
      "Hooli              30\n",
      "Initech            30\n",
      "Mediacore          45\n",
      "Streeplex          36\n",
      "\n",
      "\n",
      "Company   Product  Units\n",
      "Date                                           \n",
      "2015-02-02 21:00:00  Mediacore  Hardware      9\n",
      "2015-02-04 15:30:00  Streeplex  Software     13\n",
      "2015-02-09 09:00:00  Streeplex   Service     19\n",
      "2015-02-09 13:00:00  Mediacore  Software      7\n",
      "2015-02-19 11:00:00  Mediacore  Hardware     16\n",
      "2015-02-19 16:00:00  Mediacore   Service     10\n",
      "2015-02-21 05:00:00  Mediacore  Software      3\n",
      "2015-02-26 09:00:00  Streeplex   Service      4\n",
      "\n",
      "\n",
      "      group by using a filter\n",
      "\n",
      "# Create the Boolean Series: under10\n",
      "under10 = (titanic['age']<10).map({True:'under 10',False: 'over 10'})\n",
      "\n",
      "# Group by under10 and compute the survival rate\n",
      "survived_mean_1 = titanic.groupby(under10)['survived'].mean()\n",
      "print(survived_mean_1)\n",
      "\n",
      "# Group by under10 and pclass and compute the survival rate\n",
      "survived_mean_2 = titanic.groupby([under10,'pclass'])['survived'].mean()\n",
      "print(survived_mean_2)\n",
      "\n",
      "age\n",
      "over 10     0.366748\n",
      "under 10    0.609756\n",
      "Name: survived, dtype: float64\n",
      "age       pclass\n",
      "over 10   1         0.617555\n",
      "          2         0.380392\n",
      "          3         0.238897\n",
      "under 10  1         0.750000\n",
      "          2         1.000000\n",
      "          3         0.446429\n",
      "\n",
      "the pclass had a strong influence on who survived\n",
      "\n",
      "        >olympic medals\n",
      "\n",
      "\n",
      "# Select the 'NOC' column of medals: country_names\n",
      "country_names = medals['NOC']\n",
      "# Count the number of medals won by each country: medal_counts\n",
      "medal_counts = country_names.value_counts()\n",
      "\n",
      "# Print top 15 countries ranked by medals\n",
      "print(medal_counts.head(15))\n",
      "\n",
      "\n",
      "gender and event gender\n",
      "\n",
      "# Select columns: ev_gen\n",
      "ev_gen = medals[['Event_gender','Gender']]\n",
      "\n",
      "# Drop duplicate pairs: ev_gen_uniques\n",
      "ev_gen_uniques = ev_gen.drop_duplicates()\n",
      "\n",
      "# Print ev_gen_uniques\n",
      "print(ev_gen_uniques)\n",
      "\n",
      "Event_gender Gender\n",
      "0                M    Men\n",
      "348              X    Men\n",
      "416              W  Women\n",
      "639              X  Women\n",
      "23675            W    Men\n",
      "\n",
      "\n",
      "       >filtering\n",
      "\n",
      "## Create the Boolean Series: sus\n",
      "sus = (medals.Event_gender == 'W') & (medals.Gender == 'Men')\n",
      "\n",
      "# Create a DataFrame with the suspicious row: suspect\n",
      "suspect = medals[sus]\n",
      "\n",
      "# Print suspect\n",
      "print(suspect)\n",
      "\n",
      "City  Edition      Sport Discipline            Athlete  NOC Gender     Event Event_gender   Medal\n",
      "23675  Sydney     2000  Athletics  Athletics  CHEPCHUMBA, Joyce  KEN    Men  marathon            W  Bronze\n",
      "\n",
      "         >idxmax() and idxmin()\n",
      "\n",
      "idxmax() row or column label where maximum value is located\n",
      "\n",
      "idxmin() row or column label where minimum value is located\n",
      "\n",
      "weather.T.idxmax(axis='columns')\n",
      "weather.T.idxmin(axis='columns')\n",
      "\n",
      "\n",
      "  > filter the # Create a Boolean Series that is True when 'Edition' is between 1952 and 1988: during_cold_war\n",
      "during_cold_war = (medals['Edition'] >= 1952) & (medals['Edition'] <= 1988)\n",
      "\n",
      "# Extract rows for which 'NOC' is either 'USA' or 'URS': is_usa_urs\n",
      "is_usa_urs = medals.NOC.isin(['USA','URS'])\n",
      "\n",
      "# Use during_cold_war and is_usa_urs to create the DataFrame: cold_war_medals\n",
      "cold_war_medals = medals.loc[during_cold_war & is_usa_urs]\n",
      "\n",
      "# Group cold_war_medals by 'NOC'\n",
      "country_grouped = cold_war_medals.groupby('NOC')\n",
      "\n",
      "# Create Nsports\n",
      "Nsports = country_grouped['Sport'].nunique().sort_values(ascending=False)\n",
      "\n",
      "# Print Nsports\n",
      "print(Nsports)\n",
      "\n",
      "NOC\n",
      "    URS    21\n",
      "    USA    20\n",
      "\n",
      "    sample  > medals during the cold war\n",
      "\n",
      "# Create the pivot table: medals_won_by_country\n",
      "medals_won_by_country = medals.pivot_table(index=['Edition'],columns=['NOC'],values=['Athlete'],aggfunc=['count'])\n",
      "\n",
      "# Slice medals_won_by_country: cold_war_usa_urs_medals\n",
      "cold_war_usa_urs_medals = medals_won_by_country.loc[1952:1988, ['USA','URS']]\n",
      "\n",
      "# Create most_medals \n",
      "most_medals = cold_war_usa_urs_medals.idxmax(axis='columns')\n",
      "\n",
      "# Print most_medals.value_counts()\n",
      "print(most_medals.value_counts())\n",
      "\n",
      "Edition\n",
      "URS    8\n",
      "USA    2\n",
      "\n",
      "      unstack is a reshaping of the multi-index\n",
      "grouped=df.groupby(['Year'])['Name'].count().plot()\n",
      "\n",
      "france_medals = france_grps['Athlete'].count().unstack()\n",
      "\n",
      "   sample usa medals\n",
      "\n",
      "medals.Medal = pd.Categorical(values=medals.Medal,\n",
      "categories=['Bronze','Silver','Gold'])\n",
      "\n",
      "# Create the DataFrame: usa\n",
      "usa = medals[medals['NOC']=='USA']\n",
      "\n",
      "# Group usa by ['Edition', 'Medal'] and aggregate over 'Athlete'\n",
      "usa_medals_by_year = usa.groupby(['Edition','Medal'])['Athlete'].count()\n",
      "\n",
      "# Reshape usa_medals_by_year by unstacking\n",
      "usa_medals_by_year = usa_medals_by_year.unstack(level='Medal')\n",
      "\n",
      "# Plot the DataFrame usa_medals_by_year\n",
      "usa_medals_by_year.plot()\n",
      "plt.show()\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\dataframe pivoting.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\dataframe pivoting.txt\n",
      "pivoting dataframes\n",
      "\n",
      "id \ttreatment\tgender\tresponse\n",
      "1\tA\t\tF\t5\n",
      "2\tA\t\tM\t3\n",
      "3\tB\t\tF\t8\n",
      "4\tB\t\tM\t9\n",
      "\n",
      "\n",
      "trials.pivot(index='treatment',\n",
      "\tcolumns='gender',\n",
      "\tvalues='response')\n",
      "\n",
      "gender\tF\tM\n",
      "treatment\n",
      "A\t5\t3\n",
      "B\t8\t9\n",
      "\n",
      "\n",
      "\n",
      "trials.pivot(index='treatment', columns='gender')\n",
      "\n",
      "  >  weekdays by city sum visitors\n",
      "\n",
      "print(users)\n",
      "# Pivot the users DataFrame: visitors_pivot\n",
      "visitors_pivot = users.pivot(index='weekday', columns='city',values='visitors')\n",
      "\n",
      "# Print the pivoted DataFrame\n",
      "print(visitors_pivot)\n",
      "\n",
      "\n",
      "city     Austin  Dallas\n",
      "weekday                \n",
      "Mon         326     456\n",
      "Sun         139     237\n",
      "\n",
      "\n",
      "      >\n",
      "\n",
      "# Pivot users with signups indexed by weekday and city: signups_pivot\n",
      "signups_pivot = users.pivot(index='weekday',columns='city',values='signups')\n",
      "\n",
      "# Print signups_pivot\n",
      "print(signups_pivot)\n",
      "\n",
      "# Pivot users pivoted by both signups and visitors: pivot\n",
      "pivot = users.pivot(index='weekday',columns='city')\n",
      "\n",
      "# Print the pivoted DataFrame\n",
      "print(pivot)\n",
      "\n",
      "\n",
      "city     Austin  Dallas\n",
      "weekday                \n",
      "Mon           3       5\n",
      "Sun           7      12\n",
      "        visitors        signups       \n",
      "city      Austin Dallas  Austin Dallas\n",
      "weekday                               \n",
      "Mon          326    456       3      5\n",
      "Sun          139    237       7     12\n",
      "\n",
      "\n",
      "         Stacking and unstacking dataframes\n",
      "\n",
      "trials.unstack(level='gender')\n",
      "trials.unstack(level=1)  #are equivalent\n",
      "\n",
      "work with hierarchial columns\n",
      "\n",
      "stackd=trials.stack(level='gender')\n",
      "\n",
      "to swap levels\n",
      "\n",
      "swapped =stacked.swaplevel(0,1)\n",
      "\n",
      "sorted_trials=swapped.sort_index()\n",
      "\n",
      "\n",
      "  > unstack and stack\n",
      "\n",
      "# Unstack users by 'weekday': byweekday\n",
      "byweekday = users.unstack(level='weekday')\n",
      "\n",
      "# Print the byweekday DataFrame\n",
      "print(byweekday)\n",
      "\n",
      "# Stack byweekday by 'weekday' and print it\n",
      "print(byweekday.stack(level='weekday'))\n",
      "\n",
      "visitors      signups    \n",
      "weekday      Mon  Sun     Mon Sun\n",
      "city                             \n",
      "Austin       326  139       3   7\n",
      "Dallas       456  237       5  12\n",
      "                visitors  signups\n",
      "city   weekday                   \n",
      "Austin Mon           326        3\n",
      "       Sun           139        7\n",
      "Dallas Mon           456        5\n",
      "       Sun           237       12\n",
      "In [1]:\n",
      "\n",
      "\n",
      "  >sample\n",
      "\n",
      "# Stack 'city' back into the index of bycity: newusers\n",
      "newusers = bycity.stack(level='city')\n",
      "\n",
      "# Swap the levels of the index of newusers: newusers\n",
      "newusers = newusers.swaplevel(0,1)\n",
      "\n",
      "# Print newusers and verify that the index is not sorted\n",
      "print(newusers)\n",
      "\n",
      "# Sort the index of newusers: newusers\n",
      "newusers = newusers.sort_index()\n",
      "\n",
      "# Print newusers and verify that the index is now sorted\n",
      "print(newusers)\n",
      "\n",
      "# Verify that the new DataFrame is equal to the original\n",
      "print(newusers.equals(users))\n",
      "\n",
      "\n",
      "visitors  signups\n",
      "city   weekday                   \n",
      "Austin Mon           326        3\n",
      "Dallas Mon           456        5\n",
      "Austin Sun           139        7\n",
      "Dallas Sun           237       12\n",
      "                visitors  signups\n",
      "city   weekday                   \n",
      "Austin Mon           326        3\n",
      "       Sun           139        7\n",
      "Dallas Mon           456        5\n",
      "       Sun           237       12\n",
      "True\n",
      "\n",
      "\n",
      "         melt dataframes\n",
      "\n",
      "melt is used to move from pivoted data summary form columnar\n",
      "\n",
      "\n",
      "two columns result from the columns\n",
      "\n",
      "variable and value\n",
      "\n",
      "pd.melt(new_trials, id_vars=['treatment'],\n",
      "\tvalue_vars=['F','M']\n",
      "\tvar_name='gender'\n",
      "\tvalue_name='response'\n",
      "\t)\n",
      "\n",
      "  >sample melt id_vars and value_name\n",
      "\n",
      "# Reset the index: visitors_by_city_weekday\n",
      "visitors_by_city_weekday = visitors_by_city_weekday.reset_index()\n",
      "\n",
      "# Print visitors_by_city_weekday\n",
      "print(visitors_by_city_weekday)\n",
      "\n",
      "# Melt visitors_by_city_weekday: visitors\n",
      "visitors = pd.melt(visitors_by_city_weekday, id_vars=['weekday'], value_name='visitors')\n",
      "\n",
      "# Print visitors\n",
      "print(visitors)\n",
      "\n",
      "city  level_0  index weekday  Austin  Dallas\n",
      "0           0      0     Mon     326     456\n",
      "1           1      1     Sun     139     237\n",
      "  weekday     city  visitors\n",
      "0     Mon  level_0         0\n",
      "1     Sun  level_0         1\n",
      "2     Mon    index         0\n",
      "3     Sun    index         1\n",
      "4     Mon   Austin       326\n",
      "5     Sun   Austin       139\n",
      "6     Mon   Dallas       456\n",
      "7     Sun   Dallas       237\n",
      "In [1]:\n",
      ";\n",
      "\n",
      "   >sample id_vars muliple index\n",
      "\n",
      "# Melt users: skinny\n",
      "skinny = pd.melt(users,id_vars=['weekday','city'])\n",
      "\n",
      "# Print skinny\n",
      "print(skinny)\n",
      "\n",
      "\n",
      "    sample set_index\n",
      "\n",
      "# Set the new index: users_idx\n",
      "users_idx = users.set_index(['city','weekday'])\n",
      "\n",
      "# Print the users_idx DataFrame\n",
      "print(users_idx)\n",
      "\n",
      "# Obtain the key-value pairs: kv_pairs\n",
      "kv_pairs = pd.melt(users_idx,col_level=0)\n",
      "\n",
      "# Print the key-value pairs\n",
      "print(kv_pairs)\n",
      "\n",
      "\n",
      "variable  value\n",
      "0  visitors    139\n",
      "1  visitors    237\n",
      "2  visitors    326\n",
      "3  visitors    456\n",
      "4   signups      7\n",
      "5   signups     12\n",
      "6   signups      3\n",
      "7   signups      5\n",
      "\n",
      "\n",
      "        >Pivot Tables\n",
      "\n",
      "id\n",
      "treatment\n",
      "gender\n",
      "response\n",
      "\n",
      "more_trials.pivot(index='treatment',\n",
      "\tcolumns='gender',\n",
      "\tvalues='response')\n",
      "\n",
      "\n",
      "pivot requires unique index pairing\n",
      "\n",
      "pivot table summarizes by value not index\n",
      "\n",
      "more_trials.pivot_table(index='treatment',\n",
      "\tcolumns='gender',\n",
      "\tvalues='response',\n",
      "\taggfunc='count'\n",
      ")\n",
      "\n",
      "   >sample\n",
      "\n",
      "# Create the DataFrame with the appropriate pivot table: by_city_day\n",
      "by_city_day = pd.pivot_table(users,index='weekday',columns='city')\n",
      "\n",
      "# Print by_city_day\n",
      "print(by_city_day)\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "# Use a pivot table to display the count of each column: count_by_weekday1\n",
      "count_by_weekday1 = pd.pivot_table(users,index='weekday',aggfunc='count')\n",
      "\n",
      "# Print count_by_weekday\n",
      "print(count_by_weekday1)\n",
      "\n",
      "# Replace 'aggfunc='count'' with 'aggfunc=len': count_by_weekday2\n",
      "count_by_weekday2 = pd.pivot_table(users,index='weekday',aggfunc=len)\n",
      "\n",
      "# Verify that the same result is obtained\n",
      "print('==========================================')\n",
      "print(count_by_weekday1.equals(count_by_weekday2))\n",
      "\n",
      "<<<<<<<<<<<sample margins=True\n",
      "\n",
      "# Create the DataFrame with the appropriate pivot table: signups_and_visitors\n",
      "signups_and_visitors = pd.pivot_table(users,index='weekday',aggfunc='sum')\n",
      "\n",
      "\n",
      "# Print signups_and_visitors\n",
      "print(signups_and_visitors)\n",
      "\n",
      "# Add in the margins: signups_and_visitors_total \n",
      "signups_and_visitors_total = pd.pivot_table(users,index='weekday',aggfunc='sum',margins=True)\n",
      "\n",
      "# Print signups_and_visitors_total\n",
      "print(signups_and_visitors_total)\n",
      "\n",
      "\n",
      "signups  visitors\n",
      "weekday                   \n",
      "Mon            8       782\n",
      "Sun           19       376\n",
      "         signups  visitors\n",
      "weekday                   \n",
      "Mon            8       782\n",
      "Sun           19       376\n",
      "All           27      1158\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\dataframe selecting data with .query().txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\dataframe selecting data with .query().txt\n",
      ".query('selection selection statement')\n",
      "\n",
      "similar to where\n",
      "\n",
      "table: stocks\n",
      "1. date\n",
      "2. disney\n",
      "3. nike\n",
      "\n",
      "stocks.query('nike>=90')\n",
      "\n",
      "multiple conditions \"and\" and \"or\"\n",
      "\n",
      "stocks.query('nike>90 and disney <140')\n",
      "\n",
      "stocks.query('nike>96 or disney < 98')\n",
      "\n",
      "stocks_long.query('stocks==\"disney\" or (stocks==\"nike\" and close<90)')\n",
      "\n",
      "\n",
      " >Sample   > merge ordered\n",
      "\n",
      "# Merge gdp and pop on date and country with fill\n",
      "gdp_pop = pd.merge_ordered(gdp,pop,on=['country','date'],fill_method=\"ffill\")\n",
      "print(gdp_pop)\n",
      "\n",
      "gdp_pop['gdp_per_capita']=gdp_pop['gdp']/gdp_pop['pop']\n",
      "\n",
      "# Pivot table of gdp_per_capita, where index is date and columns is country\n",
      "gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n",
      "\n",
      "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
      "\n",
      "# Pivot data so gdp_per_capita, where index is date and columns is country\n",
      "gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n",
      "\n",
      "# Select dates equal to or greater than 2016-01-01\n",
      "recent_gdp_pop = gdp_pivot.query('date>=\"2016-01-01\"')\n",
      "\n",
      "# Plot recent_gdp_pop\n",
      "recent_gdp_pop.plot(rot=90)\n",
      "plt.show()\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\kpi.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\kpi.txt\n",
      "import pandas as pd\n",
      "\n",
      "customer_demographics=pd.read_csv('customer_demographics.csv')\n",
      "\n",
      "uid\n",
      "reg_date\n",
      "device\n",
      "gender\n",
      "country\n",
      "age\n",
      "\n",
      "\n",
      "#customer actions\n",
      "customer_subscriptions=pd.read_csv('customer_subscriptions.csv')\n",
      "\n",
      "print(customer_subscriptions.head())\n",
      "\n",
      "uid\n",
      "lapse_date\n",
      "subscription_date\n",
      "price\n",
      "\n",
      "KPI : conversion rate\n",
      "\n",
      "importance across different user groups\n",
      "\n",
      "sub_data_demo=customer_demographics.merge(\n",
      "\tcustomer_subscriptions,\n",
      "\thow='inner',\n",
      "\ton=['uid']\n",
      "\t)\n",
      "\n",
      "\n",
      "\n",
      "   > sample\n",
      "\n",
      "# Import pandas \n",
      "import pandas as pd\n",
      "\n",
      "# Load the customer_data\n",
      "customer_data = pd.read_csv('customer_data.csv')\n",
      "\n",
      "# Load the app_purchases\n",
      "app_purchases = pd.read_csv('inapp_purchases.csv')\n",
      "\n",
      "# Print the columns of customer data\n",
      "print(customer_data.columns)\n",
      "\n",
      "# Print the columns of app_purchases\n",
      "print(app_purchases.columns)\n",
      "\n",
      "\n",
      "Index(['uid', 'reg_date', 'device', 'gender', 'country', 'age'], dtype='object')\n",
      "\n",
      "Index(['date', 'uid', 'sku', 'price'], dtype='object')\n",
      "\n",
      "# Merge on the 'uid' field\n",
      "uid_combined_data = app_purchases.merge(customer_data, on=['uid'], how='inner')\n",
      "\n",
      "# Examine the results \n",
      "print(uid_combined_data.head())\n",
      "print(len(uid_combined_data))\n",
      "\n",
      "\n",
      "date_x       uid            sku  price      date_y device gender country  age\n",
      "0  2017-07-10  41195147  sku_three_499    499  2017-06-26    and      M     BRA   17\n",
      "1  2017-07-15  41195147  sku_three_499    499  2017-06-26    and      M     BRA   17\n",
      "2  2017-11-12  41195147   sku_four_599    599  2017-06-26    and      M     BRA   17\n",
      "3  2017-09-26  91591874    sku_two_299    299  2017-01-05    and      M     TUR   17\n",
      "4  2017-12-01  91591874   sku_four_599    599  2017-01-05    and      M     TUR   17\n",
      "9006\n",
      "In [1]:\n",
      "\n",
      "\n",
      "# Merge on the 'uid' and 'date' field\n",
      "uid_date_combined_data = app_purchases.merge(customer_data, on=['uid', 'date'], how='inner')\n",
      "\n",
      "# Examine the results \n",
      "print(uid_date_combined_data.head())\n",
      "print(len(uid_date_combined_data))\n",
      "\n",
      "\n",
      " uid             sku  price device gender country  age\n",
      "0  2016-03-30  94055095    sku_four_599    599    iOS      F     BRA   16\n",
      "1  2015-10-28  69627745     sku_one_199    199    and      F     BRA   18\n",
      "2  2017-02-02  11604973  sku_seven_1499    499    and      F     USA   16\n",
      "3  2016-06-05  22495315    sku_four_599    599    and      F     USA   19\n",
      "4  2018-02-17  51365662     sku_two_299    299    iOS      M     TUR   16\n",
      "\n",
      "      . exploratory analysis of kpi\n",
      "\n",
      "1. most companies will have many kpis\n",
      "2. each serves a different purpose\n",
      "\n",
      "#axis=0 is columns\n",
      "#as_index will use group labels as index\n",
      "\n",
      "sub_data_grp=sub_data_deep.groupby(by=['country','device'], axis=0, as_index=False)\n",
      "\n",
      "sub_data_grp.mean()\n",
      "or\n",
      "sub_data_grp.agg('mean')\n",
      "or\n",
      "sub_data_grp.agg(['mean','median'])\n",
      "or\n",
      "sub_data_grp.agg({'price':['mean','median','max'],\n",
      "\t'age':['mean','median','max']\n",
      "\t})\n",
      "\n",
      "def truncate_mean(data):\n",
      "\ttop_val=data.quantile(.9)\n",
      "\tbot_val=data.quantile(.1)\n",
      "\ttrunc_data=data[(data<=top_val) & (data>=bot_val)]\n",
      "\tmean=trunc_data.mean()\n",
      "\treturn (mean)\n",
      "\n",
      "\n",
      "sub_data_grp.agg({'age':[truncated_mean]})\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "# Calculate the mean and median purchase price \n",
      "purchase_price_summary = purchase_data.price.agg(['mean', 'median'])\n",
      "\n",
      "# Examine the output \n",
      "print(purchase_price_summary)\n",
      "\n",
      "mean      406.772596\n",
      "median    299.000000\n",
      "\n",
      "# Calculate the mean and median of price and age\n",
      "purchase_summary = purchase_data.agg({'price': ['mean', 'median'], 'age': ['mean', 'median']})\n",
      "\n",
      "# Examine the output \n",
      "print(purchase_summary)\n",
      "\n",
      "             price        age\n",
      "mean    406.772596  23.922274\n",
      "median  299.000000  21.000000\n",
      "\n",
      "\n",
      "Notice how the mean is higher than the median? This suggests that we have some users who are making a lot of purchases!\n",
      "\n",
      "# Group the data \n",
      "grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])\n",
      "\n",
      "# Aggregate the data\n",
      "purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})\n",
      "\n",
      "# Examine the results\n",
      "print(purchase_summary)\n",
      "\n",
      "\n",
      "price                   \n",
      "                     mean median         std\n",
      "device gender                               \n",
      "and    F       400.747504    299  179.984378\n",
      "       M       416.237308    499  195.001520\n",
      "iOS    F       404.435330    299  181.524952\n",
      "       M       405.272401    299  196.843197\n",
      "\n",
      "       calculating a conversion rate\n",
      "\n",
      "import pandas as pd\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "current_date=pd.to_datetime('2018-03-17')\n",
      "\n",
      "#what is the maximum lapse date in our dataset\n",
      "\n",
      "max_lapse_date = current_date - timedelta(days=7)\n",
      "\n",
      "conv_sub_data=sub_data_demo[(sub_data_demo.lapse_date<max_lapse_date)]\n",
      "\n",
      "\n",
      "total_users_count=conv_sub_data.price.count()\n",
      "print(total_users_count)\n",
      "\n",
      "max_sub_date=conv_sub_data.lapse_date+timedelta(days=7)\n",
      "\n",
      "total_subs=conv_sub_data[\n",
      "(conv_sub_data.price>0) &\n",
      "(conv_sub_data.subscription_data<=max_sub_data)\n",
      "]\n",
      "\n",
      "total_sub_count=total_sub.price.count()\n",
      "print(total_subs_count)\n",
      "\n",
      "conversion rate = Total subscribers/potential subscribers\n",
      "\n",
      "conversion_rate = total_subs_count / total_users_count\n",
      "print(conversion_rate)\n",
      "\n",
      "\n",
      "      cohort conversion rate\n",
      "\n",
      "conv_sub_data = conv_sub_data.copy()\n",
      "\n",
      "#keep users who lapsed prior to the last 14 days\n",
      "\n",
      "max_lapse_date = current_date - timedelta(days=14)\n",
      "\n",
      "conv_sub_data = sub_data_demo[\n",
      " (sub_data_demo.lapse_date <=max_lapse_date)\n",
      "]\n",
      "\n",
      "sub time is the number of days been the lapse date and the subscription date\n",
      "\n",
      "np.where receives a number to return a true and one to return a false\n",
      "\n",
      "sub_time = np. where(\n",
      "\tconv_sub_data.subscription_date.notnull(),\n",
      "\t#then find how many days since their lapse\n",
      "\t(conv_sub_data.scription_date - conv_sub_data.lapse_date).dt.days,\n",
      "\t#else set the value to pd.NaT\n",
      "\tpd.NaT)\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "\n",
      "find the conversion rate gcr7() and gcr14()\n",
      "\n",
      "purchase_cohorts=conv_sub_data.groupby(by=['gender','device'],as_index=False)\n",
      "\n",
      "#find the conversion rate for each cohort using gcr7 and gcr14\n",
      "\n",
      "purchase_cohorts.agg({sub_time:[gcr7,gcr14]})\n",
      "\n",
      "     How to choose KPI metrics\n",
      "\n",
      "how long does it take to gain insight on a metric\n",
      "\n",
      "what is an actionable time scale\n",
      "\n",
      "monthly conversion rate = 1 month wait time\n",
      "\n",
      "leverage exploratory data analysis\n",
      "* reveals relationships between metrics and key results\n",
      "\n",
      "KPI should measure strong growth\n",
      "* potential early warning sign of problems\n",
      "* senstive to changes in the overall ecosystem\n",
      "\n",
      "       sample\n",
      "\n",
      "# Compute max_purchase_date \n",
      "max_purchase_date = current_date - timedelta(days=28)\n",
      "\n",
      "# Filter to only include users who registered before our max date\n",
      "purchase_data_filt = purchase_data[purchase_data.reg_date < max_purchase_date]\n",
      "\n",
      "# Filter to contain only purchases within the first 28 days of registration\n",
      "purchase_data_filt = purchase_data_filt[(purchase_data_filt.date <=\n",
      "                                         purchase_data_filt.reg_date + \n",
      "                                         timedelta(days=28))]\n",
      "\n",
      "# Output the mean price paid per purchase\n",
      "print(purchase_data_filt.price.mean())\n",
      "\n",
      "414.4237288135593\n",
      "\n",
      "\n",
      "      find a 1 month of data\n",
      "\n",
      "# Set the max registration date to be one month before today\n",
      "max_reg_date = current_date - timedelta(days=28)\n",
      "\n",
      "# Find the month 1 values:\n",
      "month1 = np.where((purchase_data.reg_date < max_reg_date) &\n",
      "                    (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),\n",
      "                  purchase_data.price, \n",
      "                  np.NaN)\n",
      "                 \n",
      "# Update the value in the DataFrame \n",
      "purchase_data['month1'] = month1\n",
      "\n",
      "print(month1)\n",
      "\n",
      "# Group the data by gender and device \n",
      "purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False)\n",
      "\n",
      "# Aggregate the month1 and price data \n",
      "purchase_summary = purchase_data_upd.agg(\n",
      "                        {'month1': ['mean', 'median'],\n",
      "                        'price': ['mean', 'median']})\n",
      "\n",
      "# Examine the results \n",
      "print(purchase_summary)\n",
      "\n",
      "gender device      month1              price       \n",
      "                       mean median        mean median\n",
      "0      F    and  388.204545  299.0  400.747504    299\n",
      "1      F    iOS  432.587786  499.0  404.435330    299\n",
      "2      M    and  413.705882  399.0  416.237308    499\n",
      "3      M    iOS  433.313725  499.0  405.272401    299\n",
      "\n",
      "\n",
      "\n",
      "      >.Working with time series\n",
      "\n",
      "exploratory data analysis\n",
      "\n",
      "2nd week subscribers\n",
      "\n",
      "exclude customers who have not been on the platform for two weeks\n",
      "\n",
      "\n",
      "current_date=pd.to_datetime('2018-03-17')\n",
      "max_lapse_date=current_date - timedelta(days=14)\n",
      "\n",
      "#find all uses who have not been on the platform in the last two weeks\n",
      "\n",
      "conv_sub_data= sub_data_demo[\n",
      "  sub_data_demo.lapse_date < max_lapse_date\n",
      "]\n",
      "\n",
      "#how many days before the user subscribed\n",
      "\n",
      "sub_time = conv_sub_data.subscription_date -\n",
      "conv_sub_data.lapse_date\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "or\n",
      "conv_sub_data['sub_time']= conv_sub_data.sub_time.dt.days\n",
      "\n",
      "#find the users who have not subscribed in week one but have been on the platform for two or more weeks\n",
      "\n",
      "conv_base=conv_sub_data[\n",
      " (conv_sub_data.sub_time.notnull()) |\n",
      "\t(conv_sub_data.sub_time > 7)]\n",
      "\n",
      "total_users=len(conv_base)\n",
      "\n",
      "total_sub=np.where(conv_sub_data.sub_time.notnull()) &\n",
      "\t(conv_sub_data.sub_time <= 14,1,0)\n",
      "\n",
      "total_subs=sum(total_subs)\n",
      "\n",
      "conversion_rate = total_subs/total_users\n",
      "\n",
      "output\n",
      "0.009\n",
      "\n",
      "    pandas date parser on read_csv\n",
      "\n",
      "pandas.read_csv(\n",
      "\n",
      "\tparse_dates=False\n",
      "\tinfer_datetime_format=False\n",
      "\tkeep_date_col=False\n",
      "\tdate_parser=None\n",
      "\tdayFirst=False\n",
      "\t)\n",
      "\n",
      "strftime\n",
      "\"%Y-%m-%d\"\n",
      "\"%H:%M:%S\"\n",
      "\n",
      "\"%B %d, %Y\"\n",
      "\n",
      "to_datetime\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_one = pd.to_datetime(date_data_one, format=\"%A %B %d, %Y\")\n",
      "print(date_data_one)\n",
      "\n",
      "output:\n",
      "DatetimeIndex(['2017-01-27', '2017-12-02'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_two = pd.to_datetime(date_data_two, format=\"%Y-%m-%d\")\n",
      "print(date_data_two)\n",
      "\n",
      "output:\n",
      "'2017-01-01', '2016-05-03']\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_three = pd.to_datetime(date_data_three, format=\"%m/%d/%Y\")\n",
      "print(date_data_three)\n",
      "\n",
      "output:\n",
      "'1978-08-17', '1976-01-07'\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_four = pd.to_datetime(date_data_four, format=\"%Y %B %d %H:%M\")\n",
      "print(date_data_four)\n",
      "\n",
      "output:\n",
      "2016-03-01 01:56:00', '2016-01-04 02:16:00'\n",
      "\n",
      "    Creating time series graphs with matplotlib\n",
      "\n",
      "#find all uses who have not been on the platform in the last two weeks\n",
      "\n",
      "conv_sub_data= sub_data_demo[\n",
      "  sub_data_demo.lapse_date < max_lapse_date\n",
      "]\n",
      "\n",
      "#how many days before the user subscribed\n",
      "\n",
      "sub_time = conv_sub_data.subscription_date -\n",
      "conv_sub_data.lapse_date\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "or\n",
      "conv_sub_data['sub_time']= conv_sub_data.sub_time.dt.days\n",
      "\n",
      "#find the users who have not subscribed in week one but have been on the platform for two or more weeks\n",
      "\n",
      "conv_base=conv_sub_data[\n",
      " (conv_sub_data.sub_time.notnull()) |\n",
      "\t(conv_sub_data.sub_time > 7)]\n",
      "\n",
      "total_users=len(conv_base)\n",
      "\n",
      "total_sub=np.where(conv_sub_data.sub_time.notnull()) &\n",
      "\t(conv_sub_data.sub_time <= 14,1,0)\n",
      "\n",
      "total_subs=sum(total_subs)\n",
      "\n",
      "conversion_rate = total_subs/total_users\n",
      "\n",
      "   new stuff\n",
      "conversion_data = conv_sub_data.groupby(\n",
      "\tby=['lapse_date'], as_index=False\n",
      ").agg('sub_time': [gc7]})\n",
      "\n",
      "#produces the week one conversion rate by conversion date.\n",
      "\n",
      "\n",
      "conversion_data.plot(x='lapse_date',y='sub_time')\n",
      "\n",
      "* compare users of different genders\n",
      "* evaluate the impact of a change across regions\n",
      "* see the impact for different devices\n",
      "\n",
      "reformatted_cntry_data=pd.pivot_table(\n",
      "\tconversion_data,\n",
      "\tvalues=['sub_time'],\n",
      "\tcolumns=['country'],\n",
      "\tindex=['reg_data'],\n",
      "\tfill_value=0\n",
      ")\n",
      "\n",
      "reformat_cntry_data.plot(\n",
      "\tx='reg_date',\n",
      "\ty=['BRA','FRA','DEU','TUR','USA','CAN']\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >sample   graph reg_date by first_week_purchases\n",
      "\n",
      "# Group the data and aggregate first_week_purchases\n",
      "\n",
      "user_purchases columns: 'reg_date', 'first_week_purchases'\n",
      "\n",
      "user_purchases = user_purchases.groupby(by=['reg_date', 'uid']).agg({'first_week_purchases': ['sum']})\n",
      "\n",
      "# Reset the indexes\n",
      "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
      "user_purchases.reset_index(inplace=True)\n",
      "\n",
      "# Find the average number of purchases per day by first-week users\n",
      "user_purchases = user_purchases.groupby(by=['reg_date']).agg({'first_week_purchases': ['mean']})\n",
      "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
      "user_purchases.reset_index(inplace=True)\n",
      "\n",
      "# Plot the results\n",
      "user_purchases.plot(x='reg_date', y='first_week_purchases')\n",
      "plt.show()\n",
      "\t\n",
      "\n",
      "   sample pivot table on the first_week_purchases by country\n",
      "\n",
      "# Pivot the data\n",
      "country_pivot = pd.pivot_table(user_purchases_country, values=['first_week_purchases'], columns=['country'], index=['reg_date'])\n",
      "print(country_pivot.head())\n",
      "\n",
      "\n",
      "# Pivot the data\n",
      "device_pivot = pd.pivot_table(user_purchases_device, values=['first_week_purchases'], columns=['device'], index=['reg_date'])\n",
      "print(device_pivot.head())\n",
      "\n",
      "\n",
      "          first_week_purchases          \n",
      "device                      and       iOS\n",
      "reg_date                                 \n",
      "2017-06-01             0.714286  1.000000\n",
      "2017-06-02             1.400000  1.285714\n",
      "2017-06-03             1.545455  1.000000\n",
      "2017-06-04             1.600000  1.833333\n",
      "2017-06-05             1.625000  2.000000\n",
      "\n",
      "\n",
      "# Plot the average first week purchases for each country by registration date\n",
      "country_pivot.plot(x='reg_date', y=['USA', 'CAN', 'FRA', 'BRA', 'TUR', 'DEU'])\n",
      "plt.show()\n",
      "\n",
      "# Plot the average first week purchases for each device by registration date\n",
      "device_pivot.plot(x='reg_date', y=['and', 'iOS'])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     understanding and visualizing trends in customer data\n",
      "\n",
      "usa_subscriptions['sub_day']=(usa_subscriptions.sub_date - usa_subscriptions.lapse_date).dt.days\n",
      "\n",
      "\n",
      "usa_subscriptions = usa_subscriptions[usa_subscriptions.sub_day <=7]\n",
      "\n",
      "usa_subscriptions = usa_subscriptions.groupby(\n",
      "\tby=['sub_date'],as_index=False\n",
      ").agg({'subs':['sum']})\n",
      "\n",
      "\n",
      "     >looking for seasonal change in buying movement\n",
      "\n",
      "Trailing average smoothing technique that averages over a lagging window\n",
      "1. reveal hidden trends by smoothing out seasonality\n",
      "2. average across the period of seasonality\n",
      "3. 7-day window to smooth weekly seasonality\n",
      "4. average out day level effects to produce the average week effect\n",
      "\n",
      "calculate the rolling average over the usa subscribers data with .rolling()\n",
      "\n",
      "rolling_subs = usa_subscriptions.subs.rolling(\n",
      "\twindow=7,\n",
      "\t#specify to average backwards\n",
      "\tcenter=False\n",
      ")\n",
      "\n",
      "usa_subscriptions['rolling_subs']\n",
      "\t=rolling_subs.mean()\n",
      "usa_subscriptions.tail()\n",
      "\n",
      "high_sku_purchases = pd.read_csv(\n",
      "\t'high_sku_purchases.csv',\n",
      "\tparse_dates=True,\n",
      "\tinfer_datetime_format=True\n",
      ")\n",
      "\n",
      "high_sku_purchases.plot(x='date', y='purchases')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "       exponential moving average\n",
      "\n",
      "1. weighted moving (rolling) average\n",
      "\n",
      "* weights more recent items in the window more\n",
      "* applies weights according to an exponential distribution\n",
      "* average back to a central trend without masking any recent movements\n",
      "\n",
      ".ewm() : exponential weighting function\n",
      "\n",
      "\n",
      "window to apply weights over\n",
      "\n",
      "exp_mean=high_sku_purchases.purchases.ewm(span=30)\n",
      "\n",
      "high_sku_purchases['exp_mean'] = exp_mean.mean()\n",
      "\n",
      "\n",
      "   >  sample  > rolling window 7, 28, 365\n",
      "\n",
      "# Compute 7_day_rev\n",
      "daily_revenue['7_day_rev'] = daily_revenue.revenue.rolling(window=7,center=False).mean()\n",
      "\n",
      "# Compute 28_day_rev\n",
      "daily_revenue['28_day_rev'] = daily_revenue.revenue.rolling(window=28,center=False).mean()\n",
      "    \n",
      "# Compute 365_day_rev\n",
      "daily_revenue['365_day_rev'] = daily_revenue.revenue.rolling(window=365,center=False).mean()\n",
      "    \n",
      "# Plot date, and revenue, along with the 3 rolling functions (in order)    \n",
      "daily_revenue.plot(x='date', y=['revenue', '7_day_rev', '28_day_rev', '365_day_rev', ])\n",
      "plt.show()\n",
      "\n",
      "   > sample ewm\n",
      "\n",
      "# Calculate 'small_scale'\n",
      "daily_revenue['small_scale'] = daily_revenue.revenue.ewm(span=10).mean()\n",
      "\n",
      "# Calculate 'medium_scale'\n",
      "daily_revenue['medium_scale'] = daily_revenue.revenue.ewm(span=100).mean()\n",
      "\n",
      "# Calculate 'large_scale'\n",
      "daily_revenue['large_scale'] = daily_revenue.revenue.ewm(span=500).mean()\n",
      "\n",
      "# Plot 'date' on the x-axis and, our three averages and 'revenue'\n",
      "# on the y-axis\n",
      "daily_revenue.plot(x = 'date', y =['revenue', 'small_scale', 'medium_scale', 'large_scale'])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >Events and releases\n",
      "\n",
      "discover the cause of an issue\n",
      "\n",
      "visualizing the drop in conversion rate (3 years)\n",
      "\n",
      "we notice a dip in new user retention\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "conv_sub_data = sub_data_demo(\n",
      "\tsub_data_demo.lapse_date <= max_lapse_date]\n",
      "\n",
      "sub_time = (conv_sub_data.subscription_date -\n",
      "\tconv_sub_data.lapse_date).dt.days\n",
      "\n",
      "conv_sub_date['sub_time']=sub_time\n",
      "\n",
      "conversion_data = conv_sub_data.groupby(\n",
      "\tby=['lapse_data'], as_index=False)\n",
      ".agg({sub_time':[gc7]})\n",
      "\n",
      "conversion_data.plot()\n",
      "plt.show()\n",
      "\n",
      "    >look at the recent six months\n",
      "\n",
      "current_date = pd.to_date('2018-03-17')\n",
      "\n",
      "start_date=current_date - timedelta(days=(6*28))\n",
      "\n",
      "conv_filter=(\n",
      "\tconversion_data.lapse_date >= start_date)\n",
      "\t& (conversion_data.lapse_date <= current_date)\n",
      ")\n",
      "\n",
      "con_data_filt=conversion_data[conv_filter]\n",
      "\n",
      "conv_data_filt.plot(x='lapse_date', y='sub_time')\n",
      "plt.show()\n",
      "\n",
      "* is this drop impacting all users or just specific cohort\n",
      "\n",
      "* this could provide clues on what the issue may be\n",
      "\n",
      "* ecosystems within our data\n",
      "1. distinct countries\n",
      "2. specific device (android or ios)\n",
      "\n",
      "\n",
      "\n",
      "#pivot the results to have one column per country\n",
      "\n",
      "conv_data_cntry = pd.pivot_table(\n",
      "\tconv_data_cntry, values=['sub_time'],\n",
      "\tcolumns=['country'], index=['lapse_date'], fill_value=0\n",
      ")\n",
      "\n",
      "#pivot the results to have one column per device\n",
      "\n",
      "\n",
      "conv_data_device = pd.pivot_table(\n",
      "\tconv_data_cntry, values=['sub_time'],\n",
      "\tcolumns=['device'], index=['lapse_date'], fill_value=0\n",
      ")\n",
      "\n",
      "* all countries experience the drop\n",
      "\n",
      "* most pronounced in Brazil & Turkey\n",
      "\n",
      "* breaking out by device\n",
      "1 the drop only appears on android devices\n",
      "\n",
      "events: holidays and events impacting user behavior\n",
      "\n",
      "events=pd.read_csv('events.csv')\n",
      "1. Date\n",
      "2. Event\n",
      "\n",
      "releases: ios and android software releases\n",
      "\n",
      "releases = pd.read_csv('releases.csv')\n",
      "\n",
      "     >Plot the conversion rate trend per device\n",
      "\n",
      "conv_data_dev.plot(\n",
      "\tx=['lapse_date'], y=['iOS','and']\n",
      ")\n",
      "\n",
      "events.Date = pd.to_datetime(events.Date)\n",
      "\n",
      "#iterate through events and plot each one\n",
      "\n",
      "for row in events.iterrows():\n",
      "\ttmp=row[1]\n",
      "\tplt.axvline(\n",
      "\tx=tmp.Date, color='k', linestyle='---'\n",
      ")\n",
      "\n",
      "\n",
      "#iterate through the releases and plot each one\n",
      "\n",
      "releases.Date = pd.to_datetime(releases.Date)\n",
      "\n",
      "\n",
      "for row in releases.iterrows():\n",
      "\ttmp=row[1]\n",
      "\tif tmp.Event== 'iOS Release':\n",
      "\n",
      "\t\tplt.axvline(\n",
      "\t\tx=tmp.Date, color='b', linestyle='---'\n",
      ")\n",
      "\telse:\n",
      "\t\tplt.axvline(\n",
      "\t\tx=tmp.Date, color='r', linestyle='---'\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "There was an android release in feb/mar aligns with our dip in conversion rate\n",
      "\n",
      "\n",
      "visualizing data over time to uncover hidden trends\n",
      "\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "user_revenue:\n",
      "1. device\n",
      "2. gender\n",
      "3. country\n",
      "4. date \n",
      "5. revenue\n",
      "6. month\n",
      "\n",
      "\n",
      "# Pivot user_revenue\n",
      "pivoted_data = pd.pivot_table(user_revenue, values ='revenue', columns=['device', 'gender'], index='month')\n",
      "pivoted_data = pivoted_data[1:(len(pivoted_data) -1 )]\n",
      "\n",
      "# Create and show the plot\n",
      "pivoted_data.plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "more female bought ios devices\n",
      "\n",
      "      Introduction to A/B testing\n",
      "\n",
      "discoverying causal relationships\n",
      "\n",
      "test two or more variants against each other\n",
      "\n",
      "to evaluate which one performs best\n",
      "\n",
      "in context of a randomized experiment\n",
      "\n",
      "testing two more ideas against each other\n",
      "\n",
      "control: the current state of your product\n",
      "\n",
      "treatment: the variant that you want to test\n",
      "\n",
      "current paywall: I hope you enjoyed your free-trial please consider subscribing\n",
      "\n",
      "proposed paywall: your free-trial has ended, don't miss out, subscribe today\n",
      "\n",
      "randomly select a subset of users and show one set the control and on e the treatment\n",
      "\n",
      "monitor the conversion rates of each group to see which is better\n",
      "\n",
      "by randomly assigning the user we isolate the impact of the change and reduce the potential impact of confounding variables\n",
      "\n",
      "using an assignment criteria may introduce confounders\n",
      "\n",
      "A/B testing can be used to \n",
      "1. improve sales within a mobile application\n",
      "2. increase user interactions with a website\n",
      "3. identify the impact of a medical treatment\n",
      "4. optimize an assembly lines efficiency\n",
      "\n",
      "good problems for ab testing\n",
      "1. where users are being impacted individually\n",
      "2. testing changes that can directly impact their behavior\n",
      "\n",
      "bad problems for ab testing\n",
      "1. challenging to segment the users into groups\n",
      "2. difficult to untangle the impact of the test\n",
      "\n",
      "\n",
      "     >initial ab test design\n",
      "\n",
      "increasing our apps revenue with a/b testing\n",
      "\n",
      "1. test change to our consumable purchase paywall\n",
      "2. increase revenue by increasing the purchase rate\n",
      "\n",
      "general concepts\n",
      "1. a/b testing techniques transfer across a variety of context\n",
      "2. keep in mind how you would apply these techniques\n",
      "\n",
      "    paywall views & demographics data\n",
      "\n",
      "demographics_data = pd.read_csv('user_demographics.csv')\n",
      "demographics_data.head(n=2)\n",
      "\n",
      "1.uid\n",
      "2.reg_date\n",
      "3.device\n",
      "4.gender\n",
      "5.country\n",
      "6.age\n",
      "\n",
      "\n",
      "paywall_views = pd.read_csv('paywall_views.csv')\n",
      "\n",
      "1.uid\n",
      "2.date\n",
      "3.purchase\n",
      "4.sku\n",
      "5.price\n",
      "\n",
      "\n",
      "   >Response variable\n",
      "1. A response variable is used to measure the impact of your change\n",
      "2. should either be a kpi or directly related to a kpi\n",
      "3. something that is easy to measure\n",
      "\n",
      "factors:\n",
      "1. the paywall color\n",
      "\n",
      "variants:\n",
      "1. particular changes you are testing\n",
      "\n",
      "Experimental unit of our test\n",
      "1. the smallest unit you are measuring the change over\n",
      "2. Individual users make a convenient experimental unit\n",
      "\n",
      "purchase_data = demographics_data.merge(\n",
      "\tpaywall_views, how='left', on=['uid'])\n",
      "\n",
      "#find the total purchases for each user\n",
      "\n",
      "total_purchases = purchase_data.groupby(\n",
      "\tby=['uid'], as_index=False).purchase.sum()\n",
      "\n",
      "#find the mean number of purchase per user\n",
      "total_purchases.purchase.mean()\n",
      "\n",
      "print('total purchases average does not make alot of sense, instead try min and max')\n",
      "\n",
      "\n",
      "#find the min and max number of purchases per users in the time period\n",
      "\n",
      "total_purchases.purchase.min()\n",
      "total_purchases.purchase.max()\n",
      "\n",
      "    user days\n",
      "\n",
      "user interactions on a given day\n",
      "1. more convenient than users by itself\n",
      "2. not required to track users actions across time\n",
      "3. can treat simpler actions as responses to the test\n",
      "\n",
      "\n",
      "total_purchases = purchase_data.groupby(\n",
      "\tby=['uid','date'], as_index=False).purchase.sum()\n",
      "\n",
      "total_purchases.purchase.mean()\n",
      "users in the time period\n",
      "total_purchases.purchase.min()\n",
      "total_purchases.purchase.max()\n",
      "\n",
      "\n",
      "   Randomize by user\n",
      "1. best to randomize by individuals regardless of our experimental unit\n",
      "2. otherwise users can have inconsistent experience\n",
      "\n",
      "important to build intuition about your users and data overall\n",
      "\n",
      "\n",
      "   sample  > calculate the user average purchase per day\n",
      "\n",
      "# Extract the 'day'; value from the timestamp\n",
      "purchase_data.date = purchase_data.date.dt.floor('d')\n",
      "\n",
      "# Replace the NaN price values with 0 \n",
      "purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)\n",
      "\n",
      "# Aggregate the data by 'uid' & 'date'\n",
      "purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)\n",
      "revenue_user_day = purchase_data_agg.sum()\n",
      "\n",
      "# Calculate the final average\n",
      "revenue_user_day = revenue_user_day.price.mean()\n",
      "print(revenue_user_day) \n",
      "\n",
      "\n",
      "output:\n",
      "407.33800579385104\n",
      "\n",
      "\n",
      "    Preparing to run an ab test\n",
      "\n",
      "current paywall: \"I hope you are enjoying the relaxing benefits of our app.  Consider making a purchase\"\n",
      "\n",
      "proposed Paywall: \"don't miss out! try one of our new products!\"\n",
      "\n",
      "Questions:\n",
      "Will updating the paywall text impact our revenue\n",
      "How do our three different consumable prices impact this?\n",
      "\n",
      "Considerations in test design\n",
      "1. can our test be run well in practice\n",
      "2. will we be able to derive meaningful results from it\n",
      "\n",
      "Test sensitivity\n",
      "1. What size of impact is meaningful to detect\n",
      "\n",
      "smaller changes are more difficult to detect and can be hidden by randomness\n",
      "\n",
      "Sensitivity is the minimum level of change we want to be able to detect in our tests\n",
      "\n",
      "     Calculating the revenue per user\n",
      "\n",
      "purchase_data = demographics_data.merge(\n",
      "\tpaywall_views, how='left', on=['uid'])\n",
      "\n",
      "total_revenue = purchase_data.groupby(by=['uid'], as_index=False).price.sum()\n",
      "\n",
      "total_revenue.price = np.where(\n",
      "\tnp.isnan(total_revenue.price),0, total_revenue.price)\n",
      "\n",
      "#calculate the average revenue per user\n",
      "\n",
      "avg_revenue = total_revenue.price.mean()\n",
      "\n",
      "print(avg_revenue)\n",
      "16\n",
      "\n",
      "#find the 1% 10% and 20% change in revenue\n",
      "\n",
      "avg_revenue *1 1.01\n",
      "16.32\n",
      "avg_revenue *1 1.10\n",
      "17.77\n",
      "avg_revenue *1 1.20\n",
      "19.39\n",
      "\n",
      "    Data variability\n",
      "1. important to understand the variability in the data\n",
      "2. does the amount spent vary alot among users\n",
      "a. if it does not then it will be easier to detect a change\n",
      "\n",
      "\n",
      "#calculate the standard deviation of revenue per user\n",
      "\n",
      "revenue_variation = total_revenue.price.std()\n",
      "\n",
      "print(revenue_variation)\n",
      "\n",
      "17.520\n",
      "\n",
      "notice the standard deviation is roughly 100% of what the mean average of 16 is.\n",
      "\n",
      "revenue_variation/avg_revenue\n",
      "1.084\n",
      "\n",
      "\n",
      "#find the average number of purchases per user\n",
      "avg_purchases = total_purchases.purchase.mean()\n",
      "3.15\n",
      "\n",
      "purchase_variation = total_purchases.purchase.std()\n",
      "2.68\n",
      "\n",
      "purchase_variation/avg_purchases\n",
      "0.850\n",
      "\n",
      "Primary goal is the increase revenue\n",
      "1. paywall view to purchase conversion rate\n",
      "a. more granular than overall revenue\n",
      "b. directly related to our test\n",
      "\n",
      "Experimental unit: paywall views\n",
      "1. simplest to work with\n",
      "2. assuming these interactions are independent\n",
      "\n",
      "\n",
      "     finding the baseline conversion rate\n",
      "\n",
      "purchase_data = demographic_data.merge(\n",
      "\tpaywall_views, how='inner', on=['uid']\n",
      ")\n",
      "\n",
      "conversion_rate = (sum(purchase_data.purchase) /\n",
      "\tpurchase_data.purchase.count())\n",
      "\n",
      "print(conversion_rate)\n",
      "\n",
      "0.347\n",
      "\n",
      "      sample get the sum and count\n",
      "\n",
      "# Merge and group the datasets\n",
      "purchase_data = demographics_data.merge(paywall_views,  how='left', on=['uid'])\n",
      "purchase_data.date = purchase_data.date.dt.floor('d')\n",
      "\n",
      "# Group and aggregate our combined dataset \n",
      "daily_purchase_data = purchase_data.groupby(by=['uid'], as_index=False)\n",
      "daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})\n",
      "\n",
      "# Find the mean of each field and then multiply by 1000 to scale the result\n",
      "daily_purchases = daily_purchase_data.purchase['sum'].mean()\n",
      "daily_paywall_views = daily_purchase_data.purchase['count'].mean()\n",
      "daily_purchases = daily_purchases * 1000\n",
      "daily_paywall_views = daily_paywall_views * 1000\n",
      "\n",
      "print(daily_purchases)\n",
      "print(daily_paywall_views)\n",
      "\n",
      "3150.0 (purchases)\n",
      "90814.54545454546 (number of views)\n",
      "\n",
      "\n",
      "        calculating lift dependent upon sensitivity\n",
      "\n",
      "small_sensitivity = 0.1 \n",
      "\n",
      "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
      "small_conversion_rate = conversion_rate * (1 + small_sensitivity) \n",
      "\n",
      "# Apply the new conversion rate to find how many more users per day that translates to\n",
      "small_purchasers = daily_paywall_views * small_conversion_rate\n",
      "\n",
      "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
      "purchaser_lift = small_purchasers - daily_purchases\n",
      "\n",
      "print(small_conversion_rate)\n",
      "print(small_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "\n",
      "0.03814800000000001 (small conversion rate)\n",
      "3499.384706400001 (small purchasers)\n",
      "317.58470640000087 (lift)\n",
      "\n",
      "  > medium sensitivity\n",
      "\n",
      "medium_sensitivity = 0.2\n",
      "\n",
      "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
      "medium_conversion_rate = conversion_rate * (1 + medium_sensitivity) \n",
      "\n",
      "# Apply the new conversion rate to find how many more users per day that translates to\n",
      "medium_purchasers = daily_paywall_views * medium_conversion_rate\n",
      "\n",
      "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
      "purchaser_lift = medium_purchasers - daily_purchases\n",
      "\n",
      "print(medium_conversion_rate)\n",
      "print(medium_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "0.041616 (4% conversion rate)\n",
      "3817.5105888000003 (purchasers)\n",
      "635.7105888000001 (lift)\n",
      "\n",
      "     large sensitivity\n",
      "\n",
      "large_sensitivity = 0.5\n",
      "\n",
      "# Find the conversion rate lift with the sensitivity above\n",
      "large_conversion_rate = conversion_rate * (1 + large_sensitivity)\n",
      "\n",
      "# Find how many more users per day that translates to\n",
      "large_purchasers = daily_paywall_views * large_conversion_rate\n",
      "purchaser_lift = large_purchasers - daily_purchases\n",
      "\n",
      "print(large_conversion_rate)\n",
      "print(large_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "0.052020000000000004\n",
      "4771.888236000001\n",
      "1590.0882360000005\n",
      "\n",
      "Awesome! While it seems that a 50% increase may be too drastic and unreasonable to expect, the small and medium sensitivities both seem very reasonable.\n",
      "\n",
      "\n",
      "       standard error\n",
      "\n",
      "\n",
      "# Find the n & v quantities\n",
      "n = purchase_data.purchase.count()\n",
      "\n",
      "# Calculate the quantity \"v\"\n",
      "v = conversion_rate * (1 - conversion_rate) \n",
      "\n",
      "# Calculate the variance and standard error of the estimate\n",
      "var = v / n \n",
      "se = var**0.5\n",
      "\n",
      "print(var)\n",
      "print(se)\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "3.351780834114284e-07\n",
      "0.0005789456653360731\n",
      "\n",
      "     >calculating sample size\n",
      "\n",
      "what is the null hypothesis\n",
      "\n",
      "1. hypothesis that control and treatment have the same impact on response\n",
      "a. updated paywall does not improve conversion rate\n",
      "b. any observed difference is due to randomness\n",
      "\n",
      "rejecting the null hypothesis\n",
      "a. determine their is a difference between the treatment and control\n",
      "b. we say the test has statistical significances\n",
      "\n",
      "\n",
      "\n",
      "Null hypothesis\n",
      "\n",
      "     \ttrue   \t\tfalse\n",
      "accept\tcorrect\t\ttype II error\n",
      "reject\ttype I error\tcorrect\n",
      "\n",
      "types of error & confidence level\n",
      "1. probablilty of not making type 1 error\n",
      "2. higher this value, larger the test sample needed\n",
      "\n",
      "common values is 0.95\n",
      "\n",
      "     >Statistical power\n",
      "\n",
      "statistical power is the probability of finding a statistically siginificant result when the null hypothesis is false\n",
      "\n",
      "confidence level\n",
      "standard error\n",
      "statistical power\n",
      "test sensitivity\n",
      "\n",
      "\n",
      "as the sample size increases so does our power increase\n",
      "\n",
      "\n",
      "    calculating our needed sample size\n",
      "\n",
      "baseline conversion rate 0.3468\n",
      "confidence level: 0.95\n",
      "desired power: 0.80\n",
      "sensitivity=0.1\n",
      "\n",
      "sample_size_group=get_sample(size(0.8, conversion_rate *1.1, 0.95)\n",
      "\n",
      "print(sample_size_per_group)\n",
      "\n",
      "output:\n",
      "45788\n",
      "\n",
      "\n",
      "      >generality of this function\n",
      "\n",
      "function shown specific to conversion rate calculations\n",
      "\n",
      "different response variables have different buy analogous formulas\n",
      "\n",
      "\n",
      "  > decreasing the need sample size\n",
      "\n",
      "* choose a unit of observation with lower variability\n",
      "\n",
      "* excluding users irrelevant to the process/change\n",
      "\n",
      "* think through how different factors relate to the sample size\n",
      "\n",
      "\n",
      "\n",
      "       increase the confidence level\n",
      "\n",
      "# Look at the impact of sample size increase on power\n",
      "n_param_one = get_power(n=1000, p1=p1, p2=p2, cl=cl)\n",
      "n_param_two = get_power(n=2000, p1=p1, p2=p2, cl=cl)\n",
      "\n",
      "# Look at the impact of confidence level increase on power\n",
      "alpha_param_one = get_power(n=n1, p1=p1, p2=p2, cl=0.8)\n",
      "alpha_param_two = get_power(n=n1, p1=p1, p2=p2, cl=0.95)\n",
      "    \n",
      "# Compare the ratios\n",
      "print(n_param_two / n_param_one)\n",
      "print(alpha_param_one / alpha_param_two)\n",
      "\n",
      "\n",
      "1.7596440001351992  (change sample size)\n",
      "1.8857367092232278  (change confidence levels)\n",
      "\n",
      "\n",
      "\n",
      "With these particular values it looks like decreasing our confidence level has a slightly larger impact on the power than increasing our sample size\n",
      "\n",
      "      calculate the conversion rate\n",
      "\n",
      "# Merge the demographics and purchase data to only include paywall views\n",
      "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
      "                            \n",
      "# Find the conversion rate\n",
      "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
      "            \n",
      "print(conversion_rate)\n",
      "\n",
      "0.03468607351645712\n",
      "\n",
      "    > calculate sample size\n",
      "\n",
      "# Merge the demographics and purchase data to only include paywall views\n",
      "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
      "                            \n",
      "# Find the conversion rate\n",
      "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
      "\n",
      "# Desired Power: 0.8\n",
      "# CL: 0.90\n",
      "# Percent Lift: 0.1\n",
      "p2 = conversion_rate * (1 + 0.1)\n",
      "sample_size = get_sample_size(0.8, conversion_rate, p2, 0.90)\n",
      "print(sample_size)\n",
      "\n",
      "36101\n",
      "\n",
      "\n",
      "# Desired Power: 0.95\n",
      "# CL 0.90\n",
      "# Percent Lift: 0.1\n",
      "p2 = conversion_rate * (1 + 0.1)\n",
      "sample_size = get_sample_size(0.95, conversion_rate, p2, 0.90)\n",
      "print(sample_size)\n",
      "\n",
      "63201\n",
      "\n",
      "\n",
      "      analyzing the ab test results\n",
      "\n",
      "compare the two groups purchase rates\n",
      "\n",
      "test_demographics = pd.read_csv('test_demographics.csv')\n",
      "\n",
      "#results for our ab test\n",
      "#group column c for control | v for variant\n",
      "\n",
      "test_results=pd.read_csv('ab_test_results.csv')\n",
      "test_results.head()\n",
      "\n",
      "uid\n",
      "date\n",
      "purchase\n",
      "sku\n",
      "price\n",
      "group\n",
      "\n",
      "\n",
      "    confirming our test results\n",
      "\n",
      "does the data look reasonable\n",
      "\n",
      "\n",
      "test_results_grpd = test_results.groupby(\n",
      "\tby=['group'], as_index=False)\n",
      "\n",
      "test_results_grpd.uid.count()\n",
      "\n",
      "48236\n",
      "49867\n",
      "\n",
      "\n",
      "test_results_demo = test_results.merge(\n",
      "\ttest_demo, how='inner', on='uid')\n",
      "\n",
      "test_results_grpd = test_results_demo.groupby(\n",
      "\tby=['country','gender','device','group'],\n",
      "as_index=False)\n",
      "\n",
      "test_results_grd.uid.count()\n",
      "\n",
      "\n",
      "    > find the mean conversion\n",
      "\n",
      "test_results_summary= test_results_demo.groupby(\n",
      "\tby=['group'], as_index=False\n",
      ").agg({'purchase':['count','sum']})\n",
      "\n",
      "test_results_summary['conv'] = (test_results_summary.purchase['sum']/\n",
      "\ttest_results_summary.purchase['count'])\n",
      "\n",
      "test_results_summary\n",
      "\n",
      "grp  sum   count   conversion\n",
      "c    48236 1657    0.034351\n",
      "v    49867 2094    0.041984\n",
      "\n",
      "Is the result statistically significant\n",
      "1. are the conversion rates different enough\n",
      "2. if yes then reject the null hypothesis\n",
      "3. conclude that the paywalls have different effects\n",
      "4. if no then it may just be randomness\n",
      "\n",
      "    p -value\n",
      "\n",
      "probability if the null hypothesis is true\n",
      "\n",
      "of observing a value as or more extreme\n",
      " \n",
      "what does a low p-value mean\n",
      "1. the power is low\n",
      "2. the observation is unlikely to happen due to randomness\n",
      "\n",
      "\n",
      "<0.01 very strong evidence against the null hypothesis\n",
      "\n",
      "0.01-0.5 strong evidence against the null hypothesis\n",
      "0.05-1. very weak evidence against the null hypothesis\n",
      ">0.1 small or no evidence against the null hypothesis\n",
      "\n",
      "\n",
      "?     sample test the null hypothesis\n",
      "\n",
      "# Compute and print the results\n",
      "results = ab_test_results.groupby('group').agg({'uid':pd.Series.nunique}) \n",
      "print(results)\n",
      "\n",
      "\n",
      "   uid\n",
      "group        \n",
      "C      2825.0\n",
      "V      2834.0\n",
      "\n",
      "\n",
      "# Find the unique users in each group \n",
      "results = ab_test_results.groupby('group').agg({'uid': pd.Series.nunique}) \n",
      "\n",
      "# Find the overall number of unique users using \"len\" and \"unique\"\n",
      "unique_users = len(ab_test_results.uid.unique()) \n",
      "\n",
      "# Find the percentage in each group\n",
      "results = results / unique_users * 100\n",
      "print(results)\n",
      "\n",
      "     uid\n",
      "group           \n",
      "C      49.920481\n",
      "V      50.079519\n",
      "\n",
      "\n",
      "   find the number of users in group device and gender\n",
      "\n",
      "# Find the unique users in each group, by device and gender\n",
      "results = ab_test_results.groupby(by=['group', 'device', 'gender']).agg({'uid': pd.Series.nunique}) \n",
      "\n",
      "# Find the overall number of unique users using \"len\" and \"unique\"\n",
      "unique_users = len(ab_test_results.uid.unique())\n",
      "\n",
      "# Find the percentage in each group\n",
      "results = results / unique_users * 100\n",
      "print(results)\n",
      "\n",
      "uid\n",
      "group device gender           \n",
      "C     and    F       14.896625\n",
      "             M       13.518289\n",
      "      iOS    F       11.309419\n",
      "             M       10.196148\n",
      "V     and    F       14.861283\n",
      "             M       13.659657\n",
      "      iOS    F       10.920657\n",
      "             M       10.637922\n",
      "\n",
      "\n",
      "     understanding statistical significance\n",
      "\n",
      "distribution of expected difference between control and test groups _if_ the null hypothesis is true\n",
      "\n",
      "The red line is the observed difference in the conversion rates from our tests\n",
      "\n",
      "p-value: probability of being as or more extreme than the red line on either side of the distribution.\n",
      "\n",
      "\n",
      "def get_pvalue ( con_conv, test_conv, con_size, test_size):\n",
      "\n",
      "\tlift= - abs(test_conv - con_conv)\n",
      "\tscale_one = con_conv * (1-con_conv) * (1/con_size)\n",
      "\tscale_two= test_conv * (1-test_conv) * (1/test_size)\n",
      "\tscale_val = (scale_one + scale_two) **0.5\n",
      "\tp_value=2*stats.norm.cdf(lift, loc=0, scale=scale_val)\n",
      "\treturn p_value\n",
      "\n",
      "\n",
      "con_conv=0.034351\n",
      "test_conv=0.041984\n",
      "con_size=48236\n",
      "test_size=49867\n",
      "\n",
      "p_value=get_pvalue(con_conv,test_conv,con_size,test_size)\n",
      "print(p_value)\n",
      "\n",
      "4.2572974 e-10  (extremely small p-value)\n",
      "\n",
      "accept the null hypothesis\n",
      "\n",
      "\n",
      "    find the power of the test\n",
      "\n",
      "def get_power(n, p1, p2, cl):\n",
      "    alpha = 1 - cl\n",
      "    qu = stats.norm.ppf(1 - alpha/2)\n",
      "    diff = abs(p2-p1)\n",
      "    bp = (p1+p2) / 2\n",
      "    \n",
      "    v1 = p1 * (1-p1)\n",
      "    v2 = p2 * (1-p2)\n",
      "    bv = bp * (1-bp)\n",
      "    \n",
      "    power_part_one = stats.norm.cdf((n**0.5 * diff - qu * (2 * bv)**0.5) / (v1+v2) ** 0.5)\n",
      "    power_part_two = 1 - stats.norm.cdf((n**0.5 * diff + qu * (2 * bv)**0.5) / (v1+v2) ** 0.5)\n",
      "    \n",
      "    power = power_part_one + power_part_two\n",
      "    \n",
      "    return (power)\n",
      "\n",
      "\n",
      "power= get_power (test_size, con_conv, test_conv, 0.95)\n",
      "print(power)\n",
      "0.9999925941372282\n",
      "\n",
      "\n",
      "small p-value and nearly perfect power\n",
      "\n",
      "        confidence interval\n",
      "\n",
      "ranges of values for our estimation rather than a single number\n",
      "\n",
      "provides context for our estimation process\n",
      "\n",
      "series of repeated experiments\n",
      "1. the calculated intervals will contain the true parameter x% of the time\n",
      "2. the true conversion rate is fixed quantity, it is the interval that is random not the conversion rate.\n",
      "\n",
      "\n",
      "The estimated parameter or difference in conversion rate follows a normal distribution\n",
      "\n",
      "1. we can estimate the standard deviation\n",
      "2. the mean of this distribution\n",
      "\n",
      "alpha is the desired confidence interval width\n",
      "\n",
      "bounds containing X% of hte probabilty around the mean (95%) of that distribution\n",
      "\n",
      "\n",
      "from scipy import stats\n",
      "\n",
      "def get_ci(test_conv, con_conv, test_size, con_size, ci):\n",
      "\n",
      "\tsd=((test_conv * (1-test_conv))/test_size+\n",
      "\t(con_conv * (1-con_conv)) / con_size)**0.5\n",
      "\n",
      "\tlift=test_conv - con_conv\n",
      "\n",
      "\tval=stats.norm.isf((1-ci)/2)\n",
      "\tlwr_bnd=lift - val *sd\n",
      "\tupr_bnd=lift+ val*sd\n",
      "\treturn ((lwr_bnd,upr_bnd))\n",
      "\n",
      "\n",
      "    get p-value\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=0.1, test_conv=0.17, con_size=1000, test_size=1000)\n",
      "print(p_value)\n",
      "\n",
      "4.131297741047306e-06\n",
      "\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=.1, test_conv=.15, con_size=100, test_size=100)\n",
      "print(p_value) \n",
      "\n",
      "0.28366948940702086\n",
      "\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=.48, test_conv=.5, con_size=1000, test_size=1000)\n",
      "print(p_value)\n",
      "\n",
      "0.370901935824383\n",
      "\n",
      "\n",
      "To recap we observed that a large lift makes us confident in our observed result, while a small sample size makes us less so, and ultimately high variance can lead to a high p-value!\n",
      "\n",
      "\n",
      "    check for statistically signficant\n",
      "\n",
      "\n",
      "cont_conv=0.09096495570387314 \n",
      "test_conv=0.1020053238686779 \n",
      "con_size=5329 \n",
      "test_size=5748\n",
      "\n",
      "# Compute the p-value\n",
      "p_value = get_pvalue(con_conv=cont_conv, test_conv=test_conv, con_size=cont_size, test_size=test_size)\n",
      "print(p_value)\n",
      "\n",
      "# Check for statistical significance\n",
      "if p_value >= 0.05:\n",
      "    print(\"Not Significant\")\n",
      "else:\n",
      "    print(\"Significant Result\")\n",
      "\n",
      "\n",
      "  > confidence interval\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, 0.975, 0.5)\n",
      "print(confidence_interval)\n",
      "\n",
      "(0.9755040421682947, 1.0244959578317054)\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, .95, 2)\n",
      "print(confidence_interval)\n",
      "\n",
      "2 standard deviations\n",
      "\n",
      "(0.6690506448818785, 1.3309493551181215)\n",
      "\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, 0.95, .001)\n",
      "print(confidence_interval)\n",
      "\n",
      "(1.0, 1.0)\n",
      "\n",
      "\n",
      "As our standard deviation decreases so too does the width of our confidence interval. Great work!\n",
      "\n",
      "\n",
      "con_conv=0.034351\n",
      "test_conv=0.041984\n",
      "con_size=48236\n",
      "test_size=49867\n",
      "ci=.95\n",
      "\n",
      "# Calculate the mean of our lift distribution \n",
      "lift_mean = test_conv -cont_conv\n",
      "\n",
      "# Calculate variance and standard deviation \n",
      "lift_variance = (1 - test_conv) * test_conv /test_size + (1 - cont_conv) * cont_conv / cont_size\n",
      "lift_sd = lift_variance**0.5\n",
      "\n",
      "# Find the confidence intervals with cl = 0.95\n",
      "confidence_interval = get_ci(lift_mean, 0.95,lift_sd)\n",
      "print(confidence_interval)\n",
      "\n",
      "confidence interval:\n",
      "(0.011039999822042502, 0.011040000177957487)\n",
      "\n",
      "Notice that our interval is very narrow thanks to our substantial lift and large sample size.\n",
      "\n",
      "      interpreting your results\n",
      "\n",
      "report \n",
      "\t\tTest Group\tControl Group\n",
      "1. Sample size  \t7030\t6970\n",
      "2. run time\t2 weeks\t\t2 weeks\n",
      "3. mean\t\t3.12\t\t2.69\n",
      "4. variance\t3.20\t\t2.64\n",
      "5. est lift\t0.56\n",
      "6. conf level\t0.56 += 0.4\n",
      "\n",
      "* significant at the 0.05 level\n",
      "\n",
      "visualization\n",
      "\n",
      "histograms: bucketed counts of observations across values\n",
      "\n",
      "user data rolled up to group and user level\n",
      "uid\n",
      "group\n",
      "purchase\n",
      "\n",
      "var=results[results.group=='V']\n",
      "con=results[results.group=='C']\n",
      "\n",
      "plt.hist(var['purchase'],color='yellow',\n",
      "\talpha=0.8, bins=50, label='Test')\n",
      "plt.hist(con['purchase'], color='blue',\n",
      "\talpha=0.8, bins=50, label='Control')\n",
      "plt.legend(loc='upper right')\n",
      "\n",
      "\n",
      "plt.axvline(x= np.mean(results.purchase),\n",
      "\tcolor='red')\n",
      "plt.axvline(x=np.mean(results.purchase),\n",
      "\tcolor='green')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >plotting a distribution\n",
      "\n",
      "mean_con=0.090965\n",
      "mean_test=0.102005\n",
      "var_con=(mean_con * (1-mean_con))/58583\n",
      "var_test=(mean_test *(1-mean_test))/56350\n",
      "\n",
      "con_line = np.linspace(-3*var_con**0.5+mean_con, 3*var_con**0.5+mean_con, 100)\n",
      "test_line=np.linspace(-3*var_test**0.5+mean_test, 3*var_test**0.5+mean_test, 100)\n",
      "\n",
      "\n",
      "\n",
      "#plot the probabilities across the distribution of conversion rates\n",
      "\n",
      "plt.plot(con_line, norm.pdf(\n",
      "    con_line, mean_con, var_con**0.5)\n",
      ")\n",
      "\n",
      "plt.plot(test_line, norm.pdf(\n",
      "    test_line, mean_test, var_test**0.5)\n",
      ")\n",
      "plt.show()\n",
      "\n",
      "mlab.normpdf(): converts values to probablities from Normal Distribution\n",
      "\n",
      "   plotting the difference of conversion rates\n",
      "\n",
      "lift= mean_test - mean_control\n",
      "var = var_test + var_control\n",
      "\n",
      "variance is the sum of variances\n",
      "\n",
      "diff_line = np.linspace(-3*var**0.5 + lift,\n",
      "3*var**0.5 + lift, 100)\n",
      "\n",
      "plt.plot(diff_line, mlab.normpdf(\n",
      "\tdiff_line, lift, var**0.5)\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     plotting the confidence interval\n",
      "\n",
      "section = np.arange(0.007624, 0.01445,1/10000)\n",
      "\n",
      "\n",
      "#fill in between these boundaries\n",
      "\n",
      "plt.fill_between(\n",
      "\tsection,\n",
      "\tmlab.normpdf(section, lift, var**0.5)\n",
      ")\n",
      "\n",
      "    sample   > show the control and test distributions do not intersect\n",
      "\n",
      "# Compute the standard deviations\n",
      "control_sd = cont_var**0.5\n",
      "test_sd = test_var**0.5\n",
      "\n",
      "# Create the range of x values \n",
      "control_line = np.linspace( cont_conv - 3 * control_sd, cont_conv + 3 * control_sd , 100)\n",
      "test_line = np.linspace( test_conv - 3 * test_sd,  test_conv + 3 * test_sd , 100)\n",
      "\n",
      "# Plot the distribution \n",
      "plt.plot(control_line, mlab.normpdf(control_line, cont_conv, control_sd))\n",
      "plt.plot(test_line, mlab.normpdf(test_line,test_conv, test_sd))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "We see no overlap, which intuitively implies that our test and control conversion rates are significantly distinct.\n",
      "\n",
      "     sample show  > show the confidence intervals and show the lift mean\n",
      "\n",
      "# Find the lift mean and standard deviation\n",
      "lift_mean = np.mean(test_conv-con_conv)\n",
      "lift_sd = (var_test + var_con) ** 0.5\n",
      "\n",
      "# Generate the range of x-values\n",
      "lift_line = np.linspace(lift_mean - 3 * lift_sd, lift_mean + 3 * lift_sd, 100)\n",
      "\n",
      "# Plot the lift distribution\n",
      "plt.plot(lift_line, norm.pdf(lift_line, lift_mean, lift_sd))\n",
      "\n",
      "ci_lower,ci_upper = get_ci(test_conv, con_conv, test_size, con_size,ci)\n",
      "\n",
      "# Add the annotation lines\n",
      "plt.axvline(x = lift_mean, color = 'green')\n",
      "plt.axvline(x = ci_lower, color = 'red')\n",
      "plt.axvline(x = ci_upper, color = 'red')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pandas data manipulation.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pandas data manipulation.txt\n",
      "\n",
      " Create dataframe using a list of dictionaries\n",
      "\n",
      "avocados_list = [\n",
      "    {'date': '2019-11-03', 'small_sold':10376832, 'large_sold': 7835071},\n",
      "     {'date': '2019-11-10', 'small_sold':10717154, 'large_sold': 8561348},\n",
      "]\n",
      "\n",
      "# Convert list into DataFrame\n",
      "avocados_2019 = avocados_list\n",
      "\n",
      "# Print the new DataFrame\n",
      "print(avocados_2019)\n",
      "\n",
      "  create dataframe using a list of lists\n",
      "# Create a dictionary of lists with new data\n",
      "avocados_dict = {\n",
      "  \"date\": ['2019-11-17','2019-12-01'],\n",
      "  \"small_sold\": [10859987,9291631],\n",
      "  \"large_sold\": [7674135,6238096]\n",
      "}\n",
      "\n",
      "# Convert dictionary into DataFrame\n",
      "avocados_2019 = pd.DataFrame(avocados_dict)\n",
      "\n",
      "# Print the new DataFrame\n",
      "print(avocados_2019)\n",
      "\n",
      " Read and group\n",
      "\n",
      "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
      "print(airline_bumping.head())\n",
      "\n",
      "# For each airline, select nb_bumped and total_passengers and sum\n",
      "airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
      "\n",
      " Dataframe to_csv\n",
      "\n",
      "airline_totals_sorted = airline_totals.sort_values(\"bumps_per_10k\", ascending=False)\n",
      "\n",
      "# Print airline_totals_sorted\n",
      "print(airline_totals_sorted)\n",
      "\n",
      "# Save as airline_totals_sorted.csv\n",
      "airline_totals_sorted.to_csv(\"airline_totals_sorted.csv\")\n",
      "\n",
      "\n",
      " pandas read_csv\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Create the list of file names: filenames\n",
      "filenames = ['Gold.csv', 'Silver.csv', 'Bronze.csv']\n",
      "\n",
      "# Create the list of three DataFrames: dataframes\n",
      "dataframes = []\n",
      "for filename in filenames:\n",
      "    dataframes.append(pd.read_csv(filename))\n",
      "\n",
      "# Print top 5 rows of 1st DataFrame in dataframes\n",
      "print(dataframes[0].head())\n",
      "\n",
      " >Sort Values\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Read 'monthly_max_temp.csv' into a DataFrame: weather1\n",
      "weather1 = pd.read_csv('monthly_max_temp.csv', index_col='Month')\n",
      "\n",
      "# Print the head of weather1\n",
      "print(weather1.head())\n",
      "\n",
      "# Sort the index of weather1 in alphabetical order: weather2\n",
      "weather2 = weather1.sort_index()\n",
      "\n",
      "# Print the head of weather2\n",
      "print(weather2.head())\n",
      "\n",
      "# Sort the index of weather1 in reverse alphabetical order: weather3\n",
      "weather3 = weather1.sort_index(ascending=False)\n",
      "\n",
      "# Print the head of weather3\n",
      "print(weather3.head())\n",
      "\n",
      "# Sort weather1 numerically using the values of 'Max TemperatureF': weather4\n",
      "weather4 = weather1.sort_values('Max TemperatureF')\n",
      "\n",
      "# Print the head of weather4\n",
      "print(weather4.head())\n",
      "\n",
      " pct_change\n",
      "df_cust=pd.DataFrame({'name':['A','B','C'], 'values':[1,3,6]})\n",
      "print(df_cust)\n",
      "values=df_cust.loc[:,'values']\n",
      "change=values.pct_change()\n",
      "print(change)\n",
      "\n",
      " columns.str.replace\n",
      "\n",
      "# Extract selected columns from weather as new DataFrame: temps_f\n",
      "temps_f = weather[['Min TemperatureF','Mean TemperatureF','Max TemperatureF']]\n",
      "\n",
      "# Convert temps_f to celsius: temps_c\n",
      "temps_c = (temps_f - 32) * 5/9\n",
      "\n",
      "# Rename 'F' in column names with 'C': temps_c.columns\n",
      "temps_c.columns = temps_c.columns.str.replace('F', 'C')\n",
      "\n",
      "# Print first 5 rows of temps_c\n",
      "print(temps_c.head())\n",
      "\n",
      " multiple()\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "# Read 'sp500.csv' into a DataFrame: sp500\n",
      "sp500 = pd.read_csv('sp500.csv', parse_dates=True, index_col='Date')\n",
      "\n",
      "# Read 'exchange.csv' into a DataFrame: exchange\n",
      "exchange = pd.read_csv('exchange.csv', parse_dates=True, index_col='Date')\n",
      "\n",
      "print(sp500)\n",
      "# Subset 'Open' & 'Close' columns from sp500: dollars\n",
      "dollars = sp500[['Open','Close']]\n",
      "\n",
      "# Print the head of dollars\n",
      "print(dollars.head())\n",
      "\n",
      "# Convert dollars to pounds: pounds\n",
      "pounds = dollars.multiply(exchange['GBP/USD'], axis='rows')\n",
      "\n",
      "# Print the head of pounds\n",
      "print(pounds.head())\n",
      "\n",
      "\n",
      " append\n",
      "import pandas as pd\n",
      "\n",
      "# Load 'sales-jan-2015.csv' into a DataFrame: jan\n",
      "jan = pd.read_csv('sales-jan-2015.csv', parse_dates=True, index_col='Date')\n",
      "\n",
      "# Load 'sales-feb-2015.csv' into a DataFrame: feb\n",
      "feb = pd.read_csv('sales-feb-2015.csv', parse_dates=True, index_col='Date')\n",
      "\n",
      "# Load 'sales-mar-2015.csv' into a DataFrame: mar\n",
      "mar = pd.read_csv('sales-mar-2015.csv', parse_dates=True, index_col='Date')\n",
      "\n",
      "# Extract the 'Units' column from jan: jan_units\n",
      "jan_units = jan['Units']\n",
      "\n",
      "# Extract the 'Units' column from feb: feb_units\n",
      "feb_units = feb['Units']\n",
      "\n",
      "# Extract the 'Units' column from mar: mar_units\n",
      "mar_units = mar['Units']\n",
      "\n",
      "# Append feb_units and then mar_units to jan_units: quarter1\n",
      "quarter1 = jan_units.append(feb_units).append(mar_units)\n",
      "\n",
      "# Print the first slice from quarter1\n",
      "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
      "\n",
      "# Print the second slice from quarter1\n",
      "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])\n",
      "\n",
      "# Compute & print total sales in quarter1\n",
      "print(quarter1.sum())\n",
      "\n",
      "  >concat\n",
      "\n",
      "units = []\n",
      "\n",
      "# Build the list of Series\n",
      "for month in [jan, feb, mar]:\n",
      "    units.append(month['Units'])\n",
      "\n",
      "# Concatenate the list: quarter1\n",
      "quarter1 = pd.concat(units, axis='rows')\n",
      "\n",
      "# Print slices from quarter1\n",
      "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
      "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])\n",
      "\n",
      "  >Concatenation Horizontally\n",
      "\n",
      "# Create a list of weather_max and weather_mean\n",
      "weather_list = [weather_max, weather_mean]\n",
      "\n",
      "# Concatenate weather_list horizontally\n",
      "weather = pd.concat(weather_list, axis=1)\n",
      "\n",
      "# Print weather\n",
      "print(weather)\n",
      "\n",
      " >append several iseries then pivot them into columns\n",
      "\n",
      "medals =[]\n",
      "\n",
      "for medal in medal_types:\n",
      "    # Create the file name: file_name\n",
      "    file_name = \"%s_top5.csv\" % medal\n",
      "    # Create list of column names: columns\n",
      "    columns = ['Country', medal]\n",
      "    # Read file_name into a DataFrame: medal_df\n",
      "    medal_df = pd.read_csv(file_name, header=0, index_col='Country', names=columns)\n",
      "    # Append medal_df to medals\n",
      "    medals.append(medal_df)\n",
      "    print(medals)\n",
      "\n",
      "# Concatenate medals horizontally: medals_df\n",
      "medals_df = pd.concat(medals, axis='columns')\n",
      "\n",
      "# Print medals_df\n",
      "print(medals_df)\n",
      "\n",
      " Concatenation using keys[]\n",
      "\n",
      "medals = pd.concat(medals, keys=['bronze', 'silver', 'gold'])\n",
      "\n",
      " Looking at percent change between years\n",
      "\n",
      "print(china)\n",
      "print(us)\n",
      "# Resample and tidy china: china_annual\n",
      "china_annual = china.resample('A').last().pct_change(10).dropna()\n",
      "\n",
      "# Resample and tidy us: us_annual\n",
      "us_annual = us.resample('A').last().pct_change(10).dropna()\n",
      "\n",
      "# Concatenate china_annual and us_annual: gdp\n",
      "gdp = pd.concat([china_annual, us_annual], axis=1, join='inner')\n",
      "\n",
      "# Resample gdp and print\n",
      "print(gdp.resample('10A').last())\n",
      "\n",
      " >Print head and tail\n",
      "\n",
      "file_path = 'Summer Olympic medallists 1896 to 2008 - IOC COUNTRY CODES.csv'\n",
      "\n",
      "# Load DataFrame from file_path: ioc_codes\n",
      "ioc_codes = pd.read_csv(file_path)\n",
      "\n",
      "# Extract the relevant columns: ioc_codes\n",
      "ioc_codes = ioc_codes[['Country','NOC']]\n",
      "\n",
      "# Print first and last 5 rows of ioc_codes\n",
      "print(ioc_codes.head())\n",
      "print(ioc_codes.tail())\n",
      "\n",
      " Pivot Table with the aggfunc='count'\n",
      "# Construct the pivot_table: medal_counts\n",
      "medal_counts = medals.pivot_table(index='Edition', values='Athlete', columns='NOC', aggfunc='count')\n",
      "\n",
      "# Print the first & last 5 rows of medal_counts\n",
      "print(medal_counts.head())\n",
      "print(medal_counts.tail())\n",
      "\n",
      " > computing fraction of medals\n",
      "\n",
      "# Set Index of editions: totals\n",
      "print(editions.head(5))\n",
      "print(medals.head(5))\n",
      "print(medal_counts.head(5))\n",
      "totals = editions.set_index('Edition')\n",
      "\n",
      "# Reassign totals['Grand Total']: totals\n",
      "totals = totals['Grand Total']\n",
      "\n",
      "# Divide medal_counts by totals: fractions\n",
      "fractions = medal_counts.divide(totals,axis='rows')\n",
      "\n",
      "# Print first & last 5 rows of fractions\n",
      "print(fractions.head())\n",
      "print(fractions.tail())\n",
      "\n",
      "  plot change\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Extract influence['Change']: change\n",
      "change = influence['Change']\n",
      "\n",
      "# Make bar plot of change: ax\n",
      "ax = change.plot(kind='bar')\n",
      "\n",
      "# Customize the plot to improve readability\n",
      "ax.set_ylabel(\"% Change of Host Country Medal Count\")\n",
      "ax.set_title(\"Is there a Host Country Advantage?\")\n",
      "ax.set_xticklabels(editions['City'])\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pivot table.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pivot table.txt\n",
      "  Pivot Table\n",
      "# Add a year column to temperatures\n",
      "temperatures[\"year\"] = temperatures[\"date\"].dt.year\n",
      "\n",
      "# Pivot avg_temp_c by country and city vs year\n",
      "temp_by_country_city_vs_year = temperatures.pivot_table(\"avg_temp_c\", index = [\"country\", \"city\"], columns = \"year\")\n",
      "\n",
      "# See the result\n",
      "print(temp_by_country_city_vs_year)\n",
      "\n",
      "  Subsetting with loc\n",
      "\n",
      "temp_by_country_city_vs_year.loc[\"Egypt\":\"India\"]\n",
      "\n",
      "# Subset for Egypt, Cairo to India, Delhi\n",
      "temp_by_country_city_vs_year.loc[(\"Egypt\", \"Cairo\"):(\"India\", \"Delhi\")]\n",
      "\n",
      "# Subset in both directions at once\n",
      "temp_by_country_city_vs_year.loc[(\"Egypt\", \"Cairo\"):(\"India\", \"Delhi\"), \"2005\":\"2010\"]\n",
      "\n",
      " Unpivot a table\n",
      "\n",
      "\n",
      "def unpivot(frame):\n",
      "    N, K = frame.shape\n",
      "    data = {'value': frame.to_numpy().ravel('F'),\n",
      "            'variable': np.asarray(frame.columns).repeat(N),\n",
      "            'date': np.tile(np.asarray(frame.index), K)}\n",
      "    return pd.DataFrame(data, columns=['date', 'variable', 'value'])\n",
      "\n",
      " Find the max mean in a data set\n",
      "\n",
      "# Get the worldwide mean temp by year\n",
      "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
      "\n",
      "# Find the year that had the highest mean temp\n",
      "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
      "\n",
      "# Get the mean temp by city\n",
      "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=\"columns\")\n",
      "\n",
      "# Find the city that had the lowest mean temp\n",
      "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])\n",
      "\n",
      "\\\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['pivot']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
