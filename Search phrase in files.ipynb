{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\kpi.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\kpi.txt\n",
      "import pandas as pd\n",
      "\n",
      "customer_demographics=pd.read_csv('customer_demographics.csv')\n",
      "\n",
      "uid\n",
      "reg_date\n",
      "device\n",
      "gender\n",
      "country\n",
      "age\n",
      "\n",
      "\n",
      "#customer actions\n",
      "customer_subscriptions=pd.read_csv('customer_subscriptions.csv')\n",
      "\n",
      "print(customer_subscriptions.head())\n",
      "\n",
      "uid\n",
      "lapse_date\n",
      "subscription_date\n",
      "price\n",
      "\n",
      "KPI : conversion rate\n",
      "\n",
      "importance across different user groups\n",
      "\n",
      "sub_data_demo=customer_demographics.merge(\n",
      "\tcustomer_subscriptions,\n",
      "\thow='inner',\n",
      "\ton=['uid']\n",
      "\t)\n",
      "\n",
      "\n",
      "\n",
      "   > sample\n",
      "\n",
      "# Import pandas \n",
      "import pandas as pd\n",
      "\n",
      "# Load the customer_data\n",
      "customer_data = pd.read_csv('customer_data.csv')\n",
      "\n",
      "# Load the app_purchases\n",
      "app_purchases = pd.read_csv('inapp_purchases.csv')\n",
      "\n",
      "# Print the columns of customer data\n",
      "print(customer_data.columns)\n",
      "\n",
      "# Print the columns of app_purchases\n",
      "print(app_purchases.columns)\n",
      "\n",
      "\n",
      "Index(['uid', 'reg_date', 'device', 'gender', 'country', 'age'], dtype='object')\n",
      "\n",
      "Index(['date', 'uid', 'sku', 'price'], dtype='object')\n",
      "\n",
      "# Merge on the 'uid' field\n",
      "uid_combined_data = app_purchases.merge(customer_data, on=['uid'], how='inner')\n",
      "\n",
      "# Examine the results \n",
      "print(uid_combined_data.head())\n",
      "print(len(uid_combined_data))\n",
      "\n",
      "\n",
      "date_x       uid            sku  price      date_y device gender country  age\n",
      "0  2017-07-10  41195147  sku_three_499    499  2017-06-26    and      M     BRA   17\n",
      "1  2017-07-15  41195147  sku_three_499    499  2017-06-26    and      M     BRA   17\n",
      "2  2017-11-12  41195147   sku_four_599    599  2017-06-26    and      M     BRA   17\n",
      "3  2017-09-26  91591874    sku_two_299    299  2017-01-05    and      M     TUR   17\n",
      "4  2017-12-01  91591874   sku_four_599    599  2017-01-05    and      M     TUR   17\n",
      "9006\n",
      "In [1]:\n",
      "\n",
      "\n",
      "# Merge on the 'uid' and 'date' field\n",
      "uid_date_combined_data = app_purchases.merge(customer_data, on=['uid', 'date'], how='inner')\n",
      "\n",
      "# Examine the results \n",
      "print(uid_date_combined_data.head())\n",
      "print(len(uid_date_combined_data))\n",
      "\n",
      "\n",
      " uid             sku  price device gender country  age\n",
      "0  2016-03-30  94055095    sku_four_599    599    iOS      F     BRA   16\n",
      "1  2015-10-28  69627745     sku_one_199    199    and      F     BRA   18\n",
      "2  2017-02-02  11604973  sku_seven_1499    499    and      F     USA   16\n",
      "3  2016-06-05  22495315    sku_four_599    599    and      F     USA   19\n",
      "4  2018-02-17  51365662     sku_two_299    299    iOS      M     TUR   16\n",
      "\n",
      "      . exploratory analysis of kpi\n",
      "\n",
      "1. most companies will have many kpis\n",
      "2. each serves a different purpose\n",
      "\n",
      "#axis=0 is columns\n",
      "#as_index will use group labels as index\n",
      "\n",
      "sub_data_grp=sub_data_deep.groupby(by=['country','device'], axis=0, as_index=False)\n",
      "\n",
      "sub_data_grp.mean()\n",
      "or\n",
      "sub_data_grp.agg('mean')\n",
      "or\n",
      "sub_data_grp.agg(['mean','median'])\n",
      "or\n",
      "sub_data_grp.agg({'price':['mean','median','max'],\n",
      "\t'age':['mean','median','max']\n",
      "\t})\n",
      "\n",
      "def truncate_mean(data):\n",
      "\ttop_val=data.quantile(.9)\n",
      "\tbot_val=data.quantile(.1)\n",
      "\ttrunc_data=data[(data<=top_val) & (data>=bot_val)]\n",
      "\tmean=trunc_data.mean()\n",
      "\treturn (mean)\n",
      "\n",
      "\n",
      "sub_data_grp.agg({'age':[truncated_mean]})\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "# Calculate the mean and median purchase price \n",
      "purchase_price_summary = purchase_data.price.agg(['mean', 'median'])\n",
      "\n",
      "# Examine the output \n",
      "print(purchase_price_summary)\n",
      "\n",
      "mean      406.772596\n",
      "median    299.000000\n",
      "\n",
      "# Calculate the mean and median of price and age\n",
      "purchase_summary = purchase_data.agg({'price': ['mean', 'median'], 'age': ['mean', 'median']})\n",
      "\n",
      "# Examine the output \n",
      "print(purchase_summary)\n",
      "\n",
      "             price        age\n",
      "mean    406.772596  23.922274\n",
      "median  299.000000  21.000000\n",
      "\n",
      "\n",
      "Notice how the mean is higher than the median? This suggests that we have some users who are making a lot of purchases!\n",
      "\n",
      "# Group the data \n",
      "grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])\n",
      "\n",
      "# Aggregate the data\n",
      "purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})\n",
      "\n",
      "# Examine the results\n",
      "print(purchase_summary)\n",
      "\n",
      "\n",
      "price                   \n",
      "                     mean median         std\n",
      "device gender                               \n",
      "and    F       400.747504    299  179.984378\n",
      "       M       416.237308    499  195.001520\n",
      "iOS    F       404.435330    299  181.524952\n",
      "       M       405.272401    299  196.843197\n",
      "\n",
      "       calculating a conversion rate\n",
      "\n",
      "import pandas as pd\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "current_date=pd.to_datetime('2018-03-17')\n",
      "\n",
      "#what is the maximum lapse date in our dataset\n",
      "\n",
      "max_lapse_date = current_date - timedelta(days=7)\n",
      "\n",
      "conv_sub_data=sub_data_demo[(sub_data_demo.lapse_date<max_lapse_date)]\n",
      "\n",
      "\n",
      "total_users_count=conv_sub_data.price.count()\n",
      "print(total_users_count)\n",
      "\n",
      "max_sub_date=conv_sub_data.lapse_date+timedelta(days=7)\n",
      "\n",
      "total_subs=conv_sub_data[\n",
      "(conv_sub_data.price>0) &\n",
      "(conv_sub_data.subscription_data<=max_sub_data)\n",
      "]\n",
      "\n",
      "total_sub_count=total_sub.price.count()\n",
      "print(total_subs_count)\n",
      "\n",
      "conversion rate = Total subscribers/potential subscribers\n",
      "\n",
      "conversion_rate = total_subs_count / total_users_count\n",
      "print(conversion_rate)\n",
      "\n",
      "\n",
      "      cohort conversion rate\n",
      "\n",
      "conv_sub_data = conv_sub_data.copy()\n",
      "\n",
      "#keep users who lapsed prior to the last 14 days\n",
      "\n",
      "max_lapse_date = current_date - timedelta(days=14)\n",
      "\n",
      "conv_sub_data = sub_data_demo[\n",
      " (sub_data_demo.lapse_date <=max_lapse_date)\n",
      "]\n",
      "\n",
      "sub time is the number of days been the lapse date and the subscription date\n",
      "\n",
      "np.where receives a number to return a true and one to return a false\n",
      "\n",
      "sub_time = np. where(\n",
      "\tconv_sub_data.subscription_date.notnull(),\n",
      "\t#then find how many days since their lapse\n",
      "\t(conv_sub_data.scription_date - conv_sub_data.lapse_date).dt.days,\n",
      "\t#else set the value to pd.NaT\n",
      "\tpd.NaT)\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "\n",
      "find the conversion rate gcr7() and gcr14()\n",
      "\n",
      "purchase_cohorts=conv_sub_data.groupby(by=['gender','device'],as_index=False)\n",
      "\n",
      "#find the conversion rate for each cohort using gcr7 and gcr14\n",
      "\n",
      "purchase_cohorts.agg({sub_time:[gcr7,gcr14]})\n",
      "\n",
      "     How to choose KPI metrics\n",
      "\n",
      "how long does it take to gain insight on a metric\n",
      "\n",
      "what is an actionable time scale\n",
      "\n",
      "monthly conversion rate = 1 month wait time\n",
      "\n",
      "leverage exploratory data analysis\n",
      "* reveals relationships between metrics and key results\n",
      "\n",
      "KPI should measure strong growth\n",
      "* potential early warning sign of problems\n",
      "* senstive to changes in the overall ecosystem\n",
      "\n",
      "       sample\n",
      "\n",
      "# Compute max_purchase_date \n",
      "max_purchase_date = current_date - timedelta(days=28)\n",
      "\n",
      "# Filter to only include users who registered before our max date\n",
      "purchase_data_filt = purchase_data[purchase_data.reg_date < max_purchase_date]\n",
      "\n",
      "# Filter to contain only purchases within the first 28 days of registration\n",
      "purchase_data_filt = purchase_data_filt[(purchase_data_filt.date <=\n",
      "                                         purchase_data_filt.reg_date + \n",
      "                                         timedelta(days=28))]\n",
      "\n",
      "# Output the mean price paid per purchase\n",
      "print(purchase_data_filt.price.mean())\n",
      "\n",
      "414.4237288135593\n",
      "\n",
      "\n",
      "      find a 1 month of data\n",
      "\n",
      "# Set the max registration date to be one month before today\n",
      "max_reg_date = current_date - timedelta(days=28)\n",
      "\n",
      "# Find the month 1 values:\n",
      "month1 = np.where((purchase_data.reg_date < max_reg_date) &\n",
      "                    (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),\n",
      "                  purchase_data.price, \n",
      "                  np.NaN)\n",
      "                 \n",
      "# Update the value in the DataFrame \n",
      "purchase_data['month1'] = month1\n",
      "\n",
      "print(month1)\n",
      "\n",
      "# Group the data by gender and device \n",
      "purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False)\n",
      "\n",
      "# Aggregate the month1 and price data \n",
      "purchase_summary = purchase_data_upd.agg(\n",
      "                        {'month1': ['mean', 'median'],\n",
      "                        'price': ['mean', 'median']})\n",
      "\n",
      "# Examine the results \n",
      "print(purchase_summary)\n",
      "\n",
      "gender device      month1              price       \n",
      "                       mean median        mean median\n",
      "0      F    and  388.204545  299.0  400.747504    299\n",
      "1      F    iOS  432.587786  499.0  404.435330    299\n",
      "2      M    and  413.705882  399.0  416.237308    499\n",
      "3      M    iOS  433.313725  499.0  405.272401    299\n",
      "\n",
      "\n",
      "\n",
      "      >.Working with time series\n",
      "\n",
      "exploratory data analysis\n",
      "\n",
      "2nd week subscribers\n",
      "\n",
      "exclude customers who have not been on the platform for two weeks\n",
      "\n",
      "\n",
      "current_date=pd.to_datetime('2018-03-17')\n",
      "max_lapse_date=current_date - timedelta(days=14)\n",
      "\n",
      "#find all uses who have not been on the platform in the last two weeks\n",
      "\n",
      "conv_sub_data= sub_data_demo[\n",
      "  sub_data_demo.lapse_date < max_lapse_date\n",
      "]\n",
      "\n",
      "#how many days before the user subscribed\n",
      "\n",
      "sub_time = conv_sub_data.subscription_date -\n",
      "conv_sub_data.lapse_date\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "or\n",
      "conv_sub_data['sub_time']= conv_sub_data.sub_time.dt.days\n",
      "\n",
      "#find the users who have not subscribed in week one but have been on the platform for two or more weeks\n",
      "\n",
      "conv_base=conv_sub_data[\n",
      " (conv_sub_data.sub_time.notnull()) |\n",
      "\t(conv_sub_data.sub_time > 7)]\n",
      "\n",
      "total_users=len(conv_base)\n",
      "\n",
      "total_sub=np.where(conv_sub_data.sub_time.notnull()) &\n",
      "\t(conv_sub_data.sub_time <= 14,1,0)\n",
      "\n",
      "total_subs=sum(total_subs)\n",
      "\n",
      "conversion_rate = total_subs/total_users\n",
      "\n",
      "output\n",
      "0.009\n",
      "\n",
      "    pandas date parser on read_csv\n",
      "\n",
      "pandas.read_csv(\n",
      "\n",
      "\tparse_dates=False\n",
      "\tinfer_datetime_format=False\n",
      "\tkeep_date_col=False\n",
      "\tdate_parser=None\n",
      "\tdayFirst=False\n",
      "\t)\n",
      "\n",
      "strftime\n",
      "\"%Y-%m-%d\"\n",
      "\"%H:%M:%S\"\n",
      "\n",
      "\"%B %d, %Y\"\n",
      "\n",
      "to_datetime\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_one = pd.to_datetime(date_data_one, format=\"%A %B %d, %Y\")\n",
      "print(date_data_one)\n",
      "\n",
      "output:\n",
      "DatetimeIndex(['2017-01-27', '2017-12-02'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_two = pd.to_datetime(date_data_two, format=\"%Y-%m-%d\")\n",
      "print(date_data_two)\n",
      "\n",
      "output:\n",
      "'2017-01-01', '2016-05-03']\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_three = pd.to_datetime(date_data_three, format=\"%m/%d/%Y\")\n",
      "print(date_data_three)\n",
      "\n",
      "output:\n",
      "'1978-08-17', '1976-01-07'\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_four = pd.to_datetime(date_data_four, format=\"%Y %B %d %H:%M\")\n",
      "print(date_data_four)\n",
      "\n",
      "output:\n",
      "2016-03-01 01:56:00', '2016-01-04 02:16:00'\n",
      "\n",
      "    Creating time series graphs with matplotlib\n",
      "\n",
      "#find all uses who have not been on the platform in the last two weeks\n",
      "\n",
      "conv_sub_data= sub_data_demo[\n",
      "  sub_data_demo.lapse_date < max_lapse_date\n",
      "]\n",
      "\n",
      "#how many days before the user subscribed\n",
      "\n",
      "sub_time = conv_sub_data.subscription_date -\n",
      "conv_sub_data.lapse_date\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "or\n",
      "conv_sub_data['sub_time']= conv_sub_data.sub_time.dt.days\n",
      "\n",
      "#find the users who have not subscribed in week one but have been on the platform for two or more weeks\n",
      "\n",
      "conv_base=conv_sub_data[\n",
      " (conv_sub_data.sub_time.notnull()) |\n",
      "\t(conv_sub_data.sub_time > 7)]\n",
      "\n",
      "total_users=len(conv_base)\n",
      "\n",
      "total_sub=np.where(conv_sub_data.sub_time.notnull()) &\n",
      "\t(conv_sub_data.sub_time <= 14,1,0)\n",
      "\n",
      "total_subs=sum(total_subs)\n",
      "\n",
      "conversion_rate = total_subs/total_users\n",
      "\n",
      "   new stuff\n",
      "conversion_data = conv_sub_data.groupby(\n",
      "\tby=['lapse_date'], as_index=False\n",
      ").agg('sub_time': [gc7]})\n",
      "\n",
      "#produces the week one conversion rate by conversion date.\n",
      "\n",
      "\n",
      "conversion_data.plot(x='lapse_date',y='sub_time')\n",
      "\n",
      "* compare users of different genders\n",
      "* evaluate the impact of a change across regions\n",
      "* see the impact for different devices\n",
      "\n",
      "reformatted_cntry_data=pd.pivot_table(\n",
      "\tconversion_data,\n",
      "\tvalues=['sub_time'],\n",
      "\tcolumns=['country'],\n",
      "\tindex=['reg_data'],\n",
      "\tfill_value=0\n",
      ")\n",
      "\n",
      "reformat_cntry_data.plot(\n",
      "\tx='reg_date',\n",
      "\ty=['BRA','FRA','DEU','TUR','USA','CAN']\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >sample   graph reg_date by first_week_purchases\n",
      "\n",
      "# Group the data and aggregate first_week_purchases\n",
      "\n",
      "user_purchases columns: 'reg_date', 'first_week_purchases'\n",
      "\n",
      "user_purchases = user_purchases.groupby(by=['reg_date', 'uid']).agg({'first_week_purchases': ['sum']})\n",
      "\n",
      "# Reset the indexes\n",
      "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
      "user_purchases.reset_index(inplace=True)\n",
      "\n",
      "# Find the average number of purchases per day by first-week users\n",
      "user_purchases = user_purchases.groupby(by=['reg_date']).agg({'first_week_purchases': ['mean']})\n",
      "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
      "user_purchases.reset_index(inplace=True)\n",
      "\n",
      "# Plot the results\n",
      "user_purchases.plot(x='reg_date', y='first_week_purchases')\n",
      "plt.show()\n",
      "\t\n",
      "\n",
      "   sample pivot table on the first_week_purchases by country\n",
      "\n",
      "# Pivot the data\n",
      "country_pivot = pd.pivot_table(user_purchases_country, values=['first_week_purchases'], columns=['country'], index=['reg_date'])\n",
      "print(country_pivot.head())\n",
      "\n",
      "\n",
      "# Pivot the data\n",
      "device_pivot = pd.pivot_table(user_purchases_device, values=['first_week_purchases'], columns=['device'], index=['reg_date'])\n",
      "print(device_pivot.head())\n",
      "\n",
      "\n",
      "          first_week_purchases          \n",
      "device                      and       iOS\n",
      "reg_date                                 \n",
      "2017-06-01             0.714286  1.000000\n",
      "2017-06-02             1.400000  1.285714\n",
      "2017-06-03             1.545455  1.000000\n",
      "2017-06-04             1.600000  1.833333\n",
      "2017-06-05             1.625000  2.000000\n",
      "\n",
      "\n",
      "# Plot the average first week purchases for each country by registration date\n",
      "country_pivot.plot(x='reg_date', y=['USA', 'CAN', 'FRA', 'BRA', 'TUR', 'DEU'])\n",
      "plt.show()\n",
      "\n",
      "# Plot the average first week purchases for each device by registration date\n",
      "device_pivot.plot(x='reg_date', y=['and', 'iOS'])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     understanding and visualizing trends in customer data\n",
      "\n",
      "usa_subscriptions['sub_day']=(usa_subscriptions.sub_date - usa_subscriptions.lapse_date).dt.days\n",
      "\n",
      "\n",
      "usa_subscriptions = usa_subscriptions[usa_subscriptions.sub_day <=7]\n",
      "\n",
      "usa_subscriptions = usa_subscriptions.groupby(\n",
      "\tby=['sub_date'],as_index=False\n",
      ").agg({'subs':['sum']})\n",
      "\n",
      "\n",
      "     >looking for seasonal change in buying movement\n",
      "\n",
      "Trailing average smoothing technique that averages over a lagging window\n",
      "1. reveal hidden trends by smoothing out seasonality\n",
      "2. average across the period of seasonality\n",
      "3. 7-day window to smooth weekly seasonality\n",
      "4. average out day level effects to produce the average week effect\n",
      "\n",
      "calculate the rolling average over the usa subscribers data with .rolling()\n",
      "\n",
      "rolling_subs = usa_subscriptions.subs.rolling(\n",
      "\twindow=7,\n",
      "\t#specify to average backwards\n",
      "\tcenter=False\n",
      ")\n",
      "\n",
      "usa_subscriptions['rolling_subs']\n",
      "\t=rolling_subs.mean()\n",
      "usa_subscriptions.tail()\n",
      "\n",
      "high_sku_purchases = pd.read_csv(\n",
      "\t'high_sku_purchases.csv',\n",
      "\tparse_dates=True,\n",
      "\tinfer_datetime_format=True\n",
      ")\n",
      "\n",
      "high_sku_purchases.plot(x='date', y='purchases')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "       exponential moving average\n",
      "\n",
      "1. weighted moving (rolling) average\n",
      "\n",
      "* weights more recent items in the window more\n",
      "* applies weights according to an exponential distribution\n",
      "* average back to a central trend without masking any recent movements\n",
      "\n",
      ".ewm() : exponential weighting function\n",
      "\n",
      "\n",
      "window to apply weights over\n",
      "\n",
      "exp_mean=high_sku_purchases.purchases.ewm(span=30)\n",
      "\n",
      "high_sku_purchases['exp_mean'] = exp_mean.mean()\n",
      "\n",
      "\n",
      "   >  sample  > rolling window 7, 28, 365\n",
      "\n",
      "# Compute 7_day_rev\n",
      "daily_revenue['7_day_rev'] = daily_revenue.revenue.rolling(window=7,center=False).mean()\n",
      "\n",
      "# Compute 28_day_rev\n",
      "daily_revenue['28_day_rev'] = daily_revenue.revenue.rolling(window=28,center=False).mean()\n",
      "    \n",
      "# Compute 365_day_rev\n",
      "daily_revenue['365_day_rev'] = daily_revenue.revenue.rolling(window=365,center=False).mean()\n",
      "    \n",
      "# Plot date, and revenue, along with the 3 rolling functions (in order)    \n",
      "daily_revenue.plot(x='date', y=['revenue', '7_day_rev', '28_day_rev', '365_day_rev', ])\n",
      "plt.show()\n",
      "\n",
      "   > sample ewm\n",
      "\n",
      "# Calculate 'small_scale'\n",
      "daily_revenue['small_scale'] = daily_revenue.revenue.ewm(span=10).mean()\n",
      "\n",
      "# Calculate 'medium_scale'\n",
      "daily_revenue['medium_scale'] = daily_revenue.revenue.ewm(span=100).mean()\n",
      "\n",
      "# Calculate 'large_scale'\n",
      "daily_revenue['large_scale'] = daily_revenue.revenue.ewm(span=500).mean()\n",
      "\n",
      "# Plot 'date' on the x-axis and, our three averages and 'revenue'\n",
      "# on the y-axis\n",
      "daily_revenue.plot(x = 'date', y =['revenue', 'small_scale', 'medium_scale', 'large_scale'])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >Events and releases\n",
      "\n",
      "discover the cause of an issue\n",
      "\n",
      "visualizing the drop in conversion rate (3 years)\n",
      "\n",
      "we notice a dip in new user retention\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "conv_sub_data = sub_data_demo(\n",
      "\tsub_data_demo.lapse_date <= max_lapse_date]\n",
      "\n",
      "sub_time = (conv_sub_data.subscription_date -\n",
      "\tconv_sub_data.lapse_date).dt.days\n",
      "\n",
      "conv_sub_date['sub_time']=sub_time\n",
      "\n",
      "conversion_data = conv_sub_data.groupby(\n",
      "\tby=['lapse_data'], as_index=False)\n",
      ".agg({sub_time':[gc7]})\n",
      "\n",
      "conversion_data.plot()\n",
      "plt.show()\n",
      "\n",
      "    >look at the recent six months\n",
      "\n",
      "current_date = pd.to_date('2018-03-17')\n",
      "\n",
      "start_date=current_date - timedelta(days=(6*28))\n",
      "\n",
      "conv_filter=(\n",
      "\tconversion_data.lapse_date >= start_date)\n",
      "\t& (conversion_data.lapse_date <= current_date)\n",
      ")\n",
      "\n",
      "con_data_filt=conversion_data[conv_filter]\n",
      "\n",
      "conv_data_filt.plot(x='lapse_date', y='sub_time')\n",
      "plt.show()\n",
      "\n",
      "* is this drop impacting all users or just specific cohort\n",
      "\n",
      "* this could provide clues on what the issue may be\n",
      "\n",
      "* ecosystems within our data\n",
      "1. distinct countries\n",
      "2. specific device (android or ios)\n",
      "\n",
      "\n",
      "\n",
      "#pivot the results to have one column per country\n",
      "\n",
      "conv_data_cntry = pd.pivot_table(\n",
      "\tconv_data_cntry, values=['sub_time'],\n",
      "\tcolumns=['country'], index=['lapse_date'], fill_value=0\n",
      ")\n",
      "\n",
      "#pivot the results to have one column per device\n",
      "\n",
      "\n",
      "conv_data_device = pd.pivot_table(\n",
      "\tconv_data_cntry, values=['sub_time'],\n",
      "\tcolumns=['device'], index=['lapse_date'], fill_value=0\n",
      ")\n",
      "\n",
      "* all countries experience the drop\n",
      "\n",
      "* most pronounced in Brazil & Turkey\n",
      "\n",
      "* breaking out by device\n",
      "1 the drop only appears on android devices\n",
      "\n",
      "events: holidays and events impacting user behavior\n",
      "\n",
      "events=pd.read_csv('events.csv')\n",
      "1. Date\n",
      "2. Event\n",
      "\n",
      "releases: ios and android software releases\n",
      "\n",
      "releases = pd.read_csv('releases.csv')\n",
      "\n",
      "     >Plot the conversion rate trend per device\n",
      "\n",
      "conv_data_dev.plot(\n",
      "\tx=['lapse_date'], y=['iOS','and']\n",
      ")\n",
      "\n",
      "events.Date = pd.to_datetime(events.Date)\n",
      "\n",
      "#iterate through events and plot each one\n",
      "\n",
      "for row in events.iterrows():\n",
      "\ttmp=row[1]\n",
      "\tplt.axvline(\n",
      "\tx=tmp.Date, color='k', linestyle='---'\n",
      ")\n",
      "\n",
      "\n",
      "#iterate through the releases and plot each one\n",
      "\n",
      "releases.Date = pd.to_datetime(releases.Date)\n",
      "\n",
      "\n",
      "for row in releases.iterrows():\n",
      "\ttmp=row[1]\n",
      "\tif tmp.Event== 'iOS Release':\n",
      "\n",
      "\t\tplt.axvline(\n",
      "\t\tx=tmp.Date, color='b', linestyle='---'\n",
      ")\n",
      "\telse:\n",
      "\t\tplt.axvline(\n",
      "\t\tx=tmp.Date, color='r', linestyle='---'\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "There was an android release in feb/mar aligns with our dip in conversion rate\n",
      "\n",
      "\n",
      "visualizing data over time to uncover hidden trends\n",
      "\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "user_revenue:\n",
      "1. device\n",
      "2. gender\n",
      "3. country\n",
      "4. date \n",
      "5. revenue\n",
      "6. month\n",
      "\n",
      "\n",
      "# Pivot user_revenue\n",
      "pivoted_data = pd.pivot_table(user_revenue, values ='revenue', columns=['device', 'gender'], index='month')\n",
      "pivoted_data = pivoted_data[1:(len(pivoted_data) -1 )]\n",
      "\n",
      "# Create and show the plot\n",
      "pivoted_data.plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "more female bought ios devices\n",
      "\n",
      "      Introduction to A/B testing\n",
      "\n",
      "discoverying causal relationships\n",
      "\n",
      "test two or more variants against each other\n",
      "\n",
      "to evaluate which one performs best\n",
      "\n",
      "in context of a randomized experiment\n",
      "\n",
      "testing two more ideas against each other\n",
      "\n",
      "control: the current state of your product\n",
      "\n",
      "treatment: the variant that you want to test\n",
      "\n",
      "current paywall: I hope you enjoyed your free-trial please consider subscribing\n",
      "\n",
      "proposed paywall: your free-trial has ended, don't miss out, subscribe today\n",
      "\n",
      "randomly select a subset of users and show one set the control and on e the treatment\n",
      "\n",
      "monitor the conversion rates of each group to see which is better\n",
      "\n",
      "by randomly assigning the user we isolate the impact of the change and reduce the potential impact of confounding variables\n",
      "\n",
      "using an assignment criteria may introduce confounders\n",
      "\n",
      "A/B testing can be used to \n",
      "1. improve sales within a mobile application\n",
      "2. increase user interactions with a website\n",
      "3. identify the impact of a medical treatment\n",
      "4. optimize an assembly lines efficiency\n",
      "\n",
      "good problems for ab testing\n",
      "1. where users are being impacted individually\n",
      "2. testing changes that can directly impact their behavior\n",
      "\n",
      "bad problems for ab testing\n",
      "1. challenging to segment the users into groups\n",
      "2. difficult to untangle the impact of the test\n",
      "\n",
      "\n",
      "     >initial ab test design\n",
      "\n",
      "increasing our apps revenue with a/b testing\n",
      "\n",
      "1. test change to our consumable purchase paywall\n",
      "2. increase revenue by increasing the purchase rate\n",
      "\n",
      "general concepts\n",
      "1. a/b testing techniques transfer across a variety of context\n",
      "2. keep in mind how you would apply these techniques\n",
      "\n",
      "    paywall views & demographics data\n",
      "\n",
      "demographics_data = pd.read_csv('user_demographics.csv')\n",
      "demographics_data.head(n=2)\n",
      "\n",
      "1.uid\n",
      "2.reg_date\n",
      "3.device\n",
      "4.gender\n",
      "5.country\n",
      "6.age\n",
      "\n",
      "\n",
      "paywall_views = pd.read_csv('paywall_views.csv')\n",
      "\n",
      "1.uid\n",
      "2.date\n",
      "3.purchase\n",
      "4.sku\n",
      "5.price\n",
      "\n",
      "\n",
      "   >Response variable\n",
      "1. A response variable is used to measure the impact of your change\n",
      "2. should either be a kpi or directly related to a kpi\n",
      "3. something that is easy to measure\n",
      "\n",
      "factors:\n",
      "1. the paywall color\n",
      "\n",
      "variants:\n",
      "1. particular changes you are testing\n",
      "\n",
      "Experimental unit of our test\n",
      "1. the smallest unit you are measuring the change over\n",
      "2. Individual users make a convenient experimental unit\n",
      "\n",
      "purchase_data = demographics_data.merge(\n",
      "\tpaywall_views, how='left', on=['uid'])\n",
      "\n",
      "#find the total purchases for each user\n",
      "\n",
      "total_purchases = purchase_data.groupby(\n",
      "\tby=['uid'], as_index=False).purchase.sum()\n",
      "\n",
      "#find the mean number of purchase per user\n",
      "total_purchases.purchase.mean()\n",
      "\n",
      "print('total purchases average does not make alot of sense, instead try min and max')\n",
      "\n",
      "\n",
      "#find the min and max number of purchases per users in the time period\n",
      "\n",
      "total_purchases.purchase.min()\n",
      "total_purchases.purchase.max()\n",
      "\n",
      "    user days\n",
      "\n",
      "user interactions on a given day\n",
      "1. more convenient than users by itself\n",
      "2. not required to track users actions across time\n",
      "3. can treat simpler actions as responses to the test\n",
      "\n",
      "\n",
      "total_purchases = purchase_data.groupby(\n",
      "\tby=['uid','date'], as_index=False).purchase.sum()\n",
      "\n",
      "total_purchases.purchase.mean()\n",
      "users in the time period\n",
      "total_purchases.purchase.min()\n",
      "total_purchases.purchase.max()\n",
      "\n",
      "\n",
      "   Randomize by user\n",
      "1. best to randomize by individuals regardless of our experimental unit\n",
      "2. otherwise users can have inconsistent experience\n",
      "\n",
      "important to build intuition about your users and data overall\n",
      "\n",
      "\n",
      "   sample  > calculate the user average purchase per day\n",
      "\n",
      "# Extract the 'day'; value from the timestamp\n",
      "purchase_data.date = purchase_data.date.dt.floor('d')\n",
      "\n",
      "# Replace the NaN price values with 0 \n",
      "purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)\n",
      "\n",
      "# Aggregate the data by 'uid' & 'date'\n",
      "purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)\n",
      "revenue_user_day = purchase_data_agg.sum()\n",
      "\n",
      "# Calculate the final average\n",
      "revenue_user_day = revenue_user_day.price.mean()\n",
      "print(revenue_user_day) \n",
      "\n",
      "\n",
      "output:\n",
      "407.33800579385104\n",
      "\n",
      "\n",
      "    Preparing to run an ab test\n",
      "\n",
      "current paywall: \"I hope you are enjoying the relaxing benefits of our app.  Consider making a purchase\"\n",
      "\n",
      "proposed Paywall: \"don't miss out! try one of our new products!\"\n",
      "\n",
      "Questions:\n",
      "Will updating the paywall text impact our revenue\n",
      "How do our three different consumable prices impact this?\n",
      "\n",
      "Considerations in test design\n",
      "1. can our test be run well in practice\n",
      "2. will we be able to derive meaningful results from it\n",
      "\n",
      "Test sensitivity\n",
      "1. What size of impact is meaningful to detect\n",
      "\n",
      "smaller changes are more difficult to detect and can be hidden by randomness\n",
      "\n",
      "Sensitivity is the minimum level of change we want to be able to detect in our tests\n",
      "\n",
      "     Calculating the revenue per user\n",
      "\n",
      "purchase_data = demographics_data.merge(\n",
      "\tpaywall_views, how='left', on=['uid'])\n",
      "\n",
      "total_revenue = purchase_data.groupby(by=['uid'], as_index=False).price.sum()\n",
      "\n",
      "total_revenue.price = np.where(\n",
      "\tnp.isnan(total_revenue.price),0, total_revenue.price)\n",
      "\n",
      "#calculate the average revenue per user\n",
      "\n",
      "avg_revenue = total_revenue.price.mean()\n",
      "\n",
      "print(avg_revenue)\n",
      "16\n",
      "\n",
      "#find the 1% 10% and 20% change in revenue\n",
      "\n",
      "avg_revenue *1 1.01\n",
      "16.32\n",
      "avg_revenue *1 1.10\n",
      "17.77\n",
      "avg_revenue *1 1.20\n",
      "19.39\n",
      "\n",
      "    Data variability\n",
      "1. important to understand the variability in the data\n",
      "2. does the amount spent vary alot among users\n",
      "a. if it does not then it will be easier to detect a change\n",
      "\n",
      "\n",
      "#calculate the standard deviation of revenue per user\n",
      "\n",
      "revenue_variation = total_revenue.price.std()\n",
      "\n",
      "print(revenue_variation)\n",
      "\n",
      "17.520\n",
      "\n",
      "notice the standard deviation is roughly 100% of what the mean average of 16 is.\n",
      "\n",
      "revenue_variation/avg_revenue\n",
      "1.084\n",
      "\n",
      "\n",
      "#find the average number of purchases per user\n",
      "avg_purchases = total_purchases.purchase.mean()\n",
      "3.15\n",
      "\n",
      "purchase_variation = total_purchases.purchase.std()\n",
      "2.68\n",
      "\n",
      "purchase_variation/avg_purchases\n",
      "0.850\n",
      "\n",
      "Primary goal is the increase revenue\n",
      "1. paywall view to purchase conversion rate\n",
      "a. more granular than overall revenue\n",
      "b. directly related to our test\n",
      "\n",
      "Experimental unit: paywall views\n",
      "1. simplest to work with\n",
      "2. assuming these interactions are independent\n",
      "\n",
      "\n",
      "     finding the baseline conversion rate\n",
      "\n",
      "purchase_data = demographic_data.merge(\n",
      "\tpaywall_views, how='inner', on=['uid']\n",
      ")\n",
      "\n",
      "conversion_rate = (sum(purchase_data.purchase) /\n",
      "\tpurchase_data.purchase.count())\n",
      "\n",
      "print(conversion_rate)\n",
      "\n",
      "0.347\n",
      "\n",
      "      sample get the sum and count\n",
      "\n",
      "# Merge and group the datasets\n",
      "purchase_data = demographics_data.merge(paywall_views,  how='left', on=['uid'])\n",
      "purchase_data.date = purchase_data.date.dt.floor('d')\n",
      "\n",
      "# Group and aggregate our combined dataset \n",
      "daily_purchase_data = purchase_data.groupby(by=['uid'], as_index=False)\n",
      "daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})\n",
      "\n",
      "# Find the mean of each field and then multiply by 1000 to scale the result\n",
      "daily_purchases = daily_purchase_data.purchase['sum'].mean()\n",
      "daily_paywall_views = daily_purchase_data.purchase['count'].mean()\n",
      "daily_purchases = daily_purchases * 1000\n",
      "daily_paywall_views = daily_paywall_views * 1000\n",
      "\n",
      "print(daily_purchases)\n",
      "print(daily_paywall_views)\n",
      "\n",
      "3150.0 (purchases)\n",
      "90814.54545454546 (number of views)\n",
      "\n",
      "\n",
      "        calculating lift dependent upon sensitivity\n",
      "\n",
      "small_sensitivity = 0.1 \n",
      "\n",
      "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
      "small_conversion_rate = conversion_rate * (1 + small_sensitivity) \n",
      "\n",
      "# Apply the new conversion rate to find how many more users per day that translates to\n",
      "small_purchasers = daily_paywall_views * small_conversion_rate\n",
      "\n",
      "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
      "purchaser_lift = small_purchasers - daily_purchases\n",
      "\n",
      "print(small_conversion_rate)\n",
      "print(small_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "\n",
      "0.03814800000000001 (small conversion rate)\n",
      "3499.384706400001 (small purchasers)\n",
      "317.58470640000087 (lift)\n",
      "\n",
      "  > medium sensitivity\n",
      "\n",
      "medium_sensitivity = 0.2\n",
      "\n",
      "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
      "medium_conversion_rate = conversion_rate * (1 + medium_sensitivity) \n",
      "\n",
      "# Apply the new conversion rate to find how many more users per day that translates to\n",
      "medium_purchasers = daily_paywall_views * medium_conversion_rate\n",
      "\n",
      "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
      "purchaser_lift = medium_purchasers - daily_purchases\n",
      "\n",
      "print(medium_conversion_rate)\n",
      "print(medium_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "0.041616 (4% conversion rate)\n",
      "3817.5105888000003 (purchasers)\n",
      "635.7105888000001 (lift)\n",
      "\n",
      "     large sensitivity\n",
      "\n",
      "large_sensitivity = 0.5\n",
      "\n",
      "# Find the conversion rate lift with the sensitivity above\n",
      "large_conversion_rate = conversion_rate * (1 + large_sensitivity)\n",
      "\n",
      "# Find how many more users per day that translates to\n",
      "large_purchasers = daily_paywall_views * large_conversion_rate\n",
      "purchaser_lift = large_purchasers - daily_purchases\n",
      "\n",
      "print(large_conversion_rate)\n",
      "print(large_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "0.052020000000000004\n",
      "4771.888236000001\n",
      "1590.0882360000005\n",
      "\n",
      "Awesome! While it seems that a 50% increase may be too drastic and unreasonable to expect, the small and medium sensitivities both seem very reasonable.\n",
      "\n",
      "\n",
      "       standard error\n",
      "\n",
      "\n",
      "# Find the n & v quantities\n",
      "n = purchase_data.purchase.count()\n",
      "\n",
      "# Calculate the quantity \"v\"\n",
      "v = conversion_rate * (1 - conversion_rate) \n",
      "\n",
      "# Calculate the variance and standard error of the estimate\n",
      "var = v / n \n",
      "se = var**0.5\n",
      "\n",
      "print(var)\n",
      "print(se)\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "3.351780834114284e-07\n",
      "0.0005789456653360731\n",
      "\n",
      "     >calculating sample size\n",
      "\n",
      "what is the null hypothesis\n",
      "\n",
      "1. hypothesis that control and treatment have the same impact on response\n",
      "a. updated paywall does not improve conversion rate\n",
      "b. any observed difference is due to randomness\n",
      "\n",
      "rejecting the null hypothesis\n",
      "a. determine their is a difference between the treatment and control\n",
      "b. we say the test has statistical significances\n",
      "\n",
      "\n",
      "\n",
      "Null hypothesis\n",
      "\n",
      "     \ttrue   \t\tfalse\n",
      "accept\tcorrect\t\ttype II error\n",
      "reject\ttype I error\tcorrect\n",
      "\n",
      "types of error & confidence level\n",
      "1. probablilty of not making type 1 error\n",
      "2. higher this value, larger the test sample needed\n",
      "\n",
      "common values is 0.95\n",
      "\n",
      "     >Statistical power\n",
      "\n",
      "statistical power is the probability of finding a statistically siginificant result when the null hypothesis is false\n",
      "\n",
      "confidence level\n",
      "standard error\n",
      "statistical power\n",
      "test sensitivity\n",
      "\n",
      "\n",
      "as the sample size increases so does our power increase\n",
      "\n",
      "\n",
      "    calculating our needed sample size\n",
      "\n",
      "baseline conversion rate 0.3468\n",
      "confidence level: 0.95\n",
      "desired power: 0.80\n",
      "sensitivity=0.1\n",
      "\n",
      "sample_size_group=get_sample(size(0.8, conversion_rate *1.1, 0.95)\n",
      "\n",
      "print(sample_size_per_group)\n",
      "\n",
      "output:\n",
      "45788\n",
      "\n",
      "\n",
      "      >generality of this function\n",
      "\n",
      "function shown specific to conversion rate calculations\n",
      "\n",
      "different response variables have different buy analogous formulas\n",
      "\n",
      "\n",
      "  > decreasing the need sample size\n",
      "\n",
      "* choose a unit of observation with lower variability\n",
      "\n",
      "* excluding users irrelevant to the process/change\n",
      "\n",
      "* think through how different factors relate to the sample size\n",
      "\n",
      "\n",
      "\n",
      "       increase the confidence level\n",
      "\n",
      "# Look at the impact of sample size increase on power\n",
      "n_param_one = get_power(n=1000, p1=p1, p2=p2, cl=cl)\n",
      "n_param_two = get_power(n=2000, p1=p1, p2=p2, cl=cl)\n",
      "\n",
      "# Look at the impact of confidence level increase on power\n",
      "alpha_param_one = get_power(n=n1, p1=p1, p2=p2, cl=0.8)\n",
      "alpha_param_two = get_power(n=n1, p1=p1, p2=p2, cl=0.95)\n",
      "    \n",
      "# Compare the ratios\n",
      "print(n_param_two / n_param_one)\n",
      "print(alpha_param_one / alpha_param_two)\n",
      "\n",
      "\n",
      "1.7596440001351992  (change sample size)\n",
      "1.8857367092232278  (change confidence levels)\n",
      "\n",
      "\n",
      "\n",
      "With these particular values it looks like decreasing our confidence level has a slightly larger impact on the power than increasing our sample size\n",
      "\n",
      "      calculate the conversion rate\n",
      "\n",
      "# Merge the demographics and purchase data to only include paywall views\n",
      "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
      "                            \n",
      "# Find the conversion rate\n",
      "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
      "            \n",
      "print(conversion_rate)\n",
      "\n",
      "0.03468607351645712\n",
      "\n",
      "    > calculate sample size\n",
      "\n",
      "# Merge the demographics and purchase data to only include paywall views\n",
      "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
      "                            \n",
      "# Find the conversion rate\n",
      "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
      "\n",
      "# Desired Power: 0.8\n",
      "# CL: 0.90\n",
      "# Percent Lift: 0.1\n",
      "p2 = conversion_rate * (1 + 0.1)\n",
      "sample_size = get_sample_size(0.8, conversion_rate, p2, 0.90)\n",
      "print(sample_size)\n",
      "\n",
      "36101\n",
      "\n",
      "\n",
      "# Desired Power: 0.95\n",
      "# CL 0.90\n",
      "# Percent Lift: 0.1\n",
      "p2 = conversion_rate * (1 + 0.1)\n",
      "sample_size = get_sample_size(0.95, conversion_rate, p2, 0.90)\n",
      "print(sample_size)\n",
      "\n",
      "63201\n",
      "\n",
      "\n",
      "      analyzing the ab test results\n",
      "\n",
      "compare the two groups purchase rates\n",
      "\n",
      "test_demographics = pd.read_csv('test_demographics.csv')\n",
      "\n",
      "#results for our ab test\n",
      "#group column c for control | v for variant\n",
      "\n",
      "test_results=pd.read_csv('ab_test_results.csv')\n",
      "test_results.head()\n",
      "\n",
      "uid\n",
      "date\n",
      "purchase\n",
      "sku\n",
      "price\n",
      "group\n",
      "\n",
      "\n",
      "    confirming our test results\n",
      "\n",
      "does the data look reasonable\n",
      "\n",
      "\n",
      "test_results_grpd = test_results.groupby(\n",
      "\tby=['group'], as_index=False)\n",
      "\n",
      "test_results_grpd.uid.count()\n",
      "\n",
      "48236\n",
      "49867\n",
      "\n",
      "\n",
      "test_results_demo = test_results.merge(\n",
      "\ttest_demo, how='inner', on='uid')\n",
      "\n",
      "test_results_grpd = test_results_demo.groupby(\n",
      "\tby=['country','gender','device','group'],\n",
      "as_index=False)\n",
      "\n",
      "test_results_grd.uid.count()\n",
      "\n",
      "\n",
      "    > find the mean conversion\n",
      "\n",
      "test_results_summary= test_results_demo.groupby(\n",
      "\tby=['group'], as_index=False\n",
      ").agg({'purchase':['count','sum']})\n",
      "\n",
      "test_results_summary['conv'] = (test_results_summary.purchase['sum']/\n",
      "\ttest_results_summary.purchase['count'])\n",
      "\n",
      "test_results_summary\n",
      "\n",
      "grp  sum   count   conversion\n",
      "c    48236 1657    0.034351\n",
      "v    49867 2094    0.041984\n",
      "\n",
      "Is the result statistically significant\n",
      "1. are the conversion rates different enough\n",
      "2. if yes then reject the null hypothesis\n",
      "3. conclude that the paywalls have different effects\n",
      "4. if no then it may just be randomness\n",
      "\n",
      "    p -value\n",
      "\n",
      "probability if the null hypothesis is true\n",
      "\n",
      "of observing a value as or more extreme\n",
      " \n",
      "what does a low p-value mean\n",
      "1. the power is low\n",
      "2. the observation is unlikely to happen due to randomness\n",
      "\n",
      "\n",
      "<0.01 very strong evidence against the null hypothesis\n",
      "\n",
      "0.01-0.5 strong evidence against the null hypothesis\n",
      "0.05-1. very weak evidence against the null hypothesis\n",
      ">0.1 small or no evidence against the null hypothesis\n",
      "\n",
      "\n",
      "?     sample test the null hypothesis\n",
      "\n",
      "# Compute and print the results\n",
      "results = ab_test_results.groupby('group').agg({'uid':pd.Series.nunique}) \n",
      "print(results)\n",
      "\n",
      "\n",
      "   uid\n",
      "group        \n",
      "C      2825.0\n",
      "V      2834.0\n",
      "\n",
      "\n",
      "# Find the unique users in each group \n",
      "results = ab_test_results.groupby('group').agg({'uid': pd.Series.nunique}) \n",
      "\n",
      "# Find the overall number of unique users using \"len\" and \"unique\"\n",
      "unique_users = len(ab_test_results.uid.unique()) \n",
      "\n",
      "# Find the percentage in each group\n",
      "results = results / unique_users * 100\n",
      "print(results)\n",
      "\n",
      "     uid\n",
      "group           \n",
      "C      49.920481\n",
      "V      50.079519\n",
      "\n",
      "\n",
      "   find the number of users in group device and gender\n",
      "\n",
      "# Find the unique users in each group, by device and gender\n",
      "results = ab_test_results.groupby(by=['group', 'device', 'gender']).agg({'uid': pd.Series.nunique}) \n",
      "\n",
      "# Find the overall number of unique users using \"len\" and \"unique\"\n",
      "unique_users = len(ab_test_results.uid.unique())\n",
      "\n",
      "# Find the percentage in each group\n",
      "results = results / unique_users * 100\n",
      "print(results)\n",
      "\n",
      "uid\n",
      "group device gender           \n",
      "C     and    F       14.896625\n",
      "             M       13.518289\n",
      "      iOS    F       11.309419\n",
      "             M       10.196148\n",
      "V     and    F       14.861283\n",
      "             M       13.659657\n",
      "      iOS    F       10.920657\n",
      "             M       10.637922\n",
      "\n",
      "\n",
      "     understanding statistical significance\n",
      "\n",
      "distribution of expected difference between control and test groups _if_ the null hypothesis is true\n",
      "\n",
      "The red line is the observed difference in the conversion rates from our tests\n",
      "\n",
      "p-value: probability of being as or more extreme than the red line on either side of the distribution.\n",
      "\n",
      "\n",
      "def get_pvalue ( con_conv, test_conv, con_size, test_size):\n",
      "\n",
      "\tlift= - abs(test_conv - con_conv)\n",
      "\tscale_one = con_conv * (1-con_conv) * (1/con_size)\n",
      "\tscale_two= test_conv * (1-test_conv) * (1/test_size)\n",
      "\tscale_val = (scale_one + scale_two) **0.5\n",
      "\tp_value=2*stats.norm.cdf(lift, loc=0, scale=scale_val)\n",
      "\treturn p_value\n",
      "\n",
      "\n",
      "con_conv=0.034351\n",
      "test_conv=0.041984\n",
      "con_size=48236\n",
      "test_size=49867\n",
      "\n",
      "p_value=get_pvalue(con_conv,test_conv,con_size,test_size)\n",
      "print(p_value)\n",
      "\n",
      "4.2572974 e-10  (extremely small p-value)\n",
      "\n",
      "accept the null hypothesis\n",
      "\n",
      "\n",
      "    find the power of the test\n",
      "\n",
      "def get_power(n, p1, p2, cl):\n",
      "    alpha = 1 - cl\n",
      "    qu = stats.norm.ppf(1 - alpha/2)\n",
      "    diff = abs(p2-p1)\n",
      "    bp = (p1+p2) / 2\n",
      "    \n",
      "    v1 = p1 * (1-p1)\n",
      "    v2 = p2 * (1-p2)\n",
      "    bv = bp * (1-bp)\n",
      "    \n",
      "    power_part_one = stats.norm.cdf((n**0.5 * diff - qu * (2 * bv)**0.5) / (v1+v2) ** 0.5)\n",
      "    power_part_two = 1 - stats.norm.cdf((n**0.5 * diff + qu * (2 * bv)**0.5) / (v1+v2) ** 0.5)\n",
      "    \n",
      "    power = power_part_one + power_part_two\n",
      "    \n",
      "    return (power)\n",
      "\n",
      "\n",
      "power= get_power (test_size, con_conv, test_conv, 0.95)\n",
      "print(power)\n",
      "0.9999925941372282\n",
      "\n",
      "\n",
      "small p-value and nearly perfect power\n",
      "\n",
      "        confidence interval\n",
      "\n",
      "ranges of values for our estimation rather than a single number\n",
      "\n",
      "provides context for our estimation process\n",
      "\n",
      "series of repeated experiments\n",
      "1. the calculated intervals will contain the true parameter x% of the time\n",
      "2. the true conversion rate is fixed quantity, it is the interval that is random not the conversion rate.\n",
      "\n",
      "\n",
      "The estimated parameter or difference in conversion rate follows a normal distribution\n",
      "\n",
      "1. we can estimate the standard deviation\n",
      "2. the mean of this distribution\n",
      "\n",
      "alpha is the desired confidence interval width\n",
      "\n",
      "bounds containing X% of hte probabilty around the mean (95%) of that distribution\n",
      "\n",
      "\n",
      "from scipy import stats\n",
      "\n",
      "def get_ci(test_conv, con_conv, test_size, con_size, ci):\n",
      "\n",
      "\tsd=((test_conv * (1-test_conv))/test_size+\n",
      "\t(con_conv * (1-con_conv)) / con_size)**0.5\n",
      "\n",
      "\tlift=test_conv - con_conv\n",
      "\n",
      "\tval=stats.norm.isf((1-ci)/2)\n",
      "\tlwr_bnd=lift - val *sd\n",
      "\tupr_bnd=lift+ val*sd\n",
      "\treturn ((lwr_bnd,upr_bnd))\n",
      "\n",
      "\n",
      "    get p-value\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=0.1, test_conv=0.17, con_size=1000, test_size=1000)\n",
      "print(p_value)\n",
      "\n",
      "4.131297741047306e-06\n",
      "\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=.1, test_conv=.15, con_size=100, test_size=100)\n",
      "print(p_value) \n",
      "\n",
      "0.28366948940702086\n",
      "\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=.48, test_conv=.5, con_size=1000, test_size=1000)\n",
      "print(p_value)\n",
      "\n",
      "0.370901935824383\n",
      "\n",
      "\n",
      "To recap we observed that a large lift makes us confident in our observed result, while a small sample size makes us less so, and ultimately high variance can lead to a high p-value!\n",
      "\n",
      "\n",
      "    check for statistically signficant\n",
      "\n",
      "\n",
      "cont_conv=0.09096495570387314 \n",
      "test_conv=0.1020053238686779 \n",
      "con_size=5329 \n",
      "test_size=5748\n",
      "\n",
      "# Compute the p-value\n",
      "p_value = get_pvalue(con_conv=cont_conv, test_conv=test_conv, con_size=cont_size, test_size=test_size)\n",
      "print(p_value)\n",
      "\n",
      "# Check for statistical significance\n",
      "if p_value >= 0.05:\n",
      "    print(\"Not Significant\")\n",
      "else:\n",
      "    print(\"Significant Result\")\n",
      "\n",
      "\n",
      "  > confidence interval\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, 0.975, 0.5)\n",
      "print(confidence_interval)\n",
      "\n",
      "(0.9755040421682947, 1.0244959578317054)\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, .95, 2)\n",
      "print(confidence_interval)\n",
      "\n",
      "2 standard deviations\n",
      "\n",
      "(0.6690506448818785, 1.3309493551181215)\n",
      "\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, 0.95, .001)\n",
      "print(confidence_interval)\n",
      "\n",
      "(1.0, 1.0)\n",
      "\n",
      "\n",
      "As our standard deviation decreases so too does the width of our confidence interval. Great work!\n",
      "\n",
      "\n",
      "con_conv=0.034351\n",
      "test_conv=0.041984\n",
      "con_size=48236\n",
      "test_size=49867\n",
      "ci=.95\n",
      "\n",
      "# Calculate the mean of our lift distribution \n",
      "lift_mean = test_conv -cont_conv\n",
      "\n",
      "# Calculate variance and standard deviation \n",
      "lift_variance = (1 - test_conv) * test_conv /test_size + (1 - cont_conv) * cont_conv / cont_size\n",
      "lift_sd = lift_variance**0.5\n",
      "\n",
      "# Find the confidence intervals with cl = 0.95\n",
      "confidence_interval = get_ci(lift_mean, 0.95,lift_sd)\n",
      "print(confidence_interval)\n",
      "\n",
      "confidence interval:\n",
      "(0.011039999822042502, 0.011040000177957487)\n",
      "\n",
      "Notice that our interval is very narrow thanks to our substantial lift and large sample size.\n",
      "\n",
      "      interpreting your results\n",
      "\n",
      "report \n",
      "\t\tTest Group\tControl Group\n",
      "1. Sample size  \t7030\t6970\n",
      "2. run time\t2 weeks\t\t2 weeks\n",
      "3. mean\t\t3.12\t\t2.69\n",
      "4. variance\t3.20\t\t2.64\n",
      "5. est lift\t0.56\n",
      "6. conf level\t0.56 += 0.4\n",
      "\n",
      "* significant at the 0.05 level\n",
      "\n",
      "visualization\n",
      "\n",
      "histograms: bucketed counts of observations across values\n",
      "\n",
      "user data rolled up to group and user level\n",
      "uid\n",
      "group\n",
      "purchase\n",
      "\n",
      "var=results[results.group=='V']\n",
      "con=results[results.group=='C']\n",
      "\n",
      "plt.hist(var['purchase'],color='yellow',\n",
      "\talpha=0.8, bins=50, label='Test')\n",
      "plt.hist(con['purchase'], color='blue',\n",
      "\talpha=0.8, bins=50, label='Control')\n",
      "plt.legend(loc='upper right')\n",
      "\n",
      "\n",
      "plt.axvline(x= np.mean(results.purchase),\n",
      "\tcolor='red')\n",
      "plt.axvline(x=np.mean(results.purchase),\n",
      "\tcolor='green')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >plotting a distribution\n",
      "\n",
      "mean_con=0.090965\n",
      "mean_test=0.102005\n",
      "var_con=(mean_con * (1-mean_con))/58583\n",
      "var_test=(mean_test *(1-mean_test))/56350\n",
      "\n",
      "con_line = np.linspace(-3*var_con**0.5+mean_con, 3*var_con**0.5+mean_con, 100)\n",
      "test_line=np.linspace(-3*var_test**0.5+mean_test, 3*var_test**0.5+mean_test, 100)\n",
      "\n",
      "\n",
      "\n",
      "#plot the probabilities across the distribution of conversion rates\n",
      "\n",
      "plt.plot(con_line, norm.pdf(\n",
      "    con_line, mean_con, var_con**0.5)\n",
      ")\n",
      "\n",
      "plt.plot(test_line, norm.pdf(\n",
      "    test_line, mean_test, var_test**0.5)\n",
      ")\n",
      "plt.show()\n",
      "\n",
      "mlab.normpdf(): converts values to probablities from Normal Distribution\n",
      "\n",
      "   plotting the difference of conversion rates\n",
      "\n",
      "lift= mean_test - mean_control\n",
      "var = var_test + var_control\n",
      "\n",
      "variance is the sum of variances\n",
      "\n",
      "diff_line = np.linspace(-3*var**0.5 + lift,\n",
      "3*var**0.5 + lift, 100)\n",
      "\n",
      "plt.plot(diff_line, mlab.normpdf(\n",
      "\tdiff_line, lift, var**0.5)\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     plotting the confidence interval\n",
      "\n",
      "section = np.arange(0.007624, 0.01445,1/10000)\n",
      "\n",
      "\n",
      "#fill in between these boundaries\n",
      "\n",
      "plt.fill_between(\n",
      "\tsection,\n",
      "\tmlab.normpdf(section, lift, var**0.5)\n",
      ")\n",
      "\n",
      "    sample   > show the control and test distributions do not intersect\n",
      "\n",
      "# Compute the standard deviations\n",
      "control_sd = cont_var**0.5\n",
      "test_sd = test_var**0.5\n",
      "\n",
      "# Create the range of x values \n",
      "control_line = np.linspace( cont_conv - 3 * control_sd, cont_conv + 3 * control_sd , 100)\n",
      "test_line = np.linspace( test_conv - 3 * test_sd,  test_conv + 3 * test_sd , 100)\n",
      "\n",
      "# Plot the distribution \n",
      "plt.plot(control_line, mlab.normpdf(control_line, cont_conv, control_sd))\n",
      "plt.plot(test_line, mlab.normpdf(test_line,test_conv, test_sd))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "We see no overlap, which intuitively implies that our test and control conversion rates are significantly distinct.\n",
      "\n",
      "     sample show  > show the confidence intervals and show the lift mean\n",
      "\n",
      "# Find the lift mean and standard deviation\n",
      "lift_mean = np.mean(test_conv-con_conv)\n",
      "lift_sd = (var_test + var_con) ** 0.5\n",
      "\n",
      "# Generate the range of x-values\n",
      "lift_line = np.linspace(lift_mean - 3 * lift_sd, lift_mean + 3 * lift_sd, 100)\n",
      "\n",
      "# Plot the lift distribution\n",
      "plt.plot(lift_line, norm.pdf(lift_line, lift_mean, lift_sd))\n",
      "\n",
      "ci_lower,ci_upper = get_ci(test_conv, con_conv, test_size, con_size,ci)\n",
      "\n",
      "# Add the annotation lines\n",
      "plt.axvline(x = lift_mean, color = 'green')\n",
      "plt.axvline(x = ci_lower, color = 'red')\n",
      "plt.axvline(x = ci_upper, color = 'red')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\predictive analytics build the basetable.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\predictive analytics build the basetable.txt\n",
      "A predictive model can be used to predict and event\n",
      "\n",
      "basetable\n",
      "1. population\n",
      "2. candidate predictors\n",
      "3. target (0 or 1)\n",
      "\n",
      "draw a time line\n",
      "\n",
      "\n",
      "target period (50 euros in the next three months)\n",
      "\n",
      "\n",
      "start_target = datetime(year=2018, month=5, day=1)\n",
      "end_target = datetime(year=2018, month=8, day=1)\n",
      "\n",
      "gifts_target=gifts[(gifts[\"date\"]>= start_target) & gifts[\"date\"]<end_target)]\n",
      "\n",
      "\n",
      "Donation Id\n",
      "donor id\n",
      "donation date\n",
      "donation amount.group\n",
      "\n",
      "donations for 2016 and 2017\n",
      "\n",
      "  Sample   > Filter by start date\n",
      "\n",
      "# Start of the target is January 1st 2017\n",
      "start_target = datetime(year=2017, month=1, day=1)\n",
      "\n",
      "print(gifts.columns)\n",
      "# Select gifts made before start_target\n",
      "gifts_before_2017 = gifts[gifts['date'] < start_target]\n",
      "\n",
      "# Print the number of donations in gifts_before_2017\n",
      "print(len(gifts_before_2017))\n",
      "\n",
      "\n",
      "There are two columns in the pandas dataframe basetable: \"amount_2017\" is the total amount of donations in 2017, and \"target\" is 1 if this amount is larger than 30 and 0 else.\n",
      "\n",
      "\n",
      "  >Sample    build the basetable\n",
      "\n",
      "# Select the relevant predictors and the target\n",
      "X = basetable[[\"amount_2017\"]]\n",
      "y = basetable[[\"target\"]]\n",
      "\n",
      "# Build the logistic regression model\n",
      "logreg = linear_model.LogisticRegression()\n",
      "logreg.fit(X, y)\n",
      "\n",
      "# Make predictions for X\n",
      "predictions = logreg.predict_proba(X)[:,1]\n",
      "\n",
      "# Calculate and print the AUC value\n",
      "auc = roc_auc_score(y, predictions)\n",
      "print(round(auc, 2))\n",
      "\n",
      "       >Population requirements\n",
      "\n",
      "the population should be eligible for being a target\n",
      "* address available\n",
      "* privacy settings\n",
      "\n",
      "\n",
      "Age, Gender, previous gifts, and donated(target)\n",
      "\n",
      "\n",
      "get donation counts for the previous year\n",
      "\n",
      "\n",
      "   >using set\n",
      "\n",
      "donations_2016=gifts[gifts[\"date].dt.year==2016]\n",
      "donors_include=set(donations_2016[\"id\"])\n",
      "\n",
      "print(donors_include)\n",
      "\n",
      "\n",
      "next\n",
      "\n",
      "\n",
      "donations_2017=gifts[(gifts[\"date].dt.year==2017)\n",
      "& (gifts[\"date\"].dt.month<5)]\n",
      "dono\n",
      "donors_include=set(donations_2017[\"id\"])\n",
      "\n",
      "population= donors_include.difference(donors_exclude)\n",
      "\n",
      "the population include a set of donors that made a donation in 2016, but not between may 1 2017\n",
      "\n",
      "\n",
      "  >sample  donors between 2013 and 2018 that made a donation but did not make a donation after 2017 \n",
      "\n",
      "# Gifts made in 2013 or later\n",
      "gifts_include = gifts[gifts[\"date\"].dt.year >= 2013]\n",
      "\n",
      "# Gifts made in 2017 or later\n",
      "gifts_exclude = gifts[gifts[\"date\"].dt.year >= 2017]\n",
      "\n",
      "# Set with ids in gifts_include\n",
      "donors_include = set(gifts_include[\"id\"])\n",
      "\n",
      "# Set with ids in gifts_exclude\n",
      "donors_exclude = set(gifts_exclude[\"id\"])\n",
      "\n",
      "# Population\n",
      "population = donors_include.difference(donors_exclude)\n",
      "print(len(population))\n",
      "\n",
      "\n",
      "  > Sample   > filter a population\n",
      "\n",
      "# Create a dataframe donors_population\n",
      "donors_population = donors[(donors[\"address\"] == 1) & (donors[\"letter_allowed\"] == 1)]\n",
      "\n",
      "# Create a list of donor IDs\n",
      "population_list = list(donors_population[\"donor_id\"])\n",
      "\n",
      "# Select unique donors in population_list\n",
      "population = set(population_list)\n",
      "print(len(population))\n",
      "\n",
      "\n",
      "        >Add a target to the basetable\n",
      "\n",
      "1. if the conditions of the event occur than the target is 1 otherwise it is 0\n",
      "\n",
      "2.  The target should be based on a previous time line\n",
      "a. aug 2017 to sept 2017\n",
      "b. check if the donor donated\n",
      "\n",
      "unsubscribed_2017[:5]\n",
      "a. donor_id that unsubscribed in 2017\n",
      "\n",
      "basetable[\"target\"] = pd.Series([1 if donor_id in unsubscribe_2017 else 0 for donor_id in basetable[\"donor_id\"]])\n",
      "\n",
      "\n",
      "  filter donation in 2017\n",
      "\n",
      "start_target=datetime(year=2017, month=1, day=1)\n",
      "end_target=datetime(year=2018, month=1, day=1)\n",
      "\n",
      "\n",
      "gifts_target=gifts(gifts[\"date\"]>=start_target)\n",
      "& (gifts[\"date\"]<end_target)]\n",
      "\n",
      "\n",
      "gifts_target_byid=gifts_target.groupby(\"id\")[\"amount\"].sum().reset_index()\n",
      "\n",
      "mask=gifts_target_byid[\"amount\"]>500\n",
      "\n",
      "targets=list(gifts_target_byid[\"id\"][mask])\n",
      "\n",
      "basetable[\"target\"] = pd.Series([1 if donor_id in targets else 0 for donor_id in basetable[\"donor_id\"]])\n",
      "\n",
      "    sample    create target and calculate incidence\n",
      "\n",
      "\n",
      "basetable = pd.DataFrame(population, columns=[\"donor_id\"])\n",
      "\n",
      "# Add target to the basetable\n",
      "basetable[\"target\"] = pd.Series([1 if donor_id in attend_event else 0 for donor_id in basetable[\"donor_id\"]])\n",
      "\n",
      "# Calculate and print the target incidence\n",
      "print(round(basetable[\"target\"].sum() / len(basetable), 2))\n",
      "\n",
      "\n",
      "   sample  > create target by filter and calculate incidence\n",
      "\n",
      "print(basetable)\n",
      "print(gifts_201701)\n",
      "# Sum of donations for each donor in gifts_201701\n",
      "gifts_summed = gifts_201701.groupby(\"id\")[\"amount\"].sum().reset_index()\n",
      "\n",
      "# List with targets\n",
      "targets = list(gifts_summed[\"id\"][gifts_summed[\"amount\"] > 50])\n",
      "\n",
      "# Add targets to the basetable\n",
      "basetable[\"target\"] = pd.Series([1 if donor_id in targets else 0 for donor_id in basetable[\"donor_id\"]])\n",
      "\n",
      "# Calculate and print the target incidence\n",
      "print(round(basetable[\"target\"].sum() / len(basetable), 2))\n",
      "\n",
      "\n",
      "  Predictive varaibles\n",
      "\n",
      "demographics: \n",
      "age, gender, living place,\n",
      "spending behavior\n",
      "watching behavior\n",
      "product usage\n",
      "surfing behavior\n",
      "payment information\n",
      "\n",
      "variables need to be compliant with the time line\n",
      "\n",
      "  lifetime date\n",
      "\n",
      "where 4/1/2018 is the beginning of the target period\n",
      "\n",
      "reference_date = datetime.date(2018,4,1)\n",
      "basetable[\"lifetime\"]=reference_date - basetable[\"member_since\"]\n",
      "print(basetable.head())\n",
      "\n",
      "\n",
      "  >Contact channel\n",
      "\n",
      "donor_id\n",
      "start_valid_date\n",
      "end_valid_date\n",
      "contact_channel\n",
      "\n",
      "reference_date=datetime.date(2018,4,1)\n",
      "contact_change_reference_date=\n",
      "living_places[\n",
      "(contact_channel['start_valid_date']<=reference_date)\n",
      "&\n",
      "(living_places[\"end_valid_date\"]>reference_date)\n",
      "]\n",
      "\n",
      "\n",
      "  >merge\n",
      "\n",
      "basetable=\n",
      "\tpd.merge(\n",
      "\tbasetable,\n",
      "\tliving_places_reference_date[[\"donor_ID\",\"contact_channel\"]],\n",
      "\ton=\"donor_ID\"\n",
      ")\n",
      "\n",
      "print(basetable.head())\n",
      "\n",
      "  > Sample   >  calculating age\n",
      "\n",
      "predictive variables (may 1, 2017) target period (jul 1, 2017)\n",
      "# Reference date\n",
      "\n",
      "reference_date = datetime.date(2017, 5, 1)\n",
      "\n",
      "# Add age to the basetable\n",
      "basetable[\"age\"] = (pd.Series([calculate_age(date_of_birth, reference_date)\n",
      "                              for date_of_birth in basetable[\"date_of_birth\"]]))\n",
      "                              \n",
      "print(basetable[\"age\"])                              \n",
      "\n",
      "# Calculate mean age\n",
      "print(round(basetable[\"age\"].mean()))\n",
      "\n",
      "\n",
      "  Sample  > merge\n",
      "\n",
      "donor_id segment\n",
      "5491     18728  silver\n",
      "276        729    gold\n",
      "8139     27922  silver\n",
      "1449     70087  silver\n",
      "7180     24611    gold\n",
      "2935     75278    gold\n",
      "1732      5489  silver\n",
      "1344     69718  bronze\n",
      "3237     43454  bronze\n",
      "2661     74276  silver\n",
      "9329     32171  bronze\n",
      "3873     78448    gold\n",
      "7470     58376  bronze\n",
      "6401     87520    gold\n",
      "\n",
      "\n",
      "\n",
      "basetable = pd.merge(basetable,segments, on =[\"donor_id\"], how=\"left\")\n",
      "\n",
      "# Count the number of donors in each segment\n",
      "basetable.groupby(\"segment\").size()\n",
      "\n",
      "# Count the number of donors with no segment assigned\n",
      "print(basetable[\"segment\"].isna().sum())\n",
      "\n",
      "\n",
      "\n",
      "  >sample add living places\n",
      "\n",
      "The living place must have a start date less than the reference date and an end date greater than the reference date.\n",
      "\n",
      "\n",
      "# Reference date\n",
      "reference_date = datetime.date(2017, 5, 1)\n",
      "\n",
      "# Select living place reference date\n",
      "living_places_reference_date = living_places[(living_places[\"start_date\"] <= reference_date) & \n",
      "                                            (living_places[\"end_date\"] > reference_date)]\n",
      "\n",
      "print(living_places_reference_date)\n",
      "# Add living place to the basetable\n",
      "basetable = pd.merge(basetable, living_places_reference_date[[\"donor_ID\", \"living_place\"]], on=\"donor_ID\")\n",
      "\n",
      "\n",
      "         Aggregation      >\n",
      "\n",
      "id\n",
      "date\n",
      "amount\n",
      "\n",
      "\n",
      "start_date=datetime.date(2016,1,1)\n",
      "end_date=datetime.date(2017,1,1)\n",
      "\n",
      "gifts_2016=gifts(gifts[\"date\"]>=start_target)\n",
      "& (gifts[\"date\"]<end_target)]\n",
      "\n",
      "gifts_2016_bydonor=gifts.groupby([\"id\"])[\"amount\"].sum().reset_index()\n",
      "\n",
      "gifts_2016_bydonor=[\"donor_ID\",\"sum_2016\"]\n",
      "\n",
      "basetable = pd.merge(basetable, gifts_2016_bydonor, how=\"left\", on =\"donor_ID\")\n",
      "print(basetable.head())\n",
      "\n",
      "\n",
      "and frequency\n",
      "\n",
      "[\"amount\"].sum().reset_index()\n",
      "\n",
      "gifts_2016_bydonor=[\"donor_ID\",\"count_2016\"]\n",
      "\n",
      "gifts_2016_bydonor=gifts[gifts[\"amount\"]>0]\n",
      ".groupby([\"id\"])[\"amount\"].count().reset_index()\n",
      "\n",
      "basetable = pd.merge(basetable, gifts_2016_bydonor, how=\"left\", on =\"donor_ID\")\n",
      "print(basetable.head())\n",
      "\n",
      "\n",
      "  Sample     aggregate   max amount\n",
      "\n",
      "# Start and end date of the aggregation period\n",
      "start_date = datetime.date(2017,1,1)\n",
      "end_date = datetime.date(2017,5, 1)\n",
      "\n",
      "# Select gifts made in 2017\n",
      "gifts_2017 = gifts[(gifts[\"date\"] >= start_date) & (gifts[\"date\"] < end_date)]\n",
      "\n",
      "print(gifts.columns)\n",
      "# Maximum gift per donor in 2017\n",
      "gifts_2017_bydonor = gifts_2017.groupby([\"id\"])[\"amount\"].max().reset_index()\n",
      "gifts_2017_bydonor.columns = [\"donor_ID\", \"max_amount\"]\n",
      "\n",
      "# Add maximum amount to the basetable\n",
      "basetable = pd.merge(basetable, gifts_2017_bydonor)\n",
      "print(basetable)\n",
      "\n",
      "   Sample    aggregate  > recency\n",
      "\n",
      "look for the last day that a donor contributed then subtract it from the reference date yielding the elapsed days since contributing.\n",
      "\n",
      "# Reference date to calculate the recency\n",
      "reference_date = datetime.date(2017, 5, 1)\n",
      "\n",
      "# Select gifts made before the reference date\n",
      "gifts_before_reference = gifts[(gifts[\"date\"] < reference_date)]\n",
      "\n",
      "# Latest gift per donor in 2017\n",
      "last_gift = gifts_before_reference.groupby([\"id\"])[\"date\"].max().reset_index()\n",
      "print(last_gift)\n",
      "last_gift[\"recency\"] = reference_date - last_gift[\"date\"]   \n",
      "\n",
      "# Add recency to the basetable\n",
      "basetable = pd.merge(basetable, last_gift[[\"id\", \"recency\"]], how=\"left\")\n",
      "\n",
      "print(basetable)\n",
      "\n",
      "\n",
      "           Adding evolutions    \n",
      "\n",
      "will a donor donate more than 5 times in the next year?\n",
      "\n",
      "look at intervals of data\n",
      "\n",
      "1/1/2015, 1/1/2016, 1/1/2017\n",
      "\n",
      "see if the trend is up or even for likely predictions\n",
      "\n",
      "start_2017 = datetime.date(2017, 1, 1)\n",
      "start_2016 = datetime.date(2016, 1, 1)\n",
      "start_2015 = datetime.date(2015, 1, 1)\n",
      "\n",
      "\n",
      "gifts_2016=gifts[(gifts[\"date\"] <start_2017) & (gifts[\"date\"] >=start_2016)]\n",
      "\n",
      "\n",
      "gifts_2015_2016=gifts[(gifts[\"date\"] <start_2017) & (gifts[\"date\"] >=start_2015)]\n",
      "\n",
      "\n",
      "number_gifts_2016=gifts_2016.groupby(\"id\")[\"amount\"].size().reset_index()\n",
      "\n",
      "number_gifts_2016.columns=[\"donor_ID\",\"number_gifts_2016\"]\n",
      "\n",
      "number_gifts_2015_2016.columns=[\"donor_ID\",\"number_gifts_2015_and_2016\"]\n",
      "\n",
      "number_gifts_2015_2016=gifts_2015_2016.groupby(\"id\")[\"amount\"].size().reset_index()\n",
      "\n",
      "\n",
      "basetable = pd.merge(basetable, gifts_2016_bydonor, how=\"left\", on =\"donor_ID\")\n",
      "\n",
      "\n",
      "basetable[\"ratio_2015_to_2015_and_2016\"]=\n",
      "basetable[\"number_gifts_2016\"]/\n",
      "basetable[\"number_gifts_2015_and_2016\"]\n",
      "\n",
      "   Sample    calculate evolutions\n",
      "\n",
      "\n",
      "# Average gift last month for each donor\n",
      "average_gift_last_month = gifts_last_month.groupby(\"id\")[\"amount\"].mean().reset_index()\n",
      "\n",
      "print(average_gift_last_month)\n",
      "\n",
      "average_gift_last_month.columns = [\"donor_ID\", \"mean_gift_last_month\"]\n",
      "\n",
      "# Average gift last year for each donor\n",
      "average_gift_last_year = gifts_last_year.groupby(\"id\")[\"amount\"].mean().reset_index()\n",
      "\n",
      "average_gift_last_year.columns = [\"donor_ID\", \"mean_gift_last_year\"]\n",
      "\n",
      "# Add average gift last month and year to basetable\n",
      "basetable = pd.merge(basetable, average_gift_last_month, on=\"donor_ID\", how=\"left\")\n",
      "basetable = pd.merge(basetable, average_gift_last_year, on=\"donor_ID\", how=\"left\")\n",
      "\n",
      "# Calculate ratio of last month's and last year's average\n",
      "basetable[\"ratio_month_year\"] = basetable[\"mean_gift_last_month\"] / basetable[\"mean_gift_last_year\"]\n",
      "print(basetable.head())\n",
      "\n",
      "\n",
      "   >sample    calculate recency\n",
      "\n",
      "# Number of gifts in 2016 and 2017 for each donor\n",
      "gifts_2016_bydonor = gifts_2016.groupby(\"id\").size().reset_index()\n",
      "gifts_2016_bydonor.columns = [\"donor_ID\", \"donations_2016\"]\n",
      "gifts_2017_bydonor = gifts_2017.groupby(\"id\").size().reset_index()\n",
      "gifts_2017_bydonor.columns = [\"donor_ID\", \"donations_2017\"]\n",
      "\n",
      "# Add number of gifts in 2016 and 2017 to the basetable\n",
      "basetable = pd.merge(basetable, gifts_2016_bydonor, on=\"donor_ID\", how=\"left\")\n",
      "basetable = pd.merge(basetable, gifts_2017_bydonor, on=\"donor_ID\", how=\"left\")\n",
      "\n",
      "# Calculate the number of gifts in 2017 minus number of gifts in 2016\n",
      "basetable.fillna(0)\n",
      "basetable[\"gifts_2017_min_2016\"] = basetable[\"donations_2017\"] - basetable[\"donations_2016\"]\n",
      "print(basetable.head())\n",
      "\n",
      "\n",
      "     Predicting using logistic regression and evolutions\n",
      "\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "variables=[\"gender\",\"age\",\"donations_last_year\",\"ratio_month_year\"]\n",
      "\n",
      "X=basetable[variables]\n",
      "y=basetable[[\"target\"]]\n",
      "\n",
      "logreg=linear_model.LogisticRegression()\n",
      "logreg.fit(X,y)\n",
      "\n",
      "predictions=logreg.predict_proba(X)[:,1]\n",
      "\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "auc=roc_auc_score(y,predictions)\n",
      "print(round(auc,2))\n",
      "\n",
      "\n",
      "    predictor insight graph\n",
      "\n",
      "\n",
      "basetable[\"ratio_month_year_disc\"]=pd.qcut(basetable[\"ratio_month_year\"],5)\n",
      "\n",
      "pig_table=create_pig_table(basetable,\"target\",\"ratio_month_year_disc\")\n",
      "\n",
      "plot_pig(pig_table,\"ratio_month_year_disc\")\n",
      "\n",
      "\n",
      "  Sample  > evolutions\n",
      "variables_evolution=\n",
      "['gender_F', 'age', 'donations_2017_min_2016']\n",
      "\n",
      "print(variables_evolution)\n",
      "# Select the evolution variables and fit the model\n",
      "X_evolution = basetable[variables_evolution]\n",
      "logreg.fit(X_evolution, y)\n",
      "\n",
      "# Make predictions and calculate the AUC\n",
      "predictions_evolution = logreg.predict_proba(X_evolution)[:,1]\n",
      "auc_evolution = roc_auc_score(y, predictions_evolution)\n",
      "\n",
      "# Print the respective AUC values\n",
      "print(round(auc_regular, 2))\n",
      "print(round(auc_evolution, 2))\n",
      "\n",
      "output \n",
      ".5\n",
      ".6\n",
      "\n",
      "  Sample  > plot discretized contineous variables\n",
      "\n",
      "# Discretize the variable in 5 bins and add to the basetable\n",
      "basetable[\"donations_2017_min_2016_disc\"] = pd.qcut(basetable[\"donations_2017_min_2016\"], 5)\n",
      "\n",
      "# Construct the predictor insight graph table\n",
      "pig_table = create_pig_table(basetable, \"target\", \"donations_2017_min_2016_disc\")\n",
      "\n",
      "# Plot the predictor insight graph\n",
      "plot_pig(pig_table, \"donations_2017_min_2016_disc\")\n",
      "\n",
      "\n",
      "       Creating dummies\n",
      "\n",
      "logit (a1x1 + a2x2 + anxn + b\n",
      "\n",
      "multicollinearity means on of the dummy variables can be constructed from the other variable\n",
      "\n",
      "\n",
      "dummies_segment = pd.get_dummies(basetable[\"segment\"], drop_first=True)\n",
      "\n",
      "basetable= pd.concat([basetable,dummies_segment],axis=1)\n",
      "del basetable[\"segment\"]\n",
      "\n",
      "\n",
      "     Sample   > gender dummies   concat new columns to basetable\n",
      "\n",
      "# Create the dummy variable\n",
      "dummies_gender = pd.get_dummies(basetable[\"gender\"], drop_first=True)\n",
      "\n",
      "# Add the dummy variable to the basetable\n",
      "basetable = pd.concat([basetable, dummies_gender], axis=1)\n",
      "\n",
      "# Delete the original variable from the basetable\n",
      "del basetable[\"gender\"]\n",
      "print(basetable.head())\n",
      "\n",
      "\n",
      "  >Sample  > dummies country\n",
      "\n",
      "# Create the dummy variable\n",
      "dummies_country = pd.get_dummies(basetable[\"country\"], drop_first=True)\n",
      "\n",
      "# Add the dummy variable to the basetable\n",
      "basetable = pd.concat([basetable, dummies_country], axis=1)\n",
      "\n",
      "# Delete the original variable from the basetable\n",
      "del basetable[\"country\"]\n",
      "print(basetable.head())\n",
      "\n",
      "\n",
      "       Missing values\n",
      "\n",
      "1. replace missing values with the aggregate of the remaining values\n",
      "\n",
      "2. if the max varies greatly from the median than it is better to use the median value\n",
      "\n",
      "3. replace with a fixed value\n",
      "\n",
      "replacement=0\n",
      "\n",
      "basetable[\"donations_last_year\"]=\n",
      "basetable[\"donations_last_year\"].fillna(replacement)\n",
      "\n",
      "\n",
      "replacement=basetable[\"age\"].mean()\n",
      "\n",
      "asetable[\"donations_last_year\"]=\n",
      "basetable[\"donations_last_year\"].fillna(replacement)\n",
      "\n",
      "\n",
      "replacement=basetable[\"age\"].median()\n",
      "\n",
      "asetable[\"donations_last_year\"]=\n",
      "basetable[\"donations_last_year\"].fillna(replacement)\n",
      "\n",
      "\n",
      " >missing email\n",
      "\n",
      "basetable[\"no_email\"]=pd.Series([0 if email==email else 1 for email in basetable[\"email\"]])\n",
      "\n",
      "\n",
      "  Sample   > no donations  > calculate percentage\n",
      "\n",
      "\n",
      "# Calculate percentage of missing values\n",
      "print(round(number_na / len(basetable), 2))Create dummy indicating missing values\n",
      "basetable[\"no_donations\"] = pd.Series([1 if b else 0 for b in basetable[\"total_donations\"].isna()])\n",
      "\n",
      "# Calculate number of missing values\n",
      "number_na = sum(basetable[\"no_donations\"] == 1)\n",
      "\n",
      "\n",
      "   sample  > replace na with median\n",
      "\n",
      "# Calculate the median of age\n",
      "median_age = basetable[\"age\"].median()\n",
      "print(median_age)\n",
      "\n",
      "# Replace missing values by the median\n",
      "basetable[\"age\"] = basetable[\"age\"].fillna(median_age)\n",
      "\n",
      "# Calculate the median of age after replacement\n",
      "median_age = basetable[\"age\"].median()\n",
      "print(median_age)\n",
      "\n",
      "  sample  > total donations 0 for na\n",
      "\n",
      "replacement = 0\n",
      "# Replace missing values by the appropriate value\n",
      "basetable[\"total_donations\"] = basetable[\"total_donations\"].fillna(replacement)\n",
      "\n",
      "\n",
      "     handling outliers\n",
      "\n",
      "winsorization concepts\n",
      "\n",
      "lower 5% and upper 5% from the mean\n",
      "\n",
      "import scipy.stats.mstats import winsorize\n",
      "\n",
      "basetable[\"variable_winsorized\"]=\n",
      "winsorize(basetable[\"variable\"],\n",
      "limits=[0.05,0.01])\n",
      "\n",
      "\n",
      "  >standard deviation method\n",
      "\n",
      "mean -3 standard deviation \n",
      "mean +3 standard deviation\n",
      "\n",
      "\n",
      "mean=basetable[\"age\"].mean()\n",
      "sd_age = basetable[\"age\"].std()\n",
      "\n",
      "lower_limit= mean_age - 3*sd_age\n",
      "upper_limit = mean_age +3*sd_age\n",
      "\n",
      "basetable[\"age_no_outliers\"]=pd.Series(\n",
      "\t[min(max(a, lower_limt),upper_limit)\n",
      "for a in basetable[\"age\"]])\n",
      "\n",
      "\n",
      "  >sample    using winsorize to handle outliers\n",
      "\n",
      "from scipy.stats.mstats import winsorize\n",
      "\n",
      "# Check minimum sum of donations\n",
      "print(basetable[\"sum_donations\"].min())\n",
      "print(basetable[\"sum_donations\"].max())\n",
      "\n",
      "# Fill out the lower limit\n",
      "lower_limit = 0\n",
      "\n",
      "# Winsorize the variable sum_donations\n",
      "basetable[\"sum_donations_winsorized\"] = winsorize(basetable[\"sum_donations\"], limits=[lower_limit, 0.05])\n",
      "\n",
      "# Check maximum sum of donations after winsorization\n",
      "print(basetable[\"sum_donations_winsorized\"].max())\n",
      "\n",
      "\n",
      "\n",
      "  >sample   replace outliers with 3*std\n",
      "\n",
      "# Show the maximum age \n",
      "print(basetable[\"age\"].max())\n",
      "\n",
      "# Calculate mean and standard deviation of age\n",
      "mean_age = basetable[\"age\"].mean()\n",
      "std_age = basetable[\"age\"].std()\n",
      "\n",
      "# Calculate the lower and upper limits\n",
      "lower_limit = mean_age - 3 * std_age\n",
      "upper_limit = mean_age + 3 * std_age\n",
      "\n",
      "# Add a variable age_no_outliers to the basetable with outliers replaced\n",
      "basetable[\"age_mod\"] = (pd.Series([min(max(a, lower_limit), upper_limit) \n",
      "                             for a in basetable[\"age\"]]))\n",
      "print(basetable[\"age_mod\"].max())\n",
      "\n",
      "\n",
      "       >Transformations\n",
      "\n",
      "\n",
      "\n",
      "finding significant differences between candidates\n",
      "\n",
      "log transformations\n",
      "\n",
      "differences between smaller amounts will have a large log number or more significance\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "basetable[\"log_variable\"]=np.log(basetable[\"variable\"])\n",
      "\n",
      "\n",
      "   interactions\n",
      "\n",
      "\n",
      "basetable[\"number_donations_int_recency\"]=\n",
      "basetable[\"number_donations\"] *\n",
      "basetable[\"recency\"]\n",
      "\n",
      "\n",
      "add interactions with variables with high predictive power on their own.\n",
      "\n",
      "\n",
      "    sample   > np.log donations\n",
      "# Add the log transformation of the variable \"donations\"\n",
      "basetable[\"donations_log\"] = np.log(basetable[\"donations\"])\n",
      "\n",
      "# Add the square root transformation of the variable \"donations\"\n",
      "basetable[\"donations_sqrt\"] = np.sqrt(basetable[\"donations\"])\n",
      "\n",
      "# Compare the transformations\n",
      "print(basetable[\"donations_log\"],basetable[\"donations_sqrt\"])\n",
      "\n",
      "\n",
      "  > sample  > auc the interactions of age * country spain and france\n",
      "\n",
      "# Calculate AUC using age only\n",
      "print(auc([\"age\"], basetable))\n",
      "\n",
      "# Calculate AUC using country_Spain only\n",
      "print(auc([\"country_Spain\"], basetable))\n",
      "\n",
      "# Calculate AUC using age and country_Spain\n",
      "print(auc([\"age\", \"country_Spain\"], basetable))\n",
      "\n",
      "# Add interactions country_Spain x age and country_France x age\n",
      "basetable[\"spain_age\"] = basetable[\"age\"] * basetable[\"country_Spain\"]\n",
      "basetable[\"france_age\"] = basetable[\"age\"] * basetable[\"country_France\"]\n",
      "\n",
      "# Calculate AUC using age, country_Spain and interactions\n",
      "print(auc([\"age\", \"country_Spain\", \"spain_age\", \"france_age\"], basetable))\n",
      "\n",
      "\n",
      "     Seasonal effects\n",
      "1. donations are higher during the holidays\n",
      "\n",
      "january 1 2019\n",
      "\n",
      "\n",
      "check for seasonality\n",
      "\n",
      "gifts.groupby(\"month\")[\"amount\"].mean()\n",
      "gifts.groupby(\"month\").size()\n",
      "\n",
      "\n",
      "  Sample   > group by mean, number, median of the donation\n",
      "\n",
      "# Calculate the mean amount donated per month\n",
      "mean_per_month = gifts.groupby(\"month\")[\"amount\"].mean().reset_index()\n",
      "print(mean_per_month)\n",
      "\n",
      "# Calculate the number of donations per month \n",
      "number_per_month = gifts.groupby(\"month\").size().reset_index()\n",
      "print(number_per_month)\n",
      "\n",
      "# Calculate the median amount donated per month \n",
      "median_per_month = gifts.groupby(\"month\")[\"amount\"].median().reset_index()\n",
      "print(median_per_month)\n",
      "\n",
      "\n",
      "# AUC of model in July:\n",
      "predictions = logreg.predict_proba(test_july[[\"age\", \"max_amount\"]])[:,1]\n",
      "auc =roc_auc_score(test_july[\"target\"], predictions)\n",
      "print(auc)\n",
      "\n",
      ".55\n",
      "\n",
      "# AUC of model in September:\n",
      "predictions = logreg.predict_proba(test_september[[\"age\", \"max_amount\"]])[:,1]\n",
      "auc = roc_auc_score(test_september[\"target\"], predictions)\n",
      "print(auc)\n",
      "\n",
      ".54\n",
      "\n",
      "# AUC of model in December:\n",
      "predictions = logreg.predict_proba(test_december[[\"age\", \"max_amount\"]])[:,1]\n",
      "auc = roc_auc_score(test_december[\"target\"], predictions)\n",
      "print(auc)\n",
      "\n",
      ".529\n",
      "\n",
      "        >Using multiple snapshots\n",
      "\n",
      "not enough data\n",
      "\n",
      "use multiple snaps\n",
      "\n",
      "1. april 1st 2019 - may 1 2019\n",
      "2. april 1st 2018 - may 1 2018 (1000 donors)\n",
      "3. mar 1st 2018 - apr 1 2018 (1000 donors)\n",
      "4. feb 1st 2018 - mar 1 2018 (1000 donors)\n",
      "\n",
      "basetable = basetable_april2018.append(basetable_march2018)\n",
      "\n",
      "  > Sample  > donations for each donor  > calculate incidence\n",
      "\n",
      "# Sum of donations for each donor\n",
      "gifts_summed_january_2017 = gifts_january_2017.groupby(\"donor_id\")[\"amount\"].sum().reset_index()\n",
      "gifts_summed_january_2018 = gifts_january_2018.groupby(\"donor_id\")[\"amount\"].sum().reset_index()\n",
      "\n",
      "# List with targets in January 2017\n",
      "targets_january_2017 = list(gifts_summed_january_2017[\"donor_id\"][gifts_summed_january_2017[\"amount\"] >= 500])\n",
      "targets_january_2018 = list(gifts_summed_january_2018[\"donor_id\"][gifts_summed_january_2018[\"amount\"] >= 500])\n",
      "\n",
      "# Add targets to the basetables\n",
      "basetable_january_2017[\"target\"] = pd.Series([1 if donor_id in targets_january_2017 else 0 for donor_id in basetable_january_2017[\"donor_id\"]])\n",
      "basetable_january_2018[\"target\"] = pd.Series([1 if donor_id in targets_january_2018 else 0 for donor_id in basetable_january_2018[\"donor_id\"]])\n",
      "\n",
      "# Target incidences\n",
      "print(round(sum(basetable_january_2017[\"target\"]) / len(basetable_january_2017), 2))\n",
      "print(round(sum(basetable_january_2018[\"target\"]) / len(basetable_january_2018), 2))\n",
      "\n",
      "\n",
      "   Sample    > find max donations in 2016 adn 217\n",
      "\n",
      "# Maximum of donations for each donor in december 2016\n",
      "gifts_max_december_2016 = gifts_december_2016.groupby(\"donor_id\")[\"amount\"].max().reset_index()\n",
      "gifts_max_december_2016.columns = [\"donor_id\", \"max_amount\"]\n",
      "\n",
      "# Maximum of donations for each donor in december 2017\n",
      "gifts_max_december_2017 = gifts_december_2017.groupby(\"donor_id\")[\"amount\"].max().reset_index()\n",
      "gifts_max_december_2017.columns = [\"donor_id\", \"max_amount\"]\n",
      "\n",
      "\n",
      "  >sample    two snapshots\n",
      "\n",
      "# Maximum of donations for each donor in december 2016\n",
      "gifts_max_december_2016 = gifts_december_2016.groupby(\"donor_id\")[\"amount\"].max().reset_index()\n",
      "gifts_max_december_2016.columns = [\"donor_id\", \"max_amount\"]\n",
      "\n",
      "# Maximum of donations for each donor in december 2017\n",
      "gifts_max_december_2017 = gifts_december_2017.groupby(\"donor_id\")[\"amount\"].max().reset_index()\n",
      "gifts_max_december_2017.columns = [\"donor_id\", \"max_amount\"]\n",
      "\n",
      "\n",
      "# Add max_amount to the basetables\n",
      "basetable_january_2017 = pd.merge(basetable_january_2017, gifts_max_december_2016, on=\"donor_id\", how=\"left\")\n",
      "basetable_january_2018 = pd.merge(basetable_january_2018, gifts_max_december_2017, on=\"donor_id\", how=\"left\")\n",
      "\n",
      "# Show the basetables\n",
      "print(basetable_january_2017.head())\n",
      "print(basetable_january_2018.head())\n",
      "\n",
      "\n",
      "   sample  > append snapshots\n",
      "\n",
      "# Add basetable_january_2017 to basetable_january_2016\n",
      "basetable = basetable_january_2016.append(basetable_january_2017)\n",
      "\n",
      "# Add basetable_january_2018 to basetable\n",
      "basetable = basetable.append(basetable_january_2018)\n",
      "\n",
      "# Number of observations in the final basetable\n",
      "print(len(basetable))\n",
      "\n",
      "\n",
      "        Time gap\n",
      "\n",
      "who will donate 50 euros\n",
      "\n",
      "timegap\n",
      "1. gather data\n",
      "2. run the model\n",
      "3. prepare the campaign\n",
      "\n",
      "mar 1 2019 to apr 1 2019\n",
      "\n",
      "timegap should also apply to the previous year\n",
      "\n",
      "mean donations last year, should be feb 2018 and not mar 2018 the time gap\n",
      "\n",
      "target period is apr 1 2018 through may 1 2018\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "time gap is May 2018\n",
      "\n",
      "# Start and end date of last month\n",
      "start_date = datetime.date(2018, 4, 1)\n",
      "end_date = datetime.date(2018,5,1)\n",
      "\n",
      "# Gifts made last month\n",
      "gifts_last_month = gifts[(gifts[\"date\"] >= start_date) & (gifts[\"date\"] < end_date)]\n",
      "\n",
      "# Mean gift made last month\n",
      "gifts_last_month_mean = gifts_last_month.groupby(\"donor_id\")[\"amount\"].mean().reset_index()\n",
      "gifts_last_month_mean.columns = [\"donor_id\", \"mean_donation_last_month\"]\n",
      "\n",
      "# Add mean_donation_last_month to the basetable\n",
      "basetable = pd.merge(basetable, gifts_last_month_mean, on=\"donor_id\", how=\"left\")\n",
      "print(basetable.head(10))\n",
      "\n",
      "    sample  > reference data 2018-5-1\n",
      "\n",
      "calculate the age as of the reference date\n",
      "\n",
      "# Reference date\n",
      "reference_date = datetime.date(2018, 5, 1)\n",
      "\n",
      "# Add age to the basetable\n",
      "basetable[\"age\"] = (pd.Series([calculate_age(date_of_birth, reference_date)\n",
      "                              for date_of_birth in basetable[\"date_of_birth\"]]))\n",
      "\n",
      "# Calculate mean age\n",
      "print(round(basetable[\"age\"].mean()))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\time series visualization.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\time series visualization.txt\n",
      "time series are a fundamental way to store and analyze many types of data\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Read in the file content in a DataFrame called discoveries\n",
      "discoveries = pd.read_csv(url_discoveries)\n",
      "\n",
      "# Display the first five lines of the DataFrame\n",
      "print(discoveries.head())\n",
      "\n",
      "# Print the data type of each column in discoveries\n",
      "print(discoveries.dtypes)\n",
      "\n",
      "df=pd.read_csv('co2-concentration.csv',parse_dates=['Date'],index_col='Date')\n",
      "print(df.columns)\n",
      "plt.clf()\n",
      "fig,ax = plt.subplots(figsize=(12,4))\n",
      "df['CO2'].plot(ax=ax, color='blue',linewidth=3, fontsize=12)\n",
      "plt.style.use('fivethirtyeight')\n",
      "plt.ylabel('CO2',fontsize=16)\n",
      "plt.title('CO2 Levels over time')\n",
      "#plt.style.use('ggplot')\n",
      "plt.show()\n",
      "\n",
      "print(plt.style.available)\n",
      "\n",
      "'Solarize_Light2', '_classic_test_patch', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10'\n",
      "\n",
      "\n",
      "# Import the matplotlib.pyplot sub-module\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Use the ggplot style\n",
      "plt.style.use('ggplot')\n",
      "ax2 = discoveries.plot()\n",
      "\n",
      "# Set the title\n",
      "ax2.set_title('ggplot Style')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Plot a line chart of the discoveries DataFrame using the specified arguments\n",
      "ax = discoveries.plot(color='blue', figsize=(8, 3), linewidth=2, fontsize=6)\n",
      "\n",
      "# Specify the title in your plot\n",
      "ax.set_title('Number of great inventions and scientific discoveries from 1860 to 1959', fontsize=8)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "         stackoverflow\n",
      "\n",
      "\n",
      "\n",
      "using barh to plot different times by start and stop ranges\n",
      "https://stackoverflow.com/questions/50883054/how-to-create-a-historical-timeline-with-python/66739012#66739012\n",
      "\n",
      "event = data_set_adj['EnglishName']\n",
      "begin = data_set_adj['Start']\n",
      "end = data_set_adj['Finish']\n",
      "length =  data_set_adj['Length']\n",
      "dynasty = data_set_adj['Dynasty']\n",
      "dynasty_col = data_set_adj['Dynasty_col']\n",
      "\n",
      "dict_dynasty = dict(zip(dynasty.unique(), range(0,4*len(dynasty.unique()),4)))\n",
      "\n",
      "levels = np.tile([-1.2,1.2, -0.8, 0.8, -0.4, 0.4],\n",
      "                 int(np.ceil(len(begin)/6)))[:len(begin)]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use('ggplot')\n",
      "plt.figure(figsize=(20,10))\n",
      "\n",
      "for x in range(len(dynasty)):   \n",
      "    plt.vlines(begin.iloc[x]+length.iloc[x]/2, dict_dynasty[dynasty.iloc[x]], dict_dynasty[dynasty.iloc[x]]+levels[x], color=\"tab:red\")\n",
      "    plt.barh(dict_dynasty[dynasty.iloc[x]], (end.iloc[x]-begin.iloc[x]), color=dynasty_col.iloc[x], height =0.3 ,left=begin.iloc[x], edgecolor = \"black\", alpha = 0.5)\n",
      "    if x%2==0:\n",
      "        plt.text(begin.iloc[x] + length.iloc[x]/2, \n",
      "                 dict_dynasty[dynasty.iloc[x]]+1.6*levels[x], event.iloc[x], \n",
      "                 ha='center', fontsize = '8')\n",
      "    else:\n",
      "        plt.text(begin.iloc[x] + length.iloc[x]/2, \n",
      "                 dict_dynasty[dynasty.iloc[x]]+1.25*levels[x], event.iloc[x], \n",
      "                 ha='center', fontsize = '8')\n",
      "plt.tick_params(axis='both', which='major', labelsize=15)\n",
      "plt.tick_params(axis='both', which='minor', labelsize=20)\n",
      "plt.title('Chinese Dynasties', fontsize = '25')\n",
      "plt.xlabel('Year', fontsize = '20')\n",
      "ax = plt.gca()\n",
      "ax.axes.yaxis.set_visible(False)\n",
      "plt.xlim(900, 1915)\n",
      "plt.ylim(-4,28)\n",
      "\n",
      "\n",
      "  > pair plots\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_theme('notebook', style='dark')\n",
      "plt.style.use(\"dark_background\")\n",
      "df = sns.load_dataset('iris')\n",
      "g = sns.PairGrid(df)\n",
      "g.map_upper(sns.scatterplot, color='crimson')\n",
      "g.map_lower(sns.scatterplot, color='limegreen')\n",
      "g.map_diag(plt.hist, color='skyblue')\n",
      "plt.show()\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "            customizing\n",
      "\n",
      "slicing time series data\n",
      "\n",
      "discoveries['1960':'1970']\n",
      "discoveries['1950-01':'1950-12']\n",
      "discoveries['1950-01-01:'1950-12-31']\n",
      "\n",
      "df_subset=discoveries['1960':'1970']\n",
      "ax=df_subset.plot(color='blue', fontsize=14)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "ax.axvline(x='1969-01-01',color='red',linestyle='--')\n",
      "ax.axhline(y=100,color='green',linestyle='--')\n",
      "ax.discoveries.plot(color='blue')\n",
      "ax.set_xlabel('Date')\n",
      "ax.set_ylabel('Number of great discoveries')\n",
      "ax.axvline('1969-01-01', color='red', linestyle='--')\n",
      "ax.axhline(4,color='green', linestyle='--')\n",
      "\n",
      "ax.axvspan('1964-01-01','1968-01-01', color='red', alpha=0.5)\n",
      "\n",
      "ax.axhspan(8,6, color='green', alpha=0.2)\n",
      "c\n",
      "\n",
      " > sample\n",
      "\n",
      "# Select the subset of data between 1945 and 1950\n",
      "discoveries_subset_1 = discoveries['1945':'1950']\n",
      "\n",
      "# Plot the time series in your DataFrame as a blue area chart\n",
      "ax = discoveries_subset_1.plot(color='blue', fontsize=15)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "# Select the subset of data between 1939 and 1958\n",
      "discoveries_subset_2 =discoveries['1939':'1958']\n",
      "\n",
      "# Plot the time series in your DataFrame as a blue area chart\n",
      "ax = discoveries_subset_2.plot(color='blue', fontsize=15)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "# Select the subset of data between 1939 and 1958\n",
      "discoveries_subset_2 =discoveries['1939':'1958']\n",
      "\n",
      "# Plot the time series in your DataFrame as a blue area chart\n",
      "ax = discoveries_subset_2.plot(color='blue', fontsize=15)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Plot your the discoveries time series\n",
      "ax = discoveries.plot(color='blue', fontsize=6)\n",
      "\n",
      "# Add a vertical red shaded region\n",
      "ax.axvspan('1900-01-01', '1915-01-01', color='red', alpha=0.3)\n",
      "\n",
      "# Add a horizontal green shaded region\n",
      "ax.axhspan(6, 8, color='green', alpha=0.3)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "      > cleaning your data\n",
      "\n",
      "finding missing values in a dataframe\n",
      "print(df.isnull)\n",
      "print(df.isnull().sum())\n",
      "\n",
      "shows as nan in the graph\n",
      "\n",
      "df-df.fillna(method='bfill') #back fill\n",
      "\n",
      "ffill #forward fill\n",
      "\n",
      "   sample\n",
      "\n",
      "# Display first seven rows of co2_levels\n",
      "print(co2_levels.head(7))\n",
      "\n",
      "datestamp    co2\n",
      "0  1958-03-29  316.1\n",
      "1  1958-04-05  317.3\n",
      "2  1958-04-12  317.6\n",
      "3  1958-04-19  317.5\n",
      "4  1958-04-26  316.4\n",
      "5  1958-05-03  316.9\n",
      "6  1958-05-10    NaN\n",
      "\n",
      "# Set datestamp column as index\n",
      "co2_levels = co2_levels.set_index('datestamp')\n",
      "\n",
      "# Print out the number of missing values\n",
      "print(co2_levels.isnull().sum())\n",
      "\n",
      "co2    59\n",
      "\n",
      "# Impute missing values with the next valid observation\n",
      "co2_levels = co2_levels.fillna(method='bfill')\n",
      "\n",
      "# Print out the number of missing values\n",
      "print(co2_levels.isnull().sum())\n",
      "\n",
      "datestamp    0\n",
      "co2          0\n",
      "\n",
      "\n",
      "        plot aggregates of your data\n",
      "\n",
      "moving average\n",
      "\n",
      "in the field of time series analysis, a moving average can be used fore many purposes: smoothing out short-term fluctuations, removing outliers, highlighting long-term trends or cycles\n",
      "\n",
      "co2_levels_mean = co2_levels.rolling(window=52).mean()\n",
      "\n",
      "ax= co2_levels_mean.plot()\n",
      "ax.set_xlabel('Date')\n",
      "ax.set_ylabel('The values of my Y axis')\n",
      "ax.set_title('52 weeks rolling mean of my time series.)\n",
      "\n",
      "\n",
      "co2_levels_mean = df.rolling(window=52)['CO2'].mean()\n",
      "co2_levels_mean.plot(ax=ax, color='red')\n",
      "\n",
      "\n",
      "co2_levels.index.year\n",
      "co2_levels.index.month\n",
      "\n",
      "index_month = co2_levels.index.month\n",
      "\n",
      "co2_levels_by_month = co2_levels.groupby(index_month).mean()\n",
      "\n",
      "co2_levels_by_month.plot()\n",
      "\n",
      "\n",
      "\n",
      "  > sample   > show 2 std deviations above and below the co2 data\n",
      "\n",
      "# Compute the 52 weeks rolling mean of the co2_levels DataFrame\n",
      "ma = co2_levels.rolling(window=52).mean()\n",
      "\n",
      "# Compute the 52 weeks rolling standard deviation of the co2_levels DataFrame\n",
      "mstd = co2_levels.rolling(window=52).std()\n",
      "\n",
      "# Add the upper bound column to the ma DataFrame\n",
      "ma['upper'] = ma['co2'] + (2 * mstd['co2'])\n",
      "\n",
      "# Add the lower bound column to the ma DataFrame\n",
      "ma['lower'] = ma['co2'] - (2 * mstd['co2'])\n",
      "\n",
      "# Plot the content of the ma DataFrame\n",
      "ax = ma.plot(linewidth=0.8, fontsize=6)\n",
      "\n",
      "# Specify labels, legend, and show the plot\n",
      "ax.set_xlabel('Date', fontsize=10)\n",
      "ax.set_ylabel('CO2 levels in Mauai Hawaii', fontsize=10)\n",
      "ax.set_title('Rolling mean and variance of CO2 levels\\nin Mauai Hawaii from 1958 to 2001', fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Get month for each dates in the index of co2_levels\n",
      "index_month = co2_levels.index.month\n",
      "\n",
      "# Compute the mean CO2 levels for each month of the year\n",
      "mean_co2_levels_by_month = co2_levels.groupby(index_month).mean()\n",
      "\n",
      "# Plot the mean CO2 levels for each month of the year\n",
      "mean_co2_levels_by_month.plot(fontsize=6)\n",
      "\n",
      "# Specify the fontsize on the legend\n",
      "plt.legend(fontsize=10)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     > summarizing the values in your time series data\n",
      "\n",
      "1. what is the average value of this data\n",
      "2. what is the maximum value observed in this time series\n",
      "\n",
      ".describe()\n",
      "\n",
      "df[['CO2','adjusted CO2']].boxplot()\n",
      "\n",
      "helps you visualize the distribution of your data\n",
      "\n",
      "summarizing your data with histograms\n",
      "\n",
      "fig,ax = plt.subplots(figsize=(12,4))\n",
      "df[['CO2','adjusted CO2']].hist(ax=ax)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   density plots\n",
      "\n",
      "df.plot(kind='density')\n",
      "\n",
      "    sample\n",
      "\n",
      "# Print out summary statistics of the co2_levels DataFrame\n",
      "print(co2_levels.describe())\n",
      "\n",
      "# Print out the minima of the co2 column in the co2_levels DataFrame\n",
      "print(co2_levels['co2'].min())\n",
      "\n",
      "# Print out the maxima of the co2 column in the co2_levels DataFrame\n",
      "print(co2_levels['co2'].max())\n",
      "\n",
      "co2\n",
      "count  2284.000000\n",
      "mean    339.657750\n",
      "std      17.100899\n",
      "min     313.000000\n",
      "25%     323.975000\n",
      "50%     337.700000\n",
      "75%     354.500000\n",
      "max     373.900000\n",
      "313.0\n",
      "373.9\n",
      "\n",
      "\n",
      "# Generate a boxplot\n",
      "ax = co2_levels.boxplot()\n",
      "\n",
      "# Set the labels and display the plot\n",
      "ax.set_xlabel('CO2', fontsize=10)\n",
      "ax.set_ylabel('Boxplot CO2 levels in Maui Hawaii', fontsize=10)\n",
      "plt.legend(fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Generate a histogram\n",
      "ax = co2_levels.plot(kind='hist',bins=50, fontsize=6)\n",
      "\n",
      "# Set the labels and display the plot\n",
      "ax.set_xlabel('CO2', fontsize=10)\n",
      "ax.set_ylabel('Histogram of CO2 levels in Maui Hawaii', fontsize=10)\n",
      "plt.legend(fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "# Display density plot of CO2 levels values\n",
      "ax = co2_levels.plot(kind='density', linewidth=4, fontsize=6)\n",
      "\n",
      "# Annotate x-axis labels\n",
      "ax.set_xlabel('CO2', fontsize=10)\n",
      "\n",
      "# Annotate y-axis labels\n",
      "ax.set_ylabel('Density plot of CO2 levels in Maui Hawaii', fontsize=10)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "          autocorrelation and partial autocorrelation\n",
      "\n",
      "autocorrelation is measured as a correlation between a time series and a delayed copy of itself\n",
      "\n",
      "values are lagged by 3 time points\n",
      "\n",
      "it is used to find repetitive patterns or periodic signal in time series\n",
      "\n",
      "autocorrelation can be applied to any signal, not just time series.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics import tsaplots\n",
      "\n",
      "fig = tsaplots.plot_acf(co2_levels['co2'], lags=40)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   > partial autocorrelation in a time series data\n",
      "1. contrary to autocorrelation, partial correlation removes the effect of previous time points.\n",
      "\n",
      "a partial autocorrelation function of order 3 returns the correlation between our time series and the lagged values of itself by 3 time points after removing all effects attributable to lags 1 and 2\n",
      "\n",
      "\n",
      "\n",
      "fig = tsaplots.plot_pacf(co2_levels['co2'], lags=40)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "print('correlations close to 1 or -1 indicate there is strong correlation between the lag time series')\n",
      "\n",
      "beyond the blue areas then the correlations are statistically significant\n",
      "\n",
      "   sample\n",
      "\n",
      "# Import required libraries\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use('fivethirtyeight')\n",
      "from statsmodels.graphics import tsaplots\n",
      "\n",
      "# Display the autocorrelation plot of your time series\n",
      "fig = tsaplots.plot_acf(co2_levels['co2'], lags=40)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "If autocorrelation values are close to 0, then values between consecutive observations are not correlated with one another. Inversely, autocorrelations values close to 1 or -1 indicate that there exists strong positive or negative correlations between consecutive observations, respectively.\n",
      "\n",
      "\n",
      "# Import required libraries\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use('fivethirtyeight')\n",
      "from statsmodels.graphics import tsaplots\n",
      "\n",
      "# Display the partial autocorrelation plot of your time series\n",
      "fig = tsaplots.plot_pacf(co2_levels['co2'], lags=40)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "If partial autocorrelation values are close to 0, then values between observations and lagged observations are not correlated with one another. Inversely, partial autocorrelations with values close to 1 or -1 indicate that there exists strong positive or negative correlations between the lagged observations of the time series.\n",
      "\n",
      "\n",
      "    > seasonality trend and noise\n",
      "\n",
      "1. seasonality: does the data display a clear periodic pattern\n",
      "\n",
      "2. trend: does the data follow a consistent upward or downward slope\n",
      "\n",
      "3. noise: are their outlier points or missing values that are not consistent with the rest of the data.\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from pylab import rcParams\n",
      "\n",
      "rcParams['figure.figsize']=11,9\n",
      "decomposition=sm.tsa.seasonal_decompose(\n",
      "\tco2_levels['co2'])\n",
      "\n",
      "fig=decomposition.plot()\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "returns: residual, seasonal, trend, observed\n",
      "\n",
      "print(dir(decomposition))\n",
      "\n",
      "\n",
      "print(type(df.index))\n",
      "rcParams['figure.figsize']=11,9\n",
      "decomposition=sm.tsa.seasonal_decompose(x=df['CO2'],model='additive', extrapolate_trend='freq', period=1)\n",
      "decomposition.plot()\n",
      "plt.show()\n",
      "\n",
      "decomposition_seasonal=decomposition.seasonal\n",
      "ax= decomposition_seasonal.plot(figsize=(14,2))\n",
      "ax.set_xlabel('Date')\n",
      "ax.set_ylabel('Seasonality of time series')\n",
      "ax.set_title('Seasonal values of the time series')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "# Import statsmodels.api as sm\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Perform time series decompositon\n",
      "decomposition = sm.tsa.seasonal_decompose(co2_levels['co2'])\n",
      "\n",
      "# Print the seasonality component\n",
      "print(decomposition.seasonal)\n",
      "\n",
      "datestamp\n",
      "1958-03-29    1.028042\n",
      "1958-04-05    1.235242\n",
      "1958-04-12    1.412344\n",
      "\n",
      "2001-10-13   -2.351296\n",
      "2001-10-20   -2.072159\n",
      "2001-10-27   -1.802325\n",
      "2001-11-03   -1.509391\n",
      "2001-11-10   -1.284167\n",
      "2001-11-17   -1.024060\n",
      "2001-11-24   -0.791949\n",
      "2001-12-01   -0.525044\n",
      "2001-12-08   -0.392799\n",
      "2001-12-15   -0.134838\n",
      "2001-12-22    0.116056\n",
      "2001-12-29    0.285354\n",
      "\n",
      "\n",
      "# Extract the trend component\n",
      "trend = decomposition.trend\n",
      "\n",
      "# Plot the values of the trend\n",
      "ax = trend.plot(figsize=(12, 6), fontsize=6)\n",
      "\n",
      "# Specify axis labels\n",
      "ax.set_xlabel('Date', fontsize=10)\n",
      "ax.set_title('Seasonal component the CO2 time-series', fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "    > case project - air passengers\n",
      "\n",
      "# Plot the time series in your dataframe\n",
      "ax = airline.plot(color='blue', fontsize=12)\n",
      "\n",
      "# Add a red vertical line at the date 1955-12-01\n",
      "ax.axvline('1955-12-01', color='red', linestyle='--')\n",
      "\n",
      "# Specify the labels in your plot\n",
      "ax.set_xlabel('Date', fontsize=12)\n",
      "ax.set_ylabel('Number of Monthly Airline Passengers', fontsize=12)\n",
      "plt.show()\n",
      "\n",
      "https://www.youtube.com/watch?v=uxxkG4uKThY\n",
      "\n",
      "# Print out the number of missing values\n",
      "print(airline.isnull().sum())\n",
      "\n",
      "# Print out summary statistics of the airline DataFrame\n",
      "print(airline.describe())\n",
      "\n",
      "# Display boxplot of airline values\n",
      "ax = airline.boxplot()\n",
      "\n",
      "# Specify the title of your plot\n",
      "ax.set_title('Boxplot of Monthly Airline\\nPassengers Count', fontsize=20)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Get month for each dates from the index of airline\n",
      "index_month = airline.index.month\n",
      "\n",
      "# Compute the mean number of passengers for each month of the year\n",
      "mean_airline_by_month = airline.groupby(index_month).mean()\n",
      "\n",
      "# Plot the mean number of passengers for each month of the year\n",
      "mean_airline_by_month.plot()\n",
      "plt.legend(fontsize=20)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Import statsmodels.api as sm\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Perform time series decompositon\n",
      "decomposition = sm.tsa.seasonal_decompose(airline)\n",
      "\n",
      "# Extract the trend and seasonal components\n",
      "trend = decomposition.trend\n",
      "seasonal = decomposition.seasonal\n",
      "\n",
      "\n",
      "# Print the first 5 rows of airline_decomposed\n",
      "print(airline_decomposed.head(5))\n",
      "\n",
      "# Plot the values of the airline_decomposed DataFrame\n",
      "ax = airline_decomposed.plot(figsize=(12, 6), fontsize=15)\n",
      "\n",
      "# Specify axis labels\n",
      "ax.set_xlabel('Date', fontsize=15)\n",
      "plt.legend(fontsize=15)\n",
      "plt.show()\n",
      "\n",
      "     working with more than one time series\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use('fivethirtyeight')\n",
      "ax=df.plot.area(figsize=(12,4), fontsize=14)\n",
      "plt.show()\n",
      "\n",
      "    sample\n",
      "\n",
      "# Read in meat DataFrame\n",
      "print(url_meat)\n",
      "meat = pd.read_csv(url_meat)\n",
      "\n",
      "# Review the first five lines of the meat DataFrame\n",
      "print(meat.head(5))\n",
      "\n",
      "# Convert the date column to a datestamp type\n",
      "meat['date'] = pd.to_datetime(meat['date'])\n",
      "\n",
      "# Set the date column as the index of your DataFrame meat\n",
      "meat = meat.set_index('date')\n",
      "\n",
      "# Print the summary statistics of the DataFrame\n",
      "print(meat.describe())\n",
      "\n",
      "# Plot time series dataset\n",
      "ax = meat.plot(linewidth=2,fontsize=12)\n",
      "\n",
      "# Additional customizations\n",
      "ax.set_xlabel('Date')\n",
      "ax.legend(fontsize=15)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "# Plot an area chart\n",
      "ax = meat.plot.area(fontsize=12)\n",
      "\n",
      "# Additional customizations\n",
      "ax.set_xlabel('Date')\n",
      "ax.legend(fontsize=15)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "       Plot multiple time series\n",
      "\n",
      "COLUMNS=['beef', 'veal', 'pork', 'lamb_and_mutton', 'broilers','other_chicken', 'turkey']\n",
      "df_summary=df[COLUMNS].agg(['mean','sum'])\n",
      "\n",
      "plt.clf()\n",
      "ax=df[COLUMNS].plot(colormap='Dark2',figsize=(10,7))\n",
      "\n",
      "ax.table(cellText=df_summary[COLUMNS].values,colWidths=[0.3]*len(df_summary[COLUMNS].columns),\\\n",
      "    rowLabels=df_summary.index,\n",
      "    colLabels=df_summary[COLUMNS].columns,\n",
      "    loc='top')\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "df.plot(subplots=True,\n",
      "\tlinewidth=0.5,\n",
      "\tlayout(2,4),\n",
      "\tfigsize=(16,10),\n",
      "\tsharex=False,\n",
      "\tsharey=False)\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Plot time series dataset using the cubehelix color palette\n",
      "ax = meat.plot(colormap='cubehelix', fontsize=15)\n",
      "\n",
      "# Additional customizations\n",
      "ax.set_xlabel('Date')\n",
      "ax.legend(fontsize=18)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "# Plot time series dataset using the cubehelix color palette\n",
      "ax = meat.plot(colormap='PuOr', fontsize=15)\n",
      "\n",
      "# Additional customizations\n",
      "ax.set_xlabel('Date')\n",
      "ax.legend(fontsize=18)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "   > with cell data\n",
      "\n",
      "# Plot the meat data\n",
      "ax = meat.plot(fontsize=6, linewidth=1)\n",
      "\n",
      "# Add x-axis labels\n",
      "ax.set_xlabel('Date', fontsize=6)\n",
      "\n",
      "# Add summary table information to the plot\n",
      "ax.table(cellText=meat_mean.values,\n",
      "         colWidths = [0.15]*len(meat_mean.columns),\n",
      "         rowLabels=meat_mean.index,\n",
      "         colLabels=meat_mean.columns,\n",
      "         loc='top')\n",
      "\n",
      "# Specify the fontsize and location of your legend\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=3, fontsize=6)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "# Create a facetted graph with 2 rows and 4 columns\n",
      "meat.plot(subplots=True, \n",
      "          layout=(2,4), \n",
      "          sharex=False, \n",
      "          sharey=False, \n",
      "          colormap='viridis', \n",
      "          fontsize=2, \n",
      "          legend=False, \n",
      "          linewidth=0.2)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "       Find relationship between multiple timeseries\n",
      "\n",
      "The correlation coefficient is a measure used to determine the strength or lack of relationship between two variables\n",
      "\n",
      "pearson coefficient can be used to compute the correlation coefficient between variables for which the relationship is thought to be linear\n",
      "\n",
      "kendall tau or spearman rank can be used to compute the correlation coefficient between variables for which the relationship is thought to be non-linear\n",
      "\n",
      "from scipy.stats.stats import pearsonr\n",
      "from scipy.stats.stats import spearmanr\n",
      "from scipy.stats.stats import kendalltau\n",
      "\n",
      "pearsonr(x,y)\n",
      "spearmanr(x,y)\n",
      "kendalltau(x,y)\n",
      "\n",
      "   > correlation matrix\n",
      "\n",
      "when computing the correlation coefficient between more than two variables, you obtain a correlation matrix\n",
      "\n",
      "range:[-1,1] (negative and positive correlation)\n",
      "\n",
      "0: no relationship\n",
      "\n",
      "corr_p = meat[['beef','veal','turkey']].corr(method='pearson')\n",
      "\n",
      "(method='spearman')\n",
      "\n",
      "\n",
      "corr_p = df[COLUMNS].corr(method='pearson')\n",
      "#print(corr_p)\n",
      "cmap=sns.diverging_palette(h_neg=10, h_pos=240, as_cmap=True)\n",
      "sns.heatmap(corr_p, center=0, cmap=cmap, linewidths=1,\n",
      "annot=True, fmt=\".2f\")\n",
      "\n",
      "sns.clustermap(corr_mat)\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Print the correlation matrix between the beef and pork columns using the spearman method\n",
      "print(meat[['beef', 'pork']].corr(method='spearman'))\n",
      "\n",
      "# Print the correlation between beef and pork columns\n",
      "print(0.827587)\n",
      "\n",
      "# Compute the correlation between the pork, veal and turkey columns using the pearson method\n",
      "print(meat[['pork', 'veal', 'turkey']].corr(method='pearson'))\n",
      "\n",
      "# Print the correlation between veal and pork columns\n",
      "print(-0.827587)\n",
      "\n",
      "# Print the correlation between veal and turkey columns\n",
      "print(-0.768366)\n",
      "\n",
      "# Print the correlation between pork and turkey columns\n",
      "print(0.835215)\n",
      "\n",
      "\n",
      "# Import seaborn library\n",
      "import seaborn as sns\n",
      "\n",
      "# Get correlation matrix of the meat DataFrame: corr_meat\n",
      "corr_meat = meat.corr(method='spearman')\n",
      "\n",
      "\n",
      "# Customize the heatmap of the corr_meat correlation matrix\n",
      "sns.heatmap(corr_meat,\n",
      "            annot=True,\n",
      "            linewidths=0.4,\n",
      "            annot_kws={\"size\": 10})\n",
      "\n",
      "plt.xticks(rotation=90)\n",
      "plt.yticks(rotation=0) \n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Import seaborn library\n",
      "import seaborn as sns\n",
      "\n",
      "# Get correlation matrix of the meat DataFrame\n",
      "corr_meat = corr_meat = meat.corr(method='spearman')\n",
      "\n",
      "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
      "fig = sns.clustermap(corr_meat,\n",
      "                     row_cluster=True,\n",
      "                     col_cluster=True,\n",
      "                     figsize=(10, 10))\n",
      "\n",
      "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
      "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "      >Case analysis\n",
      "\n",
      "jobs dataset\n",
      "\n",
      "# Read in jobs file\n",
      "jobs = pd.read_csv(url_jobs)\n",
      "\n",
      "# Print first five lines of your DataFrame\n",
      "print(jobs.head(5))\n",
      "\n",
      "# Check the type of each column in your DataFrame\n",
      "print(jobs.dtypes)\n",
      "\n",
      "# Convert datestamp column to a datetime object\n",
      "jobs['datetime'] = pd.to_datetime(jobs['datestamp'])\n",
      "\n",
      "# Set the datestamp columns as the index of your DataFrame\n",
      "jobs = jobs.set_index('datestamp')\n",
      "\n",
      "# Check the number of missing values in each column\n",
      "print(jobs.isnull().sum())\n",
      "\n",
      "\n",
      "# Generate a boxplot\n",
      "jobs.boxplot(fontsize=6, vert=False)\n",
      "plt.show()\n",
      "\n",
      "# Generate numerical summaries\n",
      "print(jobs.describe())\n",
      "\n",
      "# Print the name of the time series with the highest mean\n",
      "print('Agriculture')\n",
      "\n",
      "# Print the name of the time series with the highest variability\n",
      "print('Construction')\n",
      "\n",
      "\n",
      "     beyond summary statistics\n",
      "\n",
      "jobs.plot(subplots=True,layout=(4,4), figsize=(20,16),sharex=True,sharey=False)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "ax=jobs.plot(figsize=(20,14), colormap='Dark2')\n",
      "\n",
      "ax.axvline('2008-01-01', color='black',linestyle='--')\n",
      "\n",
      "ax.axvline('2009-01-01', color='black', linestyle='--')\n",
      "\n",
      "index_month=jobs.index.month\n",
      "jobs_by_month=jobs.groupby(index_month).mean()\n",
      "print(jobs_by_month)\n",
      "\n",
      "ax=jobs_by_month.plot(figsize=(12,5), colormap='Dark2')\n",
      "ax.legend(bbox_to_anchor=(1.0,0.5),loc='center left')\n",
      "\n",
      "\n",
      "   > sample\n",
      "\n",
      "# A subset of the jobs DataFrame\n",
      "jobs_subset = jobs[['Finance', 'Information', 'Manufacturing', 'Construction']]\n",
      "\n",
      "# Print the first 5 rows of jobs_subset\n",
      "print(jobs_subset.head(5))\n",
      "\n",
      "# Create a facetted graph with 2 rows and 2 columns\n",
      "ax = jobs_subset.plot(subplots=True,\n",
      "                      layout=(2,2),\n",
      "                      sharex=False,\n",
      "                      sharey=False,\n",
      "                      linewidth=0.7,\n",
      "                      fontsize=3,\n",
      "                      legend=False)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "# Plot all time series in the jobs DataFrame\n",
      "ax = jobs.plot(colormap='Spectral', fontsize=6, linewidth=0.8)\n",
      "\n",
      "# Set labels and legend\n",
      "ax.set_xlabel('Date', fontsize=10)\n",
      "ax.set_ylabel('Unemployment Rate', fontsize=10)\n",
      "ax.set_title('Unemployment rate of U.S. workers by industry', fontsize=10)\n",
      "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
      "\n",
      "# Annotate your plots with vertical lines\n",
      "ax.axvline('2001-07-01', color='blue', linestyle='--', linewidth=0.8)\n",
      "ax.axvline('2008-09-01', color='blue', linestyle='--', linewidth=0.8)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "      effect seasonally on jobs\n",
      "\n",
      "# Extract the month from the index of jobs\n",
      "index_month = jobs.index.month\n",
      "\n",
      "# Compute the mean unemployment rate for each month\n",
      "jobs_by_month = jobs.groupby(index_month).mean()\n",
      "\n",
      "# Plot the mean unemployment rate for each month\n",
      "ax = jobs_by_month.plot(fontsize=6, linewidth=1)\n",
      "\n",
      "# Set axis labels and legend\n",
      "ax.set_xlabel('Month', fontsize=10)\n",
      "ax.set_ylabel('Mean unemployment rate', fontsize=10)\n",
      "ax.legend(bbox_to_anchor=(0.8, 0.6), fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    2008 melt down\n",
      "\n",
      "# Extract of the year in each date indices of the jobs DataFrame\n",
      "index_year = jobs.index.year\n",
      "\n",
      "# Compute the mean unemployment rate for each year\n",
      "jobs_by_year = jobs.groupby(index_year).mean()\n",
      "\n",
      "# Plot the mean unemployment rate for each year\n",
      "ax = jobs_by_year.plot(fontsize=6, linewidth=1)\n",
      "\n",
      "# Set axis labels and legend\n",
      "ax.set_xlabel('Year', fontsize=10)\n",
      "ax.set_ylabel('Mean unemployment rate', fontsize=10)\n",
      "ax.legend(bbox_to_anchor=(0.1, 0.5), fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     > decomposing time series data\n",
      "\n",
      "\n",
      "my_dict={}\n",
      "\n",
      "import statsmodel.api as sm\n",
      "\n",
      "ts_names=df.columns\n",
      "\n",
      "for ts in ts_names:\n",
      "\tts_decomposition = sm.tsa.seasonal_decompose(jobs[ts])\n",
      "\tmy_dict[ts]=ts_decomposition\n",
      "\n",
      "\n",
      "my_dict.trend={}\n",
      "\n",
      "for ts in ts_names:\n",
      "\tmy_dict_trend[ts]=my_dict[ts].trend\n",
      "\n",
      "trend_df=pd.DataFrame.from_dict(my_dict_trend)\n",
      "\n",
      "print(trend_df)\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "\n",
      "# Initialize dictionary\n",
      "jobs_decomp={}\n",
      "\n",
      "# Get the names of each time series in the DataFrame\n",
      "ts_names=jobs.columns\n",
      "\n",
      "\n",
      "# Run time series decomposition on each time series of the DataFrame\n",
      "for ts in ts_names:\n",
      "    ts_decomposition = sm.tsa.seasonal_decompose(jobs[ts])\n",
      "    jobs_decomp[ts] =ts_decomposition\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Extract the seasonal values for the decomposition of each time series\n",
      "for ts in jobs_names:\n",
      "    jobs_seasonal[ts] = jobs_decomp[ts].seasonal\n",
      "    \n",
      "# Create a DataFrame from the jobs_seasonal dictionary\n",
      "seasonality_df = pd.DataFrame.from_dict(jobs_seasonal)\n",
      "\n",
      "# Remove the label for the index\n",
      "seasonality_df.index.name = None\n",
      "\n",
      "# Create a faceted plot of the seasonality_df DataFrame\n",
      "jobs_seasonal.plot(subplots=True,\n",
      "                   layout=(4,4),\n",
      "                   sharey=False,\n",
      "                   fontsize=2,\n",
      "                   linewidth=0.3,\n",
      "                   legend=False)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "      >compute correlations between time series\n",
      "\n",
      "\n",
      "trend_corr = trend_df.corr(method='spearman')\n",
      "\n",
      "fig=sns.clustermap(trend_corr, annot=True, linewidth=0.4)\n",
      "\n",
      "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(),\n",
      "rotation=0)\n",
      "\n",
      "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(),rotation=90)\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Get correlation matrix of the seasonality_df DataFrame\n",
      "seasonality_corr = seasonality_df.corr(method='spearman')\n",
      "\n",
      "# Customize the clustermap of the seasonality_corr correlation matrix\n",
      "fig = sns.clustermap(seasonality_corr, annot=True, annot_kws={\"size\": 4}, linewidths=.4, figsize=(15, 10))\n",
      "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
      "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
      "plt.show()\n",
      "\n",
      "# Print the correlation between the seasonalities of the Government and Education & Health industries\n",
      "print(seasonality_corr)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['seasonal']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
