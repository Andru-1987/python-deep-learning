{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\deep learning tensors and layers and encoders.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\deep learning tensors and layers and encoders.txt\n",
      "first_layers = model.layers[0]\n",
      "\n",
      "print(first_layer.input)\n",
      "print(first_layer.ouput)\n",
      "print(first_layer.weights)\n",
      "\n",
      "\n",
      "tensors are 2 dimensions\n",
      "\n",
      "T2=[[1,2,3],\n",
      "[4,5,6],\n",
      "[7,8,9]]\n",
      "\n",
      "A three dimension tensors is an array of matrices.\n",
      "\n",
      "\n",
      "import keras.backend as K\n",
      "\n",
      "inp= model.layers[0].input\n",
      "out= model.layers[0].output\n",
      "\n",
      "inp_to_out = K.function([inp],[out])\n",
      "\n",
      "print(inp_to_out([X_train]))\n",
      "\n",
      "autoencoders are models have the same inputs as outputs\n",
      "1) dimensionality reduction\n",
      "a. smaller dimensional space representation of our inputs\n",
      "2) De-noising data\n",
      "a. if trained with clean data, irrelevant noise will be filtered out during reconstruction.\n",
      "3. Anomaly detection:\n",
      "a. a poor reconstruction will result when the model is fed with unseen inputs\n",
      "\n",
      "\n",
      "autoencoder= Sequential()\n",
      "\n",
      "autoencoder.add(Dense(4, input_shape(100,), activation='relu'))\n",
      "\n",
      "autoencoder.add(Dense(100,activation='sigmoid')\n",
      "\n",
      "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
      "\n",
      "\n",
      "encoder=Sequential()\n",
      "encoder.add(autoencoder.layers[0])\n",
      "\n",
      "encoder.predict(X_test)\n",
      "\n",
      "\n",
      "  \n",
      "# Import keras backend\n",
      "import keras.backend as K\n",
      "\n",
      "# Input tensor from the 1st layer of the model\n",
      "inp =  model.layers[0].input\n",
      "\n",
      "# Output tensor from the 1st layer of the model\n",
      "out = model.layers[0].output\n",
      "\n",
      "# Define a function from inputs to outputs\n",
      "inp_to_out = K.function([inp], [out])\n",
      "\n",
      "# Print the results of passing X_test through the 1st layer\n",
      "print(inp_to_out([X_test]))\n",
      "\n",
      "for i in range(0, 21):\n",
      "  \t# Train model for 1 epoch\n",
      "    h = model.fit(X_train, y_train, batch_size=16, epochs=1, verbose=0)\n",
      "    if i%4==0: \n",
      "      # Get the output of the first layer\n",
      "      layer_output = inp_to_out([X_test])[0]\n",
      "      \n",
      "      # Evaluate model accuracy for this epoch\n",
      "      test_accuracy = model.evaluate(X_test, y_test)[1] \n",
      "      \n",
      "      # Plot 1st vs 2nd neuron output\n",
      "      plot()\n",
      "\n",
      "# Start with a sequential model\n",
      "autoencoder = Sequential()\n",
      "\n",
      "# Add a dense layer with the original image as input\n",
      "autoencoder.add(Dense(32, input_shape=(784, ), activation=\"relu\"))\n",
      "\n",
      "# Add an output layer with as many nodes as the image\n",
      "autoencoder.add(Dense(784, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile your model\n",
      "autoencoder.compile(optimizer=\"adadelta\", loss=\"binary_crossentropy\")\n",
      "\n",
      "# Take a look at your model structure\n",
      "autoencoder.summary()\n",
      "\n",
      "\n",
      "# Build your encoder\n",
      "encoder = Sequential()\n",
      "encoder.add(autoencoder.layers[0])\n",
      "\n",
      "# Encode the images and show the encodings\n",
      "preds = encoder.predict(X_test_noise)\n",
      "show_encodings(preds)\n",
      "\n",
      "# Build your encoder\n",
      "encoder = Sequential()\n",
      "encoder.add(autoencoder.layers[0])\n",
      "\n",
      "# Encode the images and show the encodings\n",
      "preds = encoder.predict(X_test_noise)\n",
      "show_encodings(preds)\n",
      "\n",
      "# Predict on the noisy images with your autoencoder\n",
      "decoded_imgs = autoencoder.predict(X_test_noise)\n",
      "\n",
      "# Plot noisy vs decoded images\n",
      "compare_plot(X_test_noise, decoded_imgs)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pytorch.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pytorch.txt\n",
      "Random tensors are very important in neural networks. Parameters of the neural networks typically are initialized with random weights (random tensors).\n",
      "\n",
      "# Import torch\n",
      "import torch\n",
      "\n",
      "# Create random tensor of size 3 by 3\n",
      "your_first_tensor = torch.rand(3, 3)\n",
      "\n",
      "# Calculate the shape of the tensor\n",
      "tensor_size = your_first_tensor.shape\n",
      "\n",
      "# Print the values of the tensor and its shape\n",
      "print(your_first_tensor)\n",
      "print(tensor_size)\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Create a matrix of ones with shape 3 by 3\n",
      "tensor_of_ones = torch.ones(3, 3)\n",
      "\n",
      "# Create an identity matrix with shape 3 by 3\n",
      "identity_tensor = torch.eye(3)\n",
      "\n",
      "print(tensor_of_ones)\n",
      "print(identity_tensor)\n",
      "\n",
      "# Do a matrix multiplication of tensor_of_ones with identity_tensor\n",
      "matrices_multiplied = torch.matmul(tensor_of_ones, identity_tensor)\n",
      "print(matrices_multiplied)\n",
      "\n",
      "# Do an element-wise multiplication of tensor_of_ones with identity_tensor\n",
      "element_multiplication = tensor_of_ones * identity_tensor\n",
      "\n",
      " >Forward Propogation\n",
      "\n",
      "Input\n",
      "a=2,b=-4,c=-2,d=2\n",
      "\n",
      "+ operator where e= a+b or 2+-4=-2\n",
      "* operator where f= c+d or -2*2 =-4\n",
      "* operator where g=-2*-4=8\n",
      "\n",
      "import torch\n",
      "\n",
      "a= torch.Tensor([2])\n",
      "b= torch.Tensor([-4])\n",
      "c= torch.Tensor([-2])\n",
      "d= torch.Tensor([2])\n",
      "\n",
      "e=a+b\n",
      "f=c*d\n",
      "g=e*f\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Initialize tensors x, y and z\n",
      "x = torch.rand(1000,1000)\n",
      "y = torch.rand(1000,1000)\n",
      "z = torch.rand(1000,1000)\n",
      "\n",
      "# Multiply x with y\n",
      "q = x*y\n",
      "\n",
      "# Multiply elementwise z with q\n",
      "f = z*q\n",
      "\n",
      "mean_f = torch.mean(f)\n",
      "print(mean_f)\n",
      "\n",
      " Backpropagation\n",
      "1. derivatives represent the rate of change in the function.\n",
      "\n",
      "derivative\n",
      "addition= (f+g)' = f'+g'\n",
      "multiplication= (f*g)'=f*dg+g*df\n",
      "powers=(x^n)' = d/dx x^n = nx^(n-2)\n",
      "Inverse=(1/x)' = -1/x^2\n",
      "Division=(f/g)' = (df*1/g) + (-1/g2 * dg * f)\n",
      "\n",
      "x=torch.tensor(-3., requires_grad=true)\n",
      "y=torch.tensor(5., requires_grad=true)\n",
      "z=torch.tensor(-2., requires_grad=true)\n",
      "\n",
      "q=x+y\n",
      "f=q*z\n",
      "\n",
      "f.backward()\n",
      "\n",
      "print(\"gradient of z is:str(z.grad))\n",
      "print(\"gradient of y is:str(y.grad))\n",
      "print(\"gradient of x is:str(x.grad))\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Initialize x, y and z to values 4, -3 and 5\n",
      "x = torch.tensor(4., requires_grad=True)\n",
      "y = torch.tensor(-3., requires_grad=True)\n",
      "z = torch.tensor(5., requires_grad=True)\n",
      "\n",
      "# Set q to sum of x and y, set f to product of q with z\n",
      "q = x+y\n",
      "f = q*z\n",
      "\n",
      "# Compute the derivatives\n",
      "f.backward()\n",
      "\n",
      "# Print the gradients\n",
      "print(\"Gradient of x is: \" + str(x.grad))\n",
      "print(\"Gradient of y is: \" + str(y.grad))\n",
      "print(\"Gradient of z is: \" + str(z.grad))\n",
      "\n",
      " Sample\n",
      "\n",
      "# Multiply tensors x and y\n",
      "q = torch.matmul(x,y)\n",
      "\n",
      "# Elementwise multiply tensors z with q\n",
      "f = z * q\n",
      "\n",
      "mean_f = torch.mean(f)\n",
      "\n",
      "# Calculate the gradients\n",
      "mean_f.backward()\n",
      "\n",
      " >Introduction to neural networks\n",
      "1. The job of the hidden layer is to get good features\n",
      "\n",
      "\n",
      "import torch\n",
      "\n",
      "input_layer = torch.rand(10)\n",
      "w1=torch.rand(10,20)\n",
      "w2=torch.rand(20,20)\n",
      "w3=torch.rand(20,4)\n",
      "\n",
      "h1=torch.matmul(input_layer, w1)\n",
      "h2=torch.matmul(h1,w2)\n",
      "output_layer=torch.matmul(h2,w3)\n",
      "print(output_layer)\n",
      "\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class Net(nn.Module):\n",
      "\tdef __init__(self):\n",
      "\t\tsuper(Net,self).__init__()\n",
      "\t\tself.fc1=nn.Linear(10,20)\n",
      "\t\tself.fc2=nn.Linear(20,20)\n",
      "\t\tself.output=nn.Linear(20,4)\n",
      "\t\n",
      "\tdef forward(self,x):\n",
      "\t\tx=self.fc1(x)\n",
      "\t\tx=self.fc2(x)\n",
      "\t\tx=self.output(x)\n",
      "\t\treturn x\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Initialize the weights of the neural network\n",
      "weight_1 = torch.rand(784, 200)\n",
      "weight_2 = torch.rand(200, 10)\n",
      "\n",
      "# Multiply input_layer with weight_1\n",
      "hidden_1 = torch.matmul(input_layer, weight_1)\n",
      "\n",
      "# Multiply hidden_1 with weight_2\n",
      "output_layer = torch.matmul(hidden_1, weight_2)\n",
      "print(output_layer)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\tensor basics.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\tensor basics.txt\n",
      "import tensorflow as tf\n",
      "\n",
      "d0 = tf.ones((1,))\n",
      "#1D\n",
      "d1 = tf.ones((2,))\n",
      "#2D\n",
      "d2 = tf.ones((2,2))\n",
      "#3d\n",
      "d3 = tf.ones((2,2,2))\n",
      "\n",
      "print(d3.numpy())\n",
      "\n",
      "#defines a 2x3 constant\n",
      "a= constant(3, shape=[2,3])\n",
      "\n",
      "#define a 2x2 constant\n",
      "b= constant([1,2,3,4], shape[2,2])\n",
      "\n",
      "operation\n",
      "tf.constant()  constant([1,2,3])\n",
      "tf.zeros() zeros([2,2])\n",
      "tf.zeros_like() zeros_like(input_tensor)\n",
      "tf.ones() ones([2,2])\n",
      "tf.ones_like() ones_like(input_tensor)\n",
      "tf.fill() fill([3,3],7)\n",
      "\n",
      "the variable shape is fixed but the values of the shape can change during run time.\n",
      "\n",
      "a0 = tf.Variable([1,2,3,4,5,6], dtype=tf.float32)\n",
      "a1 = tf.Variable([1,2,3,4,5,6], dtype=tf.int16)\n",
      "\n",
      "b=tf.constant(2,tf.float32)\n",
      "\n",
      "c0=tf.multiply(a0,b)\n",
      "c1=a0*b  #tf.multiply is overloaded allowing this expression to be used\n",
      "\n",
      "   sample   > convert a np array into a tensor flow constant\n",
      "\n",
      "# Import constant from TensorFlow\n",
      "from tensorflow import constant\n",
      "\n",
      "print(credit_numpy)\n",
      "# Convert the credit_numpy array into a tensorflow constant\n",
      "credit_constant = constant(credit_numpy)\n",
      "\n",
      "# Print constant datatype\n",
      "print('The datatype is:', credit_constant.dtype)\n",
      "\n",
      "# Print constant shape\n",
      "print('The shape is:', credit_constant.shape)\n",
      "\n",
      "   sample  > create a variable then convert it to a numpy array\n",
      "\n",
      "# Define the 1-dimensional variable A1\n",
      "A1 = Variable([1, 2, 3, 4])\n",
      "\n",
      "# Print the variable A1\n",
      "print(A1)\n",
      "\n",
      "# Convert A1 to a numpy array and assign it to B1\n",
      "B1 = A1.numpy()\n",
      "\n",
      "# Print B1\n",
      "print(B1)\n",
      "\n",
      "\n",
      "     >basic operations\n",
      "\n",
      "graphs contain edges and nodes\n",
      "\n",
      "where the edges are tensors and the nodes are operations\n",
      "\n",
      "MatMul\n",
      "Add\n",
      "\n",
      "Const & Const_1 are fed to Add_1\n",
      "Const_2 & const_3 are fed to Add resulting in Add_2\n",
      "\n",
      "Add_1 & Add_2 are fed to MatMul\n",
      "\n",
      "from tensorflow import constant, add\n",
      "\n",
      "A0=constant([1])\n",
      "B0=constant([2])\n",
      "\n",
      "1 dimensional tensors\n",
      "A1=constant([1,2])\n",
      "B1=constant([3,4])\n",
      "\n",
      "2 dimensional tensors\n",
      "A2 = constant([1,2],[3,4])\n",
      "B2 = constant([5,6],[7,8])\n",
      "\n",
      "C0=add(A0,B0)\n",
      "C1=add(A1,B1)\n",
      "C2=add(A2,B2)\n",
      "\n",
      "add requires that each tensor have the same shape\n",
      "\n",
      "from tensorflow import ones, matmul, multiply\n",
      "\n",
      "a0=ones(1)\n",
      "a31=ones([3,1])\n",
      "a34=ones([3,4])\n",
      "a43=ones([4,3])\n",
      "\n",
      "matmul(A43,A34) but not matmul(A43,A43)\n",
      "\n",
      "reduce_sum() sums over the dimension of a tensor\n",
      "\n",
      "A=ones([2,3,4])\n",
      "or 2*3*4 = 24 ones\n",
      "\n",
      "x=\n",
      "1 1 1\n",
      "1 1 1\n",
      "\n",
      "reduce_sum(x,0)\n",
      "[1,1,1]+[1,1,1]\n",
      "\n",
      "reduce_sum(x,1)\n",
      "[1,1]+[1,1]+[1,1]\n",
      "\n",
      "#sum over all dimensions\n",
      "B= reduce_sum(A) -> 24\n",
      "\n",
      "B0=reduce_sum(A,0) 3x4 of 2\n",
      "B1=reduce_sum(A,1) 2x4 of 3\n",
      "B3=reduce_sum(A,2) 2x3 of 4\n",
      "\n",
      "\n",
      "   sample  > tensor multiplication of two tensors\n",
      "\n",
      "# Define tensors A1 and A23 as constants\n",
      "A1 = constant([1, 2, 3, 4])\n",
      "A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
      "\n",
      "# Define B1 and B23 to have the correct shape\n",
      "B1 = ones_like(A1)\n",
      "B23 = ones_like(A23)\n",
      "\n",
      "# Perform element-wise multiplication\n",
      "C1 = A1*B1\n",
      "C23 = A23*B23\n",
      "\n",
      "# Print the tensors C1 and C23\n",
      "print('C1: {}'.format(C1.numpy()))\n",
      "print('C23: {}'.format(C23.numpy()))\n",
      "\n",
      "\n",
      "   sample    tensor matmul\n",
      "\n",
      "# Define features, params, and bill as constants\n",
      "features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
      "params = constant([[1000], [150]])\n",
      "bill = constant([[3913], [2682], [8617], [64400]])\n",
      "\n",
      "# Compute billpred using features and params\n",
      "billpred =matmul(features,params)\n",
      "\n",
      "# Compute and print the error\n",
      "error = bill-billpred \n",
      "print(error.numpy())\n",
      "\n",
      "output:\n",
      "[[-1687]\n",
      " [-3218]\n",
      " [-1933]\n",
      " [57850]]\n",
      "\n",
      "\n",
      "wealth=\n",
      "[11 50]\n",
      "[7  2 ]\n",
      "[4  60]\n",
      "[3  0 ]\n",
      "[25 10]\n",
      "\n",
      "    >advanced operations\n",
      "\n",
      "gradient() computes the slope of a function at a point\n",
      "reshape() reshapes a tensor\n",
      "random() populates tensor with entries drawn from a probability distribution\n",
      "\n",
      "need to find a optimum\n",
      "\n",
      "Minimum : lowest value of a loss function\n",
      "Maximum : highest value of a objective function\n",
      "\n",
      "gradient : find a point where the gradient is 0\n",
      "\n",
      "check if the gradient is increasing or decreasing\n",
      "minimum : change in gradient > 0\n",
      "maximum : change in gradient < 0\n",
      "\n",
      "x=tf.Variable(-1.0)\n",
      "\n",
      "with tf.GradientTape() as tape:\n",
      "\ttape.watch(x)\n",
      "\ty=tf.multiply(x,x)\n",
      "\n",
      "g=tape.gradient(y,x)\n",
      "print(g.numpy())\n",
      "\n",
      "     reshaping \n",
      "grayscale \n",
      "\n",
      "grays 0 - 255\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "#generate a 2x2 matrix\n",
      "gray = tf.random.uniform([2,2],maxval=255, dtype='int32')\n",
      "\n",
      "#reshape into a 4x1 vector\n",
      "gray = tf.reshape(gray,[2*2,1])\n",
      "\n",
      "color = tf.random.uniform([2,2,3],maxval=255,dtype='int32')\n",
      "\n",
      "color=tf.reshape(color,[2x2,3])\n",
      "\n",
      "\n",
      "   >sample  > sign language tensor\n",
      "\n",
      "# Reshape the grayscale image tensor into a vector\n",
      "gray_vector = reshape(gray_tensor, (28*28, 1))\n",
      "\n",
      "# Reshape the color image tensor into a vector\n",
      "color_vector = reshape(color_tensor, (28*28, 3))\n",
      "\n",
      "\n",
      "   sample  > compute the gradient for different x along the a parabolia\n",
      "\n",
      "def compute_gradient(x0):\n",
      "  \t# Define x as a variable with an initial value of x0\n",
      "\tx = Variable(x0)\n",
      "\twith GradientTape() as tape:\n",
      "\t\ttape.watch(x)\n",
      "        # Define y using the multiply operation\n",
      "\t\ty = multiply(x,x)\n",
      "    # Return the gradient of y with respect to x\n",
      "\treturn tape.gradient(y, x).numpy()\n",
      "\n",
      "# Compute and print gradients at x = -1, 1, and 0\n",
      "print(compute_gradient(-1.0))\n",
      "print(compute_gradient(1.0))\n",
      "print(compute_gradient(0.0))\n",
      "\n",
      "The slope is negative at x = -1, which means that we can lower the loss by increasing x. The slope at x = 0 is 0, which means that we cannot lower the loss by either increasing or decreasing x. This is because the loss is minimized at x = 0.\n",
      "\n",
      "\n",
      "   sample use matrix multiplication to predict a letter from an image\n",
      "\n",
      "You want to determine whether the letter is an X or a K. You don't have a trained neural network, but you do have a simple model, model, which can be used to classify letter.\n",
      "\n",
      "# Reshape model from a 1x3 to a 3x1 tensor\n",
      "model = reshape(model, (3, 1))\n",
      "\n",
      "# Multiply letter by model\n",
      "output = matmul(letter, model)\n",
      "\n",
      "# Sum over output and print prediction using the numpy method\n",
      "prediction = reduce_sum(output)\n",
      "print(prediction.numpy())\n",
      "\n",
      "model:\n",
      "[[ 1.]\n",
      " [ 0.]\n",
      " [-1.]]\n",
      "letter\n",
      "\n",
      "letter is\n",
      "[[1. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 0. 1.]]\n",
      "\n",
      "      >input data\n",
      "\n",
      "linear model\n",
      "\n",
      "convert data to a numpy array\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "housing=pd.read_csv('kc_housing.csv')\n",
      "housing=np.array(housing)\n",
      "\n",
      "\n",
      "read_csv\n",
      "1. filepath_or_buffer\n",
      "2.sep delimiter between columns\n",
      "3. delim_whitespace\n",
      "4. encoding\n",
      "\n",
      "print(housing[3][3],housing[3][17])\n",
      "\n",
      "'id', \n",
      "'date', \n",
      "'price', \n",
      "'bedrooms', \n",
      "'bathrooms',\n",
      "'sqft_living', \n",
      "'sqft_lot', \n",
      "'floors', \n",
      "'waterfront', (boolean)\n",
      "'view',   (boolean)\n",
      "'condition',\n",
      "'grade', \n",
      "'sqft_above', \n",
      "'sqft_basement', \n",
      "'yr_built', \n",
      "'yr_renovated',\n",
      "'zipcode', \n",
      "'lat', \n",
      "'long', \n",
      "'sqft_living15', \n",
      "'sqft_lot15'\n",
      "\n",
      "\n",
      "price float\n",
      "waterfront boolean\n",
      "\n",
      "price=np.array(housing['price'],np.float32)\n",
      "waterfront=np.array(housing['waterfront'],np.bool)\n",
      "\n",
      "or\n",
      "price=tf.cast(housing['price'],tf.float32)\n",
      "waterfront=tf.cast(housing['waterfront'],tf.bool)\n",
      "\n",
      "\n",
      "  sample   > load the dataframe\n",
      "\n",
      "# Import pandas under the alias pd\n",
      "import pandas as pd\n",
      "\n",
      "# Assign the path to a string variable named data_path\n",
      "data_path = 'kc_house_data.csv'\n",
      "\n",
      "# Load the dataset as a dataframe named housing\n",
      "housing = pd.read_csv(data_path)\n",
      "\n",
      "# Print the price column of housing\n",
      "print(housing['price'])\n",
      "\n",
      "\n",
      "# Import numpy and tensorflow with their standard aliases\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "# Use a numpy array to define price as a 32-bit float\n",
      "price = np.array(housing['price'], np.float32)\n",
      "\n",
      "# Define waterfront as a Boolean using cast\n",
      "waterfront = tf.cast(housing['waterfront'], tf.bool)\n",
      "\n",
      "# Print price and waterfront\n",
      "print(price)\n",
      "print(waterfront)\n",
      "\n",
      "\n",
      "        >Loss Functions\n",
      "\n",
      "how to train models\n",
      "\n",
      "loss functions tell us how well the model fits the data\n",
      "\n",
      "we want to minimize the loss function\n",
      "\n",
      "Mean squared Error (MSE)\n",
      "Mean absolute Error (MAE)\n",
      "Huber error\n",
      "\n",
      "loss functions are accessible from tf.keras.losses()\n",
      "tf.keras.losses.mse()\n",
      "tf.keras.losses.mae()\n",
      "tf.keras.losses.Huber()\n",
      "\n",
      "MSE\n",
      "1. strongly penalizes outliers\n",
      "2. high gradient sensitivity near minimum\n",
      "\n",
      "MAE\n",
      "1. Scales linearly with size of error\n",
      "2. low sensitivity near minimum\n",
      "\n",
      "Huber\n",
      "1. Similar to MSE near minimum\n",
      "2. Similar to MAE away from minimum\n",
      "\n",
      "\n",
      "import tensorflow as tf\n",
      "loss=tf.keras.losses.mse(targets, predictions)\n",
      "\n",
      "\n",
      "def linear_regression(intercept, slope=slope, features=features):\n",
      "\treturn intercept+features*slope\n",
      "\n",
      "def loss_function(intercept, slope, targets=targets, features=features):\n",
      "\tpredictions=linear_regression(intercept,slope)\n",
      "\n",
      "\treturn tf.keras.losses.mse(targets,predictions)\n",
      "\n",
      "\n",
      "loss_function(intercept, slope, test_targets, test_features)\n",
      "\n",
      "  >Sample    Price and prediction error\n",
      "\n",
      "# Import the keras module from tensorflow\n",
      "from tensorflow import keras\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Compute the mean squared error (mse)\n",
      "loss = keras.losses.mse(price, predictions)\n",
      "\n",
      "# Print the mean squared error (mse)\n",
      "print(loss.numpy())\n",
      "\n",
      "plt.clf()\n",
      "plt.plot(predictions)\n",
      "plt.plot(price,color='red',alpha=0.2)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "You may have noticed that the MAE was much smaller than the MSE, even though price and predictions were the same. This is because the different loss functions penalize deviations of predictions from price differently. MSE does not like large deviations and punishes them harshly.\n",
      "\n",
      "\n",
      "   sample  > loss function\n",
      "\n",
      "# Initialize a variable named scalar\n",
      "scalar = Variable(1.0, float32)\n",
      "\n",
      "# Define the model\n",
      "def model(scalar, features = features):\n",
      "  \treturn scalar * features\n",
      "\n",
      "# Define a loss function\n",
      "def loss_function(scaler, features = features, targets = targets):\n",
      "\t# Compute the predicted values\n",
      "\tpredictions = model(scalar, features)\n",
      "    \n",
      "\t# Return the mean absolute error loss\n",
      "\treturn keras.losses.mae(targets, predictions)\n",
      "\n",
      "# Evaluate the loss function and print the loss\n",
      "print(loss_function(scalar).numpy())\n",
      "\n",
      "        >Linear Regression\n",
      "\n",
      "assumes that the relationship between two variables can be described by a line\n",
      "\n",
      "price=intercept + size * slope + error\n",
      "\n",
      "the difference between the predicted price and the actual price is the error and it can be used to construct the loss function.\n",
      "\n",
      "\n",
      "def linear_regression(intercept, slope, features=size):\n",
      "\treturn intercept+features*slope\n",
      "\n",
      "def loss_function(intercept, slope, targets=price, features=size):\n",
      "\t# Compute the predicted values\n",
      "\tpredictions = linear_regression(intercept, slope)\n",
      "    \n",
      "\t# Return the mean absolute error loss\n",
      "\treturn keras.losses.mae(targets, predictions)\n",
      "\n",
      "\n",
      "#define an optimization operation\n",
      "\n",
      "opt=tf.keras.optimizers.Adam()\n",
      "\n",
      "\n",
      "opt=tf.keras.optimizers.Adam()\n",
      "for j in range(1000):\n",
      "    opt.minimize(lambda: loss_function(intercept,slope),\\\n",
      "    var_list=[intercept,slope])\n",
      "    print(loss_function(intercept,slope))\n",
      "\n",
      "\n",
      "print(intercept.numpy(),slope.numpy())\n",
      "\n",
      "\n",
      "    sample    mae to find loss\n",
      "\n",
      "# Define a linear regression model\n",
      "def linear_regression(intercept, slope, features = size_log):\n",
      "\treturn intercept+features*slope\n",
      "\n",
      "# Set loss_function() to take the variables as arguments\n",
      "def loss_function(intercept,slope, features = size_log, targets = price_log):\n",
      "\t# Set the predicted values\n",
      "\tpredictions = linear_regression(intercept, slope, features)\n",
      "    \n",
      "    # Return the mean squared error loss\n",
      "\treturn keras.losses.mae(targets,predictions)\n",
      "\n",
      "# Compute the loss for different slope and intercept values\n",
      "print(loss_function(0.1, 0.1).numpy())\n",
      "print(loss_function(0.1, 0.5).numpy())\n",
      "\n",
      "\n",
      "   >Sample  > plot a regression that has optimized to a solution\n",
      "\n",
      "# Initialize an adam optimizer\n",
      "opt = keras.optimizers.Adam(0.5)\n",
      "\n",
      "for j in range(100):\n",
      "\t# Apply minimize, pass the loss function, and supply the variables\n",
      "\topt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
      "\n",
      "\t# Print every 10th value of the loss\n",
      "\tif j % 10 == 0:\n",
      "\t\tprint(loss_function(intercept, slope).numpy())\n",
      "\n",
      "# Plot data and regression line\n",
      "plot_results(intercept, slope)\n",
      "\n",
      "    sample    optimize to find the regression line\n",
      "\n",
      "# Define the linear regression model\n",
      "def linear_regression(params, feature1 = size_log, feature2 = bedrooms):\n",
      "\treturn params[0] + feature1*params[1] + feature2*params[2]\n",
      "\n",
      "# Define the loss function\n",
      "def loss_function(params, targets = price_log, feature1 = size_log, feature2 = bedrooms):\n",
      "\t# Set the predicted values\n",
      "\tpredictions = linear_regression(params, feature1, feature2)\n",
      "  \n",
      "\t# Use the mean absolute error loss\n",
      "\treturn keras.losses.mae(targets, predictions)\n",
      "\n",
      "# Define the optimize operation\n",
      "opt = keras.optimizers.Adam()\n",
      "\n",
      "# Perform minimization and print trainable variables\n",
      "for j in range(10):\n",
      "\topt.minimize(lambda: loss_function(params), var_list=[params])\n",
      "\tprint_results(params)\n",
      "\n",
      "\n",
      "       >Batch training\n",
      "\n",
      "batch training\n",
      "\n",
      "the complete dataset can not fit into memory\n",
      "so pass in batches of data\n",
      "\n",
      "chunksize parameter provides batch size data blocks\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "for batch in pd.read_csv('kc_housing.csv', chunksize=100):\n",
      "\tprice=np.array(batch['price'],np.float32)\n",
      "\tsize=np.array(batch['size'],np.float32)\n",
      "\n",
      "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list[intercept, slope])\n",
      "\n",
      "\n",
      "Batch Training\n",
      "1. multiple updates per epoch\n",
      "2. requires division of dataset\n",
      "3. no limit on dataset size\n",
      "\n",
      "\n",
      "      sample  linear regression and loss function\n",
      "\n",
      "\n",
      "# Define the intercept and slope\n",
      "intercept = Variable(10.0,np.float32)\n",
      "slope = Variable(0.5, float32)\n",
      "\n",
      "# Define the model\n",
      "def linear_regression(intercept, slope, features):\n",
      "\t# Define the predicted values\n",
      "\treturn intercept+features*slope\n",
      "\n",
      "# Define the loss function\n",
      "def loss_function(intercept, slope, targets, features):\n",
      "\t# Define the predicted values\n",
      "\tpredictions = linear_regression(intercept, slope, features)\n",
      "    \n",
      " \t# Define the MSE loss\n",
      "\treturn keras.losses.mse(targets, predictions)\n",
      "\n",
      "\n",
      "\n",
      "  > sample optimize the slope and intercept using batches of price and size\n",
      "\n",
      "# Initialize adam optimizer\n",
      "opt = keras.optimizers.Adam()\n",
      "\n",
      "# Load data in batches\n",
      "for batch in pd.read_csv('kc_house_data.csv',chunksize=100):\n",
      "\tsize_batch = np.array(batch['sqft_lot'], np.float32)\n",
      "\n",
      "\t# Extract the price values for the current batch\n",
      "\tprice_batch = np.array(batch['price'], np.float32)\n",
      "\n",
      "\t# Complete the loss, fill in the variable list, and minimize\n",
      "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
      "\n",
      "# Print trained parameters\n",
      "print(intercept.numpy(), slope.numpy())\n",
      "\n",
      "\n",
      "        Dense Layers\n",
      "\n",
      "credit card default\n",
      "1. Bill Amount\n",
      "2. Married\n",
      "3. Default\n",
      "\n",
      "input,hidden, output is called forward propagation\n",
      "\n",
      "input is our features\n",
      "ouput is our prediction\n",
      "\n",
      "import tensorflow as tf\n",
      "inputs = tf.constant[[1,35]])\n",
      "weights=tf.Variable([[-0.05],[-0.01]])\n",
      "\n",
      "bias = tf.Variable([0.5])\n",
      "\n",
      "plays the role of an intercept in a simple regression model.\n",
      "\n",
      "product=tf.matmul(inputs, weights)\n",
      "\n",
      "dense=tf.keras.activations.sigmoid(product+bias)\n",
      "\n",
      "  > define the complete model\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "inputs=tf.constant(data, tf.float32)\n",
      "\n",
      "dense1 = tf.keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
      "\n",
      "dense2 = tf.keras.layers.Dense(5, activation='sigmoid')(dense1)\n",
      "\n",
      "output = tf.keras.layers.Dense(1,activation='sigmoid')(dense2)\n",
      "\n",
      "\n",
      "   sample    calculate a dense layer manually\n",
      "\n",
      "# Initialize bias1\n",
      "bias1 = Variable(1.0)\n",
      "\n",
      "# Initialize weights1 as 3x2 variable of ones\n",
      "weights1 = Variable(ones((3, 2)))\n",
      "\n",
      "# Perform matrix multiplication of borrower_features and weights1\n",
      "product1 = matmul(borrower_features,weights1)\n",
      "\n",
      "# Apply sigmoid activation function to product1 + bias1\n",
      "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
      "\n",
      "# Print shape of dense1\n",
      "print(\"\\n dense1's output shape: {}\".format(dense1.shape))\n",
      "\n",
      "\n",
      "   > sample calculate a second dense layer \n",
      "\n",
      "# From previous step\n",
      "bias1 = Variable(1.0)\n",
      "weights1 = Variable(ones((3, 2)))\n",
      "product1 = matmul(borrower_features, weights1)\n",
      "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
      "\n",
      "# Initialize bias2 and weights2\n",
      "bias2 = Variable(1.0)\n",
      "weights2 = Variable(ones((2, 1)))\n",
      "\n",
      "# Perform matrix multiplication of dense1 and weights2\n",
      "product2 = matmul(dense1,weights2)\n",
      "\n",
      "# Apply activation to product2 + bias2 and print the prediction\n",
      "prediction = keras.activations.sigmoid(product2 + bias2)\n",
      "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
      "print('\\n actual: 1')\n",
      "\n",
      "\n",
      " >sample  > shape\n",
      "\n",
      "# Compute the product of borrower_features and weights1\n",
      "products1 = matmul(borrower_features,weights1)\n",
      "\n",
      "# Apply a sigmoid activation function to products1 + bias1\n",
      "dense1 = keras.activations.sigmoid(products1+bias1)\n",
      "\n",
      "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
      "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
      "print('\\n shape of weights1: ', weights1.shape)\n",
      "print('\\n shape of bias1: ', bias1.shape)\n",
      "print('\\n shape of dense1: ', dense1.shape)\n",
      "\n",
      "shape of borrower_features:  (5, 3)\n",
      "\n",
      " shape of weights1:  (3, 2)\n",
      "\n",
      " shape of bias1:  (1,)\n",
      "\n",
      " shape of dense1:  (5, 2)\n",
      "\n",
      "borrower_features, is 5x3 because it consists of 5 examples for 3 features. The shape of weights1 is 3x2, as it was in the previous exercise, since it does not depend on the number of examples. Additionally, bias1 is a scalar. Finally, dense1 is 5x2, which means that we can multiply it by the following set of weights, weights2, which we defined to be 2x1 in the previous exercise.\n",
      "\n",
      "\n",
      "  >Sample constructing an keras model\n",
      "\n",
      "# Define the first dense layer\n",
      "dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
      "\n",
      "# Define a dense layer with 3 output nodes\n",
      "dense2 = keras.layers.Dense(3,activation='sigmoid')(dense1)\n",
      "\n",
      "# Define a dense layer with 1 output node\n",
      "predictions = keras.layers.Dense(1,activation='sigmoid')(dense2)\n",
      "\n",
      "# Print the shapes of dense1, dense2, and predictions\n",
      "print('\\n shape of dense1: ', dense1.shape)\n",
      "print('\\n shape of dense2: ', dense2.shape)\n",
      "print('\\n shape of predictions: ', predictions.shape)\n",
      "\n",
      "output:\n",
      "shape of dense1:  (100, 7)\n",
      "\n",
      " shape of dense2:  (100, 3)\n",
      "\n",
      " shape of predictions:  (100, 1)\n",
      "\n",
      "      >Activation Functions\n",
      "\n",
      "hidden layers performs matrix multiplication and applies an activation function \n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "young, old = 0.3, 0.6\n",
      "low_bill, high_bill = 0.1, 0.5\n",
      "\n",
      "young_high=1.0* young + 2*high_bill\n",
      "young_low=1.0*young + 2*low_bill\n",
      "\n",
      "old_high=1.0* old + 2.0 * high_bill\n",
      "old_low=1.0*old + 2.0 * low_bill\n",
      "\n",
      "print(young_high-young_low)\n",
      "print(old_high-old_low)\n",
      "\n",
      "output\n",
      "0.8\n",
      "0.8\n",
      "\n",
      "print(tf.keras.activations.sigmoid(young_high).numpy()-tf.keras.activations.sigmoid(young_low).numpy())\n",
      "\n",
      "print(tf.keras.activations.sigmoid(old_high).numpy()-tf.keras.activations.sigmoid(old_low).numpy())\n",
      "\n",
      "output\n",
      "0.16337568\n",
      "0.14204389\n",
      "\n",
      "activation functions\n",
      "sigmoid (binary classification problems)\n",
      "relu (all layers but the output layer (0 or max(value))\n",
      "softmax (output layer for multiple classification)\n",
      "\n",
      "relu varies between 0 and infinity\n",
      "\n",
      "\n",
      "inputs= tf.constant(borrower_features,tf.float32)\n",
      "\n",
      "dense1=tf.keras.layers.Dense(16,activation='relu')(inputs)\n",
      "\n",
      "dense2=tf.keras.layers.Dense(8,activation='sigmoid')(dense1)\n",
      "\n",
      "outputs = tf.keras.layers.Dense(4,activation='softmax')(dense2)\n",
      "\n",
      "\n",
      "   > sample  > input bill_amounts  \n",
      "\n",
      "\n",
      "# Construct input layer from features\n",
      "print(bill_amounts)\n",
      "inputs = constant(bill_amounts)\n",
      "\n",
      "# Define first dense layer\n",
      "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
      "\n",
      "# Define second dense layer\n",
      "dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
      "\n",
      "# Define output layer\n",
      "outputs = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
      "\n",
      "# Print error for first five examples\n",
      "error = default[:5] - outputs.numpy()[:5]\n",
      "print(error)\n",
      "\n",
      "input\n",
      "[77479 77057 78102]\n",
      " [  326   326   326]\n",
      " [13686  1992   604]\n",
      "\n",
      "output\n",
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]]\n",
      "\n",
      "  >Sample   > 10 inputs 8 hidden 6 output\n",
      "\n",
      "# Construct input layer from borrower features\n",
      "inputs = constant(borrower_features)\n",
      "\n",
      "#print(len(borrower_features[0]))\n",
      "\n",
      "# Define first dense layer\n",
      "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
      "\n",
      "# Define second dense layer\n",
      "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
      "\n",
      "# Define output layer\n",
      "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
      "\n",
      "# Print first five predictions\n",
      "print(outputs.numpy()[:5])\n",
      "\n",
      "\n",
      "         >Gradient Descent Optimizer\n",
      "\n",
      "tf.keras.optimizer.SGD()\n",
      "learning_rate\n",
      "\n",
      "\n",
      "RMS\n",
      "applies different learning rates to each feature\n",
      "\n",
      "tf.keras.optimizers.RMSprop()\n",
      "learning_rate\n",
      "momentum\n",
      "decay\n",
      "\n",
      "allows momentum to both build and decay\n",
      "\n",
      "adam optimizer\n",
      "\n",
      "adaptive moment (adam) optimizer\n",
      "\n",
      "tk.keras.optimizers.Adam()\n",
      "\n",
      "learning_rate\n",
      "beta1 (decay)\n",
      "\n",
      "performs well with default parameter values\n",
      "\n",
      "\n",
      "def model(bias, weights, features=borrower_features):\n",
      "\tproduct=tf.matmul(features,weights)\n",
      "\treturn tf.keras.activations.sigmoid(product+bias)\n",
      "\n",
      "def loss_function(bias, weights, targets=default, features=borrower_features):\n",
      "\tpredictions=model(bias,weights)\n",
      "\treturn tf.keras.loss.binary_crossentropy(targets,predictions)\n",
      "\n",
      "opt=tk.keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.9)\n",
      "opt.minimize(lambda: loss_function(bias,weights), var_list=[bias, weights])\n",
      "\n",
      "\n",
      "    >sample  > minimize loss over 100 iterations\n",
      "\n",
      "# Initialize x_1 and x_2\n",
      "x_1 = Variable(6.0,float32)\n",
      "x_2 = Variable(0.3,float32)\n",
      "\n",
      "# Define the optimization operation\n",
      "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
      "\n",
      "for j in range(100):\n",
      "\t# Perform minimization using the loss function and x_1\n",
      "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
      "\t# Perform minimization using the loss function and x_2\n",
      "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
      "\n",
      "# Print x_1 and x_2 as numpy arrays\n",
      "print(x_1.numpy(), x_2.numpy())\n",
      "\n",
      "output\n",
      "4.3801394 0.42052683\n",
      "\n",
      "\n",
      "The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as loss_function()\n",
      "\n",
      "\n",
      " > sample rms with momentum\n",
      "\n",
      "# Initialize x_1 and x_2\n",
      "x_1 = Variable(0.05,float32)\n",
      "x_2 = Variable(0.05,float32)\n",
      "\n",
      "# Define the optimization operation for opt_1 and opt_2\n",
      "opt_1 = keras.optimizers.RMSprop(learning_rate=.01, momentum=0.99)\n",
      "opt_2 = keras.optimizers.RMSprop(learning_rate=.01, momentum=0.0)\n",
      "\n",
      "for j in range(100):\n",
      "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
      "    # Define the minimization operation for opt_2\n",
      "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
      "\n",
      "# Print x_1 and x_2 as numpy arrays\n",
      "print(x_1.numpy(), x_2.numpy())\n",
      "\n",
      "\n",
      "      Training a network in tensorflow\n",
      "\n",
      "the eggholder function\n",
      "\n",
      "there exists a global minimum but it is difficult to identify by inspection.\n",
      "\n",
      "use random and algorithmic selection of the initial values.\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "weights= tf.Variable(tf.random.normal([500],[500])\n",
      "\n",
      "\n",
      "weights= tf.Variable(tf.random.truncated_normal([500],[500])\n",
      "\n",
      "\n",
      "\n",
      "#define a dense layer with the default initializer\n",
      "dense=tf.keras.layers.Dense(32, activation='relu')\n",
      "\n",
      "dense=tf.keras.layers.Dense(32, activation='relu', \\\n",
      "\tkernel_initializer='zeros')\n",
      "\n",
      "\n",
      "dropout \n",
      "this will random drop weights on the network\n",
      "\n",
      "inputs=np.array(borrow_features, np.float32)\n",
      "dense1=tf.keras.layers.Dense(32, activation='relu')(inputs)\n",
      "\n",
      "dense2 = tf.keras.layers.Dense(16,activation='relu')(dense1)\n",
      "\n",
      "#drop 25 percent of the weights randomly\n",
      "dropout1=tf.keras.layers.Dropout(0.25)(dense2)\n",
      "\n",
      "outputs=tf.keras.layers.Dense(1, activation='sigmoid') (dropout1)\n",
      "\n",
      "\n",
      "   >sample initializing the weights of the neural network\n",
      "\n",
      "# Define the layer 1 weights\n",
      "w1 = Variable(random.normal([23, 7]))\n",
      "\n",
      "# Initialize the layer 1 bias\n",
      "b1 = Variable(ones([7]))\n",
      "\n",
      "# Define the layer 2 weights\n",
      "w2 = Variable(random.normal([7,1]))\n",
      "\n",
      "# Define the layer 2 bias\n",
      "b2 = Variable(0)\n",
      "\n",
      "   sample\n",
      "\n",
      "# Define the model\n",
      "def model(w1, b1, w2, b2, features = borrower_features):\n",
      "\t# Apply relu activation functions to layer 1\n",
      "\tlayer1 = keras.activations.sigmoid(matmul(features, w1) + b1)\n",
      "    # Apply dropout\n",
      "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
      "\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
      "\n",
      "# Define the loss function\n",
      "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
      "\tpredictions = model(w1, b1, w2, b2)\n",
      "\t# Pass targets and predictions to the cross entropy loss\n",
      "\treturn keras.losses.binary_crossentropy(targets, predictions)\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "# Train the model\n",
      "for j in range(100):\n",
      "    # Complete the optimizer\n",
      "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
      "                 var_list=[w1, b1, w2, b2])\n",
      "\n",
      "# Make predictions with model\n",
      "model_predictions = model(w1, b1, w2, b2, test_features)\n",
      "\n",
      "# Construct the confusion matrix\n",
      "confusion_matrix(test_targets, model_predictions)\n",
      "\n",
      "        Sequential\n",
      "\n",
      "28x28 image matrix\n",
      "\n",
      "16 input\n",
      "8 hidden\n",
      "4 output\n",
      "\n",
      "from tensorflow import keras\n",
      "\n",
      "model = keras.Sequential()\n",
      "\n",
      "model.add(keras.layers.Dense(16,activation='relu', input_shape=(28*28,)))\n",
      "\n",
      "model.add(Dense(8,activation='relu'))\n",
      "\n",
      "model.add(Dense(4,activation='softmax'))\n",
      "\n",
      "\n",
      "        Functional api\n",
      "\n",
      "model1_inputs=tf.keras.Input(shape=(28*28,))\n",
      "model2_inputs=tf.keras.Input(shape=(10,))\n",
      "\n",
      "model1_layer1=Dense(12,activation='relu')(model1_inputs)\n",
      "model1_layer2=Dense(4,activation='softmax')(model1_layer1)\n",
      "\n",
      "\n",
      "model2_layer1=Dense(12,activation='relu')(model2_inputs)\n",
      "model2_layer2=Dense(4,activation='softmax')(model2_layer1)\n",
      "\n",
      "merged=tf.keras.layers.add([model1_layer2,model2_layer2])\n",
      "\n",
      "\n",
      "model.compile('adam',loss='categorical_crossentropy')\n",
      "\n",
      "   Sign language    >  4 images\n",
      "\n",
      "# Define a Keras sequential model\n",
      "model=keras.Sequential()\n",
      "\n",
      "# Define the first dense layer 28x28\n",
      "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
      "\n",
      "# Define the second dense layer\n",
      "model.add(keras.layers.Dense(8,activation='relu'))\n",
      "\n",
      "# Define the output layer\n",
      "model.add(keras.layers.Dense(4,activation='softmax'))\n",
      "\n",
      "# Print the model architecture\n",
      "print(model.summary())\n",
      "\n",
      "\n",
      "\n",
      "   sign language with dropout rate of .25\n",
      "\n",
      "# Define the first dense layer\n",
      "model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))\n",
      "\n",
      "# Apply dropout to the first layer's output\n",
      "model.add(keras.layers.Dropout(0.25))\n",
      "\n",
      "# Define the output layer\n",
      "model.add(keras.layers.Dense(4, activation='softmax'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile('adam', loss='categorical_crossentropy')\n",
      "\n",
      "# Print a model summary\n",
      "print(model.summary())\n",
      "\n",
      "\n",
      "   > creating two inputs and Merge the outputs\n",
      "\n",
      "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
      "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
      "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
      "\n",
      "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
      "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
      "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
      "\n",
      "# Merge model outputs and define a functional model\n",
      "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
      "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
      "\n",
      "# Print a model summary\n",
      "print(model.summary())\n",
      "\n",
      "\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 12)           9420        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 12)           9420        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 4)            52          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 4)            52          dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 4)            0           dense_9[0][0]                    \n",
      "                                                                 dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 18,944\n",
      "Trainable params: 18,944\n",
      "Non-trainable params: 0\n",
      "______________________________\n",
      "\n",
      "    Training with Keras\n",
      "\n",
      "train and evaluate\n",
      "\n",
      "1. load and clean the data\n",
      "2. define the model\n",
      "3. train and validate model\n",
      "4. evaluate model\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "model=tf.keras.Sequential()\n",
      "\n",
      "model.add(tf.keras.layers.Dense(16,activation='relu', input_shape(784,)))\n",
      "\n",
      "model.add(tf.keras.layers.Dense(4,activation='softmax'))\n",
      "\n",
      "model.compile('adam',loss='categorical_crossentropy')\n",
      "\n",
      "model.fit(image_features, image_labels)\n",
      "\n",
      "batch_size (example 32 by default)\n",
      "epochs (times trained)\n",
      "validation_split\n",
      "1) training\n",
      "2. validation\n",
      "\n",
      "validation_split=0.20\n",
      "\n",
      "\n",
      "performing validation\n",
      "1. loss\n",
      "2. val_loss\n",
      "\n",
      "if training loss becames substantially less than validation loss than overfitting is occurring. \n",
      "\n",
      "1. Add dropout\n",
      "2. or regularize the data\n",
      "\n",
      "metrics['accuracy']\n",
      "\n",
      "model.fit(features,labels, epochs=10, validation_split=0.20)\n",
      "\n",
      "model.evaluate(test)\n",
      "\n",
      "\n",
      "\n",
      "    sample     compile the model\n",
      "\n",
      "# Define a sequential model\n",
      "model=keras.Sequential()\n",
      "\n",
      "# Define a hidden layer\n",
      "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
      "\n",
      "# Define the output layer\n",
      "model.add(keras.layers.Dense(4,activation='softmax'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile('SGD', loss='categorical_crossentropy',metrics=['accuracy'])\n",
      "\n",
      "# Complete the fitting operation\n",
      "model.fit(sign_language_features, sign_language_labels, epochs=5)\n",
      "\n",
      "\n",
      "    sample    RMSprop optimizer\n",
      "\n",
      "# Define sequential model\n",
      "model = keras.Sequential()\n",
      "\n",
      "# Define the first layer\n",
      "model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))\n",
      "\n",
      "# Add activation function to classifier\n",
      "model.add(keras.layers.Dense(4, activation='softmax'))\n",
      "\n",
      "# Set the optimizer, loss function, and metrics\n",
      "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "# Add the number of epochs and the validation split\n",
      "model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=.10)\n",
      "\n",
      "\n",
      "   > sample  > large number of neurons\n",
      "\n",
      "# Define sequential model\n",
      "model=keras.Sequential()\n",
      "\n",
      "# Define the first layer\n",
      "model.add(keras.layers.Dense(1024, activation='relu', input_shape=(784,)))\n",
      "# Add activation function to classifier\n",
      "model.add(keras.layers.Dense(4, activation='softmax'))\n",
      "\n",
      "# Finish the model compilation\n",
      "model.compile(optimizer=keras.optimizers.Adam(lr=0.001), \n",
      "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "# Complete the model fit operation\n",
      "model.fit(sign_language_features, sign_language_labels, epochs=50, validation_split=.5)\n",
      "\n",
      "if val_loss started to increase before the training process was terminated, then we may have overfitted. when this happens decrease the number of epochs.\n",
      "\n",
      "\n",
      "    sample  comparing a small model to a large model\n",
      "\n",
      "# Evaluate the small model using the train data\n",
      "small_train = small_model.evaluate(train_features, train_labels)\n",
      "\n",
      "# Evaluate the small model using the test data\n",
      "small_test = small_model.evaluate(test_features, test_labels)\n",
      "\n",
      "# Evaluate the large model using the train data\n",
      "large_train = large_model.evaluate(train_features, train_labels)\n",
      "\n",
      "# Evaluate the large model using the test data\n",
      "large_test = large_model.evaluate(test_features, test_labels)\n",
      "\n",
      "# Print losses\n",
      "print('\\n Small - Train: {}, Test: {}'.format(small_train, small_test))\n",
      "print('Large - Train: {}, Test: {}'.format(large_train, large_test))\n",
      "\n",
      "\n",
      "      Estimators api\n",
      "\n",
      "\n",
      "high level submodule:estimators\n",
      "mid level: layers, datasets, metrics\n",
      "low level: python\n",
      "\n",
      "\n",
      "estimators enforce best practices\n",
      "\n",
      "1. define feature columns\n",
      "2. load and transform data\n",
      "3. define an estimator\n",
      "4. apply train operation\n",
      "\n",
      "Defining feature columns:\n",
      "\n",
      "for an image\n",
      "features_list[tf.feature_column.numeric_column('image',shape(784,))]\n",
      "\n",
      "\n",
      "     Loading and transforming data\n",
      "\n",
      "def input_fn():\n",
      "    features={\"size\":[1340,1690,2720],'rooms':[1,3,4]}\n",
      "    layers=[221900,538000,180000]\n",
      "\n",
      "    return features, labels\n",
      "\n",
      "size=tf.feature_column.numeric_column(\"size\")\n",
      "rooms=tf.feature_column.categorical_column_with_vocabulary_list(\"rooms\",[\"1\",\"2\",\"3\",\"4\",\"5\"],default_value=0)\n",
      "feature_list=[size,\n",
      "              tf.feature_column.indicator_column(rooms)]\n",
      "\n",
      "model0=tf.estimator.DNNRegressor(feature_columns=feature_list,hidden_units=[10,6,6,1])\n",
      "model0.train(input_fn,steps=20)\n",
      "\n",
      "\n",
      "model1=tf.estimator.DNNClassifier(feature_columns=feature_list, hidden_units=[32,16,8],n_classes=4)\n",
      "\n",
      "model0.train(input_fn,steps=20)\n",
      "\n",
      "    >sample   > define feature columns and build an input_fn\n",
      "\n",
      "# Define feature columns for bedrooms and bathrooms\n",
      "bedrooms = feature_column.numeric_column(\"bedrooms\")\n",
      "bathrooms = feature_column.numeric_column(\"bathrooms\")\n",
      "\n",
      "# Define the list of feature columns\n",
      "feature_list = [bedrooms, bathrooms]\n",
      "\n",
      "def input_fn():\n",
      "\t# Define the labels\n",
      "\tlabels = np.array(housing['price'])\n",
      "\t# Define the features\n",
      "\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \n",
      "                'bathrooms':np.array(housing['bathrooms'])}\n",
      "\treturn features, labels\n",
      "\n",
      "  > use a linear regressor\n",
      "\n",
      "# Define the model and set the number of steps\n",
      "model = estimator.LinearRegressor(feature_columns=feature_list) \n",
      "model.train(input_fn, steps=2)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['tensor']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
