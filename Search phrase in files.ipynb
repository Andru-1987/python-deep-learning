{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\analysis for traffic stops by police officers.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\analysis for traffic stops by police officers.txt\n",
      "the dataset for stops by police officers in the state of rhode island.\n",
      "\n",
      "1. State\n",
      "2. stop_date\n",
      "3. stop_time\n",
      "4. county_name (contains nan values)\n",
      "5. driver_gender\n",
      "6. driver_race\n",
      "\n",
      "\n",
      "ri=pd.read_csv('police.csv')\n",
      "ri.isnull()\n",
      "\n",
      "ri.isnull().sum()\n",
      "county_name=91741\n",
      "\n",
      "ri.shape()\n",
      "output: 91741,15\n",
      "\n",
      "drop county_name column\n",
      "\n",
      "ri.drop('county_name',axis='columns', inplace=True)\n",
      "\n",
      ".dropna() : drops rows based on the presence of missing values.\n",
      "\n",
      "\n",
      "\n",
      "   sample  > dropping a column\n",
      "\n",
      "# Examine the shape of the DataFrame\n",
      "print(ri.shape)\n",
      "\n",
      "# Drop the 'county_name' and 'state' columns\n",
      "ri.drop(['county_name', 'state'], axis='columns', inplace=True)\n",
      "\n",
      "# Examine the shape of the DataFrame (again)\n",
      "print(ri.shape)\n",
      "\n",
      "   sample  > drop na subset\n",
      "\n",
      "# Count the number of missing values in each column\n",
      "print(ri.isnull().sum())\n",
      "\n",
      "# Drop all rows that are missing 'driver_gender'\n",
      "ri.dropna(subset=['driver_gender'], inplace=True)\n",
      "\n",
      "# Count the number of missing values in each column (again)\n",
      "print(ri.isnull().sum())\n",
      "\n",
      "# Examine the shape of the DataFrame\n",
      "print(ri.shape)\n",
      "\n",
      "\n",
      "removing columns and rows that will not be useful.\n",
      "\n",
      "\n",
      "   Examining the data types\n",
      "read_csv creates an inferred datatype\n",
      "\n",
      "print(ri.dtypes)\n",
      "\n",
      "dtype:\n",
      "1.object\n",
      "2.bool\n",
      "3.int\n",
      "4.float\n",
      "5.datetime\n",
      "6.category\n",
      "\n",
      "datatype affect opeations you can perform\n",
      "\n",
      "math operations can be performed on int and floats\n",
      "\n",
      "datetime \n",
      "category uses less memory and runs faster\n",
      "bool enables logical and mathematical operations\n",
      "\n",
      "\n",
      "apple\n",
      "1. date\n",
      "2. time\n",
      "3. price\n",
      "\n",
      "apple.price.dtype\n",
      "output dtype('O') means object\n",
      "\n",
      "apple['price']= apple.price.astype('float')\n",
      "\n",
      "\n",
      "  >Sample  > convert object dtype to bool\n",
      "\n",
      "# Examine the head of the 'is_arrested' column\n",
      "print(ri.is_arrested.dtype)\n",
      "\n",
      "# Change the data type of 'is_arrested' to 'bool'\n",
      "ri['is_arrested'] = ri.is_arrested.astype(bool)\n",
      "\n",
      "# Check the data type of 'is_arrested' \n",
      "print(ri.is_arrested.dtype)\n",
      "\n",
      "\n",
      "   sample  > value_counts and unique\n",
      "\n",
      "# Count the unique values in 'violation'\n",
      "print(ri['violation'].unique())\n",
      "\n",
      "# Express the counts as proportions\n",
      "print(ri['violation'].value_counts(normalize=True))\n",
      "\n",
      "['Equipment' 'Speeding' 'Other' 'Moving violation' 'Registration/plates'\n",
      " 'Seat belt']\n",
      "Speeding               48423\n",
      "Moving violation       16224\n",
      "Equipment              10921\n",
      "Other                   4409\n",
      "Registration/plates     3703\n",
      "Seat belt               2856\n",
      "Name: violation, dtype: int64\n",
      "\n",
      "  >normalized=True    output\n",
      "\n",
      "Speeding               0.559571\n",
      "Moving violation       0.187483\n",
      "Equipment              0.126202\n",
      "Other                  0.050950\n",
      "Registration/plates    0.042791\n",
      "Seat belt              0.033004\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "\n",
      "  >sample    women have more speeding violations\n",
      "\n",
      "# Create a DataFrame of female drivers\n",
      "female = ri[ri['driver_gender']=='F']\n",
      "\n",
      "# Create a DataFrame of male drivers\n",
      "male = ri[ri['driver_gender']=='M']\n",
      "\n",
      "print(female.violation.value_counts(normalize=True))\n",
      "\n",
      "# Compute the violations by male drivers (as proportions)\n",
      "print(male.violation.value_counts(normalize=True))\n",
      "\n",
      "output:\n",
      "\n",
      "Speeding               0.658114\n",
      "Moving violation       0.138218\n",
      "Equipment              0.105199\n",
      "Registration/plates    0.044418\n",
      "Other                  0.029738\n",
      "Seat belt              0.024312\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "Speeding               0.522243\n",
      "Moving violation       0.206144\n",
      "Equipment              0.134158\n",
      "Other                  0.058985\n",
      "Registration/plates    0.042175\n",
      "Seat belt              0.036296\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "\n",
      "Filtering a dataframe using multiple conditions\n",
      "\n",
      "female = ri[ri.driver_gender=='F']\n",
      "female.shape\n",
      "\n",
      "or\n",
      "\n",
      "female = ri[\n",
      "(ri.driver_gender=='F') &\n",
      "(ri.is_arrested==True)\n",
      "]\n",
      "female.shape\n",
      "\n",
      "\n",
      "each condition is surround by parentheses and the & separates the conditions\n",
      "\n",
      "only female drivers who were arrested\n",
      "\n",
      "| represents the or condition\n",
      "\n",
      "|| represents the and condition\n",
      "\n",
      "\n",
      " sample  > filtering\n",
      "\n",
      "ri[(ri.driver_gender=='F') & (ri.violation=='Speeding')]\n",
      "\n",
      "\n",
      " > Sample  > Stop outcomes\n",
      "\n",
      "\n",
      "# Create a DataFrame of female drivers stopped for speeding\n",
      "female_and_speeding = ri[(ri.driver_gender=='F') & (ri.violation=='Speeding')]\n",
      "\n",
      "# Create a DataFrame of male drivers stopped for speeding\n",
      "male_and_speeding = ri[(ri.driver_gender=='M') & (ri.violation=='Speeding')]\n",
      "\n",
      "print(\"male\")\n",
      "# Compute the stop outcomes for female drivers (as proportions)\n",
      "print(female_and_speeding.stop_outcome.value_counts(normalize=True))\n",
      "print(\"female\")\n",
      "# Compute the stop outcomes for male drivers (as proportions)\n",
      "print(male_and_speeding.stop_outcome.value_counts(normalize=True))\n",
      "\n",
      "Output::  (95% of stops resulting in a ticket)\n",
      "\n",
      "male\n",
      "Citation            0.952192\n",
      "Warning             0.040074\n",
      "Arrest Driver       0.005752\n",
      "N/D                 0.000959\n",
      "Arrest Passenger    0.000639\n",
      "No Action           0.000383\n",
      "Name: stop_outcome, dtype: float64\n",
      "\n",
      "female\n",
      "Citation            0.944595\n",
      "Warning             0.036184\n",
      "Arrest Driver       0.015895\n",
      "Arrest Passenger    0.001281\n",
      "No Action           0.001068\n",
      "N/D                 0.000976\n",
      "Name: stop_outcome, dtype: float64\n",
      "\n",
      "\n",
      "   >Does gender affect the vehicles that are searched?\n",
      "\n",
      "\n",
      "ri.isnull().sum()\n",
      "\n",
      "true is 1\n",
      "false is 0\n",
      "then sum the rows\n",
      "\n",
      "the mean of a boolean series represents the percentage of True values\n",
      "\n",
      "ri.is_arrested.value_counts(normalized=True)\n",
      ".03\n",
      "ri.is_arrested.mean()\n",
      ".03\n",
      "\n",
      "\n",
      "find the unique districts\n",
      "\n",
      "ri.district.unique()\n",
      "\n",
      "print(df_sas[df_sas['District'].isin(districts)]['ArrestInt'].mean())\n",
      "\n",
      "\n",
      "print(df_sas.groupby('District')['ArrestInt'].mean())\n",
      "\n",
      "print(df_sas.groupby(['District','Ward'])['ArrestInt'].mean())\n",
      "\n",
      "  >Sample    search_conducted\n",
      "\n",
      "# Check the data type of 'search_conducted'\n",
      "print(ri['search_conducted'].dtype)\n",
      "\n",
      "# Calculate the search rate by counting the values\n",
      "print(ri['search_conducted'].value_counts(normalize=True))\n",
      "\n",
      "# Calculate the search rate by taking the mean\n",
      "print(ri.search_conducted.mean())\n",
      "\n",
      "\n",
      "output\n",
      "bool\n",
      "False    0.961785\n",
      "True     0.038215\n",
      "Name: search_conducted, dtype: float64\n",
      "0.0382153092354627\n",
      "\n",
      "\n",
      "  Sample  > female\n",
      "\n",
      "# Calculate the search rate for female drivers\n",
      "print(ri[ri.driver_gender=='F'].search_conducted.mean())\n",
      "\n",
      "output: 0.019180617481282074 (female)\n",
      "output: 0.04542557598546892 (male)\n",
      "\n",
      " >Sample  > groupby\n",
      "\n",
      "# Calculate the search rate for both groups simultaneously\n",
      "print(ri.groupby('driver_gender').search_conducted.mean())\n",
      "\n",
      " >Sample  > groupby multiple column\n",
      "\n",
      "print(ri.groupby(['driver_gender','violation']).search_conducted.mean())\n",
      "\n",
      "driver_gender  violation          \n",
      "F              Equipment              0.039984\n",
      "               Moving violation       0.039257\n",
      "               Other                  0.041018\n",
      "               Registration/plates    0.054924\n",
      "               Seat belt              0.017301\n",
      "               Speeding               0.008309\n",
      "\n",
      "M              Equipment              0.071496\n",
      "               Moving violation       0.061524\n",
      "               Other                  0.046191\n",
      "               Registration/plates    0.108802\n",
      "               Seat belt              0.035119\n",
      "               Speeding               0.027885\n",
      "Name: search_conducted, dtype: float64\n",
      "\n",
      "\n",
      "       >Gender affect frisking\n",
      "\n",
      "ri.search_type.value_counts(dropna=False)\n",
      "1. Incident to Arrest\n",
      "2. Probable cause\n",
      "3. Inventory\n",
      "4. Reasonable Suspicion\n",
      "5. Protective Frisk\n",
      "6. Incident to Arrest, Inventory\n",
      "7. Incident to Arrest, Probable Cause\n",
      "\n",
      "\n",
      "ri['inventory']=ri.search_type.str.contains('Inventory',na=False)\n",
      "\n",
      "na=False means return a false when it finds a missing value\n",
      "ri.inventory.sum()\n",
      "\n",
      "\n",
      "search=ri[ri.searched_conducted==True]\n",
      "searched.inventory.mean()\n",
      "\n",
      "\n",
      " >Sample    > search type count, frisk in the search_type\n",
      "\n",
      "# Count the 'search_type' values\n",
      "print(len(ri.search_type.unique()))\n",
      "\n",
      "# Check if 'search_type' contains the string 'Protective Frisk'\n",
      "ri['frisk'] = ri.search_type.str.contains('Protective Frisk', na=False)\n",
      "\n",
      "# Check the data type of 'frisk'\n",
      "print(ri['frisk'].dtype)\n",
      "\n",
      "# Take the sum of 'frisk'\n",
      "print(ri['frisk'].sum())\n",
      "\n",
      "\n",
      " >Sample  > search conduction    frisk average per gender\n",
      "\n",
      "# Create a DataFrame of stops in which a search was conducted\n",
      "searched = ri[ri.search_conducted == True]\n",
      "\n",
      "# Calculate the overall frisk rate by taking the mean of 'frisk'\n",
      "print(searched.frisk.mean())\n",
      "\n",
      "# Calculate the frisk rate for each gender\n",
      "print(searched.groupby(\"driver_gender\").frisk.mean())\n",
      "\n",
      "      Does the time of day affect arrest rate\n",
      "\n",
      "analyzing datetime data\n",
      "\n",
      "apple\n",
      "1. price\n",
      "2. volume (shares traded)\n",
      "3. date_and_time\n",
      "\n",
      "\n",
      "dt.month\n",
      "dt.week\n",
      "dt.dayofweek\n",
      "dt.hour\n",
      "\n",
      "apple.set_index('date_and_time', inplace=True)\n",
      "apple.index.month\n",
      "apple.price.mean()\n",
      "\n",
      "month_price=apple.groupby(apple.index.month).price.mean()\n",
      "\n",
      "monthly_price.plot()\n",
      "plt.xlabel('Month')\n",
      "plt.ylabel('Price')\n",
      "\n",
      "df_sas['Year']=pd.DatetimeIndex(df_sas['date']).year\n",
      "arrest_year=df_sas.groupby(['Year'])['ArrestInt'].sum()\n",
      "\n",
      "\n",
      "  >Sample   > arrest rate as a time of day\n",
      "\n",
      "# Calculate the overall arrest rate\n",
      "print(ri.is_arrested.mean())\n",
      "\n",
      "# Calculate the hourly arrest rate\n",
      "print(ri.groupby(ri.index.hour).is_arrested.mean())\n",
      "\n",
      "# Save the hourly arrest rate\n",
      "hourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean()\n",
      "\n",
      "\n",
      "  Sample  > plot arrest time\n",
      "\n",
      "# Import matplotlib.pyplot as plt\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Create a line plot of 'hourly_arrest_rate'\n",
      "hourly_arrest_rate.plot()\n",
      "\n",
      "# Add the xlabel, ylabel, and title\n",
      "plt.xlabel('Hour')\n",
      "plt.ylabel('Arrest Rate')\n",
      "plt.title('Arrest Rate by Time of Day')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "         Are drug related stops on the rise\n",
      "1. We will use a subplot to see how two variables change over time\n",
      "\n",
      "2. Resampling is when you change the frequency of the time series\n",
      "\n",
      "monthly_price=apple.price.resample('M').mean()\n",
      "\n",
      "resample by month\n",
      "\n",
      "the output is the last day of the month rather than a number\n",
      "\n",
      "monthly_volume=apple.volume.resample('M').mean()\n",
      "\n",
      "\n",
      "pd.concat([monthly_price,monthly_volume],axis='columns')\n",
      "\n",
      "concatenates along a specified axis\n",
      "\n",
      "monthly.plot(subplots=True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  Sample    drug related stops   resampling\n",
      "\n",
      "# Calculate the annual rate of drug-related stops\n",
      "print(ri.drugs_related_stop.resample('A').mean())\n",
      "\n",
      "# Save the annual rate of drug-related stops\n",
      "annual_drug_rate = ri.drugs_related_stop.resample('A').mean()\n",
      "\n",
      "# Create a line plot of 'annual_drug_rate'\n",
      "annual_drug_rate.plot(subplots=True)\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "  Sample  > concatenate the two columns\n",
      "\n",
      "# Calculate and save the annual search rate\n",
      "annual_search_rate = ri.search_conducted.resample('A').mean()\n",
      "\n",
      "# Concatenate 'annual_drug_rate' and 'annual_search_rate'\n",
      "annual = pd.concat([annual_drug_rate,annual_search_rate], axis='columns')\n",
      "\n",
      "# Create subplots from 'annual'\n",
      "annual.plot(subplots=True)\n",
      "\n",
      "# Display the subplots\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    What violations are caught in each district\n",
      "\n",
      "result=df_sas.groupby(['Year','Month','fbi_code'])['ArrestInt'].sum().reset_index()\n",
      "#print(top20.columns)\n",
      "mask=result['ArrestInt']>30\n",
      "fbi_codes=result[mask]['fbi_code'].unique()\n",
      "\n",
      "filter=df_sas['fbi_code'].isin(fbi_codes) \n",
      "fbi_codes=df_sas['fbi_code'].unique()\n",
      "\n",
      "arrest_breakdown=df_sas[filter].groupby(['Year','Month','fbi_code'])['ArrestInt'].sum().reset_index()\n",
      "keys=arrest_breakdown.keys()\n",
      "#print(arrest_breakdown)\n",
      "\n",
      "g = sns.factorplot(data=arrest_breakdown, x='Year', y='ArrestInt', \n",
      "                  hue='fbi_code',  kind='point',size=8,aspect=2)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "         cross tab\n",
      "\n",
      "table=pd.crosstab(ri.driver_race, ri_driver_gender)\n",
      "\n",
      "creates a pivot table building a frequency table\n",
      "\n",
      "ri[(ri.driver_race=='Asian') & (ri.driver_gender=='F')].shape\n",
      "\n",
      "\n",
      "range=table.loc['Asian':'Hispanic']\n",
      "\n",
      "range.plot(kind='bar')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " > stack bar plot\n",
      "\n",
      "range.plot(kind='bar', stacked=True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    Sample\n",
      "\n",
      "# Create a frequency table of districts and violations\n",
      "print(pd.crosstab(ri.district,ri.violation))\n",
      "\n",
      "# Save the frequency table as 'all_zones'\n",
      "all_zones = pd.crosstab(ri.district,ri.violation)\n",
      "\n",
      "# Select rows 'Zone K1' through 'Zone K3'\n",
      "print(all_zones.loc['Zone K1':'Zone K3'])\n",
      "\n",
      "# Save the smaller table as 'k_zones'\n",
      "k_zones = all_zones.loc['Zone K1':'Zone K3']\n",
      "\n",
      "k_zone.plot(kind='bar', stacked=True)\n",
      "plt.show()\n",
      "\n",
      "     How long might you be stopped\n",
      "\n",
      "apple\n",
      "date_and_time\n",
      "price\n",
      "volume\n",
      "change\n",
      "\n",
      "change when  change\n",
      "\n",
      "True if the price went up\n",
      "\n",
      "calculate how often the price went up taking the column mean\n",
      "\n",
      "\n",
      "Stefan Jansen\n",
      "https://www.amazon.com/dp/B08D9SP6MB/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1\n",
      "\n",
      "mapping = {'up':True, 'down':False}\n",
      "apple['is_up']=apple.chage.map(mapping)\n",
      "\n",
      "apple.is_up.mean()\n",
      "\n",
      "  >how often searches occur after each violation\n",
      "\n",
      "search_rate=ri.groupby('violation').search_conducted.mean()\n",
      "\n",
      "search_rate.plot(kind='bar')\n",
      "plt.show()\n",
      "\n",
      "search rate is on the y axis\n",
      "the violation is on the x axis\n",
      "\n",
      "\n",
      "search_rate.plot(kind='barh')\n",
      "plt.show()\n",
      "\n",
      "  sample    mapping\n",
      "\n",
      "# Print the unique values in 'stop_duration'\n",
      "print(ri.stop_duration.unique())\n",
      "\n",
      "# Create a dictionary that maps strings to integers\n",
      "mapping = {\n",
      "    '0-15 Min': 8,\n",
      "    '16-30 Min': 23,\n",
      "    '30+ Min': 45\n",
      "    }\n",
      "\n",
      "# Convert the 'stop_duration' strings to integers using the 'mapping'\n",
      "ri['stop_minutes'] = ri.stop_duration.map(mapping)\n",
      "\n",
      "# Print the unique values in 'stop_minutes'\n",
      "print(ri['stop_minutes'].unique())\n",
      "\n",
      "\n",
      "   sample  >  groupby    average  > sort\n",
      "\n",
      "\n",
      "# Calculate the mean 'stop_minutes' for each value in 'violation_raw'\n",
      "print(ri.groupby('violation_raw')['stop_minutes'].mean())\n",
      "\n",
      "# Save the resulting Series as 'stop_length'\n",
      "stop_length = ri.groupby('violation_raw')['stop_minutes'].mean()\n",
      "\n",
      "# Sort 'stop_length' by its values and create a horizontal bar plot\n",
      "stop_length.sort_values().plot(kind='barh')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   Exploring the weather dataset\n",
      "\n",
      "do weather conditions affect police behavior\n",
      "\n",
      "noaa : national centers for environmental information\n",
      "\n",
      "single station in rhode islands to give weather information\n",
      "\n",
      "weather = pd.read_csv('weather.csv')\n",
      "\n",
      "weather.head(3)\n",
      "\n",
      "TAVG, TMIN, TMAX: Temperature\n",
      "AWND. WSF2: Wind Speed (average, fastest wind speed in a 2 minute interval)\n",
      "WT01, WT022: Bad Weather conditions\n",
      "\n",
      "https://mesonet.agron.iastate.edu/request/download.phtml?network=ID_ASOS\n",
      "\n",
      "increased convinced the data is trustworthy\n",
      "\n",
      "\n",
      "station:three or four character site identifier\n",
      "valid:timestamp of the observation\n",
      "tmpf:Air Temperature in Fahrenheit, typically @ 2 meters\n",
      "dwpf:Dew Point Temperature in Fahrenheit, typically @ 2 meters\n",
      "relh:Relative Humidity in %\n",
      "drct:Wind Direction in degrees from north\n",
      "sknt:Wind Speed in knots \n",
      "p01i:One hour precipitation for the period from the observation time to the time of the previous hourly precipitation reset. This varies slightly by site. Values are in inches. This value may or may not contain frozen precipitation melted by some device on the sensor or estimated by some other means. Unfortunately, we do not know of an authoritative database denoting which station has which sensor.\n",
      "alti:Pressure altimeter in inches\n",
      "mslp:Sea Level Pressure in millibar\n",
      "vsby:Visibility in miles\n",
      "gust:Wind Gust in knots\n",
      "skyc1:Sky Level 1 Coverage\n",
      "skyc2:Sky Level 2 Coverage\n",
      "skyc3:Sky Level 3 Coverage\n",
      "skyc4:Sky Level 4 Coverage\n",
      "skyl1:Sky Level 1 Altitude in feet\n",
      "skyl2:Sky Level 2 Altitude in feet\n",
      "skyl3:Sky Level 3 Altitude in feet\n",
      "skyl4:Sky Level 4 Altitude in feet\n",
      "wxcodes:Present Weather Codes (space seperated)\n",
      "feel:Apparent Temperature (Wind Chill or Heat Index) in Fahrenheit\n",
      "ice_accretion_1hr:Ice Accretion over 1 Hour (inches)\n",
      "ice_accretion_3hr:Ice Accretion over 3 Hours (inches)\n",
      "ice_accretion_6hr:Ice Accretion over 6 Hours (inches)\n",
      "peak_wind_gust:Peak Wind Gust (from PK WND METAR remark) (knots)\n",
      "peak_wind_drct:Peak Wind Gust Direction (from PK WND METAR remark) (deg)\n",
      "peak_wind_time:Peak Wind Gust Time (from PK WND METAR remark)\n",
      "metar:unprocessed reported observation in METAR format\n",
      "\n",
      "weather[['AWND','WSF2']].describe()\n",
      "\n",
      "create a box plot\n",
      "weather[['AWND','WSF2']].plot(kind='box')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "take the fast wind speed minus the average wind speed\n",
      "\n",
      "weather['WDIFF']= weather.WSF2-weather.AWND\n",
      "\n",
      "weather.WDIFF.plot(kind='hist', bins =20)\n",
      "plt.show()\n",
      "\n",
      "  Sample   > box plot temperatures\n",
      "\n",
      "# Read 'weather.csv' into a DataFrame named 'weather'\n",
      "df=pd.read_csv('weather.csv')\n",
      "\n",
      "# Describe the temperature columns\n",
      "print(df[['TMIN','TAVG','TMAX']].describe())\n",
      "\n",
      "# Create a box plot of the temperature columns\n",
      "df[['TMIN','TAVG','TMAX']].plot(kind='box')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "  >Sample   histogram to confirm temperature range distribution\n",
      "\n",
      "# Create a 'TDIFF' column that represents temperature difference\n",
      "weather['TDIFF']=weather.TMAX- weather.TMIN\n",
      "\n",
      "# Describe the 'TDIFF' column\n",
      "print(weather['TDIFF'].describe())\n",
      "\n",
      "# Create a histogram with 20 bins to visualize 'TDIFF'\n",
      "weather['TDIFF'].plot(kind='hist',bins=20)\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "            >Categorizing the weather\n",
      "\n",
      "\n",
      "slicing columns of the original dataframe\n",
      "\n",
      "temp=weather.loc[:,'TAVG':'TMAX']\n",
      "\n",
      "\n",
      "temp.sum(axis='columns').head()  \n",
      "\n",
      "this sums all the columns\n",
      "\n",
      "ri.stop_duration.unique()\n",
      "\n",
      "mapping = {\n",
      "    '0-15 Min': 'short',\n",
      "    '16-30 Min': 'medium',\n",
      "    '30+ Min': 'long'\n",
      "    }\n",
      "\n",
      "# Convert the 'stop_duration' strings to integers using the 'mapping'\n",
      "ri['stop_length'] = ri.stop_duration.map(mapping)\n",
      "\n",
      "ri.stop_length.dtype\n",
      "outputs object type because it contains string data\n",
      "\n",
      "ri.stop_length.unique()\n",
      "\n",
      "\n",
      "cats=['short','medium','long']\n",
      "ri.stop_length.astype('category',ordered=True,categories=cats)\n",
      "\n",
      "\n",
      "1. stores more efficiently\n",
      "2. allows a logical order\n",
      "\n",
      "ri.stop_length.memory_usage(deep=True)  #memory used to store the column\n",
      "\n",
      "cats=['short','medium','long']\n",
      "\n",
      "ri[ri.stop_length>'short']\n",
      "output data with categories of medium or long stop_length\n",
      "\n",
      "ri.groupby('stop_length').is_arrested.mean()\n",
      "\n",
      "\n",
      "   Sample  > bad weather conditions\n",
      "\n",
      "# Copy 'WT01' through 'WT22' to a new DataFrame\n",
      "WT = weather.loc[:,'WT01':'WT22']\n",
      "\n",
      "# Calculate the sum of each row in 'WT'\n",
      "weather['bad_conditions'] = WT.sum(axis='columns')\n",
      "\n",
      "# Replace missing values in 'bad_conditions' with '0'\n",
      "weather['bad_conditions'] = weather.bad_conditions.fillna(0).astype('int')\n",
      "\n",
      "# Create a histogram to visualize 'bad_conditions'\n",
      "\n",
      "\n",
      "weather['bad_conditions'].plot(kind='hist')\n",
      "plt.show()\n",
      "# Display the plot\n",
      "\n",
      "   sample   > bad_conditions by category\n",
      "\n",
      "# Count the unique values in 'bad_conditions' and sort the index\n",
      "print(weather.bad_conditions.value_counts().sort_index())\n",
      "\n",
      "# Create a dictionary that maps integers to strings\n",
      "mapping = {0:'good', 1:'bad', 2:'bad', 3:'bad', 4:'bad', 5:'worse', 6:'worse', 7:'worse', 8:'worse', 9:'worse'}\n",
      "\n",
      "# Convert the 'bad_conditions' integers to strings using the 'mapping'\n",
      "weather['rating'] = weather.bad_conditions.map(mapping)\n",
      "\n",
      "# Count the unique values in 'rating'\n",
      "print(weather['rating'].unique())\n",
      "\n",
      "print(weather.rating.value_counts())\n",
      "\n",
      "output:\n",
      "bad      1836\n",
      "good     1749\n",
      "worse     432\n",
      "Name: rating, dtype: int64\n",
      "\n",
      "  Sample   > create a column as a category\n",
      "\n",
      "# Create a list of weather ratings in logical order\n",
      "cats=['good','bad','worse']\n",
      "\n",
      "# Change the data type of 'rating' to category\n",
      "weather['rating'] = weather.rating.astype('category', ordered=True, categories=cats)\n",
      "\n",
      "# Examine the head of 'rating'\n",
      "print(weather['rating'].head())\n",
      "\n",
      "\n",
      "       Merging Datasets\n",
      "\n",
      "reset_index returns the index to an autonumber\n",
      "\n",
      "\n",
      "high=high_low[['DATE','HIGH']]\n",
      "\n",
      "we only need the high column\n",
      "\n",
      "apple_high=pd.merge(left=apple, right=high, left_on='date', right_on='DATE', how=left)\n",
      "\n",
      "apple_high.set_index('date_and_time', inplace=True)\n",
      "\n",
      "\n",
      "\n",
      "<<<<<<<Sample  > reset the index to the autonumber,  extract the DATE and rating columns from the weather dataframe\n",
      "\n",
      "# Reset the index of 'ri'\n",
      "ri.reset_index(inplace=True)\n",
      "\n",
      "# Examine the head of 'ri'\n",
      "print(ri.head())\n",
      "\n",
      "# Create a DataFrame from the 'DATE' and 'rating' columns\n",
      "weather_rating=weather[['DATE','rating']]\n",
      "\n",
      "# Examine the head of 'weather_rating'\n",
      "print(weather_rating.head())\n",
      "\n",
      "\n",
      "   Sample  > merge columns on stop_date and date\n",
      "\n",
      "# Examine the shape of 'ri'\n",
      "print(ri.shape)\n",
      "\n",
      "# Merge 'ri' and 'weather_rating' using a left join\n",
      "ri_weather = pd.merge(left=ri, right=weather_rating, left_on='stop_date', right_on='DATE', how='left')\n",
      "\n",
      "# Examine the shape of 'ri_weather'\n",
      "print(ri_weather.shape)\n",
      "\n",
      "# Set 'stop_datetime' as the index of 'ri_weather'\n",
      "ri_weather.set_index('stop_datetime', inplace=True)\n",
      "\n",
      "\n",
      "https://datatofish.com/multiple-linear-regression-python/\n",
      "\n",
      "\n",
      " > weather and behavior\n",
      "\n",
      "search_rate = ri.groupby(['violation','driver_gender']).search_conducted.mean()\n",
      "\n",
      "multi-index\n",
      "pandas.core.indexes.multi.multiindex (second dimension)\n",
      "\n",
      "level=0\n",
      "level=1\n",
      "\n",
      "search_rate.loc['Equipment'] #level 0\n",
      "search_rate.loc['Equipment','M'] #level 1\n",
      "\n",
      "search_rate.unstack()\n",
      "\n",
      "results in a dataframe\n",
      "\n",
      "ri.pivot_table(index='violation',\n",
      "\tcolumns='driver_gender',\n",
      "\tvalues='search_conducted')\n",
      "\n",
      "  >Sample\n",
      "print(ri_weather.is_arrested.mean())\n",
      "0.0355690117407784\n",
      "\n",
      "overall arrest rate\n",
      "\n",
      "\n",
      "# Calculate the arrest rate for each 'rating'\n",
      "print(ri_weather.groupby('rating').is_arrested.mean())\n",
      "\n",
      "stop_minutes  \n",
      "rating                \n",
      "good     0.033715\n",
      "bad      0.036261\n",
      "worse    0.041667\n",
      "\n",
      "  Sample   > create a multi index series    violation and rating for is_arrested mean\n",
      "\n",
      "# Calculate the arrest rate for each 'violation' and 'rating'\n",
      "print(ri_weather.groupby(['violation','rating']).is_arrested.mean())\n",
      "\n",
      "violation            rating\n",
      "*Equipment            good      0.059007\n",
      "                     bad       0.066311\n",
      "                     worse     0.097357\n",
      "*Moving violation     good      0.056227\n",
      "                     bad       0.058050\n",
      "                     worse     0.065860\n",
      "*Other                good      0.076966\n",
      "                     bad       0.087443\n",
      "                     worse     0.062893\n",
      "*Registration/plates  good      0.081574\n",
      "                     bad       0.098160\n",
      "                     worse     0.115625\n",
      "Seat belt            good      0.028587\n",
      "                     bad       0.022493\n",
      "                     worse     0.000000\n",
      "Speeding             good      0.013405\n",
      "                     bad       0.013314\n",
      "                     worse     0.016886\n",
      "\n",
      "  sample  > slicing the multi-index series\n",
      "\n",
      "# Save the output of the groupby operation from the last exercise\n",
      "arrest_rate = ri_weather.groupby(['violation', 'rating']).is_arrested.mean()\n",
      "\n",
      "# Print the 'arrest_rate' Series\n",
      "print(arrest_rate)\n",
      "\n",
      "# Print the arrest rate for moving violations in bad weather\n",
      "print(arrest_rate.loc['Moving violation','bad'])\n",
      "\n",
      "# Print the arrest rates for speeding violations in all three weather conditions\n",
      "print(arrest_rate.loc['Speeding'])\n",
      "\n",
      "\n",
      "  sample  > unstack and pivot\n",
      "\n",
      "# Unstack the 'arrest_rate' Series into a DataFrame\n",
      "print(arrest_rate.unstack())\n",
      "\n",
      "# Create the same DataFrame using a pivot table\n",
      "print(ri_weather.pivot_table(index='violation', columns='rating', values='is_arrested'))\n",
      "\n",
      "practice answering questions using data\n",
      "\n",
      "https://openpolicing.stanford.edu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\kaggle.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\kaggle.txt\n",
      "1. problem\n",
      "2. data\n",
      "3. model\n",
      "4. submission\n",
      "5. leaderboard\n",
      "\n",
      "download the data\n",
      "build your models\n",
      "\n",
      "\n",
      "taxi_train=pd.read_csv('taxi_train.csv')\n",
      "\n",
      "taxi_train.columns.to_list()\n",
      "\n",
      "taxi_test=pd.read_csv('taxi_test.csv')\n",
      "\n",
      "taxi_test.columns.to_list()\n",
      "\n",
      "submission file:\n",
      "\n",
      "taxi_sample_submission.csv\n",
      "\n",
      "taxi_sample_sub=pd.read_csv('taxi_sample_submission.csv')\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Read train data\n",
      "train = pd.read_csv('train.csv')\n",
      "\n",
      "# Look at the shape of the data\n",
      "print('Train shape:', train.shape)\n",
      "\n",
      "# Look at the head() of the data\n",
      "print(train.head())\n",
      "\n",
      " Read the test data\n",
      "test = pd.read_csv('test.csv')\n",
      "\n",
      "# Print train and test columns\n",
      "print('Train columns:', train.columns.tolist())\n",
      "print('Test columns:', test.columns.tolist())\n",
      "\n",
      "output:\n",
      "Train columns: ['id', 'date', 'store', 'item', 'sales']\n",
      "Test columns: ['id', 'date', 'store', 'item']\n",
      "\n",
      "\n",
      "# Read the sample submission file\n",
      "sample_submission = pd.read_csv('sample_submission.csv')\n",
      "\n",
      "# Look at the head() of the sample submission\n",
      "print(sample_submission.head())\n",
      "\n",
      "\n",
      "            >Prepare your first submission\n",
      "\n",
      "taxi_train=pd.read_csv('taxi_train.csv')\n",
      "taxi_train.columns.to_list()\n",
      "\n",
      "what is the problem type (regression, classification)\n",
      "\n",
      "\n",
      "taxi_train.fare_amount.hist(bins=30, alpha=0.5)\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lr=LinearRegression()\n",
      "\n",
      "lr.fit(X=taxi_train[['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']],\n",
      "\ty=taxi_train['fare_amount'])\n",
      "\n",
      "features=['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']\n",
      "\n",
      "taxi_test['fare_amount']=lr.predict(taxi_test[features])\n",
      "\n",
      "  submission file\n",
      "\n",
      "key\n",
      "fare_amount\n",
      "\n",
      "taxi_submission=taxi_test[['key','fare_amount']]\n",
      "\n",
      "taxi_submission.to_csv('first_sub.csv',index=False)\n",
      "\n",
      "\n",
      "   sample submission\n",
      "\n",
      "features=['id', 'date', 'store', 'item', 'sales']\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "# Read the train data\n",
      "train = pd.read_csv('train.csv')\n",
      "\n",
      "print(train.columns.to_list())\n",
      "\n",
      "# Create a Random Forest object\n",
      "rf = RandomForestRegressor()\n",
      "\n",
      "# Train a model\n",
      "rf.fit(X=train[['store', 'item']], y=train['sales'])\n",
      "\n",
      "# Read test and sample submission data\n",
      "test = pd.read_csv('test.csv')\n",
      "sample_submission = pd.read_csv('sample_submission.csv')\n",
      "\n",
      "# Show the head() of the sample_submission\n",
      "print(sample_submission.head())\n",
      "\n",
      "# Get predictions for the test set\n",
      "test['sales'] = rf.predict(test[['store', 'item']])\n",
      "\n",
      "# Write test predictions using the sample_submission format\n",
      "test[['id', 'sales']].to_csv('kaggle_submission.csv', index=False)\n",
      "\n",
      "\n",
      "  > public vs private leaderboard\n",
      "\n",
      "evaluation metric\n",
      "\n",
      "1. area under the roc (auc) (classification)\n",
      "2. f1 score (classification)\n",
      "3. mean log loss (logloss) (classification)\n",
      "4. mean absolute error (mae) (regression)\n",
      "5. mean squared error (mse) (regression)\n",
      "6. mean average precision Ranking (ranking)\n",
      "\n",
      "test split\n",
      "1. public test\n",
      "2. private test\n",
      "\n",
      "\n",
      "submission[['id','target']].to_csv('submission_1.csv',index=False)\n",
      "\n",
      "\n",
      "as model complexity increases the train data error goes down but the test data error goes up\n",
      "\n",
      "private LB\n",
      "public LB\n",
      "\n",
      "shake-up\n",
      "\n",
      "   > sample  > xgboost  > max_depth=2\n",
      "\n",
      "import xgboost as xgb\n",
      "\n",
      "# Create DMatrix on train data\n",
      "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
      "                     label=train['sales'])\n",
      "\n",
      "# Define xgboost parameters\n",
      "params = {'objective': 'reg:linear',\n",
      "          'max_depth': 2,\n",
      "          'silent': 1}\n",
      "\n",
      "# Train xgboost model\n",
      "xg_depth_2 = xgb.train(params=params, dtrain=dtrain)\n",
      "\n",
      "\n",
      "     sample  > xgboost  > max_depth=8\n",
      "\n",
      "import xgboost as xgb\n",
      "\n",
      "# Create DMatrix on train data\n",
      "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
      "                     label=train['sales'])\n",
      "\n",
      "# Define xgboost parameters\n",
      "params = {'objective': 'reg:linear',\n",
      "          'max_depth': 8,\n",
      "          'silent': 1}\n",
      "\n",
      "# Train xgboost model\n",
      "xg_depth_8 = xgb.train(params=params, dtrain=dtrain)\n",
      "\n",
      "dtrain = xgb.DMatrix(data=train[['store', 'item']])\n",
      "dtest = xgb.DMatrix(data=test[['store', 'item']])\n",
      "\n",
      "# For each of 3 trained models\n",
      "for model in [xg_depth_2, xg_depth_8, xg_depth_15]:\n",
      "    # Make predictions\n",
      "    train_pred = model.predict(dtrain)     \n",
      "    test_pred = model.predict(dtest)          \n",
      "    \n",
      "    # Calculate metrics\n",
      "    mse_train = mse(train['sales'], train_pred)                  \n",
      "    mse_test =mse(test['sales'], test_pred)\n",
      "    print('MSE Train: {:.3f}. MSE Test: {:.3f}'.format(mse_train, mse_test))\n",
      "\n",
      "output:\n",
      "\n",
      "MSE Train: 631.275. MSE Test: 558.522\n",
      "MSE Train: 183.771. MSE Test: 337.337\n",
      "MSE Train: 134.984. MSE Test: 355.534\n",
      "\n",
      "       Understand the problem\n",
      "\n",
      "solution workflow\n",
      "1. understand the problem\n",
      "2. eda - explore data analysis\n",
      "3. local validation\n",
      "4. modeling\n",
      "\n",
      "\n",
      "data type: tabular, time series, image, text\n",
      "\n",
      "problem type: classification, regression, ranking\n",
      "\n",
      "evaluation metric: roc au, f1 score, mae, mse\n",
      "\n",
      "from sklearn.metrics import roc_auc_score, f1_score, mean_squared_error\n",
      "\n",
      "def rmsl(y_true, y_pred):\n",
      "\tdiffs=np.log(y_true+1) - np.log(y_pred+1)\n",
      "\tsquares=np.power(diffs,2)\n",
      "\terr=np.sqrt(np.mean(squares))\n",
      "\treturn err\n",
      "\n",
      "\n",
      "def own_mse(y_true, y_pred):\n",
      "  \t# Raise differences to the power of 2\n",
      "    squares = np.power(y_true - y_pred, 2)\n",
      "    # Find mean over all observations\n",
      "    err = np.mean(squares)\n",
      "    return err\n",
      "\n",
      "\n",
      "   sample  > log loss\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "# Import log_loss from sklearn\n",
      "from sklearn.metrics import log_loss\n",
      "\n",
      "# Define your own LogLoss function\n",
      "def own_logloss(y_true, prob_pred):\n",
      "  \t# Find loss for each observation\n",
      "    terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1 - prob_pred)\n",
      "    # Find mean over all observations\n",
      "    err = np.mean(terms) \n",
      "    return -err\n",
      "\n",
      "print('Sklearn LogLoss: {:.5f}'.format(log_loss(y_classification_true, y_classification_pred)))\n",
      "print('Your LogLoss: {:.5f}'.format(own_logloss(y_classification_true, y_classification_pred)))\n",
      "\n",
      "\n",
      "\n",
      "        Initial EDA\n",
      "\n",
      "Exploratory Data Analysis\n",
      "\n",
      "1. size of the data\n",
      "2. properties of the target variable\n",
      "3. properties of the features\n",
      "4. generate ideas for feature engineering\n",
      "\n",
      "predict the popularity of an apartment rental listing\n",
      "\n",
      "target_variable\n",
      "1. interest_level\n",
      "\n",
      "two sigma connect\n",
      "\n",
      "id\n",
      "bathrooms\n",
      "bedrooms\n",
      "building_id\n",
      "latitude\n",
      "longitude\n",
      "manager_id\n",
      "price\n",
      "interest_level\n",
      "\n",
      "df.interest_level.value_counts()\n",
      "\n",
      "df.describe()\n",
      "1. count\n",
      "2. std\n",
      "3. min\n",
      "4. 25%\n",
      "5. 50%\n",
      "6. 75%\n",
      "7. max\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.style.use('ggplot')\n",
      "\n",
      "prices = df.groupby('interest_level', as_index=False)['price'].median()\n",
      "\n",
      "fig=plt.figure(figsize=(7,5))\n",
      "\n",
      "plt.bar(prices.interest_level, prices.price, width=0.5, alpha=0.8)\n",
      "\n",
      "plt.xlabel('Interest level')\n",
      "plt.ylabel('Median price')\n",
      "plt.title('Median listing price across interest level')\n",
      "\n",
      "plt.show()\n",
      "\n",
      " > sample \n",
      "\n",
      "# Shapes of train and test data\n",
      "print('Train shape:', train.shape)\n",
      "print('Test shape:', test.shape)\n",
      "\n",
      "# Train head()\n",
      "print(train.head())\n",
      "\n",
      "# Describe the target variable\n",
      "print(train.fare_amount.describe())\n",
      "\n",
      "# Train distribution of passengers within rides\n",
      "print(train.passenger_count.value_counts())\n",
      "\n",
      "\n",
      "# Calculate the ride distance\n",
      "train['distance_km'] = haversine_distance(train)\n",
      "\n",
      "# Draw a scatterplot\n",
      "plt.scatter(x=train['fare_amount'], y=train['distance_km'], alpha=0.5)\n",
      "plt.xlabel('Fare amount')\n",
      "plt.ylabel('Distance, km')\n",
      "plt.title('Fare amount based on the distance')\n",
      "\n",
      "# Limit on the distance\n",
      "plt.ylim(0, 50)\n",
      "plt.show()\n",
      "\n",
      "  > sample fare amount on day time\n",
      "\n",
      "# Create hour feature\n",
      "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n",
      "train['hour'] = train.pickup_datetime.dt.hour\n",
      "\n",
      "# Find median fare_amount for each hour\n",
      "hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()\n",
      "\n",
      "# Plot the line plot\n",
      "plt.plot(hour_price['hour'], hour_price['fare_amount'], marker='o')\n",
      "plt.xlabel('Hour of the day')\n",
      "plt.ylabel('Median fare amount')\n",
      "plt.title('Fare amount based on day time')\n",
      "plt.xticks(range(24))\n",
      "plt.show()\n",
      "\n",
      "     >Local validation\n",
      "\n",
      "private LB overfitting\n",
      "\n",
      "holdout set\n",
      "\n",
      "train data-> train set and holdout set\n",
      "\n",
      "train set -> training ->model ->predicting -> holdout set   (assess model quality)\n",
      "\n",
      "       cross validation using k-folding\n",
      "\n",
      "the test on each fold is on data the model has never seen before\n",
      "\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "kdf=KFold(n_splits=5, shuffle=True, random_state=123)\n",
      "\n",
      "for train_index, test_index = kdf.split(train):\n",
      "\tcv_train,cv_test=train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "str_kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
      "\n",
      "for train_index, test_index = str_kf.split(train):\n",
      "\tcv_train,cv_test=train.iloc[train_index], train.iloc[test_index]\n",
      "\t\n",
      "\t\n",
      "\n",
      "    sample    KFold\n",
      "\n",
      "# Import KFold\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "# Create a KFold object\n",
      "kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
      "\n",
      "# Loop through each split\n",
      "fold = 0\n",
      "for train_index, test_index in kf.split(train):\n",
      "    # Obtain training and testing folds\n",
      "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "    print('Fold: {}'.format(fold))\n",
      "    print('CV train shape: {}'.format(cv_train.shape))\n",
      "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
      "    fold += 1\n",
      "\n",
      "   > sample    stratified Fold\n",
      "\n",
      "# Import StratifiedKFold\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "# Create a StratifiedKFold object\n",
      "str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
      "\n",
      "# Loop through each split\n",
      "fold = 0\n",
      "for train_index, test_index in str_kf.split(train, train['interest_level']):\n",
      "    # Obtain training and testing folds\n",
      "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "    print('Fold: {}'.format(fold))\n",
      "    print('CV train shape: {}'.format(cv_train.shape))\n",
      "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
      "    fold += 1\n",
      "\n",
      "\n",
      "       >Validation Usage\n",
      "\n",
      "data leakage\n",
      "1. leak in the features: using data that will not be available in the real setting\n",
      "\n",
      "2. leak in validation strategy - validation strategy differs from the real-world situation\n",
      "\n",
      "\n",
      "    Time K-fold cross validation\n",
      "\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "\n",
      "time_kfold=TimeSeriesSpit(n_splits=5)\n",
      "\n",
      "train=train.sort_values('date')\n",
      "\n",
      "from train_index, test_index in time_kfold.split(train):\n",
      "\t cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "\n",
      "    validation pipeline CV_STRATEGY\n",
      "\n",
      "#list for results\n",
      "\n",
      "fold_metrics=[]\n",
      "\n",
      "for train_index, test_index in CV_STRATEGY.split(train):\n",
      "\t cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "\tmodel.fit(cv_train)\n",
      "\tpredictions=model.predict(cv_test)\n",
      "\t\n",
      "\tmetric=evaluate(cv_test, predictions)\n",
      "\n",
      "\tfold_metrics.append(metric)\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "mean_score = np.mean(fold_metrics)\n",
      "\n",
      "overall_score_minimizing = no.mean(fold_metrics)+ np.std(fold_metrics)\n",
      "\n",
      "overall_score_maximizing = no.mean(fold_metrics)- np.std(fold_metrics)\n",
      "\n",
      "\n",
      "  > sample  > timeseries fold\n",
      "\n",
      "# Create TimeSeriesSplit object\n",
      "time_kfold = TimeSeriesSplit(n_splits=3)\n",
      "\n",
      "# Sort train data by date\n",
      "train = train.sort_values('date')\n",
      "\n",
      "# Iterate through each split\n",
      "fold = 0\n",
      "for train_index, test_index in time_kfold.split(train):\n",
      "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "    \n",
      "    print('Fold :', fold)\n",
      "    print('Train date range: from {} to {}'.format(cv_train.date.min(), cv_train.date.max()))\n",
      "    print('Test date range: from {} to {}\\n'.format(cv_test.date.min(), cv_test.date.max()))\n",
      "    fold += 1\n",
      "\n",
      "\n",
      "\n",
      "Fold : 0\n",
      "Train date range: from 2017-12-01 to 2017-12-08\n",
      "Test date range: from 2017-12-08 to 2017-12-16\n",
      "\n",
      "Fold : 1\n",
      "Train date range: from 2017-12-01 to 2017-12-16\n",
      "Test date range: from 2017-12-16 to 2017-12-24\n",
      "\n",
      "Fold : 2\n",
      "Train date range: from 2017-12-01 to 2017-12-24\n",
      "Test date range: from 2017-12-24 to 2017-12-31\n",
      "\n",
      "\n",
      "the test date ranges do not overlap\n",
      "\n",
      "\n",
      "  > sample get the mean validation mse:\n",
      "\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "import numpy as np\n",
      "\n",
      "# Sort train data by date\n",
      "train = train.sort_values('date')\n",
      "\n",
      "# Initialize 3-fold time cross-validation\n",
      "kf = TimeSeriesSplit(n_splits=3)\n",
      "\n",
      "# Get MSE scores for each cross-validation split\n",
      "mse_scores = get_fold_mse(train, kf)\n",
      "\n",
      "print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))\n",
      "\n",
      "print('MSE by fold: {}'.format(mse_scores))\n",
      "\n",
      "print('Overall validation MSE: {:.5f}'.format(np.mean(mse_scores) + np.std(mse_scores)))\n",
      "\n",
      "output:\n",
      "Mean validation MSE: 955.49186\n",
      "MSE by fold: [890.30336, 961.65797, 1014.51424]\n",
      "Overall validation MSE: 1006.38784\n",
      "\n",
      "\n",
      "        feature engineering\n",
      "\n",
      "modeling\n",
      "\n",
      "1. create new features\n",
      "2. improve models\n",
      "3. apply tricks\n",
      "4. preprocess data\n",
      "\n",
      "feature engineering is creating new features\n",
      "1. numerical\n",
      "2. categorical\n",
      "3. datetime\n",
      "4. coordinates\n",
      "5. text\n",
      "6. images\n",
      "\n",
      "data = pd.concat([train,test])\n",
      "\n",
      "train=data[data.id.isin(train_id)]\n",
      "test=data[data.id.isin(test_id)]\n",
      "\n",
      "dem['date']=pd.to_datetime(dem['date'])\n",
      "\n",
      "dem['year']=dem['date'].dt.year\n",
      "dem['month']=dem['date'].dt.month\n",
      "dem['day']=dem['date'].dt.day\n",
      "dem['dayofweek']=dem['date'].dt.dayofweek\n",
      "\n",
      "\n",
      "  sample    create a new feature\n",
      "\n",
      "# Look at the initial RMSE\n",
      "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
      "\n",
      "# Find the total area of the house\n",
      "train['TotalArea'] = train[\"TotalBsmtSF\"] + train[\"FirstFlrSF\"] + train[\"SecondFlrSF\"]\n",
      "\n",
      "# Look at the updated RMSE\n",
      "print('RMSE with total area:', get_kfold_rmse(train))\n",
      "\n",
      "# Find the area of the garden\n",
      "train['GardenArea'] = train[\"LotArea\"] - train[\"FirstFlrSF\"]\n",
      "print('RMSE with garden area:', get_kfold_rmse(train))\n",
      "\n",
      "RMSE before feature engineering: 36029.39\n",
      "RMSE with total area: 35073.2\n",
      "RMSE with garden area: 34413.55\n",
      "\n",
      "# Find total number of bathrooms\n",
      "train['TotalBath'] = train['FullBath']+train['HalfBath']\n",
      "print('RMSE with number of bathrooms:', get_kfold_rmse(train))\n",
      "\n",
      "RMSE with number of bathrooms: 34506.78\n",
      "\n",
      "Here you see that house area improved the RMSE by almost $1,000. Adding garden area improved the RMSE by another $600. However, with the total number of bathrooms, the RMSE has increased. It means that you keep the new area features, but do not add \"TotalBath\" as a new feature. Let's now work with the datetime features!\n",
      "\n",
      "\n",
      "    sample  > new datetime feature\n",
      "\n",
      "# Concatenate train and test together\n",
      "taxi = pd.concat([train, test])\n",
      "\n",
      "# Convert pickup date to datetime object\n",
      "taxi['pickup_datetime'] = pd.to_datetime(taxi['pickup_datetime'])\n",
      "\n",
      "# Create a day of week feature\n",
      "taxi['dayofweek'] = taxi['pickup_datetime'].dt.dayofweek\n",
      "\n",
      "# Create an hour feature\n",
      "taxi['hour'] = taxi['pickup_datetime'].dt.hour\n",
      "\n",
      "# Split back into train and test\n",
      "new_train = taxi[taxi['id'].isin(train['id'])]\n",
      "new_test = taxi[taxi['id'].isin(test['id'])]\n",
      "\n",
      "\n",
      "        categorical features\n",
      "\n",
      "label encoding\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "4 a\n",
      "\n",
      "encoded\n",
      "1 0\n",
      "2 1\n",
      "3 2\n",
      "4 0\n",
      "\n",
      "\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "le = LabelEncoder()\n",
      "\n",
      "df['cat_encoded'] = le.fit_transform(df['cat'])\n",
      "\n",
      "to overcome the independency between categories, one hot encoding was developed\n",
      "\n",
      "ohe=pd.get_dummies(df['cat'], prefix='ohe_cat')\n",
      "\n",
      "df.drop('cat',axis=1,inplace=True)\n",
      "df=pd.concat([df,ohe],axis=1)\n",
      "\n",
      "  > binary features\n",
      "yes or no\n",
      "\n",
      "le=LabelEncoder()\n",
      "\n",
      "binary_feature['binary_encoded']=le.fit_transform(binary_feature['binary_feat'])\n",
      "\n",
      " > other encoders\n",
      "\n",
      "target encoder\n",
      "\n",
      "  > sample  > labelEncoder\n",
      "\n",
      "# Concatenate train and test together\n",
      "houses = pd.concat([train, test])\n",
      "\n",
      "# Label encoder\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "le = LabelEncoder()\n",
      "\n",
      "# Create new features\n",
      "houses['RoofStyle_enc'] = le.fit_transform(houses[\"RoofStyle\"])\n",
      "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
      "\n",
      "# Look at new features\n",
      "print(houses[['RoofStyle', 'RoofStyle_enc', 'CentralAir', 'CentralAir_enc']].head())\n",
      "\n",
      "\n",
      "\n",
      "    problem with label encoding\n",
      "\n",
      "The problem with label encoding is that it implicitly assumes that there is a ranking dependency between the categories.\n",
      "\n",
      "\n",
      "  > sample  value counts between roof style and central air\n",
      "\n",
      "# Concatenate train and test together\n",
      "houses = pd.concat([train, test])\n",
      "\n",
      "# Look at feature distributions\n",
      "print(houses['RoofStyle'].value_counts(), '\\n')\n",
      "print(houses['CentralAir'].value_counts())\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "Name: RoofStyle, dtype: int64 \n",
      "\n",
      "Y    2723\n",
      "N     196\n",
      "\n",
      "Name: CentralAir, dtype: int64\n",
      "\n",
      "  > encode CentralAir as binary 0 or 1\n",
      "\n",
      "# Concatenate train and test together\n",
      "houses = pd.concat([train, test])\n",
      "\n",
      "# Label encode binary 'CentralAir' feature\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "le = LabelEncoder()\n",
      "houses['CentralAir_enc'] = le.fit_transform(houses[\"CentralAir\"])\n",
      "\n",
      " >one hot encode\n",
      "\n",
      "# Create One-Hot encoded features\n",
      "ohe = pd.get_dummies(houses['RoofStyle'], prefix='RoofStyle')\n",
      "\n",
      "# Concatenate OHE features to houses\n",
      "houses = pd.concat([houses, ohe], axis=1)\n",
      "\n",
      "# Look at OHE features\n",
      "print(houses[[col for col in houses.columns if 'RoofStyle' in col]].head(3))\n",
      "\n",
      "\n",
      "     >Target encoding\n",
      "\n",
      "1. label encoder provides distinct number for each category\n",
      "\n",
      "2. one-hot encoder creates new features for each category value\n",
      "\n",
      "target encoding creates a single column\n",
      "\n",
      "1. calculate mean on the train, apply to the test\n",
      "2. split train into K folds.  calculate mean on k-1 folds, apply to the k-th fold.  this prevents overfitting\n",
      "3. add mean target encoded feature to the model\n",
      "\n",
      "\n",
      "  > sample    categorical  target\n",
      "def train_mean_target_encoding(train, target, categorical, alpha=5):\n",
      "    # Create 5-fold cross-validation\n",
      "    kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
      "    train_feature = pd.Series(index=train.index)\n",
      "\n",
      "    # For each folds split\n",
      "    for train_index, test_index in kf.split(train):\n",
      "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "        # Calculate out-of-fold statistics and apply to cv_test\n",
      "        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)\n",
      "        # Save new feature for this particular fold\n",
      "        train_feature.iloc[test_index] = cv_test_feature       \n",
      "    return train_feature.values\n",
      "\n",
      "def test_mean_target_encoding(train, test, target, categorical, alpha=5):\n",
      "    # Calculate global mean on the train data\n",
      "    global_mean = train[target].mean()\n",
      "    \n",
      "    # Group by the categorical feature and calculate its properties\n",
      "    train_groups = train.groupby(categorical)\n",
      "    category_sum = train_groups[target].sum()\n",
      "    category_size = train_groups.size()\n",
      "    \n",
      "    # Calculate smoothed mean target statistics\n",
      "    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)\n",
      "    \n",
      "    # Apply statistics to the test data and fill new categories\n",
      "    test_feature = test[categorical].map(train_statistics).fillna(global_mean)\n",
      "    return test_feature.values\n",
      "\n",
      "\n",
      "def mean_target_encoding(train, test, target, categorical, alpha=5):\n",
      "  \n",
      "    # Get the train feature\n",
      "    train_feature = train_mean_target_encoding(train, target, categorical, alpha)\n",
      "  \n",
      "    # Get the test feature\n",
      "    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)\n",
      "    \n",
      "    # Return new features to add to the model\n",
      "    return train_features, test_features\n",
      "\n",
      "\n",
      " code\n",
      "\n",
      "# For each folds split\n",
      "    for train_index, test_index in kf.split(train):\n",
      "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "      \n",
      "        # Calculate out-of-fold statistics and apply to cv_test\n",
      "\n",
      "        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)\n",
      "        \n",
      "        # Save new feature for this particular fold\n",
      "        train_feature.iloc[test_index] = cv_test_feature       \n",
      "\n",
      "    return train_feature.values\n",
      "\n",
      "\n",
      "  > target categorical\n",
      "\n",
      "# Create 5-fold cross-validation\n",
      "kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
      "\n",
      "# For each folds split\n",
      "for train_index, test_index in kf.split(bryant_shots):\n",
      "    cv_train, cv_test = bryant_shots.iloc[train_index], bryant_shots.iloc[test_index]\n",
      "\n",
      "    # Create mean target encoded feature\n",
      "    cv_train['game_id_enc'], cv_test['game_id_enc'] = mean_target_encoding(train=cv_train,\n",
      "                                                                           test=cv_test,\n",
      "                                                                           target='shot_made_flag',\n",
      "                                                                           categorical='game_id',\n",
      "                                                                           alpha=5)\n",
      "    # Look at the encoding\n",
      "    print(cv_train[['game_id', 'shot_made_flag', 'game_id_enc']].sample(n=1))\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "<script.py> output:\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    7106  20500532             0.0     0.361914\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    5084  20301100             0.0     0.568395\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    6687  20500228             0.0      0.48131\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    5046  20301075             0.0     0.252103\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    4662  20300515             1.0     0.452637\n",
      "\n",
      "\n",
      "The main conclusion you should make: while using local cross-validation, you need to repeat mean target encoding procedure inside each folds split separately. Go on to try other problem types beyond binary classification!\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "# Create mean target encoded feature\n",
      "train['RoofStyle_enc'], test['RoofStyle_enc'] = mean_target_encoding(train=train,\n",
      "                                                                     test=test,\n",
      "                                                                     target='SalePrice',\n",
      "                                                                     categorical='RoofStyle',\n",
      "                                                                     alpha=10)\n",
      "\n",
      "# Look at the encoding\n",
      "print(test[['RoofStyle', 'RoofStyle_enc']].drop_duplicates())\n",
      "\n",
      "\n",
      "\n",
      "output:\n",
      "<script.py> output:\n",
      "         RoofStyle  RoofStyle_enc\n",
      "    0        Gable  171565.947836\n",
      "    1          Hip  217594.645131\n",
      "    98     Gambrel  164152.950424\n",
      "    133       Flat  188703.563431\n",
      "    362    Mansard  180775.938759\n",
      "    1053      Shed  188267.663242\n",
      "\n",
      "\n",
      "So, you observe that houses with the Hip roof are the most pricy, while houses with the Gambrel roof are the cheapest.\n",
      "\n",
      "\n",
      "   > Missing data\n",
      "\n",
      "Mean/Median imputation\n",
      "\n",
      "categorical missing data replaced with the most frequent value\n",
      "\n",
      "new category imputation\n",
      "\n",
      "df.isnull().head(1)\n",
      "\n",
      "\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "mean_imputer = SimpleImputer(strategy='mean')\n",
      "\n",
      "constant_imputer = SimpleImputer(strategy='constant', fill_value=-999)\n",
      "\n",
      "\n",
      "df[['num']] = mean_imputer.fit_transform(df[['num']])\n",
      "\n",
      "\n",
      "constant_imputer = SimpleImputer(strategy='constant', fill_value='MISS')\n",
      "\n",
      "   sample  > find columns with missing data\n",
      "\n",
      "# Read dataframe\n",
      "twosigma = pd.read_csv(\"twosigma_train.csv\")\n",
      "\n",
      "# Find the number of missing values in each column\n",
      "print(twosigma.isnull().sum())\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "id                 0\n",
      "bathrooms          0\n",
      "bedrooms           0\n",
      "building_id       13\n",
      "latitude           0\n",
      "longitude          0\n",
      "manager_id         0\n",
      "price             32\n",
      "interest_level     0\n",
      "dtype: int64\n",
      "\n",
      " # Look at the columns with the missing values\n",
      "print(twosigma[['building_id', 'price']].head())\n",
      "\n",
      "\n",
      " > sample  > imputer mean\n",
      "\n",
      "# Import SimpleImputer\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "# Create mean imputer\n",
      "mean_imputer = SimpleImputer(strategy='mean')\n",
      "\n",
      "# Price imputation\n",
      "rental_listings[['price']] = mean_imputer.fit_transform(rental_listings[['price']])\n",
      "\n",
      "   sample  > simple imputer constant\n",
      "\n",
      "# Import SimpleImputer\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "# Create constant imputer\n",
      "constant_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
      "\n",
      "# building_id imputation\n",
      "rental_listings[['building_id']] = constant_imputer.fit_transform(rental_listings[['building_id']])\n",
      "\n",
      "     >baseline model\n",
      "\n",
      "taxi_train=pd.read_csv('taxi_train.csv')\n",
      "taxi_test=pd.read_csv('taxi_test.csv')\n",
      "\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "validation_train, validation_test=train_test_split(taxi_train, test_size=0.3, random_state=123)\n",
      "\n",
      "taxi_test['fare_amount']=np.mean(taxi_train.fair_amount)\n",
      "\n",
      "#### mean_sub\n",
      "taxi_test[['id','fare_amount']].to_csv('mean_sub.csv',index=False)\n",
      "\n",
      "naive_prediction_groups=taxi_train.groupby('passenger_count').fare_amount.mean()\n",
      "\n",
      "taxi_test['fare_amount']=taxi_test.passenger_count.map(naive_prediction_groups)\n",
      "\n",
      "#map- Used for substituting each value in a Series with another value\n",
      "\n",
      "### mean group sub\n",
      "\n",
      "taxi_test[['id','fare_amount']].to_csv('mean_group_sub.csv', index=False)\n",
      "\n",
      "#select only numeric features\n",
      "\n",
      "features['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']\n",
      "\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "gb=GradientBoostingRegressor()\n",
      "\n",
      "gb.fit(taxi_train[features], taxi_train.fare_amount)\n",
      "\n",
      "taxi_test['fare_amount']=gb.predict(taxi_test[features])\n",
      "\n",
      "### gradient boost\n",
      "\n",
      "taxi_test[['id','fare_amount']].to_csv('gb_sub.csv',index=False)\n",
      "\n",
      "\n",
      "model\t\tvalidation RMSE  public LB RMSE\n",
      "\n",
      "simple mean\t9.986\t9.409\n",
      "group mean\t9.978\t9.407\n",
      "gradient boost\t5.996\t4.595\n",
      "\n",
      "\n",
      "   sample  > hold out\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from math import sqrt\n",
      "\n",
      "# Calculate the mean fare_amount on the validation_train data\n",
      "naive_prediction = np.mean(validation_train['fare_amount'])\n",
      "\n",
      "# Assign naive prediction to all the holdout observations\n",
      "validation_test['pred'] = naive_prediction\n",
      "\n",
      "# Measure the local RMSE\n",
      "rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))\n",
      "print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))\n",
      "\n",
      "Validation RMSE for Baseline I model: 9.986\n",
      "\n",
      "\n",
      "   sample Group by hour\n",
      "\n",
      "# Get pickup hour from the pickup_datetime column\n",
      "train['hour'] = train['pickup_datetime'].dt.hour\n",
      "test['hour'] = test['pickup_datetime'].dt.hour\n",
      "\n",
      "# Calculate average fare_amount grouped by pickup hour \n",
      "hour_groups = train.groupby('hour')['fare_amount'].mean()\n",
      "\n",
      "# Make predictions on the test set\n",
      "test['fare_amount'] = test.hour.map(hour_groups)\n",
      "\n",
      "# Write predictions\n",
      "test[['id','fare_amount']].to_csv('hour_mean_sub.csv', index=False)\n",
      "\n",
      "\n",
      "   hyperparameter tuning\n",
      "\n",
      "add hour feature: validation rmse 5.553\n",
      "\n",
      "add distance feature: validation rmse 5.268\n",
      "\n",
      "deep learning does not require feature engineering\n",
      "\n",
      "least squares linear regression\n",
      "1. loss = (y_i-yhat_i)**2 -> min\n",
      "\n",
      "ridge regression\n",
      "loss =  (y_i-yhat_i)**2 + alpha * weights**2\n",
      "\n",
      "popular approachs\n",
      "1. Grid Search\n",
      "2. Random Grid Search\n",
      "3. Bayesian optimization\n",
      "\n",
      "\n",
      "     >Grid search\n",
      "\n",
      "alpha_grid=[0.01,0.1,1,10]\n",
      "\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "results={}\n",
      "\n",
      "for candidate_alpha in alpha_grid:\n",
      "\tridge_regression=Ridge(alpha=candidate_alpha)\n",
      "\n",
      "\tresults[candidate_alpha]=validation_score\n",
      "\n",
      "  > sample max depth\n",
      "\n",
      "# Possible max depth values\n",
      "max_depth_grid = [3,6,9,12,15]\n",
      "results = {}\n",
      "\n",
      "# For each value in the grid\n",
      "for max_depth_candidate in max_depth_grid:\n",
      "    # Specify parameters for the model\n",
      "    params = {'max_depth': max_depth_candidate}\n",
      "\n",
      "    # Calculate validation score for a particular hyperparameter\n",
      "    validation_score = get_cv_score(train, params)\n",
      "\n",
      "    # Save the results for each max depth value\n",
      "    results[max_depth_candidate] = validation_score   \n",
      "print(results)\n",
      "\n",
      "output:\n",
      "\n",
      "{3: 6.50509, 6: 6.52138, 9: 6.64181, 12: 6.8819, 15: 6.99156}\n",
      "\n",
      "\n",
      "\n",
      "The drawback of tuning each hyperparameter independently is a potential dependency between different hyperparameters. The better approach is to try all the possible hyperparameter combinations.\n",
      "\n",
      "\n",
      "       Sample Product with parameters\n",
      "\n",
      "import itertools\n",
      "\n",
      "# Hyperparameter grids\n",
      "max_depth_grid = [3, 5, 7]\n",
      "subsample_grid = [0.8, 0.9, 1.0]\n",
      "results = {}\n",
      "\n",
      "# For each couple in the grid\n",
      "for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid):\n",
      "    params = {'max_depth': max_depth_candidate,\n",
      "              'subsample': subsample_candidate}\n",
      "    validation_score = get_cv_score(train, params)\n",
      "    # Save the results for each couple\n",
      "    results[(max_depth_candidate, subsample_candidate)] = validation_score   \n",
      "print(results)\n",
      "\n",
      "\n",
      "{(3, 0.8): 6.33917, (3, 0.9): 6.43642, (3, 1.0): 6.50509, (5, 0.8): 6.26977, (5, 0.9): 6.35116, (5, 1.0): 6.45468, (7, 0.8): 6.1635, (7, 0.9): 6.34018, (7, 1.0): 6.48436}\n",
      "\n",
      "With max_depth equal to 7 and subsample equal to 0.8, the best RMSE is now $6.16.\n",
      "\n",
      "(grid_df_class.cv_results_)\n",
      "\n",
      "applies only to classification\n",
      "\n",
      "    Model Ensembling\n",
      "\n",
      "input data:\n",
      "1. categorical\n",
      "2. one hote encoded categorical\n",
      "3. numerics\n",
      "\n",
      "inputs into\n",
      "500 modesl\n",
      "inputs into \n",
      "125 xgboost models\n",
      "\n",
      "different subsets\n",
      "40 models and 60 models\n",
      "\n",
      "input into \n",
      "5 models of keras\n",
      "\n",
      "weighted Rank Average\n",
      "\n",
      "\n",
      "  > Regression problem\n",
      "\n",
      "regression classifier\n",
      "\n",
      "train two models a and b\n",
      "\n",
      "\n",
      "  > model stacking\n",
      "\n",
      "1. split train data into two parts\n",
      "2. train multiple models on part 1\n",
      "3. make predictions on part 2\n",
      "4. make predictions on the test data\n",
      "\n",
      "5. train a new model on part 2 using predictions as features\n",
      "\n",
      "6. make predictions on the test data using the 2nd level model.\n",
      "\n",
      "train models A, b, c on part 1\n",
      "\n",
      "trainid   feature1 feature2 Target A_pred, B_pred, c_pred\n",
      "\n",
      "4\t.10\t2.87\t1\t0.71\t0.52\t.098\n",
      "\n",
      "make predictions of the test data as well\n",
      "\n",
      "testid   feature1 feature2 Target A_pred, B_pred, c_pred\n",
      "\n",
      "\n",
      "train 2nd level model on part 2 using the train and test data set from part 1\n",
      "\n",
      "resulting in a stacking prediction\n",
      "\n",
      "    sample add new feature gb and rf part 1\n",
      "\n",
      "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
      "\n",
      "# Train a Gradient Boosting model\n",
      "gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\n",
      "\n",
      "# Train a Random Forest model\n",
      "rf = RandomForestRegressor().fit(train[features], train.fare_amount)\n",
      "\n",
      "# Make predictions on the test data\n",
      "test['gb_pred'] = gb.predict(test[features])\n",
      "test['rf_pred'] = rf.predict(test[features])\n",
      "\n",
      "# Find mean of model predictions\n",
      "test['blend'] = (test['gb_pred'] + test['rf_pred']) / 2\n",
      "print(test[['gb_pred', 'rf_pred', 'blend']].head(3))\n",
      "\n",
      "\n",
      "# Split train data into two parts\n",
      "part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n",
      "\n",
      "# Train a Gradient Boosting model on Part 1\n",
      "gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n",
      "\n",
      "# Train a Random Forest model on Part 1\n",
      "rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)\n",
      "\n",
      "# Make predictions on the Part 2 data\n",
      "part_2['gb_pred'] = gb.predict(part_2[features])\n",
      "part_2['rf_pred'] = rf.predict(part_2[features])\n",
      "\n",
      "# Make predictions on the test data\n",
      "test['gb_pred'] = gb.predict(test[features])\n",
      "test['rf_pred'] = rf.predict(test[features])\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Create linear regression model without the intercept\n",
      "lr = LinearRegression(fit_intercept=False)\n",
      "\n",
      "# Train 2nd level model on the Part 2 data\n",
      "lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\n",
      "\n",
      "# Make stacking predictions on the test data\n",
      "test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\n",
      "\n",
      "# Look at the model coefficients\n",
      "print(lr.coef_)\n",
      "\n",
      "output:\n",
      "[0.72504358 0.27647395]\n",
      "\n",
      "Looking at the coefficients, it's clear that 2nd level model has more trust to the Gradient Boosting: 0.7 versus 0.3 for the Random Forest model. \n",
      "\n",
      "        >.Save Information\n",
      "\n",
      "1. save folds to the disk\n",
      "2. save model runs\n",
      "3. save model predictions to the disk\n",
      "4. save performance results\n",
      "\n",
      "\n",
      "forums\n",
      "\n",
      "Competition discussion by the participants\n",
      "\n",
      "Kaggle kernels\n",
      "scripts and notebooks shared by the participants\n",
      "\n",
      "cloud computational environment\n",
      "\n",
      "competitions last 2 to 3 months\n",
      "\n",
      "  > sample drop column and score\n",
      "\n",
      "# Drop passenger_count column\n",
      "new_train_1 = train.drop('passenger_count', axis=1)\n",
      "\n",
      "# Compare validation scores\n",
      "initial_score = get_cv_score(train)\n",
      "new_score = get_cv_score(new_train_1)\n",
      "\n",
      "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))\n",
      "\n",
      "\n",
      " Initial score is 6.50509 and the new score is 6.41902\n",
      "\n",
      "\n",
      "# Create copy of the initial train DataFrame\n",
      "new_train_2 = train.copy()\n",
      "\n",
      "# Find sum of pickup latitude and ride distance\n",
      "new_train_2['weird_feature'] = new_train_2['pickup_latitude'] + new_train_2['distance_km']\n",
      "\n",
      "# Compare validation scores\n",
      "initial_score = get_cv_score(train)\n",
      "new_score = get_cv_score(new_train_2)\n",
      "\n",
      "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))\n",
      "\n",
      "Initial score is 6.50509 and the new score is 6.5121\n",
      "\n",
      " In this particular case, dropping the \"passenger_count\" feature helped, while finding the sum of pickup latitude and ride distance did not. \n",
      "\n",
      "Machine learning models\n",
      "\n",
      "1. talk to business.  Define the problem\n",
      "2. collect the data\n",
      "3. select the metric\n",
      "4. make train and test split\n",
      "5. create the model\n",
      "6. move the model to production\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pyspark building machine learning models.txt\n",
      "\u001b[30m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnishimoto.BOISE\\python\\pyspark building machine learning models.txt\n",
      "https://github.com/conwayyao/Recipe-Analysis/blob/master/CuisineAnalyzer/cuisinedata/indian.csv\n",
      "\n",
      "Indian food recipes (tangent)\n",
      "\n",
      "more data is the objective\n",
      "\n",
      "data is divided into partitions\n",
      "the partition can fit into ram\n",
      "spark does most processing in memory\n",
      "\n",
      "the cluster composed of one or more nodes.\n",
      "\n",
      "each node is a computer with ram and physical storage.\n",
      "\n",
      "a cluster manager allocates.\n",
      "\n",
      "resources and coordinates activity across the cluster. \n",
      "\n",
      "using the spark api the driver communicates with the cluster manager.\n",
      "\n",
      "on each node, spark launches and executor tasks application.  Work is divided into tasks which are units of computation.  tasks run multiple threads across the cores in a node.\n",
      "\n",
      "Interaction with spark can be written in java, scala, python, or r\n",
      "\n",
      "\n",
      "import pyspark\n",
      "\n",
      "pyspark.__version__\n",
      "'2.4.1'\n",
      "\n",
      "Structured Data - pyspark.sql\n",
      "Streaming Data - pyspark.streaming\n",
      "Machine Learning - pyspark.ml\n",
      "\n",
      "connect to spark\n",
      "\n",
      "remote cluster\n",
      "spark://<IP address | DNS name>:<port>\n",
      "spark://13.59.151.161:7077\n",
      "\n",
      "7077 is the default port\n",
      "\n",
      "Local cluster:\n",
      "local\n",
      "local[4]\n",
      "local[*]\n",
      "\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark= SparkSession.builder\n",
      "\t.master('local[*]')\n",
      "\t.appName('first_spark_application')\n",
      "\t.getOrCreate()\n",
      "\n",
      "\n",
      "\n",
      "spark.stop()\n",
      "\n",
      "\n",
      "  sample create an spark session\n",
      "\n",
      "# Import the PySpark module\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "# Create SparkSession object\n",
      "spark = SparkSession.builder \\\n",
      "                    .master('local[*]') \\\n",
      "                    .appName('test') \\\n",
      "                    .getOrCreate()\n",
      "\n",
      "# What version of Spark?\n",
      "# (Might be different to what you saw in the presentation!)\n",
      "print(spark.version)\n",
      "\n",
      "# Terminate the cluster\n",
      "spark.stop()\n",
      "\n",
      "     >Loading data into a dataframe\n",
      "1. count() #number of rows\n",
      "2. show()\n",
      "3. printSchema()\n",
      "4. dtypes\n",
      "\n",
      "cars.csv\n",
      "1. mfr\n",
      "2. mod\n",
      "3. org\n",
      "4. type\n",
      "5. cyl\n",
      "6. size\n",
      "7. weight\n",
      "8. len\n",
      "9. rpm\n",
      "10. cons\n",
      "\n",
      "spark.read.csv parameters:\n",
      "\n",
      "1. Header=True #tells if the first row is a header row\n",
      "2. sep=','\n",
      "3. schema - explicit column data types\n",
      "4. inferSchema - deduced column data types from data (two passes over the data to infer the data types of the columns)\n",
      "5. nullValue - placeholder for missing data\n",
      "\n",
      "\n",
      "cars.printSchema()\n",
      "\n",
      "\n",
      "read.csv treats all columns as strings by default\n",
      "\n",
      "cars=spark.read.csv('cars.csv',header=True, inferSchema=True, nullValue='NA')\n",
      "\n",
      "\n",
      "schema = StructType)[\n",
      "\tStructField(\"maker\",StringType()),\n",
      "\tStructField(\"cyl\",IntegerType()),\n",
      "\tStructField(\"size\",DoubleType())\n",
      "])\n",
      "\n",
      "cars=spark.read.csv('cars.csv',header=True, schema=schema, nullValue='NA')\n",
      "\n",
      "\n",
      "  > read.csv\n",
      "\n",
      "# Read data from CSV file\n",
      "flights = spark.read.csv('flights.csv',\n",
      "                         sep=',',\n",
      "                         header=True,\n",
      "                         inferSchema=True,\n",
      "                         nullValue='NA')\n",
      "\n",
      "# Get number of records\n",
      "print(\"The data contain %d records.\" % flights.count())\n",
      "\n",
      "# View the first five records\n",
      "flights.show(5)\n",
      "\n",
      "# Check column data types\n",
      "print(flights.printSchema())\n",
      "print(flights.dtype)\n",
      "\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n",
      "\n",
      "  >sample  > create a schema for the read.csv\n",
      "\n",
      "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
      "\n",
      "# Specify column names and types\n",
      "schema = StructType([\n",
      "    StructField(\"id\", IntegerType()),\n",
      "    StructField(\"text\", StringType()),\n",
      "    StructField(\"label\", IntegerType())\n",
      "])\n",
      "\n",
      "# Load data from a delimited file\n",
      "sms = spark.read.csv('sms.csv', sep=';', header=True, schema=schema)\n",
      "\n",
      "# Print schema of DataFrame\n",
      "sms.printSchema()\n",
      "\n",
      "\n",
      "       Data preparation     >\n",
      "\n",
      "cars.csv:\n",
      "\n",
      "maker\n",
      "model\n",
      "origin\n",
      "type\n",
      "cyl\n",
      "size\n",
      "weight\n",
      "length\n",
      "rpm\n",
      "consumption\n",
      "\n",
      "cars=cars.drop('maker','model')\n",
      "cars=cars.select('origin, 'type','cyl','size','weight','length','rpm','consumption')\n",
      "\n",
      "\n",
      "cars.filter('cyl IS NULL').count()\n",
      "\n",
      "or \n",
      "\n",
      "drop all records with missing values in any column\n",
      "\n",
      "cars=cars.dropna()\n",
      "\n",
      "\n",
      "from pyspark.sql.functions import round\n",
      "\n",
      "#kilograms conversion\n",
      "cars = cars.withColumn('mass',round(cars.weight/2.205,0))\n",
      "\n",
      "#meters conversion\n",
      "cars= cars.withColumn('length',round(cars.length * 0.0254,3))\n",
      "\n",
      "   Indexing categorical data\n",
      "\n",
      "from pyspark.ml.feature import StringIndexer\n",
      "\n",
      "indexer=StringIndexer(inputCol='type',\n",
      "\t\t\toutputCol='type_idx')\n",
      "\n",
      "indexer=indexer.fit(cars)\n",
      "\n",
      "cars=indexer.transform(cars)\n",
      "\n",
      "use StringOrderType to change order\n",
      "\n",
      "cars=StringIndexer(inputCol=\"origin\", outputCol=\"label\").fit(cars).transform(cars)\n",
      "\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "\n",
      "assembler=VectorAssembler(inputCols=['cyl','size'], outputCol='features')\n",
      "\n",
      "assembler.transform(cars)\n",
      "\n",
      "All the features are assembled into a single column\n",
      "\n",
      "\n",
      "     sample     determine delay vs missing flights\n",
      "\n",
      "# Remove the 'flight' column\n",
      "flights_drop_column = flights.drop('flight')\n",
      "\n",
      "# Number of records with missing 'delay' values\n",
      "flights_drop_column.filter('delay IS NULL').count()\n",
      "\n",
      "# Remove records with missing 'delay' values\n",
      "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
      "\n",
      "# Remove records with missing values in any column and get the number of remaining rows\n",
      "flights_none_missing = flights_valid_delay.dropna()\n",
      "print(flights_none_missing.count())\n",
      "\n",
      "\n",
      "   sample  > create a new column and drop the old column\n",
      "\n",
      "# Import the required function\n",
      "from pyspark.sql.functions import round\n",
      "\n",
      "# Convert 'mile' to 'km' and drop 'mile' column\n",
      "flights_km = flights.withColumn('km', round(flights.mile * 1.60934, 0)) \\\n",
      "                    .drop('mile')\n",
      "\n",
      "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
      "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
      "\n",
      "# Check first five records\n",
      "flights_km.show(5)\n",
      "\n",
      "\n",
      "    sample  > transforming categorical string data\n",
      "\n",
      "from pyspark.ml.feature import StringIndexer\n",
      "\n",
      "# Create an indexer\n",
      "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
      "\n",
      "# Indexer identifies categories in the data\n",
      "indexer_model = indexer.fit(flights)\n",
      "\n",
      "# Indexer creates a new column with numeric index values\n",
      "flights_indexed = indexer_model.transform(flights)\n",
      "\n",
      "# Repeat the process for the other categorical feature\n",
      "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
      "\n",
      "\n",
      "   create the vector assembler\n",
      "\n",
      "# Import the necessary class\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "\n",
      "# Create an assembler object\n",
      "assembler = VectorAssembler(inputCols=[\n",
      "'mon', 'dom', 'dow',\n",
      "'carrier_idx',\n",
      "'org_idx', \n",
      "'km',\n",
      "'depart',\n",
      "'duration'\n",
      "], outputCol='features')\n",
      "\n",
      "# Consolidate predictor columns\n",
      "flights_assembled = assembler.transform(flights)\n",
      "\n",
      "# Check the resulting column\n",
      "flights_assembled.select('features', 'delay').show(5, truncate=False)\n",
      "\n",
      "\n",
      "        >Decision tree\n",
      "\n",
      "recursive partition\n",
      "\n",
      "first split is a dominate class and the non dominate class\n",
      "\n",
      "the depth of the tree along a branch need not be the same\n",
      "\n",
      "\n",
      "0 for usa manufactured cars\n",
      "1 for manufactured elsewhere\n",
      "\n",
      "called label\n",
      "\n",
      "cars_train, cars_test=cars.randomSplit([.8,.2], seed=23)\n",
      "\n",
      "cars_train.count()\n",
      "cars_test.count()\n",
      "\n",
      "\n",
      "from pyspark.ml.classification import DecisionTreeClassifier\n",
      "\n",
      "tree=DecisionTreeClassifier()\n",
      "\n",
      "\n",
      "tree.fit(cars_train)\n",
      "\n",
      "prediction=tree_model.transform(cars_test)\n",
      "1. label\n",
      "2. prediction\n",
      "3. probability\n",
      "\n",
      "   > create the confusion matrix\n",
      "\n",
      "prediction.groupBy(\"label\",\"prediction\").count().show()\n",
      "\n",
      "True positive\n",
      "False positive\n",
      "False negative\n",
      "True negative\n",
      "\n",
      "Accuracy=(TN+TP)/(TN+TP+FN+FP)\n",
      "\n",
      "  sample  > train test split\n",
      "\n",
      "# Split into training and testing sets in a 80:20 ratio\n",
      "flights_train, flights_test = flights.randomSplit([0.8,0.2],seed=17)\n",
      "\n",
      "# Check that training set has around 80% of records\n",
      "training_ratio = flights_train.count() / flights_test.count()\n",
      "print(training_ratio)\n",
      "\n",
      "\n",
      "   >sample  > Decision Tree Classifier\n",
      "\n",
      "# Import the Decision Tree Classifier class\n",
      "from pyspark.ml.classification import DecisionTreeClassifier\n",
      "\n",
      "# Create a classifier object and fit to the training data\n",
      "tree =DecisionTreeClassifier()\n",
      "tree_model = tree.fit(flights_train)\n",
      "\n",
      "# Create predictions for the testing data and take a look at the predictions\n",
      "prediction = tree_model.transform(flights_test)\n",
      "prediction.select('label', 'prediction', 'probability').show(5, False)\n",
      "\n",
      "\n",
      "  > sample  > create a confusion matrix\n",
      "\n",
      "# Create a confusion matrix\n",
      "prediction.groupBy(\"label\", 'prediction').count().show()\n",
      "\n",
      "# Calculate the elements of the confusion matrix\n",
      "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
      "TP = prediction.filter('prediction=1 AND label=prediction').count()\n",
      "FN = prediction.filter('prediction=0 AND label!=prediction').count()\n",
      "FP = prediction.filter('prediction=1 AND label!=prediction').count()\n",
      "\n",
      "# Accuracy measures the proportion of correct predictions\n",
      "accuracy = (TP+TN)/(TN+TP+FN+FP)\n",
      "print(accuracy)\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<logistic regression\n",
      "\n",
      "logistic curve models 0 or 1\n",
      "\n",
      "threshhold above .5 then predictive state is 1 else 0\n",
      "\n",
      "the curve can be more steep or more gradual or shift left or right\n",
      "\n",
      "\n",
      "from pyspark.ml.classification import LogisticRegression\n",
      "\n",
      "\n",
      "logistic=LogisticRegression()\n",
      "\n",
      "logistic = logistic.fit(cars_train)\n",
      "\n",
      "prediction=logistics.transform(car_test)\n",
      "\n",
      "precision=  tp/ (tp+fp)\n",
      "\n",
      "recall= tp/(tp+fn)\n",
      "\n",
      "from pyspark.ml.evaluation import MulticlassClassficationEvaluator\n",
      "\n",
      "evaluator=MulticlassClassificationEvaluator()\n",
      "evaluator.evaluate(prediction,{evaluator.metricName:'weightedPrecision'})\n",
      "\n",
      "1. weightedRecall\n",
      "2. accuracy\n",
      "3. f1\n",
      "\n",
      "roc and auc\n",
      "plots the true positive rate by the false positive rate\n",
      "\n",
      "  sample   > predict using logistic regression\n",
      "\n",
      "# Import the logistic regression class\n",
      "from pyspark.ml.classification import LogisticRegression\n",
      "\n",
      "\n",
      "# Create a classifier object and train on training data\n",
      "logistic = LogisticRegression().fit(flights_train)\n",
      "\n",
      "# Create predictions for the testing data and show confusion matrix\n",
      "prediction = logistic.transform(flights_test)\n",
      "prediction.groupBy(\"label\", \"prediction\").count().show()\n",
      "\n",
      "   sample  > get the roc auc value\n",
      "\n",
      "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
      "\n",
      "# Calculate the elements of the confusion matrix\n",
      "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
      "TP = prediction.filter('prediction=1 AND label=prediction').count()\n",
      "FN = prediction.filter('prediction=0 AND label!=prediction').count()\n",
      "FP = prediction.filter('prediction=1 AND label!=prediction').count()\n",
      "\n",
      "# Calculate precision and recall\n",
      "precision = TP/(TP+FP)\n",
      "recall = TP/(TP+FN)\n",
      "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
      "\n",
      "# Find weighted precision\n",
      "multi_evaluator = MulticlassClassificationEvaluator()\n",
      "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
      "\n",
      "# Find AUC\n",
      "binary_evaluator = BinaryClassificationEvaluator()\n",
      "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName:'areaUnderROC'})\n",
      "\n",
      "print(auc) \n",
      "\n",
      "       Turning text into tables\n",
      "\n",
      "80% of machine learning is data preparation\n",
      "\n",
      "one record per document.\n",
      "\n",
      "collection of documents\n",
      "\n",
      "tokenize the documents as columns in the table\n",
      "\n",
      "remove the stop words\n",
      "\n",
      "The table indicates the frequency of the word\n",
      "\n",
      "the table is known as a term document matrix\n",
      "\n",
      "remove punctuation\n",
      "\n",
      "      regular expressions\n",
      "\n",
      "from pyspark.sql.functions import regexp_replace\n",
      "\n",
      "REGEX='[,\\\\-]'\n",
      "\n",
      "escape the -\n",
      "\n",
      "books=books.withColumn('text',regexp_replace(books.text, REGEX,' '))\n",
      "\n",
      "\n",
      "books= Tokenizer(inputCol=\"text\", outputCol=\"tokens\").transform(books)\n",
      "\n",
      "stop words are common words adding very little information\n",
      "\n",
      "from pyspark.ml.feature import StopWordsRemover\n",
      "\n",
      "stopwords=StopWordsRemover()\n",
      "\n",
      "stopwords.getStopWords()\n",
      "\n",
      "stopwords=stopwords.setInputCol('tokens').setOutputCol('words')\n",
      "\n",
      "books=stopwords.transform(books)\n",
      "\n",
      "from pyspark.ml.feature import HashingTF\n",
      "\n",
      "hasher=HashingTF(inputCol=\"words\", outputCol=\"hash\", numFeatures=32)\n",
      "\n",
      "books=hasher.transform(books)\n",
      "\n",
      "from pyspark.ml.feature import IDF\n",
      "\n",
      "books=IDF(inputCol=\"hash\", outputCol=\"features\").fit(books).transform(books)\n",
      "\n",
      "IDF measure the frequency of the word across all documents\n",
      "\n",
      "inverse document frequency\n",
      "\n",
      "    sample  > Tokenize text\n",
      "\n",
      "# Import the necessary functions\n",
      "from pyspark.sql.functions import regexp_replace\n",
      "from pyspark.ml.feature import Tokenizer\n",
      "\n",
      "# Remove punctuation (REGEX provided) and numbers\n",
      "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
      "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
      "\n",
      "# Merge multiple spaces\n",
      "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
      "\n",
      "# Split the text into words\n",
      "wrangled = Tokenizer(inputCol='text', outputCol=\"words\").transform(wrangled)\n",
      "\n",
      "wrangled.show(4, truncate=False)\n",
      "\n",
      "\n",
      " > sample  > building the [[idf features]]\n",
      "\n",
      "from pyspark.ml.feature import StopWordsRemover,HashingTF,IDF\n",
      "\n",
      "# Remove stop words.\n",
      "wrangled = StopWordsRemover(inputCol=\"words\", outputCol=\"terms\")\\\n",
      "      .transform(sms)\n",
      "\n",
      "# Apply the hashing trick\n",
      "wrangled = HashingTF(inputCol=\"terms\", outputCol=\"hash\", numFeatures=1024)\\\n",
      "      .transform(wrangled)\n",
      "\n",
      "# Convert hashed symbols to TF-IDF\n",
      "tf_idf = IDF(inputCol=\"hash\", outputCol=\"features\")\\\n",
      "      .fit(wrangled).transform(wrangled)\n",
      "      \n",
      "tf_idf.select('terms', 'features').show(4, truncate=False)\n",
      "\n",
      "  > sample  > logistic regression prediction\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "sms_train, sms_test = sms.randomSplit([0.8,0.2], seed=13)\n",
      "\n",
      "# Fit a Logistic Regression model to the training data\n",
      "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
      "\n",
      "# Make predictions on the testing data\n",
      "prediction = logistic.transform(sms_test)\n",
      "\n",
      "# Create a confusion matrix, comparing predictions to known labels\n",
      "prediction.groupBy(\"label\",\"prediction\").count().show()\n",
      "\n",
      "\n",
      "\n",
      "         One-Hot Encoding\n",
      "\n",
      "categorical data\n",
      "\n",
      "create a column for each of the categorical levels\n",
      "\n",
      "dummy variables\n",
      "\n",
      "the sparse form records the column number and the value 1 for the categorical data\n",
      "\n",
      "the process of creating dummy variables is called one hot encoding because only one column is active or hot\n",
      "\n",
      "from pyspark.ml.feature import OneHotEncoderEstimator\n",
      "\n",
      "\n",
      "indexer=StringIndexer(inputCol='type',\n",
      "\t\t\toutputCol='type_idx')\n",
      "\n",
      "indexer=indexer.fit(cars)\n",
      "\n",
      "cars=indexer.transform(cars)\n",
      "\n",
      "onehot = OneHotEncoderEstimator(inputCols=['type_idx'], outputCols=['type_dummy']\n",
      "\n",
      "onehot=onehot.fit(cars)\n",
      "\n",
      "onehot.categorySizes\n",
      "\n",
      "cars=onehot.transform(cars)\n",
      "\n",
      "cars.select('type,'type_idx','type_dummy').distinct().sort('type_idx').show\n",
      "\n",
      "\n",
      "DenseVector([1,0,0,0,0,7,0,0])\n",
      "represented as\n",
      "SparseVector(8,[0,5],[1,7])\n",
      "\n",
      "\n",
      "8 items, non zero in position 0 and 5 with values 1 and 7\n",
      "\n",
      "   >sample  OneHotEncoderEstimator\n",
      "\n",
      "# Import the one hot encoder class\n",
      "from pyspark.ml.feature import OneHotEncoderEstimator\n",
      "\n",
      "# Create an instance of the one hot encoder\n",
      "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
      "\n",
      "# Apply the one hot encoder to the flights data\n",
      "onehot = onehot.fit(flights)\n",
      "flights_onehot = onehot.transform(flights)\n",
      "\n",
      "# Check the results\n",
      "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()\n",
      "\n",
      "\n",
      "          Regression\n",
      "\n",
      "scatter plot to visualize consumption verus mass\n",
      "\n",
      "\n",
      "residuals are the difference between the observed value and the corresponding value.  Vertical distance between the points and the model line\n",
      "\n",
      "The best model is found by minimizing a loss function\n",
      "\n",
      "Mean squared error 1/N (yi-y^i)**2\n",
      "\n",
      "yi=observed values\n",
      "y^i=model values\n",
      "\n",
      "predict consumption using mass, cyl, and type_dummy\n",
      "\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "\n",
      "regression = LinearRegression(labelCol=\"consumption\")\n",
      "\n",
      "regression=regression.fit(cars_train)\n",
      "\n",
      "predictions = regression.transform(cars_test)\n",
      "\n",
      "\n",
      "        >RegressionEvaluator\n",
      "\n",
      "from pyspark.ml.evalution import RegressionEvaluator\n",
      "\n",
      "RegressionEvaluator(labelCol='consumption').evalulate(predictions)\n",
      "\n",
      "the square root of the mean square error is the RMSE\n",
      "\n",
      "RMSE is the standard deviation of the residuals\n",
      "\n",
      "RegressionEvaluator\n",
      "1. mae (mean absolute error)\n",
      "2. r2\n",
      "3. mse (mean squared error)\n",
      "\n",
      "\n",
      "regression.intercept\n",
      "\n",
      "slope with each mass and consumption combination\n",
      "slope indicates how rapidly the model changes when mass and consumption change\n",
      "\n",
      "regression.coefficients\n",
      "\n",
      "there is a coefficient for each of the predictors\n",
      "mass\n",
      "cyl\n",
      "midsize\n",
      "small\n",
      "compact\n",
      "sporty\n",
      "large\n",
      "\n",
      "DenseVector([.0027,.1897,-1.309,-1.7933,-1.3594,-1.2917,-1.9693])\n",
      "\n",
      "\n",
      "mass=.0027\n",
      "cyl=.1897\n",
      "\n",
      "midsize=-1.3\n",
      "small=-1.79\n",
      "compact=-1.35\n",
      "sporty=-1.29\n",
      "large=-1.9\n",
      "\n",
      "large vehicles are the most fuel efficient for their mass.\n",
      "\n",
      "all other types consume less fuel than a large vehicle\n",
      "\n",
      "   sample build a regressor\n",
      "\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "from pyspark.ml.evaluation import RegressionEvaluator\n",
      "\n",
      "# Create a regression object and train on training data\n",
      "regression =LinearRegression(labelCol=\"duration\").fit(flights_train)\n",
      "\n",
      "# Create predictions for the testing data and take a look at the predictions\n",
      "predictions = regression.transform(flights_test)\n",
      "predictions.select('duration', 'prediction').show(5, False)\n",
      "\n",
      "# Calculate the RMSE\n",
      "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
      "\n",
      "\n",
      "   sample  > making sense of the coefficients\n",
      "\n",
      "# Intercept (average minutes on ground)\n",
      "inter = regression.intercept\n",
      "print(inter)\n",
      "\n",
      "# Coefficients\n",
      "coefs = regression.coefficients\n",
      "print(coefs)\n",
      "\n",
      "# Average minutes per km\n",
      "minutes_per_km = regression.coefficients[0]\n",
      "print(minutes_per_km)\n",
      "\n",
      "# Average speed in km per hour\n",
      "avg_speed = 60 / minutes_per_km\n",
      "print(avg_speed)\n",
      "\n",
      "\n",
      "regression\n",
      "duration= intercept + coefficient * distance\n",
      "\n",
      " > sample\n",
      "\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "from pyspark.ml.evaluation import RegressionEvaluator\n",
      "\n",
      "# Create a regression object and train on training data\n",
      "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
      "\n",
      "# Create predictions for the testing data\n",
      "predictions = regression.transform(flights_test)\n",
      "\n",
      "# Calculate the RMSE on testing data\n",
      "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
      "\n",
      "\n",
      "         intrepreting coefficients\n",
      "\n",
      "The coefficients attribute is a list, where the first element indicates how flight duration changes with flight distance\n",
      "\n",
      "0  km\n",
      "1  ORD\n",
      "2  SFO\n",
      "3  JFK\n",
      "4  LGA\n",
      "5  SMF\n",
      "6  SJC and\n",
      "7  TUS.\n",
      "\n",
      "# Average speed in km per hour\n",
      "avg_speed_hour = 60/regression.coefficients[0]\n",
      "print(avg_speed_hour)\n",
      "\n",
      "# Average minutes on ground at OGG\n",
      "inter = regression.intercept\n",
      "print(inter)\n",
      "\n",
      "# Average minutes on ground at JFK\n",
      "avg_ground_jfk = regression.intercept + regression.coefficients[3]\n",
      "print(avg_ground_jfk)\n",
      "\n",
      "# Average minutes on ground at LGA\n",
      "avg_ground_lga = regression.intercept + regression.coefficients[4]\n",
      "print(avg_ground_lga)\n",
      "\n",
      "\n",
      "output:\n",
      "807.3336599681242\n",
      "15.856628374450773\n",
      "68.53550999587868\n",
      "62.56747182033072\n",
      "\n",
      "intercept\n",
      "15.856628374450773\n",
      "\n",
      "coefficients\n",
      "[0.07431871477075411,28.399568722791717,20.55190513998231,52.678881621427905,46.710843445879945,18.28741662016716,15.721837765620768,17.737941505895947]\n",
      "\n",
      "\n",
      "        carefully manipulating features\n",
      "\n",
      "bucketing:  assigning features to buckets or bins with well defined boundaries\n",
      "\n",
      "heights in meters\n",
      "\n",
      "defined as short, average, tall\n",
      "\n",
      "from pyspark.ml.feature import Bucketizer\n",
      "\n",
      "bucketizer=Bucketizer(splits=[3500,4500,6000,6500],\n",
      "\tinputCol=\"rpm\",\n",
      "\toutputCol=\"rpm_bin\")\n",
      "\n",
      "cars=bucketizer.transform(cars)\n",
      "\n",
      "bucketed.select('rpm','rpm_bin').show(5)\n",
      "\n",
      "cars.groupBy('rpm_bin).count().show()\n",
      "\n",
      "low, medium, high [no sparse array]\n",
      "\n",
      "\n",
      "regression.coefficients\n",
      "[1.3814,0.1433])\n",
      "regression.intercept\n",
      "8.1835\n",
      "\n",
      "low RPM\n",
      "consumption=8.1835+1.3814\n",
      "\n",
      "medium RPM\n",
      "consumption=8.1835+0.1433\n",
      "\n",
      "operations on a single column\n",
      "\n",
      "log()\n",
      "sqrt()\n",
      "pow()\n",
      "\n",
      "operations on two columns\n",
      "product\n",
      "ratio\n",
      "\n",
      "\n",
      "\n",
      "bmi = mass / height**2\n",
      "\n",
      "cars = cars.withColumn('density_line', cars.mass/cars.length)\n",
      "\n",
      "cars = cars.withColumn('density_quad', cars.mass/cars.length**2)\n",
      "\n",
      "cars = cars.withColumn('density_cube', cars.mass/cars.length**3)\n",
      "\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator\n",
      "\n",
      "# Create buckets at 3 hour intervals through the day\n",
      "buckets =Bucketizer(splits=[0,3,6,9,12,15,18,21,24], inputCol='depart', outputCol='depart_bucket')\n",
      "\n",
      "# Bucket the departure times\n",
      "bucketed = buckets.transform(flights)\n",
      "bucketed.select('depart','depart_bucket').show(5)\n",
      "\n",
      "# Create a one-hot encoder\n",
      "onehot = OneHotEncoderEstimator(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
      "\n",
      "# One-hot encode the bucketed departure times\n",
      "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
      "flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').show(5)\n",
      "\n",
      "\n",
      "\n",
      "   adding departure time\n",
      "\n",
      "# Find the RMSE on testing data\n",
      "from pyspark.ml.evaluation import RegressionEvaluator\n",
      "\n",
      "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
      "\n",
      "# Average minutes on ground at OGG for flights departing between 00:00 and 03:00\n",
      "avg_night_ogg = regression.intercept + regression.coefficients[8]\n",
      "print(avg_night_ogg)\n",
      "\n",
      "# Average minutes on ground at JFK for flights departing between 00:00 and 03:00\n",
      "avg_night_jfk = regression.intercept + regression.coefficients[8] + regression.coefficients[3]\n",
      "print(avg_night_jfk)\n",
      "\n",
      "\n",
      "        Regularization\n",
      "\n",
      "\n",
      "penalized regression: model is punished for having too many coefficients.\n",
      "\n",
      "mse chooses coefficients that minimize the loss or the residual.\n",
      "\n",
      "regularization term\n",
      "Lasso - absolute value of the coefficients\n",
      "Ridge - square of the coefficients\n",
      "\n",
      "both will shrink the coefficients of non contributing coefficients\n",
      "lasso moves those coefficients to 0\n",
      "\n",
      "strength of regularization is determined by parameter alpha\n",
      "\n",
      "alpha=0 - no regularization (standard regression)\n",
      "alpha=infinity - complete regularation (all coefficients zero)\n",
      "\n",
      "\n",
      "assembler = VectorAssembler(inputCols=[\n",
      "\t'mass','cyl','type_dummy','density_line','density_quad','density_cube'], outputCol='features')\n",
      "\n",
      "cars=assembler.transform(cars)\n",
      "\n",
      "regression = LinearRegression(labelCol='consumption').fit(cars_train)\n",
      "\n",
      "regression.coefficients\n",
      "\n",
      "DenseVector([-0.012,0.174,-0.897,-1.445,-0.985,-1.071,-1.335,0.189,-0.780,1.160])\n",
      "\n",
      "every predictor is contributing to the model\n",
      "however it is unlike that all the feature are equally important in predicting consumption\n",
      "\n",
      "ridge= LinearRegression(labelCol='consumption', elasticNetParam=0, regParam=0.1)\n",
      "ridge.fit(cars_train)\n",
      "\n",
      "#RMSE\n",
      "0.72453\n",
      "\n",
      "\n",
      "lasso = LinearRegression(labelCol='consumption', elasticNetParam=1, regParam=0.1)\n",
      "lasso.fit(cars_train)\n",
      "\n",
      "\n",
      "DenseVector[0,0,0,-.056,0,0,0,0.026,0,0])\n",
      "\n",
      "all but two features are 0\n",
      "\n",
      "small type car and the linear density\n",
      "\n",
      "\n",
      "  features\n",
      "\n",
      "km\n",
      "org (origin airport, one-hot encoded, 8 levels)\n",
      "depart (departure time, binned in 3 hour intervals, one-hot encoded, 8 levels)\n",
      "dow (departure day of week, one-hot encoded, 7 levels) and\n",
      "mon (departure month, one-hot encoded, 12 levels).\n",
      "\n",
      "\n",
      "   >Sample    > create a linear Regression and a Regression Evaluator\n",
      "\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "from pyspark.ml.evaluation import RegressionEvaluator\n",
      "\n",
      "\n",
      "# Fit linear regression model to training data\n",
      "regression = LinearRegression(labelCol=\"duration\").fit(flights_train)\n",
      "\n",
      "# Make predictions on testing data\n",
      "predictions = regression.transform(flights_test)\n",
      "\n",
      "# Calculate the RMSE on testing data\n",
      "rmse = RegressionEvaluator(labelCol=\"duration\").evaluate(predictions)\n",
      "print(\"The test RMSE is\", rmse)\n",
      "\n",
      "# Look at the model coefficients\n",
      "coeffs = regression.coefficients\n",
      "print(coeffs)\n",
      "\n",
      "\n",
      "\n",
      "   > sample    lasso\n",
      "\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "from pyspark.ml.evaluation import RegressionEvaluator\n",
      "\n",
      "# Fit Lasso model (a = 1) to training data\n",
      "regression = LinearRegression(labelCol='duration', regParam=1, elasticNetParam=1).fit(flights_train)\n",
      "\n",
      "# Calculate the RMSE on testing data\n",
      "rmse = RegressionEvaluator(labelCol='duration').evaluate(regression.transform(flights_test))\n",
      "print(\"The test RMSE is\", rmse)\n",
      "\n",
      "# Look at the model coefficients\n",
      "coeffs = regression.coefficients\n",
      "print(coeffs)\n",
      "\n",
      "# Number of zero coefficients\n",
      "zero_coeff = sum([beta == 0 for beta in regression.coefficients])\n",
      "print(\"Number of coefficients equal to 0:\", zero_coeff)\n",
      "\n",
      "output\n",
      "\n",
      "The test RMSE is 11.221618112066176\n",
      "[0.07326284332459325,0.26927242574175647,-4.213823507520847,23.31411303902282,16.924833465407964,-7.538366699625629,-5.04321753247765,-20.348693139176927,0.0,0.0,0.0,0.0,0.0,1.199161974782719,0.43548357163388335,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "Number of coefficients equal to 0: 22\n",
      "\n",
      "\n",
      "         >Pipeline\n",
      "\n",
      "make it better\n",
      "\n",
      "leakage occurrs when fit is applied to any testing data\n",
      "a pipeline combines a series of steps\n",
      "\n",
      "\n",
      "manual sequence of steps\n",
      "\n",
      "#one hot encode the type categorical data\n",
      "indexer=StringIndexer(inputCol='type', outputCol='type_idx')\n",
      "\n",
      "onehot = OneHotEncoderEstimator(inputCols=['type_idx'), outputCols=['type_dummy'])\n",
      "\n",
      "#create a single features column\n",
      "\n",
      "assemble=VectorAssembler(inputCols=['mass','cyl','type_dummy'],outputCol='features')\n",
      "\n",
      "#build the regression model\n",
      "\n",
      "regression=LinearRegression(labelCol='consumption')\n",
      "\n",
      "indexer=indexer.fit(cars_train)\n",
      "cars_train=indexer.transform(cars_train)\n",
      "cars_test=indexer.transform(cars_test)\n",
      "\n",
      "cars_train=onehot.transform(cars_train)\n",
      "cars_tst=onehot.transform(cars_test)\n",
      "\n",
      "cars_train=assemble.transform(cars_train)\n",
      "cars_test=assemble.transform(cars_test)\n",
      "\n",
      "regression=regression.fit(cars_train)\n",
      "\n",
      "predictions=regression.transform(cars_test)\n",
      "\n",
      "\n",
      "from pyspark.ml import Pipeline\n",
      "\n",
      "#sequence of stages\n",
      "pipeline= Pipeline(stages=[indexer, onehot, assemble, regression])\n",
      "\n",
      "pipeline=pipeline.fit(cars_train)\n",
      "\n",
      "predictions=pipeline.transform(cars_test)\n",
      "\n",
      "pipeline.stages\n",
      "pipeline.stages[3]\n",
      "\n",
      "pipeline.stages[3].intercept\n",
      "pipeline.stages[3].coefficients\n",
      "\n",
      "\n",
      "      >sample   > setup for the pipeline\n",
      "\n",
      "# Convert categorical strings to index values\n",
      "indexer =StringIndexer(inputCol='org',outputCol='org_idx')\n",
      "\n",
      "# One-hot encode index values\n",
      "onehot =OneHotEncoderEstimator(\n",
      "    inputCols=['org_idx','dow'],\n",
      "    outputCols=['org_dummy','dow_dummy']\n",
      ")\n",
      "\n",
      "# Assemble predictors into a single column\n",
      "assembler = VectorAssembler(inputCols=['km','org_dummy','dow_dummy'], outputCol='features')\n",
      "\n",
      "# A linear regression object\n",
      "regression = LinearRegression(labelCol='duration')\n",
      "\n",
      "   Add the pipeline\n",
      "\n",
      "# Import class for creating a pipeline\n",
      "from pyspark.ml import Pipeline\n",
      "\n",
      "# Construct a pipeline\n",
      "pipeline = Pipeline(stages=[indexer,onehot,assembler,regression])\n",
      "\n",
      "# Train the pipeline on the training data\n",
      "pipeline = pipeline.fit(flights_train)\n",
      "\n",
      "# Make predictions on the testing data\n",
      "predictions = pipeline.transform(flights_test)\n",
      "\n",
      "\n",
      "      sample     setup for words for a pipeline\n",
      "\n",
      "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
      "\n",
      "# Break text into tokens at non-word characters\n",
      "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
      "\n",
      "# Remove stop words\n",
      "remover = StopWordsRemover(inputCol=\"words\", outputCol='terms')\n",
      "\n",
      "# Apply the hashing trick and transform to TF-IDF\n",
      "hasher = HashingTF(inputCol=\"terms\", outputCol=\"hash\")\n",
      "idf = IDF(inputCol=\"hash\", outputCol=\"features\")\n",
      "\n",
      "# Create a logistic regression object and add everything to a pipeline\n",
      "logistic = LogisticRegression()\n",
      "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])\n",
      "\n",
      "\n",
      "        >Cross validation\n",
      "\n",
      "data, training, testing\n",
      "\n",
      "training is split into fold called cross validation\n",
      "\n",
      "regression=LinearRegression(labelCol='consumption')\n",
      "\n",
      "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
      "\n",
      "\n",
      "params = ParamGridBuilder().build()\n",
      "\n",
      "cv=CrossValidator(estimator=regression,\n",
      "\testimatorParamMaps = params,\n",
      "\tevaluator=evaluator,\n",
      "\tnumFolds=10, seed=13)\n",
      "\n",
      "folds default to 3\n",
      "\n",
      "cv=cv.fit(cars_train)\n",
      "\n",
      "cv.avgMetrics\n",
      "\n",
      "predictions=cv.transform(cars_test)\n",
      "rmse = evaluator.evaluate(predictions)\n",
      "\n",
      "   sample     setup Cross Validation with 5 fold\n",
      "\n",
      "# Create an empty parameter grid\n",
      "params = ParamGridBuilder().build()\n",
      "\n",
      "# Create objects for building and evaluating a regression model\n",
      "regression = LinearRegression(labelCol='duration')\n",
      "evaluator = RegressionEvaluator(labelCol=\"duration\")\n",
      "\n",
      "# Create a cross validator\n",
      "cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
      "\n",
      "# Train and test model on multiple folds of the training data\n",
      "cv = cv.fit(flights_train)\n",
      "\n",
      "# NOTE: Since cross-valdiation builds multiple models, the fit() method can take a little while to complete.\n",
      "\n",
      "predictions=cv.transform(flights_test)\n",
      "rmse = evaluator.evaluate(predictions)\n",
      "print(rmse)\n",
      "\n",
      "output: 16\n",
      "\n",
      "\n",
      "\n",
      "    sample cross validating with a pipeline\n",
      "\n",
      "# Create an indexer for the org field\n",
      "indexer = StringIndexer(inputCol='org',outputCol='org_idx')\n",
      "\n",
      "# Create an one-hot encoder for the indexed org field\n",
      "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
      "\n",
      "# Assemble the km and one-hot encoded fields\n",
      "assembler = VectorAssembler(inputCols=['km','org_dummy'],outputCol='features')\n",
      "\n",
      "# Create a pipeline and cross-validator.\n",
      "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
      "\n",
      "cv = CrossValidator(estimator=pipeline,\n",
      "          estimatorParamMaps=params,\n",
      "          evaluator=evaluator)\n",
      "\n",
      "   sorting a groupby\n",
      "\n",
      "res = g.apply(lambda x: x.sort_values(ascending=False).head(3))\n",
      "          >Grid Search\n",
      "\n",
      "\n",
      "regression=LinearRegression(labelCol='consumption', fitIntercept=True)\n",
      "\n",
      "regression = regression.fit(cars_train)\n",
      "\n",
      "rmse = evaluator.evaluate(regression.transform(cars_test))\n",
      "\n",
      "output.745\n",
      "\n",
      "set fitInercept=False\n",
      "output.65\n",
      "\n",
      "from pyspark.ml.tuning import ParamGridBuilder\n",
      "\n",
      "params=ParamGridBuilder()\n",
      "\n",
      "#add grid points\n",
      "\n",
      "params=params.addGrid(regression.fitIntercept,[True,False])\n",
      "\n",
      "params=params.build()\n",
      "\n",
      "print('Number of models to be tested:', len(params))\n",
      "\n",
      "output:2\n",
      "\n",
      "\n",
      "cv=CrossValidator(estimator=regression,\n",
      "\t\testimatorParamsMaps=params,\n",
      "\t\tevaluator=evaluator)\n",
      "\n",
      "cv=cv.setNumFolds(10).setSeed(13).fit(cars_train)\n",
      "\n",
      "20 models\n",
      "\n",
      "cv.avgMetrics\n",
      "\n",
      "output:[.8006,0.9079] \n",
      "\n",
      "the model that includes an intercept does better than one without\n",
      "\n",
      "cv.bestModel\n",
      "\n",
      "predictions = cv.tranform(cars_test)\n",
      "\n",
      "cv.bestModel.explainParam('fitIntercept')\n",
      "\n",
      "output: current is True\n",
      "\n",
      "#add multiple grid points\n",
      "\n",
      "params=params.addGrid(regression.fitIntercept,[True,False]) \\\n",
      "\t.addGrid(regression.regParam,[0.001,0.01,0.1,1,10])\\\n",
      "\t.addGrid(regression.elasticNetParam,[0,0.25,0.5,0.75,1])\\\n",
      "\t.build()\n",
      "\n",
      "\n",
      "    sample    > Grid builder    CrossValidator 5 folds\n",
      "\n",
      "# Create parameter grid\n",
      "params = ParamGridBuilder()\n",
      "\n",
      "# Add grids for two parameters\n",
      "params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
      "               .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])\n",
      "\n",
      "# Build the parameter grid\n",
      "params = params.build()\n",
      "print('Number of models to be tested: ', len(params))\n",
      "\n",
      "# Create cross-validator\n",
      "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
      "\n",
      "output: number of models to be tested: 12\n",
      "\n",
      "\n",
      "    sample  > make predictions with the best model\n",
      "\n",
      "# Get the best model from cross validation\n",
      "best_model = cv.bestModel\n",
      "\n",
      "# Look at the stages in the best model\n",
      "print(best_model.stages)\n",
      "\n",
      "# Get the parameters for the LinearRegression object in the best model\n",
      "best_model.stages[3].extractParamMap()\n",
      "\n",
      "# Generate predictions on testing data using the best model then calculate RMSE\n",
      "predictions = best_model.transform(flights_test)\n",
      "evaluator.evaluate(predictions)\n",
      "\n",
      "output:\n",
      "[StringIndexer_14299b2d5472, OneHotEncoderEstimator_9a650c117f1d, VectorAssembler_933acae88a6e, LinearRegression_9f5a93965597]\n",
      "In [1]:\n",
      "\n",
      "\n",
      "        >sample build the params grid for logistic regression\n",
      "\n",
      "# Create parameter grid\n",
      "params = ParamGridBuilder()\n",
      "\n",
      "# Add grid for hashing trick parameters\n",
      "params = params.addGrid(hasher.numFeatures,[1024, 4096,16384]) \\\n",
      "               .addGrid(hasher.binary, [True,False])\n",
      "\n",
      "# Add grid for logistic regression parameters\n",
      "params = params.addGrid(logistic.regParam,[0.01, 0.1, 1.0, 10.0]) \\\n",
      "               .addGrid(logistic.elasticNetParam, [0.0, 0.5, 1.0])\n",
      "\n",
      "# Build parameter grid\n",
      "params = params.build()\n",
      "\n",
      "        >Ensemble\n",
      "\n",
      "it is a collection of models\n",
      "\n",
      "collective opinion of a group is better\n",
      "\n",
      "there must be diversity and independence \n",
      "\n",
      "\n",
      "Random Forest is a collection of trees\n",
      "\n",
      "no two trees are the same\n",
      "\n",
      "from pyspark.ml.classification import RandomForestClassifier\n",
      "\n",
      "forest=RandomForestClassifier(numTrees=5)\n",
      "\n",
      "forest=forest.fit(cars_train)\n",
      "\n",
      "forest.trees\n",
      "\n",
      "\n",
      "the model uses : cyl, size, mass, length, rpm, and consumption\n",
      "\n",
      "forest.featureImportances\n",
      "\n",
      "sparseVector(6,{0:0.0205,1:0.2701,2:0.108,3:0.1895,4:0.2939,5:0.1181})\n",
      "\n",
      "rpm is the most important\n",
      "cyl is the least important\n",
      "\n",
      "\n",
      "Gradient-Boost Trees\n",
      "\n",
      "trees working in series\n",
      "\n",
      "1. build a decision tree and add to the ensemble\n",
      "2. predict label for each training instance using ensemble\n",
      "3. compare predictions with known labels\n",
      "4. emphasize training instances with incorrect predictions\n",
      "\n",
      "5. return to 1 and train another tree which works to improve the incorrect predictions.\n",
      "\n",
      "each new tree attempts to correct the errors of the proceeding trees\n",
      "\n",
      "from pyspark.ml.classification import GBTClassifier\n",
      "\n",
      "gbt = GBTClassifier(maxIter=10)\n",
      "\n",
      "gbt=gbt.fit(cars_train)\n",
      "\n",
      "\n",
      "   sample  compare the AUC for the Decision Tree, Random Forest, and Gradient Boosted Tree\n",
      "\n",
      "\n",
      "\n",
      "# Import the classes required\n",
      "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier\n",
      "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
      "\n",
      "# Create model objects and train on training data\n",
      "tree =DecisionTreeClassifier().fit(flights_train)\n",
      "gbt = GBTClassifier().fit(flights_train)\n",
      "\n",
      "# Compare AUC on testing data\n",
      "evaluator = BinaryClassificationEvaluator()\n",
      "evaluator.evaluate(tree.transform(flights_test))\n",
      "evaluator.evaluate(gbt.transform(flights_test))\n",
      "\n",
      "# Find the number of trees and the relative importance of features\n",
      "print(gbt.getNumTrees)\n",
      "print(gbt.featureImportances)\n",
      "\n",
      "\n",
      "output\n",
      "20 (trees)\n",
      "(3,[0,1,2],[0.27857733519498645,0.3517987451488248,0.36962391965618874])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      RandomForestClassifier\n",
      "\n",
      "You'll find good values for the following parameters:\n",
      "\n",
      "featureSubsetStrategy  the number of features to consider for splitting at each node and\n",
      "maxDepth  the maximum number of splits along any branch.\n",
      "\n",
      "# Create a random forest classifier\n",
      "forest = RandomForestClassifier()\n",
      "\n",
      "# Create a parameter grid\n",
      "params = ParamGridBuilder() \\\n",
      "            .addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']) \\\n",
      "            .addGrid(forest.maxDepth, [2, 5, 10]) \\\n",
      "            .build()\n",
      "\n",
      "# Create a binary classification evaluator\n",
      "evaluator = BinaryClassificationEvaluator()\n",
      "\n",
      "# Create a cross-validator\n",
      "cv = CrossValidator(estimator=forest, estimatorParamMaps=params,evaluator=evaluator, numFolds=5)\n",
      "\n",
      "\n",
      "   random forest   > cross validation   > evaluate auc\n",
      "\n",
      "# Average AUC for each parameter combination in grid\n",
      "avg_auc = cv.avgMetrics\n",
      "\n",
      "# Average AUC for the best model\n",
      "best_model_auc =  max(avg_auc)\n",
      "\n",
      "print(avg_auc)\n",
      "print(best_model_auc)\n",
      "\n",
      "# What's the optimal parameter value?\n",
      "print(cv.bestModel.params)\n",
      "opt_max_depth = cv.bestModel.explainParam(\"maxDepth\")\n",
      "opt_feat_substrat = cv.bestModel.explainParam('featureSubsetStrategy')\n",
      "\n",
      "# AUC for best model on testing data\n",
      "best_auc = evaluator.evaluate(cv.transform(flights_test))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\statistics probability.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\statistics probability.txt\n",
      " >Probablity\n",
      "\n",
      "1. probablistic logic helps us describe uncertainty\n",
      "2. statistic inference is the process whereby we go from measured data to probabilistic conclusions based on expectation\n",
      "\n",
      "np.random.random() \n",
      "#draws a number between 0 and 1\n",
      "\n",
      "np.random.seed(42)\n",
      "#allows you to have reproducable code\n",
      "#integer fed into random number generator algorithm\n",
      "\n",
      "random_numbers=np.random.random(size=4)\n",
      "random_numbers\n",
      "\n",
      "heads=random_numbers<0.5\n",
      "heads\n",
      "np.sum(heads)\n",
      "\n",
      "\n",
      "n_all_heads=0\n",
      "\n",
      "for _ in range(1000):\n",
      "\theads=np.random.random(size=4) <0.5\n",
      "\tn_heads=np.sum(heads)\n",
      "\tif n_heads==4:\n",
      "\t\tn_all_heads+=1\n",
      "\n",
      "n_all_heads/10000\n",
      "\n",
      " Hacker statistics\n",
      "\n",
      "1. figure out how to simulate the data\n",
      "2. simulate the data many many times\n",
      "3. compute the probability as an approximate fraction of the trials with the outcome of interest\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "# Seed the random number generator\n",
      "\n",
      "np.random.seed(42)\n",
      "# Initialize random numbers: random_numbers\n",
      "random_numbers=np.empty(100000)\n",
      "\n",
      "# Generate random numbers by looping over range(100000)\n",
      "for i in range(100000):\n",
      "    random_numbers[i] = np.random.random()\n",
      "\n",
      "# Plot a histogram\n",
      "ax = plt.hist(random_numbers)\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      " Sample\n",
      "\n",
      "def perform_bernoulli_trials(n, p):\n",
      "    \"\"\"Perform n Bernoulli trials with success probability p\n",
      "    and return number of successes.\"\"\"\n",
      "    # Initialize number of successes: n_success\n",
      "    n_success = 0\n",
      "\n",
      "    # Perform trials\n",
      "    for i in range(n):\n",
      "        # Choose random number between zero and one: random_number\n",
      "        random_number = np.random.random()\n",
      "\n",
      "        # If less than p, it's a success  so add one to n_success\n",
      "        if random_number < p:\n",
      "            n_success += 1\n",
      "\n",
      "    return n_success\n",
      "\n",
      "# Seed random number generator\n",
      "np.random.seed(42)\n",
      "\n",
      "# Initialize the number of defaults: n_defaults\n",
      "\n",
      "n_defaults=np.empty(1000)\n",
      "# Compute the number of defaults\n",
      "for i in range(1000):\n",
      "    n_defaults[i] = perform_bernoulli_trials(100,0.05)\n",
      "\n",
      "\n",
      "# Plot the histogram with default number of bins; label your axes\n",
      "plt.clf()\n",
      "_ = plt.hist(n_defaults, normed=True)\n",
      "_ = plt.xlabel('number of defaults out of 100 loans')\n",
      "_ = plt.ylabel('probability')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "def ecdf(data):\n",
      "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
      "    # Number of data points: n\n",
      "    n =len(data)\n",
      "\n",
      "    # x-data for the ECDF: x\n",
      "    x = np.sort(data)\n",
      "\n",
      "    # y-data for the ECDF: y\n",
      "    y = np.arange(1,n+1) / n\n",
      "\n",
      "    return x, y\n",
      "\n",
      "# Compute ECDF: x, y\n",
      "\n",
      "x,y=ecdf(n_defaults)\n",
      "# Plot the ECDF with labeled axes\n",
      "\n",
      "_=plt.plot(x,y,marker='.', linestyle='none')\n",
      "_=plt.xlabel('Bank Defaults')\n",
      "_=plt.ylabel('ECDF')\n",
      "\n",
      "\n",
      "# Show the plot\n",
      "\n",
      "plt.show()\n",
      "\n",
      "# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money\n",
      "\n",
      "n_lose_money = np.sum(n_defaults >= 10)\n",
      "\n",
      "# Compute and print probability of losing money\n",
      "print('Probability of losing money =', n_lose_money / len(n_defaults))\n",
      "\n",
      "  The binomial distribution\n",
      "\n",
      "Probability mass function (PMF)\n",
      "1. The set of probabilities of discrete outcomes\n",
      "2.  the values are discrete because only certain values can be obtained.\n",
      "\n",
      "dice: 1,2,3,4,5,6 each with a 1/6 probability\n",
      "1. discrete uniform probablity pmf\n",
      "\n",
      "probability distribution is a mathematical descriptiion of outcomes.\n",
      "\n",
      "The binomial distribution story\n",
      "\n",
      "The number r of successes in n bernouli trials with probability p of sucess is binomially distributed.\n",
      "\n",
      "The number r of heads in 4 flips with probability 0.5 of heads is binomially distributed.\n",
      "\n",
      "print(np.random.binomial(100,0.5,size=10))\n",
      "#size  tells the function how many random numbers to sample out the distribution\n",
      "\n",
      "n=60\n",
      "p=0.1\n",
      "samples=np.random.binomial(n,p,size=10000)\n",
      "\n",
      "x,y = ecdf(samples)\n",
      "_ = plt.plot(x,y, marker='.', linestyle='none')\n",
      "plt.margins(0.02)\n",
      "_ = plt.xlabel('number of successes')\n",
      "_ = plt.ylabel('CDF')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "n=100\n",
      "p=0.05\n",
      "n_defaults=np.random.binomial(n,p,size=10000)\n",
      "\n",
      "# Compute CDF: x, y\n",
      "x,y = ecdf(n_defaults)\n",
      "\n",
      "_ = plt.plot(x,y, marker='.', linestyle='none')\n",
      "plt.margins(0.02)\n",
      "_ = plt.xlabel('number of successes')\n",
      "_ = plt.ylabel('CDF')\n",
      "plt.show()\n",
      "\n",
      "# Compute bin edges: bins\n",
      "bins = np.arange(0, max(n_defaults) + 1.5) - 0.5\n",
      "\n",
      "# Generate histogram\n",
      "plt.clf()\n",
      "_ = plt.hist(n_defaults, bins=bins, normed=True)\n",
      "_ = plt.xlabel('number of defaults out of 100 loans')\n",
      "_ = plt.ylabel('probability')\n",
      "_\n",
      "# Label axes\n",
      "\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  Poisson Process\n",
      "\n",
      "1.  The time of the next event is completely independent of when the prevous event happened\n",
      "a. natural births in a given hospital\n",
      "b. hit on a website during a given hour\n",
      "c. meteor strikes\n",
      "d. molecular collision in a gas\n",
      "e. avaition incidents\n",
      "f. buses in poissonville\n",
      "\n",
      "poisson distribution\n",
      "\n",
      "1. The number r of arrivals of a poisson process in a given time interval with a average rate of ? arrivals per interval is poisson distribution\n",
      "\n",
      "2. The number r of hits on a website in one hour with an average hit rate of 6 hits per hour is poisson distribute\n",
      "\n",
      "Poisson distribution\n",
      "1. limit of binomial distribution of low probablity of success and large number of trials\n",
      "2. That is, for rare events\n",
      "\n",
      "https://www.cnbc.com/2020/02/07/junk-bond-scare-is-rising-no-one-cares-people-are-buying-everything.html\n",
      "\n",
      "https://www.cnbc.com/2020/04/06/investing-in-hunt-for-returns-investors-are-buying-junk-bonds.html\n",
      "\n",
      "samples = np.random.poisson(6, size=10000)\n",
      "x,y=ecdf(samples)\n",
      "_=plt.plot(x,y, marker='.', linestyle='none')\n",
      "plt.margins(0.02)\n",
      "_= plt.xlabel('number of successes')\n",
      "_= plt.ylabel('cdf')\n",
      "plt.show()\n",
      "\n",
      " sample\n",
      "\n",
      "#the Poisson distribution is a limit of the Binomial distribution for rare events\n",
      "\n",
      "#Say we do a Bernoulli trial every minute for an hour, each with a success probability of 0.1. We would do 60 trials, and the number of successes is Binomially distributed, and we would expect to get about 6 successes. \n",
      "\n",
      "\n",
      " >Sample 1\n",
      "\n",
      "# Draw 10,000 samples out of Poisson distribution: samples_poisson\n",
      "\n",
      "samples_poisson = np.random.poisson(10, size=10000)\n",
      "\n",
      "# Print the mean and standard deviation\n",
      "print('Poisson:     ', np.mean(samples_poisson),\n",
      "                       np.std(samples_poisson))\n",
      "\n",
      "# Specify values of n and p to consider for Binomial: n, p\n",
      "\n",
      "n=[20,100,1000]\n",
      "p=[0.5,0.1,0.01]\n",
      "\n",
      "# Draw 10,000 samples for each n,p pair: samples_binomial\n",
      "for i in range(3):\n",
      "    samples_binomial = np.random.binomial(n[i],p[i],size=10000)\n",
      "\n",
      "    # Print results\n",
      "    print('n =', n[i], 'Binom:', np.mean(samples_binomial),\n",
      "                                 np.std(samples_binomial))\n",
      "\n",
      " Sample 2\n",
      "\n",
      "# Draw 10,000 samples out of Poisson distribution: n_nohitters\n",
      "\n",
      "n_nohitters = np.random.poisson(251/115, size=10000)\n",
      "\n",
      "# Compute number of samples that are seven or greater: n_large\n",
      "n_large = np.sum(n_nohitters>=7)\n",
      "\n",
      "# Compute probability of getting seven or more: p_large\n",
      "\n",
      "p_large=n_large/10000\n",
      "\n",
      "# Print the result\n",
      "print('Probability of seven or more no-hitters:', p_large)\n",
      "\n",
      "\n",
      " >density functions\n",
      "\n",
      "continous variables can take on any values, not just discrete values.\n",
      "\n",
      "normal distribution\n",
      "\n",
      "1. probability density function (pdf)\n",
      "a. continous analog to the pmf\n",
      "b. mathematical description of the relative likelihood of observing a value of a continous variable\n",
      "\n",
      "cdf - accumulative distribution function\n",
      "\n",
      "Normal distribution is famous\n",
      "1. describes a continous variable whose PDF has a single symmetric peak.\n",
      "a. The mean determines where the center of the peak is\n",
      "b. The standard deviation is a measure of how wide the peak is.\n",
      "\n",
      "\n",
      "we can use a histogram to compare the data to a normal probability of distribution pdf\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "mean=np.mean(michelson_speed_of_light)\n",
      "std= np.std(michelson_speed_of_light)\n",
      "\n",
      "samples= np.random.normal(mean,std, size=10000)\n",
      "x,y=ecdf(michaelson_speed_of_light)\n",
      "x_theory,y_theory=ecdf(samples)\n",
      "\n",
      "sns.set()\n",
      "\n",
      "_=plt.plot(x_theory,y_theory)\n",
      "_=plt.plot(x,y,marker='.', linestyle='none')\n",
      "_=plt.xlabel('speed of light (km/s)')\n",
      "_=plt.ylabel('cdf')\n",
      "plt.show()\n",
      "\n",
      "The michelson data is approximately normally distributed\n",
      "\n",
      " Sample of the distribution\n",
      "\n",
      "# Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10\n",
      "samples_std1 = np.random.normal(20, 1, size=100000)\n",
      "samples_std3 = np.random.normal(20, 3, size=100000)\n",
      "samples_std10 = np.random.normal(20, 10, size=100000)\n",
      "\n",
      "# Make histograms\n",
      "_ = plt.hist(samples_std1, bins=100, normed=True, histtype='step')\n",
      "_ = plt.hist(samples_std3, bins=100, normed=True, histtype='step')\n",
      "_ = plt.hist(samples_std10, bins=100, normed=True, histtype='step')\n",
      "\n",
      "# Make a legend, set limits and show plot\n",
      "_ = plt.legend(('std = 1', 'std = 3', 'std = 10'))\n",
      "plt.ylim(-0.01, 0.42)\n",
      "plt.show()\n",
      "\n",
      "# Generate CDFs\n",
      "x_std1,y_std1=ecdf(samples_std1)\n",
      "x_std3,y_std3=ecdf(samples_std3)\n",
      "x_std10,y_std10=ecdf(samples_std10)\n",
      "\n",
      "\n",
      "\n",
      "# Plot CDFs\n",
      "\n",
      "_=plt.plot(x_std1,y_std1)\n",
      "_=plt.plot(x_std3,y_std3)\n",
      "_=plt.plot(x_std10,y_std10)\n",
      "\n",
      "\n",
      "# Make a legend and show the plot\n",
      "_ = plt.legend(('std = 1', 'std = 3', 'std = 10'), loc='lower right')\n",
      "plt.show()\n",
      "\n",
      " >Normal distribution and properties and warnings\n",
      "1. The normal distribution is often referred to as the guassian distribution\n",
      "\n",
      " sample 3\n",
      "# Compute mean and standard deviation: mu, sigma\n",
      "mu = np.mean(belmont_no_outliers)\n",
      "sigma = np.std(belmont_no_outliers)\n",
      "\n",
      "# Sample out of a normal distribution with this mu and sigma: samples\n",
      "samples = np.random.normal(mu, sigma, size=10000)\n",
      "\n",
      "# Get the CDF of the samples and of the data\n",
      "x_theor, y_theor = ecdf(samples)\n",
      "x, y = ecdf(belmont_no_outliers)\n",
      "\n",
      "# Plot the CDFs and show the plot\n",
      "_ = plt.plot(x_theor, y_theor)\n",
      "_ = plt.plot(x, y, marker='.', linestyle='none')\n",
      "_ = plt.xlabel('Belmont winning time (sec.)')\n",
      "_ = plt.ylabel('CDF')\n",
      "plt.show()\n",
      "\n",
      "# Take a million samples out of the Normal distribution: samples\n",
      "samples = np.random.normal(mu, sigma, size=1000000)\n",
      "\n",
      "# Compute the fraction that are faster than 144 seconds: prob\n",
      "prob = np.sum(samples <= 144) / len(samples)\n",
      "\n",
      "# Print the result\n",
      "print('Probability of besting Secretariat:', prob)\n",
      "\n",
      "https://www.cnbc.com/2020/03/20/junk-bond-default-rate-expected-to-triple-in-next-12-months-sp-says.html\n",
      "\n",
      "S&P Global Ratings said the default rate for high-yield, or junk, bonds is heading to 10% over the next 12 months, more than triple the rate of 3.1% that closed out 2019. \n",
      "\n",
      "find junk bonds by cuspip\n",
      "https://managingfundswithpythonandsql.wordpress.com/\n",
      "\n",
      ":/Index Holdings/High Yield Archiv\n",
      "\n",
      "https://github.com/fedspendingtransparency/usaspending-website/wiki\n",
      "\n",
      "  Exponential distribution\n",
      "\n",
      "1. The waiting time between arrivals of a poisson process is exponentially distributed\n",
      "\n",
      "2. It has a single parameter the mean waiting time\n",
      "\n",
      "nuclear incidents - timing of one is independent of all others. time of days between nuclear incidents\n",
      "\n",
      "mean= np.mean(inter_times)\n",
      "samples = np.random.exponential(mean, size=10000)\n",
      "x,y=ecdf(inter_times)\n",
      "x_theor,y_theor = ecdf(samples)\n",
      "\n",
      "_ = plt.plot(x_theor, y_theor)\n",
      "_ = plt.plot(x, y, marker='.', linestyle='none')\n",
      "_ = plt.xlabel('time (days)')\n",
      "_ = plt.ylabel('CDF')\n",
      "\n",
      "\n",
      "\n",
      "speculative grade default rates\n",
      "\n",
      "https://www.schwab.com/resource-center/insights/content/corporate-defaults-what-investors-should-know-when-a-bond-issuer-goes-bankrupt\n",
      "\n",
      "1984 3%\n",
      "1988 3%\n",
      "1992 12%\n",
      "1996 4%\n",
      "2000 8%\n",
      "2002 12%\n",
      "2004 6%\n",
      "2008 2%\n",
      "2010 12%\n",
      "2012 3%\n",
      "2016 5%\n",
      "2020 2%\n",
      "\n",
      "\n",
      "if the incidents are evenly distributed the event can be modeled as a poisson process.\n",
      "\n",
      "if you can simulate a story you can get its distribution.\n",
      "\n",
      "\n",
      " sample\n",
      "\n",
      "def successive_poisson(tau1, tau2, size=1):\n",
      "    \"\"\"Compute time for arrival of 2 successive Poisson processes.\"\"\"\n",
      "    # Draw samples out of first exponential distribution: t1\n",
      "    t1 = np.random.exponential(tau1, size)\n",
      "\n",
      "    # Draw samples out of second exponential distribution: t2\n",
      "    t2 = np.random.exponential(tau2, size)\n",
      "\n",
      "    return t1 + t2\n",
      "\n",
      "\n",
      "#Recall, from the earlier exercise, that tau1 denotes the mean waiting time for a no-hitter, while tau2 denotes the mean waiting time for hitting the cycle.\n",
      "\n",
      "#The mean waiting time for a no-hitter is 764 games, and the mean waiting time for hitting the cycle is 715 games.\n",
      "\n",
      "\n",
      "waiting_times = successive_poisson(764, 715, size=100000)\n",
      "\n",
      "# Make the histogram\n",
      "_ = plt.hist(waiting_times, bins=100, histtype='step',\n",
      "             normed=True)\n",
      "\n",
      "# Label axes\n",
      "_ = plt.xlabel('total waiting time (games)')\n",
      "_ = plt.ylabel('PDF')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "# Notice that the PDF is peaked, unlike the waiting time for a single Poisson process. For fun (and enlightenment), I encourage you to also plot the CDF.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['hour']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
