{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\customer churn.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\customer churn.txt\n",
      "churn is when a customer ends a relationship with the company\n",
      "\n",
      "non-contractual churn (consumer loyalty)\n",
      "involuntary churn (expiration or non payment)\n",
      "\n",
      "Customer\n",
      "1. Lack of usage\n",
      "2. Poor service\n",
      "3. Better price\n",
      "\n",
      "Domain/industry knowledge\n",
      "\n",
      "Telco churn dataset.\n",
      "\n",
      "telecom features\n",
      "1. voice mail\n",
      "2. international calling\n",
      "3. cost for the service\n",
      "4. customer usage\n",
      "5. customer churn indicator\n",
      "\n",
      "churn is defined as the customer cancelling their cellular plan at a given point in time.\n",
      "\n",
      "print(telco['Churn'].value_counts())\n",
      "\n",
      " > Sample using groupby\n",
      "print(telco.groupby(['Churn']).count())\n",
      "print(telco.groupby(['Churn']).std())\n",
      "print(telco.groupby('State')['Churn'].value_counts())\n",
      "\n",
      " >seaborn\n",
      "understand how your variables are distributed\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "sns.distplot(telco['Account_Length')\n",
      "\n",
      "sns.boxplot(x='Churn', y='Account_Length', data=telco,sym=\"\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#The bell curve means that the data is normally distributed. This means that the data\n",
      "can be simulated by random sampling to increase the accurracy of the prediction\n",
      "\n",
      "sns.boxplot(x=\"Churn\", y='Account_length',data=telco)\n",
      "plt.show()\n",
      "\n",
      "#The line in the middle represents the median\n",
      "#The colored boxes represent the middle 50% of each group\n",
      "\n",
      "#The floating points represent outliers\n",
      "sym=\"\" removes the outliers\n",
      "\n",
      "\n",
      "sns.boxplot(x='Churn', y='Account_Length', data=telco,sym=\"\", hue='StreamingMovies')\n",
      "plt.show()\n",
      "\n",
      " >Sample\n",
      "Day_Mins\n",
      "Eve_Mins\n",
      "Night_Mins\n",
      "Intl_Mins\n",
      "\n",
      "# Import matplotlib and seaborn\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Visualize the distribution of 'Day_Mins'\n",
      "\n",
      "sns.distplot(telco['Day_Mins'])\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#If the data was not normal distributed, you would apply a feature transformation\n",
      "\n",
      "#In such cases, the extreme values could be identified and removed in order to make the distribution more Gaussian. These extreme values are often called outliers\n",
      "\n",
      "#Taking the square root and the logarithm of the observation in order to make the distribution normal belongs to a class of transforms called power transforms.\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Import matplotlib and seaborn\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Create the box plot\n",
      "sns.boxplot(x = 'Churn',\n",
      "          y = 'CustServ_Calls',\n",
      "          data = telco)\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "        >Churn Prediction Fundamentals\n",
      "\n",
      "test decision trees and logistic regression models (compare the models)\n",
      "\n",
      "churn definition depends on company\n",
      "1. churn happens when a customer stops buying or engaging with the company\n",
      "2. The business context could be contractual or non-contractual\n",
      "3. Failing to update subscription can cause involuntary churn\n",
      "4. Contractual churn happens explicitly when customers decide to terminate the relationship\n",
      "5. Non contractual churn happens on online shopping or when the customer stops shopping\n",
      "\n",
      "\n",
      "Encoding churn\n",
      "1=Churn\n",
      "0=No churn\n",
      "Or it could be a string churn and no churn\n",
      "\n",
      "Increase accuracy with under sampling or over sampling techniques\n",
      "\n",
      "train,test = train_test_split(telcom, test_size=.25)\n",
      "\n",
      "separate the independant features and the target variable\n",
      "\n",
      "target==['Churn']\n",
      "custid=['CustomerId']\n",
      "\n",
      "cols=[col for col in telcom.columns if col not in custid+target]\n",
      "\n",
      "train_X = train[cols]\n",
      "train_Y = train[target]\n",
      "test_X = test[cols]\n",
      "test_Y = test[target]\n",
      "\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Print the unique Churn values\n",
      "print(set(telcom['Churn']))\n",
      "\n",
      "# Calculate the ratio size of each churn group\n",
      "telcom.groupby(['Churn']).size() / telcom.shape[0] * 100\n",
      "\n",
      "# Import the function for splitting data to train and test\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Split the data into train and test\n",
      "train, test = train_test_split(telcom, test_size = .25)\n",
      "\n",
      "\n",
      "# Store column names from `telcom` excluding target variable and customer ID\n",
      "cols = [col for col in telcom.columns if col not in custid + target]\n",
      "\n",
      "# Extract training features\n",
      "train_X = train[cols]\n",
      "\n",
      "# Extract training target\n",
      "train_Y = train[target]\n",
      "\n",
      "# Extract testing features\n",
      "test_X = test[cols]\n",
      "\n",
      "# Extract testing target\n",
      "test_Y = test[target]\n",
      "\n",
      "\n",
      "      Predicting with Logistic Regression\n",
      "\n",
      "1. Statistical classification model for binary responses\n",
      "2. Models log-odds of the probabilty of the target\n",
      "\n",
      "odds= is the probability of the odd occurring divided by the probabiity of the event not occurring\n",
      "\n",
      "p/1-p\n",
      "\n",
      "helps to find the decision boundary between the two coeffiencts but keeping the variables linearly relatived.\n",
      "\n",
      "Accuracy - the % of correctly predicted labels (both churn and non-churn)\n",
      "Precision - the % of total models positive class predictions (here - predicted as Churn) that wee correctly classified\n",
      "Recall - The % of total positive class samples (all churned customers) that were correctly classified\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy\n",
      "\n",
      "pred_train_Y=logreg.predict(train_X)\n",
      "pred_test_Y= logreg.predict(test_X)\n",
      "\n",
      "train_accuracy = accuracy_score(train_Y, pred_train_Y)\n",
      "test_accuracy=accuracy_score(test_Y, pred_test_Y)\n",
      "\n",
      "from sklearn.metrics import precision_score, recall_score\n",
      "\n",
      "train_precision = round(precision_score(train_Y. pred_train_Y,4)\n",
      "test_precision=round(precision_score(test_Y,pred_test_Y),4)\n",
      "\n",
      "\n",
      "  Regularization\n",
      "\n",
      "* Introduces penalty coefficient in the model building phase\n",
      "* Addresses over-fitting (when patterns are memorized by the model)\n",
      "\n",
      "-- the classifier does well at recalling the predictions on the training data but does not do well on the testing data\n",
      "\n",
      "L1 Regularization and feature selection\n",
      "-- reduces the number of features and makes the model more predictable\n",
      "\n",
      "L1 regularization called LASSO can be called explicitly, and this approach performs\n",
      "feature selection by shrinking some of the model coefficients to zero\n",
      "\n",
      "logreg = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
      "logreg.fit(train_X,train_Y)\n",
      "\n",
      "C=0 to 1\n",
      "\n",
      "C=[1,.5,.25,.1,.05,.25,.01,.005,.0025]\n",
      "\n",
      "l1_metrics=np.zeros(len(C),5))\n",
      "l1_metrics[:,0]=C\n",
      "\n",
      "for index in range(0, len(C)):\n",
      "\tlogreg = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
      "\tlogreg.fit(train_X,train_Y)\n",
      "\tpred_test_Y= logreg.predict(test_X)\n",
      "\n",
      "\tl1_metrics[index,1]=np.count_nonzero(logreg.coef_)\n",
      "\tl1_metrics[index,1]=accuracy_score(test_Y, pred_test_Y)\n",
      "\tl1_metrics[index,1]=precision_score(test_Y,pred_test_Y)\n",
      "\tl1_metrics[index,1]=recall_score(test_Y,pred_test_Y)\n",
      "col_names=['C','non-zero coeffs','accuracy','precision','recall']\n",
      "print(pd.DataFrame(l1_metrics, columns=col_names)\n",
      "\n",
      "we want a model that has reduced complexity but similar performance metrics\n",
      "\n",
      "Non-Zero coeffs are feature count\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Fit logistic regression on training data\n",
      "logreg.fit(train_X, train_Y)\n",
      "\n",
      "# Predict churn labels on testing data\n",
      "pred_test_Y = logreg.predict(test_X)\n",
      "\n",
      "# Calculate accuracy score on testing data\n",
      "test_accuracy = accuracy_score(test_Y, pred_test_Y)\n",
      "\n",
      "# Print test accuracy score rounded to 4 decimals\n",
      "print('Test accuracy:', round(test_accuracy, 4))\n",
      "\n",
      "  >Sample\n",
      "\n",
      "# Initialize logistic regression instance \n",
      "logreg = LogisticRegression(penalty='l1', C=0.025, solver='liblinear')\n",
      "\n",
      "# Fit the model on training data\n",
      "logreg.fit(train_X, train_Y)\n",
      "\n",
      "# Predict churn values on test data\n",
      "pred_test_Y = logreg.predict(test_X)\n",
      "\n",
      "# Print the accuracy score on test data\n",
      "print('Test accuracy:', round(accuracy_score(test_Y, pred_test_Y), 4))\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Run a for loop over the range of C list length\n",
      "for index in range(0, len(C)):\n",
      "  # Initialize and fit Logistic Regression with the C candidate\n",
      "  logreg = LogisticRegression(penalty='l1', C=C[index], solver='liblinear')\n",
      "  logreg.fit(train_X, train_Y)\n",
      "  # Predict churn on the testing data\n",
      "  pred_test_Y = logreg.predict(test_X)\n",
      "  # Create non-zero count and recall score columns\n",
      "  l1_metrics[index,1] = np.count_nonzero(logreg.coef_)\n",
      "  l1_metrics[index,2] = recall_score(test_Y, pred_test_Y)\n",
      "\n",
      "\n",
      "         >Decision Tree\n",
      "\n",
      "if else rules\n",
      "\n",
      "dt= DecisionTreeClassifier(max_depth=2, random_state=1)\n",
      "\n",
      "dt.fit(X_train, y_train)\n",
      "\n",
      "pred_test= dt.predict(X_test)\n",
      "pred_train= dt.predict(X_train)\n",
      "\n",
      "\n",
      "buffer=pd.Series(pred_test)\n",
      "buffer.value_counts().plot(kind='pie')\n",
      "plt.show()\n",
      "\n",
      "print(\"0 none churn 1 churn\")\n",
      "\n",
      "print(\"Training accuracy:\",round(accuracy_score(y_train,pred_train),4))\n",
      "print(\"Testing accuracy:\", round(accuracy_score(y_test, pred_test),4))\n",
      "\n",
      "\n",
      "depth_list=list(range(2,15))\n",
      "depth_tuning = np.zeros((len(depth_list),4))\n",
      "depth_tuning[:,0]=depth_list\n",
      "\n",
      "for index in range(len(depth_list)):\n",
      "    mytree=DecisionTreeClassifier(max_depth=depth_list[index])\n",
      "    mytree.fit(X_train,y_train)\n",
      "    pred_test_Y= mytree.predict(X_test)\n",
      "\n",
      "    depth_tuning[index,1]=accuracy_score(y_test,pred_test_Y)\n",
      "    depth_tuning[index,2]=precision_score(y_test,pred_test_Y)\n",
      "    depth_tuning[index,3]=recall_score(y_test,pred_test_Y)\n",
      "    \n",
      "col_names=['Max_Depth','Accuracy','Precision','Recall']\n",
      "print(pd.DataFrame(depth_tuning, columns=col_names))\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Initialize decision tree classifier\n",
      "mytree = tree.DecisionTreeClassifier()\n",
      "\n",
      "# Fit the decision tree on training data\n",
      "mytree.fit(train_X, train_Y)\n",
      "\n",
      "# Predict churn labels on testing data\n",
      "pred_test_Y = mytree.predict(test_X)\n",
      "\n",
      "# Calculate accuracy score on testing data\n",
      "test_accuracy = accuracy_score(test_Y, pred_test_Y)\n",
      "\n",
      "# Print test accuracy\n",
      "print('Test accuracy:', round(test_accuracy, 4))\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Run a for loop over the range of depth list length\n",
      "for index in range(0, len(depth_list)):\n",
      "  # Initialize and fit decision tree with the `max_depth` candidate\n",
      "  mytree = DecisionTreeClassifier(max_depth=depth_list[index])\n",
      "  mytree.fit(train_X, train_Y)\n",
      "  # Predict churn on the testing data\n",
      "  pred_test_Y = mytree.predict(test_X)\n",
      "  # Calculate the recall score \n",
      "  depth_tuning[index,1] = recall_score(test_Y, pred_test_Y)\n",
      "\n",
      "\n",
      " >Identifying insights into churn\n",
      "\n",
      "from sklearn import tree\n",
      "import graphviz\n",
      "\n",
      "\n",
      "exported=tree.export_graphviz(\n",
      "\tdecision_tree=mytree,\n",
      "\tout_file=None,\n",
      "\tfeature_names=cols,\n",
      "\tprecision=1,\n",
      "\tclass_names=['Not churn','Churn'],\n",
      "\tfilled=True)\n",
      "\n",
      "graph=graphviz.Source(exported)\n",
      "display(graph)\n",
      "\n",
      "\n",
      "<<<<<Logistic regression coefficients\n",
      "\n",
      "1. Logistic regression returns beta coefficients\n",
      "2. The coeffients can to be intrepretated as the log-odds of churn associated with 1 unit increase in the feature\n",
      "\n",
      "logb p/(1-p)\n",
      "\n",
      "log of odds is hard to intrepret\n",
      "\n",
      "logreg.coef_\n",
      "\n",
      "* calculate the exponent of the coefficients\n",
      "* This gives us the change in odds associated with 1 unit increase in the feature\n",
      "\n",
      "\n",
      "coefficients = pd.concat([pd.DataFrame(train_X.columns),\n",
      "pd.DataFrame(np.transpose(logit.coef_))],\n",
      "axis=1)\n",
      "\n",
      "coefficients.columns=['Feature','Coefficient']\n",
      "\n",
      "coefficients['Exp_Coefficients']=np.exp(coefficients['Coefficient'])\n",
      "coefficients=cefficients[coefficients['Coefficients]!=0]\n",
      "print(coefficients.sort_value(by=['Coefficient']))\n",
      "\n",
      "\n",
      "*values less than 1 decrease the odds\n",
      "*values greater than 1 increase the odds\n",
      "\n",
      "One additional year of tenure decrease churn odds by 60%\n",
      "\n",
      "\n",
      "   >Customer Lifetime Value basics (CLV)\n",
      "\n",
      "*CLV is the amount of money a company expect to earn in a lifetime\n",
      "\n",
      "Historical CLV = (revenues)*Profit Margin\n",
      "\n",
      "* Does not account for tenure, retention and churn rates\n",
      "\n",
      "* Does not account for new customers and their future revenue\n",
      "\n",
      "\n",
      "CLV = Average Revenue (for a certain period of time) * Profit Margin * Average Lifespan\n",
      "\n",
      "* lifespan is knowledge about its customers or the average lifespan of the customer churn.\n",
      "\n",
      "CLV (avg.revenue per purchase * avg.frequency* profit margin) * average lifespan\n",
      "\n",
      "* does not account for customer retention rates\n",
      "\n",
      "CLV = (Average Revenue * Profit Margin) * Retention Rate/Churn Rate\n",
      "\n",
      "churn= 1- retention\n",
      "\n",
      "cohort_sizes=cohort_counts.iloc[:,0]\n",
      "retention=cohorts_counts.divide(cohort_sizes,axis=0)\n",
      "churn=1-retention\n",
      "\n",
      "sns.heatmap(retention, annot=True, vmin=0, vmax=0.5, map=\"Y1Gn\")\n",
      "\n",
      "\n",
      "  Sample (calculate retention and churn)\n",
      "\n",
      "# Extract cohort sizes from the first column of cohort_counts\n",
      "cohort_sizes = cohort_counts.iloc[:,0]\n",
      "\n",
      "# Calculate retention by dividing the counts with the cohort sizes\n",
      "retention = cohort_counts.divide(cohort_sizes, axis=0)\n",
      "\n",
      "# Calculate churn\n",
      "churn = 1 - retention\n",
      "\n",
      "# Print the retention table\n",
      "print(churn)\n",
      "print(cohort_counts.shape)\n",
      "\n",
      "\n",
      "  Sample (calculate retention rate and churn rate)\n",
      "\n",
      "Now that you have calculated the monthly retention and churn metrics for monthly customer cohorts, you can calculate the overall mean retention and churn rates. You will use the .mean() method twice in a row (this is called \"chaining\") to calculate the overall mean\n",
      "\n",
      "# Calculate the mean retention rate\n",
      "retention_rate = retention.iloc[:,1:].mean().mean()\n",
      "\n",
      "# Calculate the mean churn rate\n",
      "churn_rate = churn.iloc[:,1:].mean().mean()\n",
      "\n",
      "# Print rounded retention and churn rates\n",
      "print('Retention rate: {:.2f}; Churn rate: {:.2f}'.format(retention_rate, churn_rate))\n",
      "\n",
      "    CLV\n",
      "1. goal clv measure customers in terms of revenue or profit\n",
      "2. benchmark customers\n",
      "3. identify maximum investment to gain customer acquistion\n",
      "\n",
      "CLV = Average Revenue * Retention Rate/churn rate\n",
      "\n",
      "\n",
      "      >Basic CLV\n",
      "\n",
      "1, Calculate monthly spent by the customer\n",
      "\n",
      "monthly_revenue = online.groupby(['CustomerID','InvoiceMonth'])\n",
      "['TotalSum'].sum().mean()\n",
      "\n",
      "monthly_revenue=np.mean(month_revenue)\n",
      "\n",
      "lifespan_months=36\n",
      "\n",
      "clv_basic=monthly_revenue * lifespn_months\n",
      "\n",
      "print('Average basic CLV is (:1f) USD'.format(clv_basic))\n",
      "\n",
      "       Granular CLV calculation\n",
      "\n",
      "revenue_per_purchase= online.groupby(['InvoiceNo']).['TotalSum'].mean().mean()\n",
      "\n",
      "##overall revenue for a purchase\n",
      "\n",
      "freq=online.groupby(['CustomerId','InvoicedMonth'])['InvoiceMonth'].nunique().mean()\n",
      "\n",
      "##calculate the average number of unique invoices per customer per month\n",
      "\n",
      "lifespan_months=36\n",
      "\n",
      "clv_granular= revenue_per_purchase * freq * lifespan_months\n",
      "\n",
      "print('Average granular CLV is (:,1f) USD'.format(clv_granular))\n",
      "\n",
      "print('Revenue per purchase is (:,1f) USD'.format(revenue_per_purchase)\n",
      "\n",
      "print('Frequency per month is (:,1f) USD'.format(freq)\n",
      "\n",
      "\n",
      "       Traditional CLV calculation\n",
      "\n",
      "monthly_revenue= online.groupby(['CustomerID','InvoiceMonth'])['TotalSum'].sum().mean()\n",
      "\n",
      "retention_rate=retention.iloc[:,1].mean().mean()\n",
      "\n",
      "churn_rate=1-retention_rate\n",
      "\n",
      "clv_traditional=month_revenue * (retention_rate/churn_rate)\n",
      "\n",
      "print('Average traditional clv is (:.1f) % retention_rate'.format(clv_traditional, retention_rate*100))\n",
      "\n",
      "   Which method to use\n",
      "\n",
      "1. depends on business model\n",
      "2.traditional clv model - assumes churn is definitive - customer dies.  The customer is assumed to not come back if they have churned once.\n",
      "3. traditional model is not robust at low retention values\n",
      "4. hardest thing to predict - frequency in the future\n",
      "\n",
      "\n",
      "  Sample (basic clv of 36 months)\n",
      "\n",
      "\n",
      "# Calculate monthly spend per customer\n",
      "monthly_revenue = online.groupby(['CustomerID','InvoiceMonth'])['TotalSum'].sum().mean()\n",
      "\n",
      "# Calculate average monthly spend\n",
      "monthly_revenue = np.mean(monthly_revenue)\n",
      "\n",
      "# Define lifespan to 36 months\n",
      "lifespan_months = 36\n",
      "\n",
      "# Calculate basic CLV\n",
      "clv_basic = monthly_revenue * lifespan_months\n",
      "\n",
      "# Print the basic CLV value\n",
      "print('Average basic CLV is {:.1f} USD'.format(clv_basic))\n",
      "\n",
      "\n",
      " >Sample (granular)\n",
      "\n",
      "# Calculate average revenue per invoice\n",
      "revenue_per_purchase = online.groupby(['InvoiceNo'])['TotalSum'].mean().mean()\n",
      "\n",
      "# Calculate average number of unique invoices per customer per month\n",
      "frequency_per_month = online.groupby(['CustomerID','InvoiceMonth'])['InvoiceNo'].nunique().mean()\n",
      "\n",
      "# Define lifespan to 36 months\n",
      "lifespan_months = 36\n",
      "\n",
      "# Calculate granular CLV\n",
      "clv_granular = revenue_per_purchase * frequency_per_month * lifespan_months\n",
      "\n",
      "# Print granular CLV value\n",
      "print('Average granular CLV is {:.1f} USD'.format(clv_granular))\n",
      "\n",
      " >Sample (traditional)\n",
      "\n",
      "#Now you will calculate one of the most popular descriptive CLV models that accounts for the retention and churn rates. This gives a more robust estimate, but comes with certain assumptions that have to be validated\n",
      "\n",
      "# Calculate monthly spend per customer\n",
      "monthly_revenue = online.groupby(['CustomerID','InvoiceMonth'])['TotalSum'].sum().mean()\n",
      "\n",
      "# Calculate average monthly retention rate\n",
      "retention_rate = retention.iloc[:,1:].mean().mean()\n",
      "\n",
      "# Calculate average monthly churn rate\n",
      "churn_rate = 1 - retention_rate\n",
      "\n",
      "# Calculate traditional CLV \n",
      "clv_traditional = monthly_revenue * (retention_rate / churn_rate)\n",
      "\n",
      "# Print traditional CLV and the retention rate values\n",
      "print('Average traditional CLV is {:.1f} USD at {:.1f} % retention_rate'.format(clv_traditional, retention_rate*100))\n",
      "\n",
      "#As you can see, the traditional CLV formula yields a much lower estimate as it accounts for monthly retention which is quite low for this company.\n",
      "\n",
      "\n",
      "         Data preparation for purchase prediction\n",
      "\n",
      "* regression to predict purchasing\n",
      "* simplest model is linear regression\n",
      "* target variable is either continous or count\n",
      "\n",
      "* count data (number of active days) work better with poisson or negative binomal regression\n",
      "\n",
      "RFM - recency, frequency, or monetary features\n",
      "\n",
      "explore the sales distribution by month\n",
      "\n",
      "online.groupby(['InvoiceMonth']).size()\n",
      "\n",
      "#prints out the number of observations per month\n",
      "\n",
      "online_X=online[online['InvoiceMonth']='2011-11']\n",
      "\n",
      "#calculate the recency\n",
      "\n",
      "NOW= dt.datetime(2011,11,1)\n",
      "\n",
      "features = online_X.groupby('CustomerID').agg({\n",
      "\t'InvoiceDate': lambda x(NOW-x.max())days,\n",
      "\t'InvoiceMo': pd.Series.nunique,\n",
      "\t'TotalSum': np.sum,\n",
      "\t'Quantity': ['mean','sum']\n",
      "\n",
      "}).reset_index()\n",
      "\n",
      "features.columns=['CustomerID','recency','frequency','monetary','quantity_avg','quantity_total']\n",
      "\n",
      "#recency is the now date - the lastest invoice date\n",
      "#frequency by counting the unique number of invoice\n",
      "#sum the revenue for that customer\n",
      "#calculate the quantity and sum of the quantities\n",
      "#reindex makes sure the columns are not stored as an index for use later\n",
      "\n",
      "  Calculate the target variable\n",
      "\n",
      "#build a pivot table\n",
      "\n",
      "cust_month_tx= pd.pivot_table(data=online, index=['CustomerID'],\n",
      "\tvalues='InvoiceNo',\n",
      "\tcolumns=['InvoiceMonth'],\n",
      "\taggfunc=pd.Series.nunique, fill_value=0)\n",
      "\n",
      "print(cust_month_tx.head())\n",
      "\n",
      "#the result is a matrix of unique invoices per month by customer ID\n",
      "\n",
      "#use the last month of data\n",
      "\n",
      "#store the identifier and the target variable as separate list\n",
      "\n",
      "custid=['CustomerID']\n",
      "target=['2011-11']\n",
      "\n",
      "Y=cust_month_tx[target]\n",
      "\n",
      "cols=[col for col in features.columns if col not in custid]\n",
      "\n",
      "X=featurs(cols)\n",
      "\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "train_X,test_X,train_Y,test_Y= train_test_split(X,Y,\n",
      "\ttest_size=0.25, random_state=99)\n",
      "\n",
      "\n",
      "print(train_X.shape, train_Y.shape, test_\n",
      "X.shape, test_Y.shape)\n",
      "\n",
      "  >Sample (building features of Recency, Frequency, and Monetary)\n",
      "\n",
      "\n",
      "# Define the snapshot date\n",
      "NOW = dt.datetime(2011,11,1)\n",
      "\n",
      "# Calculate recency by subtracting current date from the latest InvoiceDate\n",
      "features = online_X.groupby('CustomerID').agg({\n",
      "  'InvoiceDate': lambda x: (NOW - x.max()).days,\n",
      "  # Calculate frequency by counting unique number of invoices\n",
      "  'InvoiceNo': pd.Series.nunique,\n",
      "  # Calculate monetary value by summing all spend values\n",
      "  'TotalSum': np.sum,\n",
      "  # Calculate average and total quantity\n",
      "  'Quantity': ['mean', 'sum']}).reset_index()\n",
      "\n",
      "# Rename the columns\n",
      "features.columns = ['CustomerID', 'recency', 'frequency', 'monetary', 'quantity_avg', 'quantity_total']\n",
      "\n",
      "\n",
      "# Build a pivot table counting invoices for each customer monthly\n",
      "cust_month_tx = pd.pivot_table(data=online, values='InvoiceNo',\n",
      "                               index=['CustomerID'], columns=['InvoiceMonth'],\n",
      "                               aggfunc=pd.Series.nunique, fill_value=0)\n",
      "\n",
      "# Store November 2011 data column name as a list\n",
      "target = ['2011-11']\n",
      "\n",
      "# Store target value as `Y`\n",
      "Y = cust_month_tx[target]\n",
      "\n",
      "# Store customer identifier column name as a list\n",
      "custid = ['CustomerID']\n",
      "\n",
      "# Select feature column names excluding customer identifier\n",
      "cols = [col for col in features.columns if col not in custid]\n",
      "\n",
      "# Extract the features as `X`\n",
      "X = features[cols]\n",
      "\n",
      "# Split data to training and testing\n",
      "Train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.25, random_state=99)\n",
      "\n",
      "\n",
      "   Predicting next months transactions\n",
      "\n",
      "Use linear regression to predict next months transactions\n",
      "initializing the model\n",
      "fit and predict\n",
      "measure\n",
      "\n",
      "\n",
      "root mean squared error (RMSE) - Square root of the average squared differences between prediction and actuals\n",
      "a. subtract the predicted and actuals\n",
      "b. square the results\n",
      "c. calculate the average\n",
      "d. take the square root to get a normalized measurement\n",
      "\n",
      "\n",
      "    Mean absolute error (MAE)\n",
      "mean absolute error - Average absolute difference between the predicted and actuals\n",
      "\n",
      "    Mean absolute percentage error (MAPE)\n",
      "average percentage difference between prediction and actuals\n",
      "normalized between 0 and 100 percent (actuals can't be zero)\n",
      "\n",
      "R-squared: statistical measure that represents the percentage proportion of variance that is explained by the model.  \n",
      "applies only to regression\n",
      "(Higer is better)\n",
      "\n",
      "coefficient p-values - probability that the regression coefficient is observed due to chance.  (lower is better)\n",
      "threshholds are 5% to 10%  (measures the significance of the null hypothesis)\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "linreg=LinearRegression()\n",
      "\n",
      "linreq.fit(train_X,train_Y)\n",
      "\n",
      "train_pred_Y= linreq.predict(train_X)\n",
      "test_pred_Y = linreq.predict(test_X)\n",
      "\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "rmse_train=np.sqrt(mean_squared_error(train_Y,train_pred_Y))\n",
      "mae_train=mean_absolute_error(train_Y,train_pred_Y)\n",
      "\n",
      "rmse_test=np.sqrt(mean_squared_error(test_Y,test_pred_Y))\n",
      "mae_test=mean_absolute_error(test_Y,test_pred_Y)\n",
      "\n",
      "print('RMSE train: (:3f): RMSE test: (:3f)\\nMAE train :{:3f}, MAE test: {:3f}'.format(rmse_train,rmse_test, mae_train, mae_test))\n",
      "\n",
      "\n",
      " >Interpreting the coefficients\n",
      "\n",
      "1. statistical significance - standard statistical significant is 95%\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "train_Y=np.array(train_Y)\n",
      "\n",
      "#Ordinary Least Square Model (curve fitting algorithm)\n",
      "\n",
      "olsreg = sm.OLS(train_Y, train_X)\n",
      "olsreg=olsreg.fit()\n",
      "\n",
      "print(olsreg.summary())\n",
      "\n",
      "#R-squared is the percentage of explained variance.  What percentage does the model explain of the variation? (higher is better)\n",
      "\n",
      "check the P-value coefficients\n",
      "(change in the output variable if one unit changed in the feature).  Some of the coeffiencts are not statistically significant. 1-significance = 100-95% or 5%  (look features for p values less than 5%)\n",
      "\n",
      "\n",
      "Sample \n",
      "\n",
      "# Initialize linear regression instance\n",
      "linreg = LinearRegression()\n",
      "\n",
      "# Fit the model to training dataset\n",
      "linreg.fit(train_X, train_Y)\n",
      "\n",
      "# Predict the target variable for training data\n",
      "train_pred_Y = linreg.predict(train_X)\n",
      "\n",
      "# Predict the target variable for testing data\n",
      "test_pred_Y = linreg.predict(test_X)\n",
      "\n",
      "\n",
      "#This is a critical step where you are measuring how \"close\" are the model predictions compared to actual values.\n",
      "\n",
      "# Calculate root mean squared error on training data\n",
      "rmse_train = np.sqrt(mean_squared_error(train_Y, train_pred_Y))\n",
      "\n",
      "# Calculate mean absolute error on training data\n",
      "mae_train = mean_absolute_error(train_Y, train_pred_Y)\n",
      "\n",
      "# Calculate root mean squared error on testing data\n",
      "rmse_test = np.sqrt(mean_squared_error(test_Y, test_pred_Y))\n",
      "\n",
      "# Calculate mean absolute error on testing data\n",
      "mae_test = mean_absolute_error(test_Y, test_pred_Y)\n",
      "\n",
      "# Print the performance metrics\n",
      "print('RMSE train: {}; RMSE test: {}\\nMAE train: {}, MAE test: {}'.format(rmse_train, rmse_test, mae_train, mae_test))\n",
      "\n",
      "\n",
      " >Sample OLS \n",
      "\n",
      "OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.488\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.487\n",
      "Method:                 Least Squares   F-statistic:                              480.3\n",
      "Date:                Mon, 07 Sep 2020   Prob (F-statistic):                        0.00\n",
      "Time:                        22:03:37   Log-Likelihood:                         -2769.8\n",
      "No. Observations:                2529   AIC:                                      5550.\n",
      "Df Residuals:                    2524   BIC:                                      5579.\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "recency            0.0002      0.000      1.701      0.089   -2.92e-05       0.000\n",
      "frequency          0.1316      0.003     38.000      0.000       0.125       0.138\n",
      "monetary        1.001e-06   3.59e-05      0.028      0.978   -6.95e-05    7.15e-05\n",
      "quantity_avg       0.0001      0.000      0.803      0.422      -0.000       0.000\n",
      "quantity_total    -0.0001   5.74e-05     -2.562      0.010      -0.000   -3.45e-05\n",
      "==============================================================================\n",
      "Omnibus:                      987.494   Durbin-Watson:                   1.978\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5536.657\n",
      "Skew:                           1.762   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.334   Cond. No.                         249.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "In [2]: \n",
      "\n",
      "\n",
      "  Customer and product segmentation on basics\n",
      "\n",
      "wholesale.head()\n",
      "\n",
      "1. Fresh\n",
      "2. Milk\n",
      "3. Grocery\n",
      "4. Frozen\n",
      "5. Detergents_Paper\n",
      "6. Delicassens\n",
      "\n",
      "Unsupervised learning models\n",
      "\n",
      "* k-means\n",
      "* non-negative matrix factorization nmf\n",
      "\n",
      "1. initialize the model\n",
      "2. fit the model\n",
      "3. assign cluster values\n",
      "\n",
      "wholesale.agg(['mean','std']).round(0)\n",
      "\n",
      "averages= wholesale.mean()\n",
      "st_dev = wholesale.std()\n",
      "x_names=wholesale.columns\n",
      "x_ix= np.arange(wholesale.shape[1])\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.bar(x_ix-0.2, averages, color='grey', label='Average', width=0.4)\n",
      "plt.bar(x_ix+0.2, std_dev, color='orange',' label='Standard Deviation', width=0.4)\n",
      "plt.xticks(x_ix, x_names, rotation=90)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "sns.pairplot(wholesale,diag_kind='kde')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      " >Sample (pairplot)\n",
      "\n",
      "# Print the header of the `wholesale` dataset\n",
      "print(wholesale.head())\n",
      "\n",
      "# Plot the pairwise relationships between the variables\n",
      "sns.pairplot(wholesale, diag_kind='kde')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "  Sample (bar plot average and standard deviation)\n",
      "\n",
      "# Create column names list and same length integer list\n",
      "x_names = wholesale.columns\n",
      "x_ix = np.arange(wholesale.shape[1])\n",
      "\n",
      "# Plot the averages data in gray and standard deviations in orange \n",
      "plt.bar(x=x_ix-0.2, height=averages, color='grey', label='Average', width=0.4)\n",
      "plt.bar(x=x_ix+0.2, height=std_devs, color='orange', label='Standard Deviation', width=0.4)\n",
      "\n",
      "# Add x-axis labels and rotate\n",
      "plt.xticks(ticks=x_ix, labels=x_names, rotation=90)\n",
      "\n",
      "# Add the legend and display the chart\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  >Data preparation for segmentation\n",
      "\n",
      "1. start with k-means\n",
      "2. k-means works well when the data is normally distributed\n",
      "a. mean=0\n",
      "b. standard deviation=1\n",
      "\n",
      "Non-negative matrix factorization works well with on draw sparse matrices\n",
      "\n",
      "wholesale_log = np.log(wholesale)\n",
      "\n",
      "sns.pairplot(wholesale_log, diag_kind='kde')\n",
      "plt.show()\n",
      "\n",
      "#result in less skewed data\n",
      "\n",
      " >Box-cox transformation\n",
      "\n",
      "\n",
      "from scipy import stats\n",
      "\n",
      "def boxcox_df(x):\n",
      "\tx_boxcox, _ = stats.boxcox(x)\n",
      "\treturn x_boxcox\n",
      "\n",
      "wholesale_boxcox = wholesale.apply(boxcox_df,axis, 0)\n",
      "\n",
      "sns.pairplot(wholesale_boxcox, diag_kind='kde')\n",
      "plt.show()\n",
      "\n",
      "   Scale the data\n",
      "1. Subtract column average from each column value\n",
      "2. Divide each column value by column standard deviation\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler=StandardScaler()\n",
      "\n",
      "scaler.fit(wholesale_boxcox)\n",
      "\n",
      "#numpy array\n",
      "wholesale_scaled= scaler.transform(wholesale_box)\n",
      "\n",
      "wholesale_scaled_df=pd.DataFrame(data=whosale_scaled,\n",
      "\tindex=wholesale_boxcox.index,\n",
      "\tcolumns=wholesale_boxcox.columns)\n",
      "\n",
      "wholesale_scaled_df.agg(['mean','[std']).round()\n",
      "\n",
      " >Sample sns pairplot\n",
      "\n",
      "# Define custom Box Cox transformation function\n",
      "def boxcox_df(x):\n",
      "    x_boxcox, _ = stats.boxcox(x)\n",
      "    return x_boxcox\n",
      "\n",
      "# Apply the function to the `wholesale` dataset\n",
      "wholesale_boxcox = wholesale.apply(boxcox_df, axis=0)\n",
      "\n",
      "# Plot the pairwise relationships between the transformed variables \n",
      "sns.pairplot(wholesale_boxcox, diag_kind='kde')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample (Scaling)\n",
      "\n",
      "# Fit the initialized `scaler` instance on the Box-Cox transformed dataset\n",
      "scaler.fit(wholesale_boxcox)\n",
      "\n",
      "# Transform and store the scaled dataset as `wholesale_scaled`\n",
      "wholesale_scaled = scaler.transform(wholesale_boxcox)\n",
      "\n",
      "# Create a `pandas` DataFrame from the scaled dataset\n",
      "wholesale_scaled_df = pd.DataFrame(data=wholesale_scaled,\n",
      "                                       index=wholesale_boxcox.index,\n",
      "                                       columns=wholesale_boxcox.columns)\n",
      "\n",
      "# Print the mean and standard deviation for all columns\n",
      "print(wholesale_scaled_df.agg(['mean','std']).round())\n",
      "\n",
      "  >Kmeans\n",
      "\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans= KMeans(n_cluster=k)\n",
      "\n",
      "kmeans.fit(wholesale_scaled_df)\n",
      "\n",
      "\n",
      "#Use the original df not the scaled one\n",
      "wholesale_kmeans4 = wholesale.assign(segment=kmeans.labels_)\n",
      "\n",
      "\n",
      " >NMF\n",
      "\n",
      "from sklearn.decomposition import NMF\n",
      "\n",
      "nmf=NMF(k)\n",
      "nmf.fit(wholesale)\n",
      "\n",
      "components=pd.DataFrame(nmf.components_, columns=wholesale.columns)\n",
      "\n",
      "segment_weights= pd.DataFrame(nmf.transform(wholesale, columns=component.index)\n",
      "\n",
      "segment_weights.index=wholesale.index\n",
      "\n",
      "wholesale_nmf= wholesale.assign(segment=segment_weights.idxmax(axis=1))\n",
      "\n",
      "#new column - which cluster weight is largest for each customer\n",
      "\n",
      "  Defining k\n",
      "elbow criterion method to get the optimal number of k clusters\n",
      "a. iterate through a number of k values\n",
      "b. running cluster for each on the same data\n",
      "c. calculate sum of squared errors (se) for each\n",
      "d. plot the sse against k and identify the elbow of diminishing incremental improvements\n",
      "\n",
      "  Samples (NMF heatmap)\n",
      "\n",
      "# Create the W matrix\n",
      "W = pd.DataFrame(data=nmf.transform(wholesale), columns=components.index)\n",
      "W.index = wholesale.index\n",
      "\n",
      "# Assign the column name where the corresponding value is the largest\n",
      "wholesale_nmf3 = wholesale.assign(segment = W.idxmax(axis=1))\n",
      "\n",
      "# Calculate the average column values per each segment\n",
      "nmf3_averages = wholesale_nmf3.groupby('segment').mean().round(0)\n",
      "\n",
      "# Plot the average values as heatmap\n",
      "sns.heatmap(nmf3_averages.T, cmap='YlGnBu')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "sse={}\n",
      "\n",
      "for k in range(1,11):\n",
      "\tkmeans=KMeans(n_clusters=k, random_state=333)\n",
      "\tkmeans.fit(wholesale_scaled_df)\n",
      "\tsse(k)=kmeans.inertia_\n",
      "\n",
      "\n",
      "plt.title('Elbow criterion method chart')\n",
      "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
      "plt.show()\n",
      "\n",
      "build meanful segmentation\n",
      "can you give the segmentation a name given the clustering.\n",
      "\n",
      " >Sample (elbow)\n",
      "\n",
      "# Create empty sse dictionary\n",
      "sse = {}\n",
      "\n",
      "# Fit KMeans algorithm on k values between 1 and 11\n",
      "for k in range(1, 11):\n",
      "    kmeans = KMeans(n_clusters=k, random_state=333)\n",
      "    kmeans.fit(wholesale_scaled_df)\n",
      "    sse[k] = kmeans.inertia_\n",
      "\n",
      "# Add the title to the plot\n",
      "plt.title('Elbow criterion method chart')\n",
      "\n",
      "# Create and display a scatter plot\n",
      "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample (KMeans)\n",
      "\n",
      "# Import `KMeans` module\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Initialize `KMeans` with 4 clusters\n",
      "kmeans=KMeans(n_clusters=4, random_state=123)\n",
      "\n",
      "# Fit the model on the pre-processed dataset\n",
      "kmeans.fit(wholesale_scaled_df)\n",
      "\n",
      "# Assign the generated labels to a new column\n",
      "wholesale_kmeans4 = wholesale.assign(segment = kmeans.labels_)\n",
      "\n",
      "  Sample (NMF)\n",
      "\n",
      "# Import the non-negative matrix factorization module\n",
      "from sklearn.decomposition import NMF\n",
      "\n",
      "# Initialize NMF instance with 4 components\n",
      "nmf = NMF(4)\n",
      "\n",
      "# Fit the model on the wholesale sales data\n",
      "nmf.fit(wholesale)\n",
      "\n",
      "# Extract the components \n",
      "components = pd.DataFrame(data=nmf.components_, columns=wholesale.columns)\n",
      "\n",
      "       >Visualize and interpret segmentation solutions\n",
      "1. Calculate average/median/other percentile values for each variable by segment\n",
      "2. Calculate relative importance for each variable by segment\n",
      "3. Visualize using a heatmap\n",
      "\n",
      "\n",
      "\n",
      "kmeans4_averages= wholesale_kmeans4.groupby(['segment']).mean().round(0)\n",
      "\n",
      "print(kmeans4_averages)\n",
      "\n",
      "The four segments have different average values for fresh, milk, grocery, frozen, detergents_paper, delicassen\n",
      "\n",
      "sns.heatmap(kmeans4_averages.T, cmap='Y1GnBu')\n",
      "plt.show()\n",
      "\n",
      " >Plot average NMF segmentation attributes\n",
      "\n",
      "nmf4_averages=wholesale_nmf4.groupby('segment').mean().round(0)\n",
      "sns.heatmap(nmf4_averages.T, cmap='Y1GnBu')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample (heatmap kmeans clusters)\n",
      "# Group by the segment label and calculate average column values\n",
      "kmeans3_averages= wholesale_kmeans3.groupby(['segment']).mean().round(0)\n",
      "\n",
      "# Print the average column values per each segment\n",
      "print(kmeans3_averages)\n",
      "\n",
      "# Create a heatmap on the average column values per each segment\n",
      "sns.heatmap(kmeans3_averages.T, cmap='YlGnBu')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<<<<<<<<  Predicting Customer Churn\n",
      "    One hot encoding\n",
      "\n",
      "1. numeric one for a category in a column\n",
      "\n",
      "print(telco.dtypes) to find the objects to encode\n",
      "\n",
      "    Standardization\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "df=StandardScaler().fit_transform(df)\n",
      "\n",
      "  Sample (encoding)\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Vmail_Plan'\n",
      "telco['Vmail_Plan'] = telco['Vmail_Plan'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Churn'\n",
      "telco['Churn'] = telco['Churn'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Print the results to verify\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Perform one hot encoding on 'State'\n",
      "telco_state = pd.get_dummies(telco['State'])\n",
      "\n",
      "print(telco_state)\n",
      "\n",
      "\n",
      " >Sample (Scaler)\n",
      "\n",
      "# Import StandardScaler\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Scale telco using StandardScaler\n",
      "telco_scaled = StandardScaler().fit_transform(telco)\n",
      "\n",
      "# Add column names back for readability\n",
      "telco_scaled_df = pd.DataFrame(telco_scaled, columns=[\"Intl_Calls\", \"Night_Mins\"])\n",
      "\n",
      "# Print summary statistics\n",
      "print(telco_scaled_df.describe())\n",
      "\n",
      "\n",
      "       Feature selection\n",
      "\n",
      "Unique identifiers that need to be dropped\n",
      "1. phone numbers\n",
      "2. customerid\n",
      "3. account numbers\n",
      "\n",
      "telco.drop(['Soc_Sec'], axis=1)\n",
      "\n",
      "Features that are highly correlated to features can be dropped because they offer no additional information to the model.\n",
      "\n",
      "telco.corr()\n",
      "\n",
      "remove features that are highly correlated\n",
      "\n",
      "should consult with business and subject matter experts\n",
      "\n",
      "A new feature could be\n",
      "\n",
      "Total_Minutes = Day_Mins+Eve_Mins+Night_Mins+Intl_Mins\n",
      "\n",
      "understanding the ratio of minutes and charge\n",
      "\n",
      "telco['Day_Cost']=telco['Day_Mins']/telco['Day_Charge']\n",
      "\n",
      "\n",
      " >Sample dropping columns\n",
      "\n",
      "# Drop the unnecessary features\n",
      "telco = telco.drop(['Area_Code','Phone'],axis=1)\n",
      "\n",
      "  Sample (new features)\n",
      "\n",
      "# Create the new feature\n",
      "telco['Avg_Night_Calls'] = telco['Night_Mins']/telco['Night_Calls']\n",
      "\n",
      "# Print the first five rows of 'Avg_Night_Calls'\n",
      "print(telco['Avg_Night_Calls'])\n",
      "\n",
      "      >Making predictions\n",
      "\n",
      "Logistic Regression: Good baseline offering simplicity and interpretability.\n",
      "\n",
      "Can not capture more complex relationships in the dataset\n",
      "\n",
      "Random forest\n",
      "\n",
      "Support vector machines\n",
      "\n",
      "  >Suport Vector Machine\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "\n",
      "svc.fit(telco[features], telco['target'])\n",
      "\n",
      "1. features must be contineous values\n",
      "2. dataframes or numpy arrays\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "decision_function_shape='ovr', degree=3, gamma='auto',\n",
      "kernel='rbf', max_iter=-1, probability=False,\n",
      "random_state=None, shrinking=True, tol=0.001,\n",
      "verbose=False)\n",
      "\n",
      "prediction = svc.predict(new_customer)\n",
      "print(prediction)\n",
      "\n",
      "\n",
      " >Sample (Logistic Regression)\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "clf = LogisticRegression()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "New customer\n",
      "\n",
      "Account_Length  Vmail_Message  Day_Mins  Eve_Mins  Night_Mins  ...  Eve_Charge  Night_Calls  Night_Charge  Intl_Calls  Intl_Charge\n",
      "0              91             23     232.4     186.0       190.5  ...       15.81          128          8.57           3         3.32\n",
      "\n",
      "\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "  >Sample (Decision Tree Classifier)\n",
      "\n",
      "# Import DecisionTreeClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "\n",
      "clf=DecisionTreeClassifier()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "# Predict the label of new_customer\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "       evaluate model performance\n",
      "1. Compute its accuracy\n",
      "2. Accuracy = Correct Predictions/total number of data points\n",
      "training data may not represent actual data\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(telco['data'],\n",
      "\ttelco['target'],\n",
      "\ttest_size=0.2,\n",
      "\trandom_state=42)\n",
      "\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "svc.fit(X_train, y_train)\n",
      "svc.predict(X_test)\n",
      "\n",
      "svc.score(X_test,y_test)\n",
      "\n",
      "#overfitting means the model has become to sensitive to noise in the training data\n",
      "\n",
      "#underfitting is means not capturing trends in the training data\n",
      "\n",
      "  Sample - Train Test Split\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(X,\n",
      "\ty,\n",
      "\ttest_size=0.3,\n",
      "\trandom_state=42)\n",
      "\n",
      "print(X_train.shape, X_test.shape)\n",
      "\n",
      " >Sample - (Random Forest Classifier)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "# Compute accuracy\n",
      "print(clf.score(X_test, y_test))\n",
      "\n",
      "        >Model Metrics\n",
      "\n",
      "\n",
      "<<<<<<<<  Predicting Customer Churn\n",
      "    One hot encoding\n",
      "\n",
      "1. numeric one for a category in a column\n",
      "\n",
      "print(telco.dtypes) to find the objects to encode\n",
      "\n",
      "    Standardization\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "df=StandardScaler().fit_transform(df)\n",
      "\n",
      "  Sample (encoding)\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Vmail_Plan'\n",
      "telco['Vmail_Plan'] = telco['Vmail_Plan'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Churn'\n",
      "telco['Churn'] = telco['Churn'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Print the results to verify\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Perform one hot encoding on 'State'\n",
      "telco_state = pd.get_dummies(telco['State'])\n",
      "\n",
      "print(telco_state)\n",
      "\n",
      "\n",
      " >Sample (Scaler)\n",
      "\n",
      "# Import StandardScaler\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Scale telco using StandardScaler\n",
      "telco_scaled = StandardScaler().fit_transform(telco)\n",
      "\n",
      "# Add column names back for readability\n",
      "telco_scaled_df = pd.DataFrame(telco_scaled, columns=[\"Intl_Calls\", \"Night_Mins\"])\n",
      "\n",
      "# Print summary statistics\n",
      "print(telco_scaled_df.describe())\n",
      "\n",
      "\n",
      "       Feature selection\n",
      "\n",
      "Unique identifiers that need to be dropped\n",
      "1. phone numbers\n",
      "2. customerid\n",
      "3. account numbers\n",
      "\n",
      "telco.drop(['Soc_Sec'], axis=1)\n",
      "\n",
      "Features that are highly correlated to features can be dropped because they offer no additional information to the model.\n",
      "\n",
      "telco.corr()\n",
      "\n",
      "remove features that are highly correlated\n",
      "\n",
      "should consult with business and subject matter experts\n",
      "\n",
      "A new feature could be\n",
      "\n",
      "Total_Minutes = Day_Mins+Eve_Mins+Night_Mins+Intl_Mins\n",
      "\n",
      "understanding the ratio of minutes and charge\n",
      "\n",
      "telco['Day_Cost']=telco['Day_Mins']/telco['Day_Charge']\n",
      "\n",
      "\n",
      " >Sample dropping columns\n",
      "\n",
      "# Drop the unnecessary features\n",
      "telco = telco.drop(['Area_Code','Phone'],axis=1)\n",
      "\n",
      "  Sample (new features)\n",
      "\n",
      "# Create the new feature\n",
      "telco['Avg_Night_Calls'] = telco['Night_Mins']/telco['Night_Calls']\n",
      "\n",
      "# Print the first five rows of 'Avg_Night_Calls'\n",
      "print(telco['Avg_Night_Calls'])\n",
      "\n",
      "      >Making predictions\n",
      "\n",
      "Logistic Regression: Good baseline offering simplicity and interpretability.\n",
      "\n",
      "Can not capture more complex relationships in the dataset\n",
      "\n",
      "Random forest\n",
      "\n",
      "Support vector machines\n",
      "\n",
      "  >Suport Vector Machine\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "\n",
      "svc.fit(telco[features], telco['target'])\n",
      "\n",
      "1. features must be contineous values\n",
      "2. dataframes or numpy arrays\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "decision_function_shape='ovr', degree=3, gamma='auto',\n",
      "kernel='rbf', max_iter=-1, probability=False,\n",
      "random_state=None, shrinking=True, tol=0.001,\n",
      "verbose=False)\n",
      "\n",
      "prediction = svc.predict(new_customer)\n",
      "print(prediction)\n",
      "\n",
      "\n",
      " >Sample (Logistic Regression)\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "clf = LogisticRegression()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "New customer\n",
      "\n",
      "Account_Length  Vmail_Message  Day_Mins  Eve_Mins  Night_Mins  ...  Eve_Charge  Night_Calls  Night_Charge  Intl_Calls  Intl_Charge\n",
      "0              91             23     232.4     186.0       190.5  ...       15.81          128          8.57           3         3.32\n",
      "\n",
      "\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "  >Sample (Decision Tree Classifier)\n",
      "\n",
      "# Import DecisionTreeClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "\n",
      "clf=DecisionTreeClassifier()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "# Predict the label of new_customer\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "       evaluate model performance\n",
      "1. Compute its accuracy\n",
      "2. Accuracy = Correct Predictions/total number of data points\n",
      "training data may not represent actual data\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(telco['data'],\n",
      "\ttelco['target'],\n",
      "\ttest_size=0.2,\n",
      "\trandom_state=42)\n",
      "\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "svc.fit(X_train, y_train)\n",
      "svc.predict(X_test)\n",
      "\n",
      "svc.score(X_test,y_test)\n",
      "\n",
      "#overfitting means the model has become to sensitive to noise in the training data\n",
      "\n",
      "#underfitting is means not capturing trends in the training data\n",
      "\n",
      "  Sample - Train Test Split\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(X,\n",
      "\ty,\n",
      "\ttest_size=0.3,\n",
      "\trandom_state=42)\n",
      "\n",
      "print(X_train.shape, X_test.shape)\n",
      "\n",
      " >Sample - (Random Forest Classifier)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "# Compute accuracy\n",
      "print(clf.score(X_test, y_test))\n",
      "\n",
      "        >Model Metrics\n",
      "\n",
      "imbalanced classes\n",
      "\n",
      "telco['Churn'].value_counts()\n",
      "\n",
      "up balancing and down balancing\n",
      "\n",
      "confusion matrix\n",
      "\n",
      "\n",
      "          Churn   \t\tNo Churn\n",
      "\n",
      "Churn\tTrue positive\t\tFalse positive\n",
      "\n",
      "No Churn false Negatives\tTrue Negatives\n",
      "\n",
      "\n",
      "Precision = true positives/(true positives+false positives)\n",
      "\n",
      "* high precision means there are not many false positives\n",
      "\n",
      "Recall sensitivity = true positives/(true positives+false negatives)\n",
      "* a high recall means it correctly recalled most churners\n",
      "* minimizes false negatives\n",
      "\n",
      "high precision when the offer is expensive\n",
      "\n",
      "high recall if losing customers is expensive\n",
      "\n",
      "\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "cm=confusion_matrix(y_test,y_pred)\n",
      "\n",
      " >Sample (confusion Matrix)\n",
      "\n",
      "# Import confusion_matrix\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "# Print the confusion matrix\n",
      "print(confusion_matrix(y_test,y_pred)\n",
      "\n",
      ")\n",
      "\n",
      "\n",
      "  Sample (confusion Matrix)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2))\n",
      "\n",
      " >Sample (confusion matrix - RandomForestClassifier)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import confusion_matrix\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "# Print confusion matrix\n",
      "print(confusion_matrix(y_test,y_pred))\n",
      "\n",
      " >Sample (score_precision)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import precision_score\n",
      "from sklearn.metrics import precision_score\n",
      "\n",
      "print(precision_score(y_test,y_pred))\n",
      "\n",
      "  >Sample (Precision and recall)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import precision_score\n",
      "from sklearn.metrics import precision_score, recall_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "# Print the precision\n",
      "print(precision_score(y_test,y_pred))\n",
      "\n",
      "print(recall_score(y_test,y_pred))\n",
      "\n",
      "print(confusion_matrix(y_test,y_pred))\n",
      "\n",
      "<<<<<<Other model metrics\n",
      "\n",
      "receiving operating curve ROC \n",
      "\n",
      "every prediction your classifier makes has an associated probability.\n",
      "\n",
      "> 50% belongs to the positive class\n",
      "\n",
      "* default probability threshold in scikit-learn is 50%\n",
      "\n",
      "measuring the true positive rate against the false positive rate we get the roc curve\n",
      "\n",
      "Area under the curve \n",
      "* a large area would have a well performing model\n",
      "\n",
      "* AUC allows you to compare the performance of different classifiers\n",
      "\n",
      "y_pred_prob= logreg.predict_proba(X_test)[:,1]\n",
      "\n",
      "*0 column- the probablity the first column will be 0\n",
      "*1 column= the probability the second column will be 1\n",
      "\n",
      "from sklearn.metrics import roc_curve\n",
      "\n",
      "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
      "\n",
      "fpr=false positive rate\n",
      "tpr=true positive rate\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.plot(fpr,tpr)\n",
      "plt.xlabel(\"False Positive Rate\")\n",
      "plt.ylabel(\"True Positive Rate\")\n",
      "plt.plot([0,1],[0,1],\"k--\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "auc= roc_auc_score(y_test, y_pred)\n",
      "\n",
      "  Sample (Print probabilities - ROC curve)\n",
      "\n",
      "# Generate the probabilities\n",
      "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
      "print(y_pred_prob)\n",
      "\n",
      "from sklearn.metrics import roc_curve\n",
      "\n",
      "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
      "\n",
      "# Plot the ROC curve\n",
      "plt.plot(fpr,tpr)\n",
      "\n",
      "# Add labels and diagonal line\n",
      "plt.xlabel(\"False Positive Rate\")\n",
      "plt.ylabel(\"True Positive Rate\")\n",
      "plt.plot([0, 1], [0, 1], \"k--\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  Sample ( auc)\n",
      "\n",
      "# Import roc_auc_score\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "# Print the AUC\n",
      "print(roc_auc_score(y_test, y_pred_prob)\n",
      "\n",
      " >Precision - recall curve\n",
      "\n",
      "Another way to evaluate model performance is using a precision-recall curve, which shows the tradeoff between precision and recall for different thresholds.\n",
      "\n",
      "  Sample F1 score\n",
      "\n",
      "\n",
      "f1=2 * (precision * recall) / (precision + recall)\n",
      "\n",
      "F1 score is it incorporates both precision and recall into a single metric\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import f1_score\n",
      "from sklearn.metrics import f1_score\n",
      "\n",
      "# Print the F1 score\n",
      "print(f1_score(y_test, y_pred))\n",
      "\n",
      "\n",
      "#a high F1 score is a sign of a well-performing model\n",
      "\n",
      "    >Tuning your model          \n",
      "\n",
      "hyper parameters of the random forest model\n",
      "1. n_estimators = number of trees\n",
      "2. criterion = quality of split\n",
      "3. max_features= number of features for best split\n",
      "4. max_depth= max depth of tree\n",
      "5. min_sample_splits=minimum samples to spit node\n",
      "6. bootstrap = whether bootstrap samples are used\n",
      "\n",
      "grid search is brute force search\n",
      "a. returns the best model fit\n",
      "\n",
      "cross validation\n",
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "param_grid={'n_estimators':np.arange(10,51)}\n",
      "\n",
      "clf_cv= GridSearchCV(RandomForestClassifier(), param_grid)\n",
      "\n",
      "clf_cv.fit(X,y)\n",
      "\n",
      "print(clf_cv.best_params_)\n",
      "\n",
      "print(clf_cv.best_score_)\n",
      "\n",
      "   >Sample GridSearchCV\n",
      "\n",
      "# Import GridSearchCV\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Create the hyperparameter grid\n",
      "param_grid = {'max_features': ['auto', 'sqrt', 'log2']}\n",
      "\n",
      "# Call GridSearchCV\n",
      "grid_search = GridSearchCV(RandomForestClassifier(), param_grid)\n",
      "\n",
      "# Fit the model\n",
      "grid_search.fit(X, y)\n",
      "\n",
      "print(grid_search.best_params_)\n",
      "print(grid_search.best_score_)\n",
      "\n",
      "  Sample (complex hyper parameter)\n",
      "\n",
      "\n",
      "# Import GridSearchCV\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Create the hyperparameter grid\n",
      "param_grid = {\"max_depth\": [3, None],\n",
      "              \"max_features\": [1, 3, 10],\n",
      "              \"bootstrap\": [True, False],\n",
      "              \"criterion\": [\"gini\", \"entropy\"]}\n",
      "\n",
      "# Call GridSearchCV\n",
      "grid_search = GridSearchCV(RandomForestClassifier(),param_grid)\n",
      "# Fit the model\n",
      "grid_search.fit(X, y)\n",
      "\n",
      "print(grid_search.best_params_)\n",
      "print(grid_search.best_score_)\n",
      "\n",
      "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': None, 'max_features': 10}\n",
      "0.9534953495349535\n",
      "\n",
      "   Sample (Random Grid Search)\n",
      "\n",
      "we could randomly jump around the grid and try different combinations.\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "# Create the hyperparameter grid\n",
      "param_dist = {\"max_depth\": [3, None],\n",
      "              \"max_features\": randint(1, 11),\n",
      "              \"bootstrap\": [True, False],\n",
      "              \"criterion\": [\"gini\", \"entropy\"]}\n",
      "\n",
      "# Call RandomizedSearchCV\n",
      "random_search = RandomizedSearchCV(RandomForestClassifier(),param_dist)\n",
      "# Fit the model)\n",
      "random_search.fit(X, y)\n",
      "\n",
      "print(random_search.best_params_)\n",
      "print(random_search.best_score_)\n",
      "\n",
      "\n",
      "         >Feature importances\n",
      "\n",
      "* scoring represents how much each feature contributes to a prediction.\n",
      "* visualization is an effective way to communicate results to stakeholders\n",
      "1. which features are important drivers of churn\n",
      "2. which features can be removed from the model\n",
      "\n",
      "interpretability might be reasons you get buyin from stakeholders\n",
      "\n",
      "\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "clf.feature_importances_\n",
      "\n",
      "  Sample (calculating feature importances)\n",
      "\n",
      "# Calculate feature importances\n",
      "importances = clf.feature_importances_\n",
      "\n",
      "# Create plot\n",
      "plt.barh(range(X.shape[1]), importances)\n",
      "plt.show()\n",
      "\n",
      "In order to make the plot more readable, we need to do achieve two goals:\n",
      "\n",
      "Re-order the bars in ascending order.\n",
      "Add labels to the plot that correspond to the feature names.\n",
      "\n",
      " >Sort features by importance\n",
      "\n",
      "# Sort importances\n",
      "sorted_index = np.argsort(importances)\n",
      "\n",
      "# Create labels\n",
      "labels = X.columns[sorted_index]\n",
      "\n",
      "# Clear current plot\n",
      "plt.clf()\n",
      "\n",
      "# Create plot\n",
      "plt.barh(range(X.shape[1]), importances[sorted_index], tick_label=labels)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    Adding New Features\n",
      "\n",
      "additional data sources: customer service, web logs, email campaigns, network, transactions, and signal strength\n",
      "\n",
      "1. can improve model performances\n",
      "2. avoid underfitting\n",
      "\n",
      "churn features:\n",
      "Region Code,\n",
      "Total Charges,\n",
      "Total Minutes,\n",
      "Minutes per Call\n",
      "Cost per Call\n",
      "Total Calls\n",
      "\n",
      "\n",
      "Compare both ROC curves\n",
      "\n",
      "Discuss with business the benefits and costs of incorporating the additional features\n",
      "1. improved return on investment\n",
      "2. decreased cost\n",
      "3. increased performance\n",
      "\n",
      "Benefits must exceed costs\n",
      "\n",
      "\n",
      "  Sample Adding additional features\n",
      "\n",
      "# Import necessary modules\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the data\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "# Print the accuracy\n",
      "print(clf.score(X_test, y_test))\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Print the F1 score\n",
      "print(f1_score(y_test,y_pred))\n",
      "\n",
      "\n",
      "\n",
      " >Exploratory Data Analysis in python\n",
      " >Designing Machine Learning Workflows in python\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\dimensions reduction techniques.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\dimensions reduction techniques.txt\n",
      "df.shape\n",
      "each row should be an observation\n",
      "\n",
      "remove columns with vary little variance\n",
      "user pd.describe()\n",
      "\n",
      "the column generation had a std of 0 and min and max values that were the same.  you can drop the generation column.\n",
      "\n",
      "pd.describe(exclude='number')\n",
      "\n",
      "describes only non numeric columns\n",
      "\n",
      "  sample    combine list of column names\n",
      "\n",
      "# Remove the feature without variance from this list\n",
      "number_cols = ['HP', 'Attack', 'Defense']\n",
      "\n",
      "# Leave this list as is for now\n",
      "non_number_cols = ['Name', 'Type', 'Legendary']\n",
      "\n",
      "print(pokemon_df.columns)\n",
      "# Sub-select by combining the lists with chosen features\n",
      "df_selected = pokemon_df[number_cols + non_number_cols]\n",
      "\n",
      "# Prints the first 5 lines of the new dataframe\n",
      "print(df_selected.head())\n",
      "\n",
      "print(df_selected.describe(exclude='number'))\n",
      "\n",
      "#remove the columns with almost all similarities\n",
      "\n",
      "\n",
      "       >Reducing dimensionality\n",
      "\n",
      "your dataset will be less complex\n",
      "your dataset will require less disk space\n",
      "\n",
      "training will require less computation time\n",
      "you will have less of chance of overfitting.\n",
      "\n",
      "decide on which features are important\n",
      "\n",
      "dropping a column\n",
      "insurance_df.drop('favorite color', axis=1)\n",
      "\n",
      "       Exploring the dataset\n",
      "\n",
      "sns.pairplot(ansur_df, hue='gender', diag_kind='hist')\n",
      "\n",
      "it provides an one by one comparison of all numeric columns in the dataframe as a scatter plot\n",
      "\n",
      "removing features with very little information prevents information loss.\n",
      "\n",
      "Extract new features from the existing features\n",
      "\n",
      "pca\n",
      "\n",
      "\n",
      "   >sample using pairplot\n",
      "\n",
      "# Create a pairplot and color the points using the 'Gender' feature\n",
      "sns.pairplot(ansur_df_1,hue='Gender', kind='reg', diag_kind='hist')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   sample  > remove stature_m\n",
      "#US Army ANSUR body measurement dataset\n",
      "\n",
      "print(ansur_df_1.columns)\n",
      "# Remove one of the redundant features\n",
      "reduced_df = ansur_df_1.drop('stature_m', axis=1)\n",
      "\n",
      "# Create a pairplot and color the points using the 'Gender' feature\n",
      "sns.pairplot(reduced_df, hue='Gender')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "Index(['Gender', 'footlength', 'headlength', 'n_legs'], dtype='object')\n",
      "\n",
      "  sample remove n_legs which has low variance\n",
      "\n",
      "# Remove the redundant feature\n",
      "reduced_df = ansur_df_2.drop('n_legs', axis=1)\n",
      "\n",
      "# Create a pairplot and color the points using the 'Gender' feature\n",
      "sns.pairplot(reduced_df, hue='Gender', diag_kind='hist')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "         >t-SNE visualization\n",
      "\n",
      "\n",
      "t-SNE is a way to visual high dimensional data using feature extraction\n",
      "\n",
      "t-SNE maximize distance in 2 dimensional space that are different in high dimensional space\n",
      "\n",
      "items that are close to each other may cluster\n",
      "\n",
      "non_numericnon_numeric=['BMI_class','Height_class','Gender','Component','Branch']\n",
      "\n",
      "df_numeric=df.drop(non_numeric,axis=1)\n",
      "\n",
      "df_numeric.shape\n",
      "\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "\n",
      "m=TSNE(learning_rate=50)\n",
      "\n",
      "learning rates 10 to 1000 range\n",
      "\n",
      "tnse_features = m.fit_transform(df_numeric)\n",
      "tsne_features[1:4,:]\n",
      "\n",
      "sns.scatterplot(x='x',y='y', hue='BMI_class', data=df)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "bmi_class: Overweight, normal, underweight\n",
      "\n",
      "\n",
      "Over weight 25 to 29.9\n",
      "Normal weight 18.5 to 24.9\n",
      "Under weight 18.5 or less\n",
      "\n",
      "weight in lbs * 703/ heightin**2\n",
      "\n",
      "\n",
      "Tall >5'9\n",
      "Normal >5'4 to <5'9\n",
      "short <5'4\n",
      "\n",
      "  > Sample tsne  higher dimensional view of the data\n",
      "\n",
      "# Non-numerical columns in the dataset\n",
      "non_numeric = ['Branch', 'Gender', 'Component']\n",
      "\n",
      "# Drop the non-numerical columns from df\n",
      "df_numeric = df.drop(non_numeric, axis=1)\n",
      "\n",
      "# Create a t-SNE model with learning rate 50\n",
      "m = TSNE(learning_rate=50)\n",
      "\n",
      "# Fit and transform the t-SNE model on the numeric dataset\n",
      "tsne_features = m.fit_transform(df_numeric)\n",
      "print(tsne_features.shape)\n",
      "\n",
      "# Color the points according to Army Component\n",
      "sns.scatterplot(x=\"x\", y=\"y\", hue=\"Component\", data=df)\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "    The curse of dimensionality\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc= SVC()\n",
      "\n",
      "svc.fit(X_train, y_train)\n",
      "print(accuracy_score(y_test,svc.predict(X_test))\n",
      "\n",
      "print(accuracy_score(y_train, svc.predict(X_train))\n",
      "\n",
      "\n",
      "features: city, price, n_floors, n_bathrooms, surface_m2\n",
      "\n",
      "increase the number of observations to ensure generalization.  otherwise the model memorize the smaller training set overfitting and it does not generalize well.\n",
      "\n",
      "observations should increase exponentially with the number of features\n",
      "\n",
      "this is called the curse of dimensionality\n",
      "\n",
      "\n",
      "  Sample load and split train and test\n",
      "\n",
      "# Import train_test_split()\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Select the Gender column as the feature to be predicted (y)\n",
      "y = ansur_df['Gender']\n",
      "\n",
      "# Remove the Gender column to create the training data\n",
      "X = ansur_df.drop('Gender', axis=1)c\n",
      "\n",
      "# Perform a 70% train and 30% test data split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
      "\n",
      "print(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))\n",
      "\n",
      "\n",
      "   sample  > fit and predict using svc\n",
      "\n",
      "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Create an instance of the Support Vector Classification class\n",
      "svc = SVC()\n",
      "\n",
      "# Fit the model to the training data\n",
      "svc.fit(X_train, y_train)\n",
      "\n",
      "# Calculate accuracy scores on both train and test data\n",
      "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
      "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
      "\n",
      "print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
      "\n",
      "output: 49.7% accuracy\n",
      "\n",
      "\n",
      "        >features with missing values or little variance\n",
      "\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "\n",
      "sel = VarianceThreshold(threshold=1)\n",
      "sel.fit(ansur_df)\n",
      "\n",
      "mask=sel.get_support()\n",
      "print(mask)\n",
      "\n",
      "reduced_df=ansur_df.loc[:,mask]\n",
      "\n",
      "print(reduced_df.shape)\n",
      "\n",
      "\n",
      "   >normalize the variance\n",
      "\n",
      "sel=VarianceThreshold(threshold=0.005)\n",
      "set.fit(ansur_df / ansur_df.mean())\n",
      "\n",
      "\n",
      "   missing values     >.repairing\n",
      "\n",
      "df.isna().sum()\n",
      "\n",
      "df.isna().sum()/len(df)\n",
      "\n",
      "mask=df.isna().sum()/len(df)<0.3\n",
      "\n",
      "reduced_df=df.loc[:,mask]\n",
      "\n",
      "reduced_df.head()\n",
      "\n",
      "\n",
      "  >sample   > create boxplot\n",
      "\n",
      "# Create the boxplot\n",
      "head_df.boxplot()\n",
      "\n",
      "\n",
      "   sample  > boxplot   normalize   print the variance\n",
      "\n",
      "# Normalize the data\n",
      "normalized_df = head_df / head_df.mean()\n",
      "\n",
      "# Print the variances of the normalized data\n",
      "print(normalized_df.var())\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  >sample  > remove columns with low variance\n",
      "\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "\n",
      "# Create a VarianceThreshold feature selector\n",
      "sel = VarianceThreshold(threshold=0.001)\n",
      "\n",
      "# Fit the selector to normalized head_df\n",
      "sel.fit(head_df / head_df.mean())\n",
      "\n",
      "# Create a boolean mask\n",
      "mask = sel.get_support()\n",
      "\n",
      "# Apply the mask to create a reduced dataframe\n",
      "reduced_df = head_df.loc[:, mask]\n",
      "\n",
      "print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))\n",
      "\n",
      "  sample remove the missing values using a mask\n",
      "\n",
      "# Create a boolean mask on whether each feature less than 50% missing values.\n",
      "mask = school_df.isna().sum() / len(school_df) < 0.5\n",
      "\n",
      "# Create a reduced dataset by applying the mask\n",
      "reduced_df = school_df.loc[:,mask]\n",
      "\n",
      "print(school_df.shape)\n",
      "print(reduced_df.shape)\n",
      "\n",
      "\n",
      "            >Pairwise correlation\n",
      "\n",
      "sns.pairplot(ansur, hue=gender)\n",
      "\n",
      "strength of correlation coefficient\n",
      "\n",
      "r=-1 and r=0  and r=1\n",
      "\n",
      "\n",
      "-1 is perfectly negative correlation\n",
      "1 is perfectly postive correlation\n",
      "0 is no correlation\n",
      "\n",
      "weights_df_corr()\n",
      "\n",
      "the dialog tells us that each feature is perfectly correlated to itself\n",
      "\n",
      "visual the correlation using the seaborn heatmap\n",
      "\n",
      "cmap=sns.diverging_palette(h_neg=10, h_pos=240, as_cmap=True)\n",
      "\n",
      "sns.heatmap(weights_df.corr(), center=0, cmap=cmap, linewidths=1,\n",
      "annot=True, fmt=\".2f\")\n",
      "\n",
      " > remove the diagonal feature referencing itself\n",
      "\n",
      "corr= weights_df.corr()\n",
      "\n",
      "mask=np.triu(np.ones_like(corr,dtype=bool))\n",
      "\n",
      "remove\n",
      "SubjectNumericRace\n",
      "DODRace\n",
      "\n",
      "\n",
      "  sample  > create a heatmap of the correlation\n",
      "\n",
      "# Create the correlation matrix\n",
      "corr = ansur_df.corr()\n",
      "\n",
      "# Draw the heatmap\n",
      "sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  sample  > add a mask\n",
      "\n",
      "# Create the correlation matrix\n",
      "corr = ansur_df.corr()\n",
      "\n",
      "# Generate a mask for the upper triangle\n",
      "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
      "\n",
      "\n",
      "sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\",mask=mask)\n",
      "plt.show()\n",
      "\n",
      "  > removing highly correlated features\n",
      "\n",
      "\n",
      "-1 and 1 and 0\n",
      "\n",
      "drop features that are close to 1 or -1\n",
      "\n",
      "cervical height and suprastermale height\n",
      "chest height and suprastermale height\n",
      "chest height and cericale height\n",
      "\n",
      "\n",
      "corr_df=chest_df.corr().abs()\n",
      "mask=np.triu(np.ones_like(corr_df,dtype=bool))\n",
      "\n",
      "\n",
      "tri_df=corr_matrix.mask(mask)\n",
      "\n",
      "to_drop=[c for c in tri_df.columns if any(tri_df[c]>0.95)]\n",
      "\n",
      "print(to_drop)\n",
      "\n",
      "reduced_df=chest_df.drop(to_drop,axis=1)\n",
      "\n",
      "\n",
      "  > sample  > dropping highly correlated features from the dataframe\n",
      "\n",
      "# Calculate the correlation matrix and take the absolute value\n",
      "corr_matrix = ansur_df.corr().abs()\n",
      "\n",
      "# Create a True/False mask and apply it\n",
      "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
      "tri_df = corr_matrix.mask(mask)\n",
      "\n",
      "# List column names of highly correlated features (r > 0.95)\n",
      "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
      "\n",
      "# Drop the features in the to_drop list\n",
      "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
      "\n",
      "print(\"The reduced dataframe has {} columns.\".format(reduced_df.shape[1]))\n",
      "\n",
      "\n",
      "  >predict gender\n",
      "\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler = StandardScaler()\n",
      "\n",
      "X_train_std= scaler.fit_transform(X_train)\n",
      "\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "lr=LogisticRegression()\n",
      "lr.fit(X_train_std, y_train)\n",
      "\n",
      "X_test_std= scaler.transform(X_test)\n",
      "\n",
      "y_pred=lr.predict(X_test_std)\n",
      "print(accurancy_score(y_test, y_pred))\n",
      "\n",
      "print(lr.coef_)\n",
      "\n",
      "output: array[[-3, 0.14, 7.46, 1.22, 0.87]])\n",
      "\n",
      "coefficients close to zero will contribute little to the end result\n",
      "\n",
      "print(dict(zip(X.column, abs(lr.coef_[0]))))\n",
      "\n",
      "{'chestdepth': 3.0,\n",
      "'handlength':0.14,\n",
      "'neckcircumference':7.46,\n",
      "'shoulderlength':1.22,\n",
      "'earlength':0.87\n",
      "}\n",
      "\n",
      "remove handlength\n",
      "\n",
      "         >Recursive Feature Elimination\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "\n",
      "rfe=RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n",
      "\n",
      "scaler = StandardScaler()\n",
      "X_train_std= scaler.fit_transform(X_train)\n",
      "\n",
      "rfe.fit(X_train_std, y_train)\n",
      "\n",
      "X.columns[rfe.support_]\n",
      "\n",
      "print(dict(zip(X.columns,rfe.ranking_)))\n",
      "\n",
      "high values mean the feature was dropped early on\n",
      "\n",
      "\n",
      "  > Sample  > test features contribution using logistic regression\n",
      "#Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression\n",
      "\n",
      "# Fit the scaler on the training features and transform these in one go\n",
      "X_train_std = scaler.fit_transform(X_train)\n",
      "\n",
      "# Fit the logistic regression model on the scaled training data\n",
      "lr=LogisticRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "\n",
      "# Scale the test features\n",
      "X_test_std = scaler.transform(X_test)\n",
      "\n",
      "# Predict diabetes presence on the scaled test set\n",
      "y_pred = lr.predict(X_test_std)\n",
      "\n",
      "# Prints accuracy metrics and feature coefficients\n",
      "print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \n",
      "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
      "\n",
      "79.6% accuracy on test set.\n",
      "{'pregnant': 0.04, 'glucose': 1.23, 'diastolic': 0.03, 'triceps': 0.24, 'insulin': 0.19, 'bmi': 0.38, 'family': 0.34, 'age': 0.34}\n",
      "\n",
      "\n",
      " >sample  > remove diastolic\n",
      "\n",
      "# Remove the feature with the lowest model coefficient\n",
      "X = diabetes_df[['pregnant', 'glucose',  'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
      "\n",
      "# Performs a 25-75% train test split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
      "\n",
      "# Scales features and fits the logistic regression model\n",
      "lr.fit(scaler.fit_transform(X_train), y_train)\n",
      "\n",
      "# Calculates the accuracy on the test set and prints coefficients\n",
      "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
      "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
      "\n",
      "\n",
      "\n",
      "  Sample  > RFE   > dropping feature columns\n",
      "\n",
      "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
      "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n",
      "\n",
      "# Fits the eliminator to the data\n",
      "rfe.fit(X_train, y_train)\n",
      "\n",
      "# Print the features and their ranking (high = dropped early on)\n",
      "print(dict(zip(X.columns, rfe.ranking_)))\n",
      "\n",
      "# Print the features that are not eliminated\n",
      "print(X.columns[rfe.support_])\n",
      "\n",
      "# Calculates the test set accuracy\n",
      "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
      "\n",
      "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
      "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n",
      "\n",
      "# Fits the eliminator to the data\n",
      "rfe.fit(X_train, y_train)\n",
      "\n",
      "# Print the features and their ranking (high = dropped early on)\n",
      "print(dict(zip(X.columns, rfe.ranking_)))\n",
      "\n",
      "# Print the features that are not eliminated\n",
      "print(X.columns[rfe.support_])\n",
      "\n",
      "# Calculates the test set accuracy\n",
      "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
      " \n",
      "\n",
      "{'pregnant': 5, 'glucose': 1, 'diastolic': 6, 'triceps': 3, 'insulin': 4, 'bmi': 1, 'family': 2, 'age': 1}\n",
      "Index(['glucose', 'bmi', 'age'], dtype='object')\n",
      "80.6% accuracy on test set.\n",
      "\n",
      "\n",
      "diastolic and pregnant dropped early\n",
      "\n",
      "tricept and bmi\n",
      "insulin and glucose\n",
      "\n",
      "        >Random forest classifer\n",
      "\n",
      "ensemble of multiple decision trees \n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "rf=RandomForestClassifier()\n",
      "rf.fit(X_train, y_train)\n",
      "\n",
      "print(rf.feature_importances_)\n",
      "print(sum(rf.feature_importances_))\n",
      "\n",
      "#always sum to 1\n",
      "\n",
      "\n",
      "mask=rf.feature_importances_ > 0.1\n",
      "\n",
      "print(mask)\n",
      "\n",
      "X_reduced=X.loc[:,mask]\n",
      "print(X_reduced.columns)\n",
      "\n",
      "  >drop the least 10 important features at a cycle\n",
      "\n",
      "rfe=RFE(esimator=RandomForestClassifier(),\n",
      "n_features_to_select=6, step=10, verbose=1)\n",
      "\n",
      "#drop the least 10 important features at a cycle\n",
      "\n",
      "print(X.columns[rfe.support_])\n",
      "\n",
      "#contains the remaining features in the model\n",
      "\n",
      "  >sample  > use a randomforestclassifier to determine feature importance\n",
      "\n",
      "# Perform a 75% training and 25% test data split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
      "\n",
      "# Fit the random forest model to the training data\n",
      "rf = RandomForestClassifier(random_state=0)\n",
      "rf.fit(X_train, y_train)\n",
      "\n",
      "# Calculate the accuracy\n",
      "acc = accuracy_score(y_test, rf.predict(X_test))\n",
      "\n",
      "# Print the importances per feature\n",
      "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
      "\n",
      "# Print accuracy\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
      "\n",
      "\n",
      "{'pregnant': 0.09, 'glucose': 0.21, 'diastolic': 0.08, 'triceps': 0.11, 'insulin': 0.13, 'bmi': 0.09, 'family': 0.12, 'age': 0.16}\n",
      "77.6% accuracy on test set.\n",
      "\n",
      "\n",
      "   >sample   > measure feature importances\n",
      "\n",
      "# Create a mask for features importances above the threshold\n",
      "mask = rf.feature_importances_>0.15\n",
      "\n",
      "# Prints out the mask\n",
      "print(mask)\n",
      "\n",
      "mask = rf.feature_importances_ > 0.15\n",
      "\n",
      "# Apply the mask to the feature dataset X\n",
      "reduced_X = X.loc[:,mask]\n",
      "\n",
      "# prints out the selected column names\n",
      "print(reduced_X.columns)\n",
      "\n",
      "output:  'glucose', 'age'\n",
      "\n",
      "   >sample  > RFE\n",
      "\n",
      "# Wrap the feature eliminator around the random forest model\n",
      "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
      "\n",
      "# Fit the model to the training data\n",
      "rfe.fit(X_train, y_train)\n",
      "\n",
      "# Create a mask using an attribute of rfe\n",
      "mask = rfe.support_\n",
      "\n",
      "# Apply the mask to the feature dataset X and print the result\n",
      "reduced_X = X.loc[:, mask]\n",
      "print(reduced_X.columns)\n",
      "\n",
      "output: Index(['glucose', 'insulin'], dtype='object')\n",
      "\n",
      "\n",
      "   Linear regressor\n",
      "linear moe\n",
      "x1,x2,x3 target y \n",
      "where y is a contineous value\n",
      "\n",
      "normal distribution\n",
      "\n",
      "the coefficients determine the affect the feature has on the target\n",
      "\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lr=LinearRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "\n",
      "print(lr.coef_)\n",
      "\n",
      "print(lr.intercept_)\n",
      "\n",
      "r2 tells us the variance of prediction and whether the data is linear or non linear\n",
      "\n",
      "the model tries to fit through the data by minimizing the loss function  (MSE)\n",
      "\n",
      "mse or mean square error creates the linear line through your data.  r2 tells you if the linear regressor is linear or non linear.  regularization helps reduce overfit of the data by smoothing your distribution to look more guassian.\n",
      "\n",
      "regularization will try to keep the model simple by keeping the coefficients low\n",
      "\n",
      "if the model is too low it might overfit, if the model is too high it might become inaccurate\n",
      "\n",
      "\n",
      "la = Lasso()\n",
      "la.fit(X_train, y_train)\n",
      "\n",
      "print(la.coef_)\n",
      "\n",
      "change the alpha\n",
      "la=Lasso(alpha=0.05)\n",
      "\n",
      "output: [4.91 1.76 0]\n",
      "\n",
      "\n",
      "  > sample  > regularize and lasso\n",
      "\n",
      "\n",
      "# Set the test size to 30% to get a 70-30% train test split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=0)\n",
      "\n",
      "# Fit the scaler on the training features and transform these in one go\n",
      "X_train_std = scaler.fit_transform(X_train,y_train)\n",
      "\n",
      "# Create the Lasso model\n",
      "la = Lasso()\n",
      "\n",
      "# Fit it to the standardized training data\n",
      "la.fit(X_train_std,y_train)\n",
      "\n",
      "\n",
      "print(la.coef_)\n",
      "\n",
      " > sample  > using R2 to determine the number of ignored features\n",
      "\n",
      "# Transform the test set with the pre-fitted scaler\n",
      "X_test_std = scaler.transform(X_test)\n",
      "\n",
      "# Calculate the coefficient of determination (R squared) on X_test_std\n",
      "r_squared = la.score(X_test_std, y_test)\n",
      "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
      "\n",
      "# Create a list that has True values when coefficients equal 0\n",
      "zero_coef = la.coef_ == 0\n",
      "\n",
      "# Calculate how many features have a zero coefficient\n",
      "n_ignored = sum(zero_coef)\n",
      "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))\n",
      "\n",
      "    combining features\n",
      "\n",
      "\n",
      "from sklearn.linear_model import Lasso\n",
      "\n",
      "la=Lasso(alpha=0.05)\n",
      "la.fit(X_train, y_train)\n",
      "\n",
      "print(la.coef_)\n",
      "\n",
      "print(la.score(X_test,y_test))\n",
      "\n",
      "  >lassoCV\n",
      "\n",
      "from sklearn.linear_model import LassoCV\n",
      "\n",
      "lcv=LassoCV()\n",
      "\n",
      "lcv.fit(X_train, y_train)\n",
      "print(lcv.alpha_)\n",
      "\n",
      "mask= lcv.coef_ !=0\n",
      "print(mask)\n",
      "\n",
      "reduced_X=X.loc[:,mask]\n",
      "\n",
      "\n",
      "  Combining feature selectors\n",
      "\n",
      "Random forest is a combination of decision trees\n",
      "It is based on the idea that a combination of models can combine to form a strong one\n",
      "\n",
      "\n",
      "from sklearn.linear_model import LassoCV\n",
      "\n",
      "lcv=LassoCV()\n",
      "\n",
      "lcv.fit(X_train, y_train)\n",
      "lcv.score(X_test, y_test)\n",
      "\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "rfe_rf= RFE(estimator=RandomForestRegressor(),\n",
      "\tn_features_to_select =66, step =5, verbose=1)\n",
      "\n",
      "rfe_rf.fit(X_train, y_train)\n",
      "\n",
      "rf_mask=rfe_rf.support_\n",
      "\n",
      "\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "rfe_gb= RFE(estimator=GradientBoostingRegressor(),\n",
      "\tn_features_to_select =66, step =5, verbose=1)\n",
      "\n",
      "rfe_gb.fit(X_train, y_train)\n",
      "\n",
      "gb_mask=rfe_rg.support_\n",
      "\n",
      "votes=np.sum([lcv_mask, rf_mask, gb_mask],axis=0)\n",
      "print(votes)\n",
      "\n",
      "mask=votes>=2\n",
      "\n",
      "reduced_X = X.loc[:,mask]\n",
      "\n",
      "\n",
      "  >Sample    lassoCV   >\n",
      "\n",
      "from sklearn.linear_model import LassoCV\n",
      "\n",
      "# Create and fit the LassoCV model on the training set\n",
      "lcv = LassoCV()\n",
      "lcv.fit(X_train,y_train)\n",
      "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
      "\n",
      "# Calculate R squared on the test set\n",
      "r_squared = lcv.score(X_test,y_test)\n",
      "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
      "\n",
      "# Create a mask for coefficients not equal to zero\n",
      "lcv_mask = lcv.coef_!=0\n",
      "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))\n",
      "\n",
      "X.loc[:,lcv_mask].columns\n",
      "\n",
      "\n",
      "Output: Optimal alpha = 0.089\n",
      "The model explains 88.2% of the test set variance\n",
      "26 features out of 32 selected\n",
      "\n",
      "['acromialheight', 'bideltoidbreadth', 'buttockcircumference', 'buttockpopliteallength', 'chestcircumference', 'chestheight', 'earprotrusion', 'footbreadthhorizontal',\n",
      "       'forearmcircumferenceflexed', 'handlength', 'headbreadth', 'heelbreadth', 'hipbreadth', 'interscyeii', 'lateralfemoralepicondyleheight', 'lateralmalleolusheight', 'radialestylionlength',\n",
      "       'shouldercircumference', 'shoulderelbowlength', 'thighcircumference', 'thighclearance', 'verticaltrunkcircumferenceusa', 'waistcircumference', 'waistdepth', 'wristheight', 'BMI'],\n",
      "      dtype='object')\n",
      "\n",
      "  >sample    RFE   > GradientBoostRegressor\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
      "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
      "             n_features_to_select=10, step=3, verbose=1)\n",
      "rfe_gb.fit(X_train, y_train)\n",
      "\n",
      "# Calculate the R squared on the test set\n",
      "r_squared = rfe_gb.score(X_test,y_test)\n",
      "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
      "\n",
      "gb_mask = rfe_gb.support_!=0\n",
      "print(X.loc[:,gb_mask].columns) \n",
      "\n",
      "\n",
      "Index(['bideltoidbreadth', 'buttockcircumference', 'chestcircumference', 'forearmcircumferenceflexed', 'hipbreadth', 'lateralmalleolusheight', 'shouldercircumference', 'thighcircumference',\n",
      "       'waistcircumference', 'BMI'],\n",
      "      dtype='object')\n",
      "\n",
      "   sample    rfe with RandomForestRegressor\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
      "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
      "             n_features_to_select=10, step=3, verbose=1)\n",
      "rfe_rf.fit(X_train, y_train)\n",
      "\n",
      "# Calculate the R squared on the test set\n",
      "r_squared = rfe_rf.score(X_test, y_test)\n",
      "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
      "\n",
      "# Assign the support array to gb_mask\n",
      "rf_mask = rfe_rf.support_\n",
      "\n",
      "  sample sum the masks\n",
      "\n",
      "# Sum the votes of the three models\n",
      "votes = np.sum([lcv_mask,rf_mask,gb_mask],axis=0)\n",
      "print(votes)\n",
      "\n",
      "meta_mask = votes>=2\n",
      "print(meta_mask)\n",
      "\n",
      "X_reduced = X.loc[:,meta_mask]\n",
      "print(X_reduced.columns)\n",
      "\n",
      "# Plug the reduced dataset into a linear regression pipeline\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
      "lm.fit(scaler.fit_transform(X_train), y_train)\n",
      "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
      "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))\n",
      "\n",
      "\n",
      "Index(['chestcircumference', 'forearmcircumferenceflexed', 'hipbreadth', 'thighcircumference', 'waistcircumference', 'wristheight', 'BMI'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "In [1]:\n",
      "\n",
      "\n",
      "    Feature Extraction\n",
      "\n",
      "feature extraction are new features resulting from the combinations of existing features.\n",
      "\n",
      "df_body['BMI']=df['Weight kg']/df_body['Height m']**2\n",
      "\n",
      "weight and height are obsolete\n",
      "\n",
      "leg_df['leg mm']=leg_df[['right leg mm','left leg mm']].mean(axis=1)\n",
      "\n",
      "    pca\n",
      "\n",
      "scaler = StandardScaler()\n",
      "\n",
      "df_std=pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
      "\n",
      "footlength and handlength\n",
      "\n",
      "people with big feet tend to have big hands\n",
      "\n",
      "principal components\n",
      "\n",
      "   sample  > combine quantity and revenue into price and drop the columns\n",
      "\n",
      "# Calculate the price from the quantity sold and revenue\n",
      "sales_df['price'] = sales_df['revenue']/sales_df['quantity']\n",
      "\n",
      "# Drop the quantity and revenue features\n",
      "reduced_df = sales_df.drop(['quantity','revenue'], axis=1)\n",
      "\n",
      "print(reduced_df.head())\n",
      "\n",
      "   sample  > add three columns into a new column and drop them\n",
      "\n",
      "# Calculate the mean height\n",
      "height_df['height'] = height_df[['height_1','height_2','height_3']].mean(axis=1)\n",
      "\n",
      "print(height_df.columns)\n",
      "# Drop the 3 original height features\n",
      "reduced_df = height_df.drop(['height_1','height_2','height_3'], axis=1)\n",
      "\n",
      "print(reduced_df.head())\n",
      "\n",
      "\n",
      "      >Principal component analysis\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler = StandardScaler()\n",
      "std_df = scaler.fit_transform(df)\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca=PCA()\n",
      "print(pca.fit_transform(std_df))\n",
      "\n",
      "pca.fit(std_df)\n",
      "\n",
      "print(pca.explained_variance_ratio_)\n",
      "\n",
      "\n",
      "\n",
      "   sample   > standard scaler\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Create the scaler and standardize the data\n",
      "scaler = StandardScaler()\n",
      "ansur_std = scaler.fit_transform(ansur_df)\n",
      "\n",
      "\n",
      "   sample  > pca fit transform\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# Create the scaler and standardize the data\n",
      "scaler = StandardScaler()\n",
      "ansur_std = scaler.fit_transform(ansur_df)\n",
      "\n",
      "# Create the PCA instance and fit and transform the data with pca\n",
      "pca = PCA()\n",
      "pc = pca.fit_transform(ansur_std)\n",
      "\n",
      "# This changes the numpy array output back to a dataframe\n",
      "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
      "\n",
      "\n",
      "sns.pairplot(data=pc_df)\n",
      "plt.show()\n",
      "\n",
      "  >sample  > pca component\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# Scale the data\n",
      "scaler = StandardScaler()\n",
      "ansur_std = scaler.fit_transform(ansur_df)\n",
      "\n",
      "# Apply PCA\n",
      "pca = PCA()\n",
      "pca.fit(ansur_std)\n",
      "\n",
      "# Inspect the explained variance ratio per component\n",
      "print(pca.explained_variance_ratio_)\n",
      "\n",
      "print(pca.explained_variance_ratio_.cumsum())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0.61449404 0.19893965 0.06803095 0.03770499 0.03031502 0.0171759\n",
      " 0.01072762 0.00656681 0.00634743 0.00436015 0.0026586  0.00202617\n",
      " 0.00065268]\n",
      "\n",
      "\n",
      "        PCA applications\n",
      "\n",
      "one downside to pca is the remaining components can be hard to intrept.\n",
      "\n",
      "print(pca.components_)\n",
      "\n",
      "this tells to what extent the component is affected by a feature\n",
      "\n",
      "PC 1 = 0.71x hand length + 0.71 foot length\n",
      "PC 2 = -071 x hand length + 0.71 x foot length\n",
      "\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "pipe=Pipeline([\n",
      "\t('scaler',StandardScaler()),\n",
      "\t('reducer',PCA())])\n",
      "\n",
      "pc=pipe.fit_transform(ansur_df)\n",
      "print(pc[:,2])\n",
      "\n",
      "ansur_categories['PC 1'] = pc[:,0]\n",
      "ansur_categories['PC 2'] = pc[:,1]\n",
      "\n",
      "\n",
      "sns.scatterplot(data=ansur_categories,\n",
      "x='PC 1', y='PC 2', hue='Height_class', alpha=0.4)\n",
      "\n",
      "\n",
      "   Add a classifier to the pipeline\n",
      "pipe=Pipeline([\n",
      "\t('scaler',StandardScaler()),\n",
      "\t('reducer',PCA(n_components=3)),\n",
      "\t('classifier', RandomForestClassifier())\n",
      "])\n",
      "\n",
      "\n",
      "pipe.fit(X_train,y_train)\n",
      "print(pipe.steps[1])\n",
      "\n",
      "print(pipe.steps[1][1].explained_variance_ratio_.cumsum())\n",
      "\n",
      "\n",
      "print(pipe.score(X_test,y_test))\n",
      "\n",
      "\n",
      "   sample  > build the pca pipeline\n",
      "\n",
      "# Build the pipeline\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "        \t\t ('reducer', PCA(n_components=2))])\n",
      "\n",
      "# Fit it to the dataset and extract the component vectors\n",
      "pipe.fit(poke_df)\n",
      "vectors = pipe.steps[1][1].components_.round(2)\n",
      "\n",
      "# Print feature effects\n",
      "print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
      "print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))\n",
      "\n",
      "  sample pca pipeline\n",
      "\n",
      "# Build the pipeline\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "                 ('reducer', PCA(n_components=2))])\n",
      "\n",
      "# Fit the pipeline to poke_df and transform the data\n",
      "pc = pipe.fit_transform(poke_df)\n",
      "\n",
      "print(pc)\n",
      "\n",
      "[[-1.5563747  -0.02148212]\n",
      " [-0.36286656 -0.05026854]\n",
      " [ 1.28015158 -0.06272022]\n",
      " ...\n",
      " [ 2.45821626 -0.51588158]\n",
      " [ 3.5303971  -0.95106516]\n",
      " [ 2.23378629  0.53762985]]\n",
      "\n",
      "Index(['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'], dtype='object')\n",
      "\n",
      "poke_cat_df['PC 1'] = pc[:, 0]\n",
      "poke_cat_df['PC 2'] = pc[:, 1]\n",
      "\n",
      "print(poke_cat_df.head())\n",
      "\n",
      "# Use the Type feature to color the PC 1 vs PC 2 scatterplot\n",
      "sns.scatterplot(data=poke_cat_df, \n",
      "                x='PC 1', y='PC 2', hue='Type')\n",
      "plt.show()\n",
      "\n",
      "  sample  > pipeline with pca and randomforest classifier\n",
      "\n",
      "# Build the pipeline\n",
      "pipe = Pipeline([\n",
      "        ('scaler', StandardScaler()),\n",
      "        ('reducer', PCA(n_components=2)),\n",
      "        ('classifier',  RandomForestClassifier(random_state=0))])\n",
      "\n",
      "\n",
      "# Fit the pipeline to the training data\n",
      "pipe.fit(X_train,y_train)\n",
      "\n",
      "# Prints the explained variance ratio\n",
      "print(pipe.steps[1][1].explained_variance_ratio_)\n",
      "\n",
      "# Score the accuracy on the test set\n",
      "accuracy = pipe.score(X_test,y_test)\n",
      "\n",
      "# Prints the model accuracy\n",
      "print('{0:.1%} test set accuracy'.format(accuracy))\n",
      "\n",
      "\n",
      "[0.45624044 0.17767414 0.12858833]\n",
      "95.0% test set accuracy\n",
      "\n",
      "     >Principal component selection\n",
      "\n",
      "pipe= Pipeline([\n",
      "('scaler', StandardScaler()),\n",
      "('reducer',PCA(n_components=0.9))])\n",
      "\n",
      "#explains 90% of the variance\n",
      "\n",
      "pipe.fit(poke_df)\n",
      "\n",
      "print(len(pipe.steps[1][1].components_))\n",
      "\n",
      "There is no right answer to the number of components i should keep. It depends on how much information you are willing to lose to reduce complexity\n",
      "\n",
      "var=pipe.steps[1][1].explain_variance_ratio_\n",
      "\n",
      "plt.plot(var)\n",
      "\n",
      "plt.xlabel('Principal component index')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.show()\n",
      "\n",
      "X=pca.inverse_transform(pc)\n",
      "\n",
      "moves from principal component space back to feature space.\n",
      "\n",
      "2914 grayscale values\n",
      "62x47 pixels=2914 grayscale values\n",
      "\n",
      "test\n",
      "(15,2914)\n",
      "15 pictures\n",
      "training\n",
      "(1333,2914)\n",
      "1333 images\n",
      "\n",
      "pipe= Pipeline([\n",
      "('scaler', StandardScaler()),\n",
      "('reducer',PCA(n_components=290))])\n",
      "\n",
      "pipe.fit(X_train)\n",
      "\n",
      "pc=pipe.fit_transform(X_test)\n",
      "\n",
      "print(pc.shape)\n",
      "15,290\n",
      "\n",
      "10 fold number reduction in features\n",
      "\n",
      "X_rebuilt=pipe.inverse_transform(pc)\n",
      "print(X_rebuilt.shape)\n",
      "\n",
      "img_plotter(X_rebuilt)\n",
      "\n",
      "\n",
      "   sample  > pipeline\n",
      "\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "        \t\t ('reducer', PCA(n_components=0.8))])\n",
      "\n",
      "# Fit the pipe to the data\n",
      "pipe.fit(ansur_df)\n",
      "\n",
      "print('{} components selected'.format(len(pipe.steps[1][1].components_)))\n",
      "\n",
      "11 components selected\n",
      "\n",
      ".9 n_components requires 23 components selected\n",
      "\n",
      " > sample pipeline     variance elbow\n",
      "\n",
      "# Pipeline a scaler and pca selecting 10 components\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "        \t\t ('reducer', PCA(n_components=10))])\n",
      "\n",
      "# Fit the pipe to the data\n",
      "pipe.fit(ansur_df)\n",
      "\n",
      "\n",
      "# Plot the explained variance ratio\n",
      "plt.plot(pipe.steps[1][1].explained_variance_ratio_)\n",
      "\n",
      "plt.xlabel('Principal component index')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.show()\n",
      "\n",
      "   sample  > hand written numbers\n",
      "\n",
      "plot_digits(X_test)\n",
      "\n",
      "print(X_test.shape)\n",
      "(16,784)\n",
      "\n",
      "\n",
      "    sample  > pc transform\n",
      "\n",
      "# Transform the input data to principal components\n",
      "pc = pipe.transform(X_test)\n",
      "\n",
      "\n",
      "# Prints the number of features per dataset\n",
      "print(\"X_test has {} features\".format(X_test.shape[1]))\n",
      "print(\"pc has {} features\".format(pc.shape[1]))\n",
      "\n",
      "X_test has 784 features\n",
      "pc has 78 features\n",
      "\n",
      "# Inverse transform the components to original feature space\n",
      "X_rebuilt = pipe.inverse_transform(pc)\n",
      "\n",
      "# Prints the number of features\n",
      "print(\"X_rebuilt has {} features\".format(X_rebuilt.shape[1]))\n",
      "\n",
      "X_rebuilt has 784 features\n",
      "\n",
      "# Plot the reconstructed data\n",
      "plot_digits(X_rebuilt)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\machine learning for finance.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\machine learning for finance.txt\n",
      "print(lng_df.head())  # examine the dataframe\n",
      "print(spy_df.head())  # examine the SPY DataFrame\n",
      "\n",
      "# Plot the Adj_Close columns for SPY and LNG\n",
      "spy_df['Adj_Close'].plot(label='SPY', legend=True)\n",
      "lng_df['Adj_Close'].plot(label='LNG', legend=True, secondary_y=True)\n",
      "plt.show()  # show the plot\n",
      "plt.clf()  # clear the plot space for the next plot\n",
      "\n",
      "# Histogram of the daily price change percent of Adj_Close for LNG\n",
      "lng_df['Adj_Close'].pct_change().plot.hist(bins=50)\n",
      "plt.xlabel('adjusted close 1-day percent change')\n",
      "plt.show()\n",
      "\n",
      " moving average is a average in the n past days\n",
      "14,50, 200 days for stocks\n",
      "\n",
      "Rsi oscilates between 0 and 100 - close to 0 rebound and close to 100 then decline\n",
      "\n",
      "RSI=100 - 100/(1+RS) where RS= Average gain over n periods / Average loss over n periods\n",
      "\n",
      "\n",
      "corr=feature_target_df.corr()\n",
      "sns.heatmap(corr, annot=True)\n",
      "\n",
      " \n",
      "\n",
      "feature_names = ['5d_close_pct']  # a list of the feature names for later\n",
      "\n",
      "# Create moving averages and rsi for timeperiods of 14, 30, 50, and 200\n",
      "for n in [14, 30, 50, 200]:\n",
      "\n",
      "    # Create the moving average indicator and divide by Adj_Close\n",
      "    lng_df['ma' + str(n)] = talib.SMA(lng_df['Adj_Close'].values,\n",
      "                              timeperiod=n) / lng_df['Adj_Close']\n",
      "    # Create the RSI indicator\n",
      "    lng_df['rsi' + str(n)] = talib.RSI(lng_df['Adj_Close'].values, timeperiod=n)\n",
      "    \n",
      "    # Add rsi and moving average to the feature name list\n",
      "    feature_names = feature_names + ['ma' + str(n), 'rsi' + str(n)]\n",
      "    \n",
      "print(feature_names)\n",
      "print(lng_df.head(5))\n",
      "\n",
      " \n",
      "\n",
      "# Plot heatmap of correlation matrix\n",
      "sns.heatmap(corr, annot= True, annot_kws = {\"size\": 14})\n",
      "plt.yticks(rotation=0, size = 14); plt.xticks(rotation=90, size = 14)  # fix ticklabel directions and size\n",
      "plt.tight_layout()  # fits plot area to the plot, \"tightly\"\n",
      "plt.show()\n",
      "\n",
      "# Drop all na values\n",
      "lng_df = lng_df.dropna()\n",
      "\n",
      "# Create features and targets\n",
      "# use feature_names for features; '5d_close_future_pct' for targets\n",
      "features = lng_df[feature_names]\n",
      "targets = lng_df['5d_close_future_pct']\n",
      "\n",
      "# Create DataFrame from target column and feature columns\n",
      "feature_and_target_cols = ['5d_close_future_pct'] + feature_names\n",
      "print(feature_and_target_cols)\n",
      "feat_targ_df = lng_df[feature_and_target_cols]\n",
      "\n",
      "# Calculate correlation matrix\n",
      "corr = feat_targ_df.corr()\n",
      "print(corr)\n",
      "\n",
      " \n",
      "\n",
      "# Plot heatmap of correlation matrix\n",
      "sns.heatmap(corr, annot=True, annot_kws = {\"size\": 14})\n",
      "plt.yticks(rotation=0, size = 14); plt.xticks(rotation=90, size = 14)  # fix ticklabel directions and size\n",
      "plt.tight_layout()  # fits plot area to the plot, \"tightly\"\n",
      "plt.show()  # show the plot\n",
      "plt.clf()  # clear the plot area\n",
      "\n",
      "# Create a scatter plot of the most highly correlated variable with the target\n",
      "plt.scatter(lng_df['ma200'], lng_df['5d_close_future_pct'])\n",
      "plt.show()\n",
      "\n",
      "\n",
      " Making train and test sets for time series\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "linear_features=sm.add_constant(features)\n",
      "\n",
      "train_size = int(0.85 * targets.shape[0])\n",
      "\n",
      "train_features = linear_features[:train_size]\n",
      "train_targets = targets[:train_size]\n",
      "test_features = linear_features[train_size:]\n",
      "test_targets = targets[train_size:]\n",
      "\n",
      "print(linear_features.shape, train_features.shape, test_features.shape)\n",
      "\n",
      "model=sm.OLS(train_targets,train_features)\n",
      "results=model.fit()\n",
      "print(results.summary())\n",
      "\n",
      "print(results.pvalues)\n",
      "\n",
      "train_predictions = results.predict(train_features)\n",
      "test_predictions = results.predict(test_features)\n",
      "\n",
      "plt.scatter(train_predictions, train_targets, alpha=0.2, color='b', label='train')\n",
      "plt.scatter(test_predictions, test_targets, alpha=0.2, color='r', label='test')\n",
      "\n",
      "# Plot the perfect prediction line\n",
      "xmin, xmax = plt.xlim()\n",
      "plt.plot(np.arange(xmin, xmax, 0.01), np.arange(xmin, xmax, 0.01), c='k')\n",
      "\n",
      "# Set the axis labels and show the plot\n",
      "plt.xlabel('predictions')\n",
      "plt.ylabel('actual')\n",
      "plt.legend()  # show the legend\n",
      "plt.show()\n",
      "\n",
      "#calculate the growth expectations of each company by calculating the eps\n",
      "\n",
      "price to earning ratio = (market price)/ earnings per share\n",
      "\n",
      "the higher price earning ratio indicates growth expectations of the stock\n",
      "\n",
      "pe= stock price/ earnings per share\n",
      "\n",
      " \n",
      "\n",
      "# Import numpy as np\n",
      "import numpy as np\n",
      "\n",
      "# Convert lists to arrays\n",
      "prices_array = np.array(prices)\n",
      "earnings_array = np.array(earnings)\n",
      "\n",
      "# Calculate P/E ratio \n",
      "pe = prices_array/earnings_array\n",
      "print(pe)\n",
      "\n",
      " \n",
      "\n",
      "boolean_array = (sectors == 'Consumer Staples')\n",
      "\n",
      "# Subset sector-specific data\n",
      "cs_names = names[boolean_array]\n",
      "cs_pe = pe[boolean_array]\n",
      "\n",
      "# Display sector names\n",
      "print(cs_names)\n",
      "print(cs_pe)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Make a scatterplot\n",
      "plt.scatter(it_id, it_pe, color='red', label='IT')\n",
      "plt.scatter(cs_id, cs_pe, color='green',label='CS')\n",
      "\n",
      "# Add legend\n",
      "plt.legend()\n",
      "\n",
      "# Add labels\n",
      "plt.xlabel('Company ID')\n",
      "plt.ylabel('P/E Ratio')\n",
      "plt.show()\n",
      "\n",
      " >\n",
      "\n",
      "amd_df['Adj_Volume_1d_change']=amd_df['Adj_Volume'].pct_change()\n",
      "\n",
      "one_day_change=amd_df['Adj_volume_1d_change'].values\n",
      "\n",
      "amd_df['Adj_Volume_1d_change_SMA']= talib.SMA(one_day_change,timeperiod=10)\n",
      "\n",
      "day of the week\n",
      "day of the month\n",
      "month\n",
      "quarter\n",
      "\n",
      "\n",
      "amd_df.index.dayofweek (0-6)\n",
      "\n",
      "days_of_week=pd.get_dummies(amd_df.index.dayofweek,\n",
      "prefix='weekday',\n",
      "drop_first=True)\n",
      "\n",
      " \n",
      "# Create 2 new volume features, 1-day % change and 5-day SMA of the % change\n",
      "new_features = ['Adj_Volume_1d_change', 'Adj_Volume_1d_change_SMA']\n",
      "feature_names.extend(new_features)\n",
      "lng_df['Adj_Volume_1d_change'] = lng_df['Adj_Volume'].pct_change()\n",
      "lng_df['Adj_Volume_1d_change_SMA'] = talib.SMA(lng_df['Adj_Volume_1d_change'].values,\n",
      "                                               timeperiod=5)\n",
      "\n",
      "# Plot histogram of volume % change data\n",
      "lng_df[new_features].plot(kind='hist', sharex=False, bins=50)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\seaborn heatplot.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\seaborn heatplot.txt\n",
      "pd.crosstab(df['mnth'],df['weekday'],\n",
      "values=df['total_rentals'], aggfunc='mean')\n",
      "\n",
      "\n",
      "  Sample  >  \n",
      "filter =(df['DRG'].isin(['682','683','684'])) & (df[' Total Discharges ']>=200)\n",
      "\n",
      "result=df[filter]\n",
      "\n",
      "ct=pd.crosstab(result['Provider Name'],result['DRG'],\n",
      "values=result[' Average Total Payments '], aggfunc='mean')\n",
      "\n",
      "print(ct)\n",
      "\n",
      "plt.figure(figsize=(22,22))\n",
      "sns.heatmap(ct,annot=True,fmt='f',\n",
      "cmap='YlGnBu', cbar=True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "Pandas corr functions correlations between columns in a dataframe.\n",
      "\n",
      "sns.heatmap(df.corr())\n",
      "\n",
      "\n",
      "    sample  > heatmaps show change over time\n",
      "\n",
      "# Create a crosstab table of the data\n",
      "pd_crosstab = pd.crosstab(df[\"Group\"], df[\"YEAR\"])\n",
      "print(pd_crosstab)\n",
      "\n",
      "# Plot a heatmap of the table\n",
      "sns.heatmap(pd_crosstab)\n",
      "\n",
      "# Rotate tick marks for visibility\n",
      "plt.yticks(rotation=0)\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "<<<<< sample  > cmap BuGn    counts guests\n",
      "\n",
      "# Create the crosstab DataFrame\n",
      "pd_crosstab = pd.crosstab(df[\"Group\"], df[\"YEAR\"])\n",
      "\n",
      "# Plot a heatmap of the table with no color bar and using the BuGn palette\n",
      "sns.heatmap(pd_crosstab, cbar=False, cmap=\"BuGn\", linewidths=.3)\n",
      "\n",
      "# Rotate tick marks for visibility\n",
      "plt.yticks(rotation=0)\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "#Show the plot\n",
      "plt.show()\n",
      "plt.clf()\n",
      "\n",
      "YEAR GoogleKnowlege_Occupation     Show   Group    Raw_Guest_List\n",
      "0  1999                     actor  1/11/99  Acting    Michael J. Fox\n",
      "1  1999                  Comedian  1/12/99  Comedy   Sandra Bernhard\n",
      "2  1999        television actress  1/13/99  Acting     Tracey Ullman\n",
      "3  1999              film actress  1/14/99  Acting  Gillian Anderson\n",
      "4  1999                     actor  1/18/99  Acting  David Alan Grier\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\time series.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\time series.txt\n",
      "points of time or periods in time\n",
      "\n",
      "import pandas as pd\n",
      "from datatime import datetime\n",
      "\n",
      "time_stamp=pd.Timestamp(datetime(2017,1,1))\n",
      "\n",
      "pd.Timestamp('2017-01-01')=time_stamp\n",
      "\n",
      "times_stamp.year\n",
      "time_stamp.weekday_name\n",
      "\n",
      "period=pd.Period('2017-01')\n",
      "\n",
      "convert pd.Period() to pd.Timestamp() and back\n",
      "\n",
      "period.asfreq(0)\n",
      "\n",
      "period+=2\n",
      "\n",
      "pd.Timestamp('2017-01-03','M')+1\n",
      "\n",
      "index= pd.date_range(start='2017-1-1',periods=12,freq='M')\n",
      "\n",
      "ns = nanoseconds\n",
      "\n",
      "\n",
      "data=np.random.random(size=(12,2))\n",
      "df=pd.DataFrame(data=data,index=index).info\n",
      "\n",
      "frequencies\n",
      "\n",
      "Period\n",
      "Hour\tH\t.second .minute .hour\n",
      "Day\tD\t.day .month .quarter .year\n",
      "Week\tW\t.weekday\n",
      "Month\tM\t.dayofweek\n",
      "Quarter\tQ\t.weekofyear\n",
      "Year\tA\t.dayofyear\n",
      "\n",
      "\n",
      "   sample date_range 7 days\n",
      "\n",
      "# Create the range of dates here\n",
      "seven_days = pd.date_range(start='2017-1-1',periods=7,freq='D')\n",
      "\n",
      "# Iterate over the dates and print the number and name of the weekday\n",
      "for day in seven_days:\n",
      "    print(day.dayofweek, day.weekday_name)\n",
      "\n",
      "\n",
      "6 Sunday\n",
      "0 Monday\n",
      "1 Tuesday\n",
      "2 Wednesday\n",
      "3 Thursday\n",
      "4 Friday\n",
      "5 Saturday\n",
      "\n",
      "   > indexing and resampling time series\n",
      "\n",
      "time series transformation\n",
      "1. parsing string dates and convert to datetime64\n",
      "2. selecting and slicing for specific subperiods\n",
      "3. setting and changing DateTimeIndex frequency\n",
      "a. upsampling involves increasing the time frequency\n",
      "b. downsampling involves decreasing the time frequency and involves aggregating data\n",
      "\n",
      "df=df.set_index('Date',inplace=True)\n",
      "\n",
      "inplace : don't create copy\n",
      "\n",
      "   > partial string indexing\n",
      "\n",
      "print(df.loc['2020-03-16','Open'])\n",
      "\n",
      ".asfreq('D)\n",
      "* convert DateTimeIndex to calendar day frequency\n",
      "\n",
      "print(df.asfreq('D').head(100))\n",
      "\n",
      "missing data is displayed as nan\n",
      "\n",
      ".asfreq('B')\n",
      "* convert DateTimeIndex to business day frequency\n",
      "\n",
      "google[google.price.isnull()]\n",
      "\n",
      "#select missing price values\n",
      "\n",
      "\n",
      "\n",
      "   sample   indexing using date\n",
      "\n",
      "data = pd.read_csv('nyc.csv')\n",
      "\n",
      "# Inspect data\n",
      "print(data.info())\n",
      "\n",
      "# Convert the date column to datetime64\n",
      "data['date']=pd.to_datetime(data['date'])\n",
      "\n",
      "# Set date column as index\n",
      "data.set_index('date',inplace=False)\n",
      "\n",
      "# Inspect data \n",
      "print(data.head())\n",
      "\n",
      "# Plot data\n",
      "data.plot(subplots=True)\n",
      "plt.show()\n",
      "\n",
      "<<<<<<< sample  > slice the data by year\n",
      "and reset the index then rename the column and concatenate the amount into a prices dataframe\n",
      "\n",
      "# Create dataframe prices here\n",
      "prices = pd.DataFrame()\n",
      "\n",
      "# Select data for each year and concatenate with prices here \n",
      "for year in ['2013','2014','2015']:\n",
      "    price_per_year = yahoo.loc[year, ['price']].reset_index(drop=True)\n",
      "    price_per_year.rename(columns={'price': year}, inplace=True)\n",
      "    prices = pd.concat([prices, price_per_year], axis=1)\n",
      "\n",
      "# Plot prices\n",
      "prices.plot()\n",
      "plt.show()\n",
      "\n",
      "#Reset the index, or a level of it. Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex, this method can remove one or more levels.\n",
      "\n",
      "\n",
      "     sample  carbon monoxide concentration in nyc\n",
      "\n",
      "Chicago        1898 non-null float64\n",
      "Los Angeles    1898 non-null float64\n",
      "New York       1898 non-null float64\n",
      "\n",
      "\n",
      "# Inspect data\n",
      "print(co.info())\n",
      "\n",
      "# Set the frequency to calendar daily\n",
      "co = co.asfreq('D')\n",
      "\n",
      "# Plot the data\n",
      "co.plot(subplots=True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Set frequency to monthly\n",
      "co = co.asfreq('M')\n",
      "\n",
      "# Plot the data\n",
      "co.plot(subplots=True)\n",
      "plt.show()\n",
      "\n",
      "      Lags and changes and returns for stock price series\n",
      "\n",
      "1. move data across time\n",
      "2. shift or lag values back or forward back in time.\n",
      "3. get the difference in value for a given time period\n",
      "4. compute the percent change any number of periods\n",
      "\n",
      "google=pd.read_csv('google.csv',parse_dates=['date'], index_col='date')\n",
      "\n",
      "google.info()\n",
      "\n",
      "     .shift(): Moving data between past & future\n",
      "\n",
      ".shift()\n",
      "* defaults to periods=1\n",
      "1 period into future\n",
      "\n",
      "google['shifted']=google.price.shift()\n",
      "\n",
      "\n",
      "df['shifted']=df.Open.shift()\n",
      "print(df)\n",
      "\n",
      "  > .shift () lag\n",
      "\n",
      ".shift(periods=-1)\n",
      "* lagged data\n",
      "* 1 period back in time\n",
      "\n",
      "google['lagged']=google.price.shift(periods=-1)\n",
      "\n",
      "calculating one-period percent change\n",
      "\n",
      "google['change']=google.price.div(google.shifted)\n",
      "\n",
      "Xt/Xt-1\n",
      "\n",
      "google['return'] = google.change.sub(1).mul(100)\n",
      "\n",
      "\n",
      ".diff() built in time-series change\n",
      "\n",
      "google['diff'] = google.price.diff()\n",
      "\n",
      "\n",
      ".pct_change : built in time-series % change\n",
      "\n",
      "google['pct_change'] = google.priced.pct_change().mul(100)\n",
      "\n",
      "google['return_3d'] = google.price.pct_change(period=3).mul(100)\n",
      "\n",
      "\n",
      "  > sample shift and lag\n",
      "\n",
      "# Import data here\n",
      "google = pd.read_csv('google.csv',parse_dates=['Date'], index_col='Date')\n",
      "\n",
      "# Set data frequency to business daily\n",
      "google = google.asfreq('B')\n",
      "\n",
      "# Create 'lagged' and 'shifted'\n",
      "google['lagged'] = google.Close.shift(periods=-90)\n",
      "google['shifted'] = google.Close.shift(periods=90)\n",
      "\n",
      "# Plot the google price series\n",
      "\n",
      "google.plot()\n",
      "\n",
      "    sample returns daily, monthly, and yearly\n",
      "\n",
      "# Create daily_return\n",
      "print(google.columns)\n",
      "google['daily_return'] = google.Close.pct_change()\n",
      "\n",
      "# Create monthly_return\n",
      "google['monthly_return'] = google.Close.pct_change(periods=30)\n",
      "\n",
      "# Create annual_return\n",
      "google['annual_return'] = google.Close.pct_change(periods=360)\n",
      "\n",
      "# Plot the result\n",
      "\n",
      "google.plot(subplots=True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "plt.show()\n",
      "\n",
      "    sample change in price   > 30 days\n",
      "\n",
      "# Created shifted_30 here\n",
      "yahoo['shifted_30'] = yahoo.price.shift(30)\n",
      "\n",
      "# Subtract shifted_30 from price\n",
      "yahoo['change_30'] = yahoo.price.sub(yahoo.shifted_30)\n",
      "\n",
      "# Get the 30-day price difference\n",
      "yahoo['diff_30'] = yahoo.price.diff(30)\n",
      "\n",
      "# Inspect the last five rows of price\n",
      "print(yahoo.tail())\n",
      "\n",
      "# Show the value_counts of the difference between change_30 and diff_30\n",
      "print(yahoo.change_30.sub(yahoo.diff_30).value_counts())\n",
      "\n",
      "   sample see the stock growth over the sp500\n",
      "\n",
      "# Create tickers\n",
      "tickers = ['MSFT', 'AAPL']\n",
      "\n",
      "# Import stock data here\n",
      "stocks = pd.read_csv('msft_aapl.csv', parse_dates=['date'], index_col='date')\n",
      "\n",
      "# Import index here\n",
      "sp500 = pd.read_csv('sp500.csv', parse_dates=['date'], index_col='date')\n",
      "\n",
      "# Concatenate stocks and index here\n",
      "data = pd.concat([stocks, sp500], axis=1).dropna()\n",
      "\n",
      "# Normalize data\n",
      "normalized = data.div(data.iloc[0]).mul(100)\n",
      "\n",
      "# Subtract the normalized index from the normalized stock prices, and plot the result\n",
      "normalized[tickers].sub(normalized['SP500'], axis=0).plot()\n",
      "plt.show()\n",
      "\n",
      "    normalize the data\n",
      "compare=compare.div(compare.iloc[0]).mul(100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  > compare stock growth\n",
      "\n",
      "# Import stock prices and index here\n",
      "stocks = pd.read_csv('nyse.csv',parse_dates=['date'],index_col='date')\n",
      "dow_jones = pd.read_csv('dow_jones.csv',parse_dates=['date'],index_col='date')\n",
      "\n",
      "# Concatenate data and inspect result here\n",
      "data = pd.concat([stocks,dow_jones],axis=1)\n",
      "print(data)\n",
      "\n",
      "# Normalize and plot your data here\n",
      "data=data.div(data.iloc[0]).mul(100)\n",
      "data.plot()\n",
      "plt.show()\n",
      "\n",
      "   > resampling   > changing the time series frequency\n",
      "\n",
      "DateTimeIndex : set & change freq using .asfreq()\n",
      "\n",
      "frequency conversion affects the data\n",
      "1. upsampling : fill or interpolate missing data\n",
      "2. downsampling: aggregate existing data\n",
      "\n",
      ".asfreq\n",
      ".reindex()\n",
      "\n",
      "\n",
      "dates = pd.date_range(start='2016', periods=4, freq='Q')\n",
      "\n",
      "data=range(1,5)\n",
      "\n",
      "quarterly = pd.Series(data=data, index=dates)\n",
      "print(quarterly)\n",
      "\n",
      "monthly=quarterly.asfreq('M')\n",
      "\n",
      "upsampling creates missing values\n",
      "\n",
      "monthly = monthly.to_frame('baseline')\n",
      "\n",
      "monthly['ffill'] = quarterly.asfreq('M', method='ffill')\n",
      "monthly['bfill'] = quarterly.asfreq('M', method='bfill')\n",
      "monthly['value']=quarterly.asfreq('M',fill_value=0)\n",
      "\n",
      "\n",
      "ffill is forward fill the nan\n",
      "bfill is backward fill the nan\n",
      "fill_value=0 is fill nan with 0\n",
      "\n",
      "dates = pd.date_range(start='2016',\n",
      "\tperiods=12,\n",
      "\tfreq='M')\n",
      "\n",
      "\n",
      "quarterly.reindex(dates)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   sample  asfreq with bfill and ffil\n",
      "\n",
      "create a summaried datetimeindex of quarterly\n",
      "then create a series by monthly. assign the data value of 1 and 2 to the two monthly dates\n",
      "\n",
      "reindex the monthly series with the weekly dates for the start and end month.\n",
      "\n",
      "# Set start and end dates\n",
      "start = '2016-1-1'\n",
      "end = '2016-2-29'\n",
      "\n",
      "# Create monthly_dates here\n",
      "monthly_dates = pd.date_range(start=start, end=end, freq='M')\n",
      "\n",
      "# Create monthly here\n",
      "monthly = pd.Series(data=[1,2], index=monthly_dates)\n",
      "print(monthly)\n",
      "\n",
      "# Create weekly_dates here\n",
      "weekly_dates = pd.date_range(start=start, end=end, freq='W')\n",
      "\n",
      "# Print monthly, reindexed using weekly_dates\n",
      "print(monthly.reindex(weekly_dates))\n",
      "print(monthly.reindex(weekly_dates, method='bfill'))\n",
      "print(monthly.reindex(weekly_dates, method='ffill'))\n",
      "\n",
      "\n",
      "  > sample   asfreq and bfill\n",
      "\n",
      "# Import data here\n",
      "data = pd.read_csv('unemployment.csv', parse_dates=['date'], index_col='date')\n",
      "\n",
      "# Show first five rows of weekly series\n",
      "print(data.asfreq('W').head())\n",
      "\n",
      "# Show first five rows of weekly series with bfill option\n",
      "print(data.asfreq('W', method='bfill').head())\n",
      "\n",
      "# Create weekly series with ffill option and show first five rows\n",
      "weekly_ffill = data.asfreq('W', method='ffill')\n",
      "print(weekly_ffill.head())\n",
      "\n",
      "# Plot weekly_fill starting 2015 here \n",
      "weekly_ffill.loc['2015':].plot()\n",
      "plt.show()\n",
      "\n",
      "        >resample method\n",
      "\n",
      "resample() follows a method similar to groupby()\n",
      "\n",
      "#groups data witin resampling period and applies one or several methods to each group\n",
      "\n",
      "new data determined by offset - start, end, etc\n",
      "\n",
      "upsampling to fill from existing or interpolate values\n",
      "\n",
      "unemployment data is reported the first day of the calendar month\n",
      "1. date\n",
      "2. unrate\n",
      "\n",
      "unrate=pd.read_csv('unrate.csv', parse_dates['Date'], index_col='Date')\n",
      "\n",
      "unrate.info()\n",
      "\n",
      "\n",
      "Resample period & frequency offsets\n",
      "M - calendar month end\n",
      "MS  calendar month start\n",
      "BM  business month end\n",
      "BMS - business month start\n",
      "\n",
      "upsampling there will be more resampling periods than data points. (fill or interpolate)\n",
      "\n",
      "downsampling there are more data points than resampling periods. (aggregate)\n",
      "\n",
      "unrate.asfreq('MS').info()\n",
      "\n",
      "returns a datetimeindex\n",
      "\n",
      "unrate.resample('MS')\n",
      "\n",
      "returns a DatetimeIndexResampler\n",
      "\n",
      ".resample() returns data only when calling another method\n",
      "\n",
      "\n",
      "both approaches yield the same data\n",
      "\n",
      "gdp_1= gdp.resample('MS').ffill().add_suffix('_ffill')\n",
      "\n",
      "gdp_2=gdp.resample('MS').interpolate().add_suffix('_inter')\n",
      "\n",
      "\n",
      "\n",
      "pd.concat([df1,df2],axis=1) \n",
      "\n",
      "concatenates horizontally the row index\n",
      "\n",
      "\n",
      "  > sample ffill versus interpolated\n",
      "\n",
      "# Inspect data here\n",
      "print(monthly.info())\n",
      "\n",
      "# Create weekly dates\n",
      "weekly_dates = pd.date_range(\n",
      "    start=monthly.index.min(),\n",
      "    end=monthly.index.max(),\n",
      "    freq=\"W\"\n",
      ")\n",
      "\n",
      "# Reindex monthly to weekly data\n",
      "weekly = monthly.reindex(weekly_dates)\n",
      "\n",
      "# Create ffill and interpolated columns\n",
      "weekly['ffill'] =weekly.ffill()\n",
      "weekly['interpolated'] = weekly.UNRATE.interpolate()\n",
      "\n",
      "# Plot weekly\n",
      "\n",
      "weekly.plot()\n",
      "plt.show()\n",
      "\n",
      "    sample interpolate unemployment vs GDP\n",
      "\n",
      "# Import & inspect data here\n",
      "data = pd.read_csv('debt_unemployment.csv',parse_dates=['date'],index_col='date')\n",
      "print(data.info())\n",
      "\n",
      "# Interpolate and inspect here\n",
      "interpolated = data.interpolate()\n",
      "print(interpolated.info())\n",
      "\n",
      "# Plot interpolated data here\n",
      "\n",
      "interpolated.plot(secondary_y='Unemployment')\n",
      "plt.show()\n",
      "\n",
      "   downsampling & aggregation\n",
      "\n",
      "how to reduce the frequency of your time series\n",
      "\n",
      "hour to day\n",
      "day to month\n",
      "\n",
      "\n",
      "how to represent the existing values at the new date\n",
      "1. mean, median, last value\n",
      "\n",
      "ozone=pd.read_csv('ozone.csv',\n",
      "\tparse_dates=['date'],\n",
      "\tindex_col='date')\n",
      "\n",
      "ozone.info()\n",
      "\n",
      "ozone=ozone.resample('D').asfreq()\n",
      "\n",
      "ozone=ozone.resample('M').mean().head()\n",
      "\n",
      "ozone=ozone.resample('M\").median().head()\n",
      "ozone=ozone.resample('M).agg(['mean','std']).head()\n",
      "\n",
      "ozone=ozone.loc['2016':]\n",
      "ax=ozone.plot()\n",
      "monthly=ozone.resample('M').mean()\n",
      "monthly.add_suffix('_monthly').plot(ax=ax)\n",
      "\n",
      "\n",
      "  > sample    > weekly, monthly, and yearly\n",
      "\n",
      "# Import and inspect data here\n",
      "ozone = pd.read_csv('ozone.csv', parse_dates=['date'], index_col='date')\n",
      "ozone.info();\n",
      "\n",
      "# Calculate and plot the weekly average ozone trend\n",
      "ozone.resample('W').mean().plot();\n",
      "plt.show()\n",
      "\n",
      "# Calculate and plot the monthly average ozone trend\n",
      "ozone.resample('M').mean().plot();\n",
      "plt.show();\n",
      "\n",
      "# Calculate and plot the annual average ozone trend\n",
      "ozone.resample('A').mean().plot();\n",
      "plt.show();\n",
      "\n",
      "  > sample  > monthly average\n",
      "\n",
      "# Import and inspect data here\n",
      "stocks = pd.read_csv('stocks.csv',parse_dates=['date'],index_col='date')\n",
      "\n",
      "print(stocks.info())\n",
      "\n",
      "monthly_average = stocks.resample('M').mean()\n",
      "\n",
      "monthly_average.plot(subplots=True)\n",
      "plt.show()\n",
      "\n",
      "   > plot the gdp growth with the dija prices\n",
      "\n",
      "# Import and inspect gdp_growth here\n",
      "gdp_growth = pd.read_csv('gdp_growth.csv',parse_dates=['date'],index_col='date')\n",
      "\n",
      "\n",
      "# Import and inspect djia here\n",
      "djia = pd.read_csv('djia.csv',parse_dates=['date'],index_col='date')\n",
      "\n",
      "\n",
      "# Calculate djia quarterly returns here \n",
      "djia_quarterly = djia.resample('QS').first()\n",
      "djia_quarterly_return = djia_quarterly.pct_change().mul(100)\n",
      "\n",
      "# Concatenate, rename and plot djia_quarterly_return and gdp_growth here \n",
      "data = pd.concat([gdp_growth,djia_quarterly_return],axis=1).plot()\n",
      "\n",
      "plt.show()\n",
      "\n",
      "   >sample squeeze\n",
      "squeeze removes single dimension entries from the shape of the array\n",
      "\n",
      "# Import data here\n",
      "sp500 = pd.read_csv('sp500.csv',parse_dates=['date'],index_col='date')\n",
      "\n",
      "sp500.info()\n",
      "\n",
      "# Calculate daily returns here\n",
      "daily_returns = sp500.squeeze().pct_change().mul(100)\n",
      "\n",
      "# Resample and calculate statistics\n",
      "stats = daily_returns.resample('M').agg(['mean','median','std'])\n",
      "\n",
      "# Plot stats here\n",
      "stats.plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >.Rolling windows functions with pandas\n",
      "\n",
      "windows operate on sub periods of your time series\n",
      "\n",
      "calculate metrics for sub periods inside the window\n",
      "\n",
      "create a new time series of metrics\n",
      "1. rolling: same size, sliding (this video)\n",
      "2. expanding: contain all prior values\n",
      "\n",
      "data.rolling(window=30).mean()\n",
      "#last 30 business days\n",
      "\n",
      "\n",
      "data.rolling(window='30D').mean()\n",
      "#last 30 calendar days\n",
      "\n",
      "\n",
      "r90=data.rolling(window='90D').mean()\n",
      "\n",
      "google.join(r90.add_suffix('_mean_90')).plot()\n",
      "\n",
      "\n",
      "r360=data.rolling(window='360D').mean()\n",
      "\n",
      "nio_rolling_360=nio.rolling(window=360).mean()\n",
      "nio=nio.join(nio_rolling_360.add_suffix('_Mean_360')) #.add_suffix('_mean_360')\n",
      "\n",
      "watch the 90 day average versus the 360 day average\n",
      "\n",
      "r360=data['price'].rolling(window='360D').mean()\n",
      "\n",
      "data['mean360']=r360\n",
      "\n",
      "data.plot()\n",
      "\n",
      "\n",
      "r90 = data.price.rolling('90D').agg(['mean','std'])\n",
      "\n",
      "r90.plot(subplots=True)\n",
      "\n",
      "rolling=data.google.rolling('360D')\n",
      "q10= rolling.quantile(0.1).to_frame('q10')\n",
      "median=rolling.median().to_frame('median')\n",
      "q90= rolling.quantile(0.9).to_frame('q90)\n",
      "\n",
      "pd.concat([q10,median,q90], axis=1).plot()\n",
      "\n",
      "\n",
      " > sample 90 day vs 360 day moving average\n",
      "\n",
      "# Import and inspect ozone data here\n",
      "data = pd.read_csv('ozone.csv',parse_dates=['date'],index_col='date')\n",
      "print(data.info())\n",
      "\n",
      "# Calculate 90d and 360d rolling mean for the last price\n",
      "data['90D'] = data.Ozone.rolling(window='90D').mean()\n",
      "data['360D'] = data.Ozone.rolling(window='360D').mean()\n",
      "\n",
      "# Plot data\n",
      "data.plot()\n",
      "plt.show()\n",
      "\n",
      "   > 360 rolling window\n",
      "\n",
      "# Import and inspect ozone data here\n",
      "data = pd.read_csv('ozone.csv',parse_dates=['date'],index_col='date').dropna()\n",
      "\n",
      "# Calculate the rolling mean and std here\n",
      "rolling_stats = data.Ozone.rolling(window=360).agg(['mean','std'])\n",
      "\n",
      "# Join rolling_stats with ozone data\n",
      "stats = pd.concat([data,rolling_stats],axis=1)\n",
      "\n",
      "# Plot stats\n",
      "stats.plot()\n",
      "plt.show()\n",
      "\n",
      "    add quantile (10%, 50%, 90%)\n",
      "\n",
      "# Resample, interpolate and inspect ozone data here\n",
      "data = data.resample('D').interpolate()\n",
      "\n",
      "data.info()\n",
      "\n",
      "# Create the rolling window\n",
      "rolling = data.rolling(window=360)['Ozone']\n",
      "\n",
      "# Insert the rolling quantiles to the monthly returns\n",
      "data['q10'] = rolling.quantile(0.1).to_frame('q10)')\n",
      "data['q50'] = rolling.quantile(0.5).to_frame('q50')\n",
      "data['q90'] = rolling.quantile(0.9).to_frame('q90')\n",
      "\n",
      "# Plot the data\n",
      "data.plot()\n",
      "plt.show()\n",
      "\n",
      "    expanding window functions with pandas\n",
      "\n",
      ".expanding()\n",
      "1. cumsum()\n",
      "2. cumprod()\n",
      "3. cummin()\n",
      "4. cummax()\n",
      "\n",
      "\n",
      "df=pd.DataFrame({'data':range(5)})\n",
      "df['expanding sum']=df.data.expanding().sum()\n",
      "df['cumulative sum']=df.data.cumsum()\n",
      "\n",
      "\n",
      "#period return\n",
      "pr= data.sp500.pct_change()\n",
      "pr_plus_one = pr.add(1)\n",
      "cumulative_return = pr_plus_one.cumprod().sub(1)\n",
      "cumulative_return.mul(100).plot()\n",
      "\n",
      "data['running_min']= data.SP500.expanding().min()\n",
      "data['running_max']=data.SP500.expanding().max()\n",
      "\n",
      "\n",
      "def multi_period_return(period_returns):\n",
      "\treturn np.prod(period_returns + 1) -1\n",
      "\n",
      "pr=data.SP500.pct_change()\n",
      "r=pr.rolling('360D').apply(multi_period_return)\n",
      "data['Rolling 1yr Return']=r.mul(100)\n",
      "\n",
      "data.plot(subplots=True)\n",
      "\n",
      "\n",
      "    sample Cumsum\n",
      "\n",
      "# Calculate differences\n",
      "differences = data.diff().dropna()\n",
      "print(differences)\n",
      "\n",
      "# Select start price\n",
      "start_price = data.first('D')\n",
      "\n",
      "# Calculate cumulative sum\n",
      "cumulative_sum = start_price.append(differences).cumsum()\n",
      "\n",
      "# Validate cumulative sum equals data\n",
      "print(cumulative_sum.equals(data))\n",
      "\n",
      "The .cumsum() method allows you to reconstruct the original data from the differences.\n",
      "\n",
      "\n",
      "    sample returns on investing a 1000 dollars\n",
      "\n",
      "# Define your investment\n",
      "investment = 1000\n",
      "\n",
      "# Calculate the daily returns here\n",
      "returns = data.pct_change()\n",
      "\n",
      "# Calculate the cumulative returns here\n",
      "returns_plus_one = returns.add(1)\n",
      "cumulative_return = returns_plus_one.cumprod()\n",
      "\n",
      "# Calculate and plot the investment return here \n",
      "cumulative_return.mul(investment).plot()\n",
      "plt.show()\n",
      "\n",
      "   rolling returns for multiple years\n",
      "\n",
      "# Import numpy\n",
      "import numpy as np\n",
      "\n",
      "# Define a multi_period_return function\n",
      "def multi_period_return(period_returns):\n",
      "    return np.prod(period_returns + 1) - 1\n",
      "    \n",
      "# Calculate daily returns\n",
      "daily_returns = data.pct_change()\n",
      "\n",
      "# Calculate rolling_annual_returns\n",
      "rolling_annual_returns = daily_returns.rolling('360D').apply(multi_period_return)\n",
      "\n",
      "# Plot rolling_annual_returns\n",
      "rolling_annual_returns.mul(100).plot();\n",
      "plt.show()\n",
      "\n",
      "   price simulation\n",
      "\n",
      "1. daily stock returns are hard to predict\n",
      "2. models often assume they are random in nature\n",
      "3. numpy allows you to generate random numbers\n",
      "\n",
      ".cumprod from random returns to prices\n",
      "\n",
      "generate random returns in the bell shaped distribution\n",
      "\n",
      "random selected actual sp500 returns\n",
      "\n",
      "to generate random numbers\n",
      "\n",
      "from numpy.random import normal, seed\n",
      "from scipy.stats import norm\n",
      "\n",
      "seed(42)\n",
      "\n",
      "random_returns= normal(loc=0, scale=0.01, size=1000)\n",
      "\n",
      "sns.distplot(random_returns, fit=norm, kde=False)\n",
      "\n",
      "\n",
      "  > create a random price path\n",
      "\n",
      "return_series=pd.Series(random_returns)\n",
      "random_prices=return_series.add(1).cumprod().sub(1)\n",
      "random_prices.mul(100).plot()\n",
      "\n",
      "data['returns']=data.SP500.pct_change()\n",
      "\n",
      "\n",
      "    random select from the sp500 dataset\n",
      "\n",
      "from numpy.random import choice\n",
      "\n",
      "sample= data.returns.dropna()\n",
      "n_obs = data.returns.count()\n",
      "\n",
      "random_walk = choice(sample, size=n_obs)\n",
      "\n",
      "random_walk = pd.Series(random_walk, index=sample.index)\n",
      "random_walk.head()\n",
      "\n",
      "start = sp500['Open'].first('D')\n",
      "\n",
      "sp500_random = start.append(random_walk.add(1))\n",
      "sp500_random.head()\n",
      "\n",
      "\n",
      "     sample 2500 random prices\n",
      "\n",
      "# Set seed here\n",
      "seed(42)\n",
      "\n",
      "# Create random_walk\n",
      "random_walk = normal(loc=.001, scale=.01, size=2500)\n",
      "\n",
      "# Convert random_walk to pd.series\n",
      "random_walk = pd.Series(random_walk)\n",
      "\n",
      "# Create random_prices\n",
      "random_prices = random_walk.add(1).cumprod()\n",
      "\n",
      "   sample plot random walk distribution\n",
      "\n",
      "# Set seed here\n",
      "\n",
      "seed(42)\n",
      "# Calculate daily_returns here\n",
      "daily_returns = fb.pct_change().dropna()\n",
      "\n",
      "# Get n_obs\n",
      "n_obs = daily_returns.count()\n",
      "\n",
      "# Create random_walk\n",
      "random_walk = choice(daily_returns,size=n_obs)\n",
      "\n",
      "# Convert random_walk to pd.series\n",
      "random_walk = pd.Series(random_walk)\n",
      "\n",
      "# Plot random_walk distribution\n",
      "plt.clf()\n",
      "sns.distplot(random_walk)\n",
      "plt.show()\n",
      "\n",
      "  > sample random walk and random price\n",
      "\n",
      "# Select fb start price here\n",
      "start = fb.price.first('D')\n",
      "\n",
      "# Add 1 to random walk and append to start\n",
      "random_walk = random_walk.add(1)\n",
      "random_price = start.append(random_walk)\n",
      "\n",
      "# Calculate cumulative product here\n",
      "random_price = random_price.cumprod()\n",
      "\n",
      "# Insert into fb and plot\n",
      "fb['random'] = random_price\n",
      "fb.plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   relationships between time series correlation\n",
      "\n",
      "correlation is the linear relationship between two variables\n",
      "\n",
      "correlation is import for prediction and risk management\n",
      "\n",
      "correlation coefficient looks at the pairwise movement of two variables around their averages\n",
      "\n",
      "covariant\n",
      "\n",
      "varies between -1 and 1\n",
      "\n",
      "data=pd.read_csv('assets.csv', parse_dates=['date'],index_col='date')\n",
      "\n",
      "data=data.dropna().info()\n",
      "\n",
      "daily_returns = data.pct_change()\n",
      "\n",
      "sns.jointplot(x='sp500', y='nasdaq', data=data_returns)\n",
      "\n",
      "\n",
      "the closer the pearson coefficient is to 1 the more correlated the data\n",
      "\n",
      "sns.jointplot(x='sp500', y='bonds', data=data_returns)\n",
      "\n",
      "\n",
      "    calculating correlation\n",
      "\n",
      "correlations = returns.corr()\n",
      "\n",
      "sns.heatmap(correlations,annot=True)\n",
      "\n",
      "\n",
      "  > sample\n",
      "aapl\n",
      "amzn\n",
      "ibm\n",
      "wmt\n",
      "xom\n",
      "\n",
      "# Inspect data here\n",
      "print(data.info())\n",
      "\n",
      "# Calculate year-end prices here\n",
      "annual_prices = data.resample('A').last()\n",
      "\n",
      "# Calculate annual returns here\n",
      "annual_returns = annual_prices.pct_change()\n",
      "\n",
      "# Calculate and print the correlation matrix here\n",
      "correlations = annual_returns.corr()\n",
      "print(correlations)\n",
      "\n",
      "# Visualize the correlations as heatmap here\n",
      "\n",
      "sns.heatmap(correlations,annot=True)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "apple and ibm correlate the highest\n",
      "\n",
      "       >select index components\n",
      "\n",
      "case study\n",
      "1. components weighted by market capitalization\n",
      "2. capitalization = share price * number of shares => market value\n",
      "\n",
      "\n",
      "listing\n",
      "1.stock symbol\n",
      "2. company name\n",
      "3. last sale\n",
      "4. market capitalization\n",
      "5. ipo year\n",
      "6. sector\n",
      "7. industry\n",
      "\n",
      "nyse = pd.read_excel('listings.xlsx' sheetname='nyse', na_values='n/a')\n",
      "\n",
      "nyse.set_index('Stock Symbol', inplace=True)\n",
      "\n",
      "nyse.dropna(subset=['Sector'], inplace=True)\n",
      "\n",
      "nyse['Market Capitalization'] /=1e6 #in million usd\n",
      "\n",
      "components = nyse.groupby(['Sector'])['Market Capitalization'].nlargest(1)\n",
      "\n",
      "tickers = components.index.get_level_values('Stock Symbol')\n",
      "\n",
      "columns=['Company Name','Market Capitalization', 'Last Sale']\n",
      "\n",
      "component_info= nyse.loc[tickers, columns]\n",
      "pd.options.display.float_format='{:.2f}'.format\n",
      "\n",
      "\n",
      "data=pd.read_csv('stocks.csv', parse_dates=['Date'], index_col='Date').loc[:,tickers.tolist()]\n",
      "\n",
      "data.info()\n",
      "\n",
      "  > sample   set_index and filter\n",
      "\n",
      "# Inspect listings\n",
      "print(listings.info())\n",
      "\n",
      "# Move 'stock symbol' into the index\n",
      "listings.set_index('Stock Symbol')\n",
      "\n",
      "# Drop rows with missing 'sector' data\n",
      "listings.dropna(subset=['Sector'],inplace=True)\n",
      "\n",
      "# Select companies with IPO Year before 2019\n",
      "listings = listings[listings['IPO Year']<2019]\n",
      "\n",
      "# Inspect the new listings data\n",
      "print(listings)\n",
      "\n",
      "# Show the number of companies per sector\n",
      "print(listings.groupby('Sector').size())\n",
      "\n",
      "Sector\n",
      "Basic Industries         104\n",
      "Capital Goods            143\n",
      "Consumer Durables         55\n",
      "Consumer Non-Durables     89\n",
      "Consumer Services        402\n",
      "Energy                   144\n",
      "Finance                  351\n",
      "Health Care              445\n",
      "Miscellaneous             68\n",
      "Public Utilities         104\n",
      "Technology               386\n",
      "Transportation            58\n",
      "\n",
      "\n",
      "   > sample  get the largest market stock capitalization per sector\n",
      "\n",
      "# Select largest company for each sector\n",
      "components = listings.groupby('Sector')['Market Capitalization'].nlargest(1)\n",
      "\n",
      "# Print components, sorted by market cap\n",
      "print(components)\n",
      "\n",
      "# Select stock symbols and print the result\n",
      "tickers = components.index.get_level_values('Stock Symbol')\n",
      "print(tickers)\n",
      "\n",
      "# Print company name, market cap, and last price for each component \n",
      "info_cols =['Company Name','Market Capitalization', 'Last Sale']\n",
      "print(listings.loc[tickers,info_cols])\n",
      "\n",
      "Company Name  Market Capitalization  Last Sale\n",
      "Stock Symbol                                                                      \n",
      "Stock Symbol                                                                      \n",
      "AAPL                                  Apple Inc.             740,024.47     141.05\n",
      "AMZN                            Amazon.com, Inc.             422,138.53     884.67\n",
      "MA                       Mastercard Incorporated             123,330.09     111.22\n",
      "AMGN                                  Amgen Inc.             118,927.21     161.61\n",
      "UPS                  United Parcel Service, Inc.              90,180.89     103.74\n",
      "GS               Goldman Sachs Group, Inc. (The)              88,840.59     223.32\n",
      "RIO                                Rio Tinto Plc              70,431.48      38.94\n",
      "TEF                                Telefonica SA              54,609.81      10.84\n",
      "EL            Estee Lauder Companies, Inc. (The)              31,122.51      84.94\n",
      "ILMN                              Illumina, Inc.              25,409.38     173.68\n",
      "PAA           Plains All American Pipeline, L.P.              22,223.00      30.72\n",
      "CPRT                                Copart, Inc.              13,620.92      29.65\n",
      "\n",
      "\n",
      "  >  plot the returns\n",
      "\n",
      "Calculate the price return for the index components by dividing the last row of stock_prices by the first, subtracting 1 and multiplying by 100. Assign the result to price_return.\n",
      "\n",
      "# Print tickers\n",
      "print(tickers)\n",
      "\n",
      "# Import prices and inspect result\n",
      "stock_prices = pd.read_csv('stock_prices.csv', parse_dates=['Date'], index_col='Date')\n",
      "print(stock_prices.info())\n",
      "\n",
      "# Calculate the returns\n",
      "price_return = stock_prices.iloc[-1].div(stock_prices.iloc[0]).sub(1).mul(100)\n",
      "\n",
      "# Plot horizontal bar chart of sorted price_return   \n",
      "price_return.sort_values().plot(kind='barh', title='Stock Price Returns')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "price_return.AAPL    278.868171\n",
      "AMGN    153.309078\n",
      "AMZN    460.022405\n",
      "CPRT    204.395604\n",
      "EL      215.162752\n",
      "GS       38.346429\n",
      "ILMN    319.116203\n",
      "MA      302.063863\n",
      "PAA      19.592593\n",
      "RIO     -31.358201\n",
      "TEF     -67.775832\n",
      "UPS      97.043658\n",
      "\n",
      "   > build the value weighted index\n",
      "\n",
      "1. number of shares\n",
      "2. stock price series\n",
      "\n",
      "aggregate market value per period\n",
      "\n",
      "normalize the index to start at 100\n",
      "\n",
      "components\n",
      "1. stock symbol\n",
      "2. company name\n",
      "3. market capitalization\n",
      "4. last price\n",
      "\n",
      "shares= components['Market Capitalization'].div(components['Last Sale'])\n",
      "\n",
      "stock_prices = pd.read_csv('stock_prices.csv', parse_dates=['Date'], index_col='Date')\n",
      "\n",
      "\n",
      "Market capitalization = Number of shares * share price\n",
      "\n",
      "market_cap_series = data.mul(no_shares)\n",
      "\n",
      "using the first day of the month with the last day of the month for the ticker symbols\n",
      "\n",
      "\n",
      "market_cap_series.first('D').append(market_cap_series.last('D'))\n",
      "\n",
      "agg_mcap= market_cap_series.sum(axis=1) #sum each row for total market cap\n",
      "agg_mcap(title='Aggregate Market Cap')\n",
      "\n",
      "\n",
      "index= agg_mcap.div(agg_mcap.iloc[0]).mul(100) #divide by 1st value\n",
      "index.plot(title='Market-Cap Weighted Index')\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "tickers=['RIO', 'ILMN', 'CPRT', 'EL', 'AMZN', 'PAA', 'GS', 'AMGN', 'MA', 'TEF', 'AAPL', 'UPS']\n",
      "\n",
      "\n",
      "components = listings.loc[tickers, ['Market Capitalization', 'Last Sale']]\n",
      "\n",
      "# Print the first rows of components\n",
      "print(components.head())\n",
      "\n",
      "Market Capitalization  Last Sale\n",
      "Stock Symbol                                  \n",
      "RIO                    70431.476895      38.94\n",
      "ILMN                   25409.384000     173.68\n",
      "CPRT                   13620.922869      29.65\n",
      "EL                     31122.510011      84.94\n",
      "AMZN                  422138.530626     884.67\n",
      "\n",
      "# Select components and relevant columns from listings\n",
      "components = listings.loc[tickers, ['Market Capitalization', 'Last Sale']]\n",
      "\n",
      "# Print the first rows of components\n",
      "print(components.head())\n",
      "\n",
      "# Calculate the number of shares here\n",
      "no_shares = components['Market Capitalization'].div(components['Last Sale'])\n",
      "\n",
      "# Print the sorted no_shares\n",
      "print(no_shares.sort_values(ascending=False))\n",
      "\n",
      "Stock Symbol\n",
      "AAPL    5246.540000\n",
      "TEF     5037.804990\n",
      "RIO     1808.717948\n",
      "MA      1108.884100\n",
      "UPS      869.297154\n",
      "AMGN     735.890171\n",
      "PAA      723.404994\n",
      "AMZN     477.170618\n",
      "CPRT     459.390316\n",
      "GS       397.817439\n",
      "EL       366.405816\n",
      "ILMN     146.300000\n",
      "\n",
      "     sample get the first price and last price and plot it on a horzontal bar chart\n",
      "\n",
      "\n",
      "# Select the number of shares\n",
      "no_shares = components['Market Capitalization'].div(components['Last Sale'])\n",
      "print(no_shares.sort_values())\n",
      "\n",
      "# Create the series of market cap per ticker\n",
      "market_cap = stock_prices.mul(no_shares)\n",
      "\n",
      "# Select first and last market cap here\n",
      "first_value = market_cap.iloc[0]\n",
      "last_value = market_cap.iloc[-1]\n",
      "\n",
      "\n",
      "# Concatenate and plot first and last market cap here\n",
      "pd.concat([first_value,last_value],axis=1).plot(kind='barh')\n",
      "plt.show()\n",
      "\n",
      "?  >  normalize the market cap per trade day\n",
      "\n",
      "# Aggregate and print the market cap per trading day\n",
      "raw_index = market_cap_series.sum(axis=1)\n",
      "print(raw_index)\n",
      "\n",
      "# Normalize the aggregate market cap here \n",
      "index = raw_index.div(raw_index.iloc[0]).mul(100)\n",
      "print(index)\n",
      "\n",
      "# Plot the index here\n",
      "index.plot(title='Market-Cap Weighted Index')\n",
      "plt.show()\n",
      "\n",
      "Market capitalization series\n",
      "\n",
      "AAPL           AMGN           AMZN         CPRT            EL  ...             MA           PAA            RIO            TEF            UPS\n",
      "Date                                                                              ...                                                                          \n",
      "2010-01-04  160386.7278   42475.580670   63893.145750  2090.225938   8892.669154  ...   28476.143688  19531.934838  101342.466626  143829.332465   50575.708420\n",
      "2010-01-05  160701.5202   42107.635585   64270.110538  2090.225938   8859.692631  ...   28398.521801  19748.956336  102916.051241  143728.576365   50662.638135\n",
      "2010-01-06  158130.7156   41791.202811   63105.814230  2081.038131   8885.341038  ...   28343.077596  19741.722286  106063.220471  142217.234868   50288.840359\n",
      "2010-01-07  157815.9232   41408.539922   62032.180340  2067.256422   8998.926841  ...   28154.567299  19502.998638  106081.307650  139799.088473   49906.349611\n",
      "2010-01-08  158865.2312   41776.485008   63711.820915  2076.444228   9035.567423  ...   28165.656140  19568.105088  107256.974316  138892.283574   52305.609756\n",
      "2010-01-11  157501.1308   41960.457550   62180.103232  2067.256422   9119.840760  ...   27699.924818  19531.934838  106316.440983  135869.600580   54609.247214\n",
      "2010-01-12  155664.8418   41231.926281   60767.678202  2053.474713   9079.536120  ...   27688.835977  19206.402591  103169.271754  134862.039582   54244.142410\n",
      "2010-01-13  157868.3886   41599.871367   61607.498490  2035.099100   9152.817284  ...   28409.610642  19314.913340  105773.825599  136272.624980   53957.274349\n",
      "2010-01-14  156976.4768   41327.592003   60767.678202  2053.474713   9141.825109  ...   28886.430805  19358.317639  107546.369188  135768.844481   54070.282979\n",
      "2010-01-15  154353.2068   41393.822119   60667.472373  2021.317390   9116.176702  ...   29119.296466  19850.233035  105647.215343  132947.673686   53835.572747\n",
      "2010-01-19  161173.7088   42350.479341   60891.742563  2039.693003   9160.145400  ...   29363.250968  20248.105782  108432.640983  135315.442031   54113.747837\n",
      "\n",
      "[1761 rows x 12 columns]\n",
      "\n",
      "\n",
      "      >evaluating index performance\n",
      "\n",
      "index return\n",
      "1. total index return\n",
      "2. contribution by component\n",
      "\n",
      "performance vs benchmark\n",
      "1. total period return\n",
      "2. rolling returns for sub periods\n",
      "\n",
      "agg_market_cap = market_cap_series.sum(axis=1)\n",
      "index=agg_market_cap.div(agg_market_cap.iloc[0]).mul(100)\n",
      "index.plot(title='Market-Cap Weighted Index')\n",
      "\n",
      "agg_market_cap.iloc[-1] - agg_market_cap.iloc[0]\n",
      "\n",
      "change = market_cap_series.first('D').append(market_cap_series.last('D'))\n",
      "change.diff().iloc[-1].sort_values()\n",
      "\n",
      "  > market-cap based weights\n",
      "\n",
      "market_cap = components['Market Capitalization']\n",
      "\n",
      "weights = market_cap.div(market_cap.sum())\n",
      "weights.sort_values().mul(100)\n",
      "\n",
      "#shows the percentage of the market capitalization\n",
      "\n",
      "index_return = (index.iloc[-1] / index.iloc[0] -1) * 100  \n",
      "\n",
      "about 14% \n",
      "\n",
      "weights_returns=weights.mul(index_returns)\n",
      "\n",
      "weighted_returns.sort_values().plot(kind='barh')\n",
      "\n",
      "\n",
      "     >Performance vs benchmark\n",
      "\n",
      "#convert the series to a dataframe\n",
      "\n",
      "data=index.to_frame('Index')\n",
      "\n",
      "data['SP500'] = pd.read_csv('sp500.csv', parse_dates=['Date'],index_col='Date')\n",
      "\n",
      "data.SP500 = data.SP500.div(data.SP500.iloc[0],axis=0).mul(100)\n",
      "\n",
      "def multi_period_return(r):\n",
      "\treturn(np.prod(r+1)-1)*100\n",
      "\n",
      "data.pct_change().rolling('30D'.apply(multi_period_return).plot()\n",
      "\n",
      "    >returns using index returns and market capitalizations\n",
      "\n",
      "# Calculate and print the index return here\n",
      "index_return = (index.iloc[-1]/index.iloc[0] - 1) * 100\n",
      "print(index_return)\n",
      "\n",
      "# Select the market capitalization\n",
      "market_cap = components['Market Capitalization']\n",
      "\n",
      "# Calculate the total market cap\n",
      "total_market_cap = market_cap.sum()\n",
      "\n",
      "# Calculate the component weights, and print the result\n",
      "weights = market_cap.div(total_market_cap)\n",
      "print(weights.sort_values())\n",
      "\n",
      "# Calculate and plot the contribution by component\n",
      "weights.mul(index_return).sort_values().plot(kind='barh')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   > Add a rolling 360D window\n",
      "\n",
      "# Inspect data\n",
      "print(data.info())\n",
      "print(data.head())\n",
      "\n",
      "# Create multi_period_return function here\n",
      "def multi_period_return(r):\n",
      "    return(np.prod(r+1)-1)*100\n",
      "\n",
      "# Calculate rolling_return_360\n",
      "rolling_return_360 = data.pct_change().rolling('360D').apply(multi_period_return)\n",
      "\n",
      "# Plot rolling_return_360 here\n",
      "rolling_return_360.plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   index correlation\n",
      "\n",
      "daily return correlations\n",
      "calculate among all components\n",
      "visualize the result as a heatmap\n",
      "\n",
      "daily_returns = data.pct_change()\n",
      "correlations = daily_returns.cor()\n",
      "\n",
      "sns.heatmap(correlations, annot=True)\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Daily Return Correlations')\n",
      "\n",
      "\n",
      "correlations.to_excel(excel_writer='correlations.xls',\n",
      "\tsheet_name='correlations',\n",
      "\tstartrow=1,\n",
      "\tstartcol=1)\n",
      "\n",
      "\n",
      "\n",
      "data.index=data.index.date\n",
      "\n",
      "with pd.ExcelWriter('stock_data.xlsx') as writer:\n",
      "\tcorr.to_excel(excel_writer=writer, sheet_name='correlations')\n",
      "\tdata.to_excel(excel_writer=writer, sheet_name='prices')\n",
      "\tdata.pct_change().to_excel(writer, sheet_name='returns')\n",
      "\n",
      "\n",
      "    sample heatmap for correlations\n",
      "\n",
      "# Inspect stock_prices here\n",
      "print(stock_prices.info())\n",
      "\n",
      "# Calculate the daily returns\n",
      "returns = stock_prices.pct_change()\n",
      "\n",
      "# Calculate and print the pairwise correlations\n",
      "correlations = returns.corr()\n",
      "print(correlations)\n",
      "\n",
      "# Plot a heatmap of daily return correlations\n",
      "\n",
      "sns.heatmap(correlations, annot=True)\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Daily Return Correlations')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   sample   > compare your index with the returns of the sp500\n",
      "\n",
      "# Convert index series to dataframe here\n",
      "data = index.to_frame('Index')\n",
      "\n",
      "# Normalize djia series and add as new column to data\n",
      "djia = djia.div(djia.iloc[0]).mul(100)\n",
      "data['DJIA'] = djia\n",
      "\n",
      "# Show total return for both index and djia\n",
      "print(data.iloc[-1].div(data.iloc[0]).sub(1).mul(100))\n",
      "\n",
      "# Plot both series\n",
      "data.plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    to excel\n",
      "\n",
      "# Inspect index and stock_prices\n",
      "print(index)\n",
      "print(stock_prices)\n",
      "\n",
      "# Join index to stock_prices, and inspect the result\n",
      "data = index.join(stock_prices)\n",
      "\n",
      "\n",
      "# Create index & stock price returns\n",
      "returns = data.pct_change()\n",
      "\n",
      "# Export data and data as returns to excel\n",
      "with pd.ExcelWriter('data.xls') as writer:\n",
      "    data.to_excel(excel_writer=writer,sheet_name='data')\n",
      "    returns.to_excel(excel_writer=writer,sheet_name='returns')\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['heatmap']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
