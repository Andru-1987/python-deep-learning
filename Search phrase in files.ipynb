{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\svm (support vector machine).txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\svm (support vector machine).txt\n",
      " Classifications using linear SVC\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "wine=sklearn.datasets.load_wine()\n",
      "\n",
      "svm= LinearSVC()\n",
      "svm.fit(wine.data, wine.target)\n",
      "svm.score(wine.date,wine.target)\n",
      "\n",
      "\n",
      "\n",
      "svm use the hinge less and l2 regularization\n",
      "\n",
      "Support vectors: a training example not in the flat part of the loss diagram\n",
      "\n",
      "support vectors include incorrectly classified examples or correctly classified examples that are close to the boundary.\n",
      "\n",
      "support vectors contribute to the fit\n",
      "\n",
      "all data points matter to the fit\n",
      "\n",
      "the length from the support vector to the boundary line is called the margin\n",
      "\n",
      "all incorrectly classified vectors are support vectors\n",
      "\n",
      "Support vectors are defined as training examples that influence the decision boundary.\n",
      "\n",
      "\n",
      " \n",
      "# Train a linear SVM\n",
      "svm = SVC(kernel=\"linear\")\n",
      "svm.fit(X,y)\n",
      "plot_classifier(X, y, svm, lims=(11,15,0,6))\n",
      "\n",
      "# Make a new data set keeping only the support vectors\n",
      "print(\"Number of original examples\", len(X))\n",
      "print(\"Number of support vectors\", len(svm.support_))\n",
      "X_small = X[svm.support_]\n",
      "y_small = y[svm.support_]\n",
      "\n",
      "# Train a new SVM using only the support vectors\n",
      "svm_small = SVC(kernel=\"linear\")\n",
      "svm_small.fit(X_small,y_small)\n",
      "plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))\n",
      "\n",
      "\n",
      " Kernel SVMs rbf kernel\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svm=SVC(gamma=1)  #gamma controls the boundary smoothness\n",
      "svm=SVC(gamma=0.01)\n",
      "\n",
      " >\n",
      "\n",
      "# Instantiate an RBF SVM\n",
      "svm = SVC()\n",
      "\n",
      "# Instantiate the GridSearchCV object and run the search\n",
      "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
      "searcher = GridSearchCV(svm, parameters)\n",
      "searcher.fit(X_train, y_train)\n",
      "\n",
      "# Report the best parameters and the corresponding score\n",
      "print(\"Best CV params\", searcher.best_params_)\n",
      "print(\"Best CV accuracy\", searcher.best_score_)\n",
      "\n",
      "# Report the test accuracy using these best parameters\n",
      "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\n",
      "\n",
      " Comparing Logistic Regression and SVM\n",
      "\n",
      "logistic regression and support vector machines are a linear classifiers, both can be used with kernels, both can be extended to multiclass\n",
      "in logistic regression all data affect the fit\n",
      "in svm only support vectors affect fit\n",
      "logistic regression can use L1 and L2 penalty\n",
      "\n",
      "logistic regression\n",
      "1. linear_model.LogisticRegression\n",
      "2. C hyper parameter (inverse regularization strength)\n",
      "3. penalty (type of regularization)\n",
      "4. multi-class using multinomial or softmax and solver =lbfgs\n",
      "\n",
      "SVM\n",
      "1. svm.LinearSVC and svm.SVC #Kernel SVM\n",
      "2. C hyper parameter (inverse regularization strength)\n",
      "3. kernel (type of kernel)\n",
      "4. gamma (inverse RBF smoothness)\n",
      "\n",
      "SGDClassifier: stocastic gradient descent\n",
      "\n",
      "logreg= SGDClassifier(loss='log')\n",
      "linsvm= SGDClassifier(loss='hinge')\n",
      "\n",
      "regularization is called alpha and is like 1/c\n",
      "a big alpha means more regularization\n",
      "\n",
      " >\n",
      "\n",
      "# We set random_state=0 for reproducibility \n",
      "linear_classifier = SGDClassifier(random_state=0)\n",
      "\n",
      "# Instantiate the GridSearchCV object and run the search\n",
      "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
      "             'loss':['hinge','log'], 'penalty':['l1','l2']}\n",
      "searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n",
      "searcher.fit(X_train, y_train)\n",
      "\n",
      "# Report the best parameters and the corresponding score\n",
      "print(\"Best CV params\", searcher.best_params_)\n",
      "print(\"Best CV accuracy\", searcher.best_score_)\n",
      "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['svm']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
