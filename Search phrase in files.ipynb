{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\analysis for traffic stops by police officers.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\analysis for traffic stops by police officers.txt\n",
      "the dataset for stops by police officers in the state of rhode island.\n",
      "\n",
      "1. State\n",
      "2. stop_date\n",
      "3. stop_time\n",
      "4. county_name (contains nan values)\n",
      "5. driver_gender\n",
      "6. driver_race\n",
      "\n",
      "\n",
      "ri=pd.read_csv('police.csv')\n",
      "ri.isnull()\n",
      "\n",
      "ri.isnull().sum()\n",
      "county_name=91741\n",
      "\n",
      "ri.shape()\n",
      "output: 91741,15\n",
      "\n",
      "drop county_name column\n",
      "\n",
      "ri.drop('county_name',axis='columns', inplace=True)\n",
      "\n",
      ".dropna() : drops rows based on the presence of missing values.\n",
      "\n",
      "\n",
      "\n",
      "   sample  > dropping a column\n",
      "\n",
      "# Examine the shape of the DataFrame\n",
      "print(ri.shape)\n",
      "\n",
      "# Drop the 'county_name' and 'state' columns\n",
      "ri.drop(['county_name', 'state'], axis='columns', inplace=True)\n",
      "\n",
      "# Examine the shape of the DataFrame (again)\n",
      "print(ri.shape)\n",
      "\n",
      "   sample  > drop na subset\n",
      "\n",
      "# Count the number of missing values in each column\n",
      "print(ri.isnull().sum())\n",
      "\n",
      "# Drop all rows that are missing 'driver_gender'\n",
      "ri.dropna(subset=['driver_gender'], inplace=True)\n",
      "\n",
      "# Count the number of missing values in each column (again)\n",
      "print(ri.isnull().sum())\n",
      "\n",
      "# Examine the shape of the DataFrame\n",
      "print(ri.shape)\n",
      "\n",
      "\n",
      "removing columns and rows that will not be useful.\n",
      "\n",
      "\n",
      "   Examining the data types\n",
      "read_csv creates an inferred datatype\n",
      "\n",
      "print(ri.dtypes)\n",
      "\n",
      "dtype:\n",
      "1.object\n",
      "2.bool\n",
      "3.int\n",
      "4.float\n",
      "5.datetime\n",
      "6.category\n",
      "\n",
      "datatype affect opeations you can perform\n",
      "\n",
      "math operations can be performed on int and floats\n",
      "\n",
      "datetime \n",
      "category uses less memory and runs faster\n",
      "bool enables logical and mathematical operations\n",
      "\n",
      "\n",
      "apple\n",
      "1. date\n",
      "2. time\n",
      "3. price\n",
      "\n",
      "apple.price.dtype\n",
      "output dtype('O') means object\n",
      "\n",
      "apple['price']= apple.price.astype('float')\n",
      "\n",
      "\n",
      "  >Sample  > convert object dtype to bool\n",
      "\n",
      "# Examine the head of the 'is_arrested' column\n",
      "print(ri.is_arrested.dtype)\n",
      "\n",
      "# Change the data type of 'is_arrested' to 'bool'\n",
      "ri['is_arrested'] = ri.is_arrested.astype(bool)\n",
      "\n",
      "# Check the data type of 'is_arrested' \n",
      "print(ri.is_arrested.dtype)\n",
      "\n",
      "\n",
      "   sample  > value_counts and unique\n",
      "\n",
      "# Count the unique values in 'violation'\n",
      "print(ri['violation'].unique())\n",
      "\n",
      "# Express the counts as proportions\n",
      "print(ri['violation'].value_counts(normalize=True))\n",
      "\n",
      "['Equipment' 'Speeding' 'Other' 'Moving violation' 'Registration/plates'\n",
      " 'Seat belt']\n",
      "Speeding               48423\n",
      "Moving violation       16224\n",
      "Equipment              10921\n",
      "Other                   4409\n",
      "Registration/plates     3703\n",
      "Seat belt               2856\n",
      "Name: violation, dtype: int64\n",
      "\n",
      "  >normalized=True    output\n",
      "\n",
      "Speeding               0.559571\n",
      "Moving violation       0.187483\n",
      "Equipment              0.126202\n",
      "Other                  0.050950\n",
      "Registration/plates    0.042791\n",
      "Seat belt              0.033004\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "\n",
      "  >sample    women have more speeding violations\n",
      "\n",
      "# Create a DataFrame of female drivers\n",
      "female = ri[ri['driver_gender']=='F']\n",
      "\n",
      "# Create a DataFrame of male drivers\n",
      "male = ri[ri['driver_gender']=='M']\n",
      "\n",
      "print(female.violation.value_counts(normalize=True))\n",
      "\n",
      "# Compute the violations by male drivers (as proportions)\n",
      "print(male.violation.value_counts(normalize=True))\n",
      "\n",
      "output:\n",
      "\n",
      "Speeding               0.658114\n",
      "Moving violation       0.138218\n",
      "Equipment              0.105199\n",
      "Registration/plates    0.044418\n",
      "Other                  0.029738\n",
      "Seat belt              0.024312\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "Speeding               0.522243\n",
      "Moving violation       0.206144\n",
      "Equipment              0.134158\n",
      "Other                  0.058985\n",
      "Registration/plates    0.042175\n",
      "Seat belt              0.036296\n",
      "Name: violation, dtype: float64\n",
      "\n",
      "\n",
      "Filtering a dataframe using multiple conditions\n",
      "\n",
      "female = ri[ri.driver_gender=='F']\n",
      "female.shape\n",
      "\n",
      "or\n",
      "\n",
      "female = ri[\n",
      "(ri.driver_gender=='F') &\n",
      "(ri.is_arrested==True)\n",
      "]\n",
      "female.shape\n",
      "\n",
      "\n",
      "each condition is surround by parentheses and the & separates the conditions\n",
      "\n",
      "only female drivers who were arrested\n",
      "\n",
      "| represents the or condition\n",
      "\n",
      "|| represents the and condition\n",
      "\n",
      "\n",
      " sample  > filtering\n",
      "\n",
      "ri[(ri.driver_gender=='F') & (ri.violation=='Speeding')]\n",
      "\n",
      "\n",
      " > Sample  > Stop outcomes\n",
      "\n",
      "\n",
      "# Create a DataFrame of female drivers stopped for speeding\n",
      "female_and_speeding = ri[(ri.driver_gender=='F') & (ri.violation=='Speeding')]\n",
      "\n",
      "# Create a DataFrame of male drivers stopped for speeding\n",
      "male_and_speeding = ri[(ri.driver_gender=='M') & (ri.violation=='Speeding')]\n",
      "\n",
      "print(\"male\")\n",
      "# Compute the stop outcomes for female drivers (as proportions)\n",
      "print(female_and_speeding.stop_outcome.value_counts(normalize=True))\n",
      "print(\"female\")\n",
      "# Compute the stop outcomes for male drivers (as proportions)\n",
      "print(male_and_speeding.stop_outcome.value_counts(normalize=True))\n",
      "\n",
      "Output::  (95% of stops resulting in a ticket)\n",
      "\n",
      "male\n",
      "Citation            0.952192\n",
      "Warning             0.040074\n",
      "Arrest Driver       0.005752\n",
      "N/D                 0.000959\n",
      "Arrest Passenger    0.000639\n",
      "No Action           0.000383\n",
      "Name: stop_outcome, dtype: float64\n",
      "\n",
      "female\n",
      "Citation            0.944595\n",
      "Warning             0.036184\n",
      "Arrest Driver       0.015895\n",
      "Arrest Passenger    0.001281\n",
      "No Action           0.001068\n",
      "N/D                 0.000976\n",
      "Name: stop_outcome, dtype: float64\n",
      "\n",
      "\n",
      "   >Does gender affect the vehicles that are searched?\n",
      "\n",
      "\n",
      "ri.isnull().sum()\n",
      "\n",
      "true is 1\n",
      "false is 0\n",
      "then sum the rows\n",
      "\n",
      "the mean of a boolean series represents the percentage of True values\n",
      "\n",
      "ri.is_arrested.value_counts(normalized=True)\n",
      ".03\n",
      "ri.is_arrested.mean()\n",
      ".03\n",
      "\n",
      "\n",
      "find the unique districts\n",
      "\n",
      "ri.district.unique()\n",
      "\n",
      "print(df_sas[df_sas['District'].isin(districts)]['ArrestInt'].mean())\n",
      "\n",
      "\n",
      "print(df_sas.groupby('District')['ArrestInt'].mean())\n",
      "\n",
      "print(df_sas.groupby(['District','Ward'])['ArrestInt'].mean())\n",
      "\n",
      "  >Sample    search_conducted\n",
      "\n",
      "# Check the data type of 'search_conducted'\n",
      "print(ri['search_conducted'].dtype)\n",
      "\n",
      "# Calculate the search rate by counting the values\n",
      "print(ri['search_conducted'].value_counts(normalize=True))\n",
      "\n",
      "# Calculate the search rate by taking the mean\n",
      "print(ri.search_conducted.mean())\n",
      "\n",
      "\n",
      "output\n",
      "bool\n",
      "False    0.961785\n",
      "True     0.038215\n",
      "Name: search_conducted, dtype: float64\n",
      "0.0382153092354627\n",
      "\n",
      "\n",
      "  Sample  > female\n",
      "\n",
      "# Calculate the search rate for female drivers\n",
      "print(ri[ri.driver_gender=='F'].search_conducted.mean())\n",
      "\n",
      "output: 0.019180617481282074 (female)\n",
      "output: 0.04542557598546892 (male)\n",
      "\n",
      " >Sample  > groupby\n",
      "\n",
      "# Calculate the search rate for both groups simultaneously\n",
      "print(ri.groupby('driver_gender').search_conducted.mean())\n",
      "\n",
      " >Sample  > groupby multiple column\n",
      "\n",
      "print(ri.groupby(['driver_gender','violation']).search_conducted.mean())\n",
      "\n",
      "driver_gender  violation          \n",
      "F              Equipment              0.039984\n",
      "               Moving violation       0.039257\n",
      "               Other                  0.041018\n",
      "               Registration/plates    0.054924\n",
      "               Seat belt              0.017301\n",
      "               Speeding               0.008309\n",
      "\n",
      "M              Equipment              0.071496\n",
      "               Moving violation       0.061524\n",
      "               Other                  0.046191\n",
      "               Registration/plates    0.108802\n",
      "               Seat belt              0.035119\n",
      "               Speeding               0.027885\n",
      "Name: search_conducted, dtype: float64\n",
      "\n",
      "\n",
      "       >Gender affect frisking\n",
      "\n",
      "ri.search_type.value_counts(dropna=False)\n",
      "1. Incident to Arrest\n",
      "2. Probable cause\n",
      "3. Inventory\n",
      "4. Reasonable Suspicion\n",
      "5. Protective Frisk\n",
      "6. Incident to Arrest, Inventory\n",
      "7. Incident to Arrest, Probable Cause\n",
      "\n",
      "\n",
      "ri['inventory']=ri.search_type.str.contains('Inventory',na=False)\n",
      "\n",
      "na=False means return a false when it finds a missing value\n",
      "ri.inventory.sum()\n",
      "\n",
      "\n",
      "search=ri[ri.searched_conducted==True]\n",
      "searched.inventory.mean()\n",
      "\n",
      "\n",
      " >Sample    > search type count, frisk in the search_type\n",
      "\n",
      "# Count the 'search_type' values\n",
      "print(len(ri.search_type.unique()))\n",
      "\n",
      "# Check if 'search_type' contains the string 'Protective Frisk'\n",
      "ri['frisk'] = ri.search_type.str.contains('Protective Frisk', na=False)\n",
      "\n",
      "# Check the data type of 'frisk'\n",
      "print(ri['frisk'].dtype)\n",
      "\n",
      "# Take the sum of 'frisk'\n",
      "print(ri['frisk'].sum())\n",
      "\n",
      "\n",
      " >Sample  > search conduction    frisk average per gender\n",
      "\n",
      "# Create a DataFrame of stops in which a search was conducted\n",
      "searched = ri[ri.search_conducted == True]\n",
      "\n",
      "# Calculate the overall frisk rate by taking the mean of 'frisk'\n",
      "print(searched.frisk.mean())\n",
      "\n",
      "# Calculate the frisk rate for each gender\n",
      "print(searched.groupby(\"driver_gender\").frisk.mean())\n",
      "\n",
      "      Does the time of day affect arrest rate\n",
      "\n",
      "analyzing datetime data\n",
      "\n",
      "apple\n",
      "1. price\n",
      "2. volume (shares traded)\n",
      "3. date_and_time\n",
      "\n",
      "\n",
      "dt.month\n",
      "dt.week\n",
      "dt.dayofweek\n",
      "dt.hour\n",
      "\n",
      "apple.set_index('date_and_time', inplace=True)\n",
      "apple.index.month\n",
      "apple.price.mean()\n",
      "\n",
      "month_price=apple.groupby(apple.index.month).price.mean()\n",
      "\n",
      "monthly_price.plot()\n",
      "plt.xlabel('Month')\n",
      "plt.ylabel('Price')\n",
      "\n",
      "df_sas['Year']=pd.DatetimeIndex(df_sas['date']).year\n",
      "arrest_year=df_sas.groupby(['Year'])['ArrestInt'].sum()\n",
      "\n",
      "\n",
      "  >Sample   > arrest rate as a time of day\n",
      "\n",
      "# Calculate the overall arrest rate\n",
      "print(ri.is_arrested.mean())\n",
      "\n",
      "# Calculate the hourly arrest rate\n",
      "print(ri.groupby(ri.index.hour).is_arrested.mean())\n",
      "\n",
      "# Save the hourly arrest rate\n",
      "hourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean()\n",
      "\n",
      "\n",
      "  Sample  > plot arrest time\n",
      "\n",
      "# Import matplotlib.pyplot as plt\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Create a line plot of 'hourly_arrest_rate'\n",
      "hourly_arrest_rate.plot()\n",
      "\n",
      "# Add the xlabel, ylabel, and title\n",
      "plt.xlabel('Hour')\n",
      "plt.ylabel('Arrest Rate')\n",
      "plt.title('Arrest Rate by Time of Day')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "         Are drug related stops on the rise\n",
      "1. We will use a subplot to see how two variables change over time\n",
      "\n",
      "2. Resampling is when you change the frequency of the time series\n",
      "\n",
      "monthly_price=apple.price.resample('M').mean()\n",
      "\n",
      "resample by month\n",
      "\n",
      "the output is the last day of the month rather than a number\n",
      "\n",
      "monthly_volume=apple.volume.resample('M').mean()\n",
      "\n",
      "\n",
      "pd.concat([monthly_price,monthly_volume],axis='columns')\n",
      "\n",
      "concatenates along a specified axis\n",
      "\n",
      "monthly.plot(subplots=True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  Sample    drug related stops   resampling\n",
      "\n",
      "# Calculate the annual rate of drug-related stops\n",
      "print(ri.drugs_related_stop.resample('A').mean())\n",
      "\n",
      "# Save the annual rate of drug-related stops\n",
      "annual_drug_rate = ri.drugs_related_stop.resample('A').mean()\n",
      "\n",
      "# Create a line plot of 'annual_drug_rate'\n",
      "annual_drug_rate.plot(subplots=True)\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "  Sample  > concatenate the two columns\n",
      "\n",
      "# Calculate and save the annual search rate\n",
      "annual_search_rate = ri.search_conducted.resample('A').mean()\n",
      "\n",
      "# Concatenate 'annual_drug_rate' and 'annual_search_rate'\n",
      "annual = pd.concat([annual_drug_rate,annual_search_rate], axis='columns')\n",
      "\n",
      "# Create subplots from 'annual'\n",
      "annual.plot(subplots=True)\n",
      "\n",
      "# Display the subplots\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    What violations are caught in each district\n",
      "\n",
      "result=df_sas.groupby(['Year','Month','fbi_code'])['ArrestInt'].sum().reset_index()\n",
      "#print(top20.columns)\n",
      "mask=result['ArrestInt']>30\n",
      "fbi_codes=result[mask]['fbi_code'].unique()\n",
      "\n",
      "filter=df_sas['fbi_code'].isin(fbi_codes) \n",
      "fbi_codes=df_sas['fbi_code'].unique()\n",
      "\n",
      "arrest_breakdown=df_sas[filter].groupby(['Year','Month','fbi_code'])['ArrestInt'].sum().reset_index()\n",
      "keys=arrest_breakdown.keys()\n",
      "#print(arrest_breakdown)\n",
      "\n",
      "g = sns.factorplot(data=arrest_breakdown, x='Year', y='ArrestInt', \n",
      "                  hue='fbi_code',  kind='point',size=8,aspect=2)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "         cross tab\n",
      "\n",
      "table=pd.crosstab(ri.driver_race, ri_driver_gender)\n",
      "\n",
      "creates a pivot table building a frequency table\n",
      "\n",
      "ri[(ri.driver_race=='Asian') & (ri.driver_gender=='F')].shape\n",
      "\n",
      "\n",
      "range=table.loc['Asian':'Hispanic']\n",
      "\n",
      "range.plot(kind='bar')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " > stack bar plot\n",
      "\n",
      "range.plot(kind='bar', stacked=True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    Sample\n",
      "\n",
      "# Create a frequency table of districts and violations\n",
      "print(pd.crosstab(ri.district,ri.violation))\n",
      "\n",
      "# Save the frequency table as 'all_zones'\n",
      "all_zones = pd.crosstab(ri.district,ri.violation)\n",
      "\n",
      "# Select rows 'Zone K1' through 'Zone K3'\n",
      "print(all_zones.loc['Zone K1':'Zone K3'])\n",
      "\n",
      "# Save the smaller table as 'k_zones'\n",
      "k_zones = all_zones.loc['Zone K1':'Zone K3']\n",
      "\n",
      "k_zone.plot(kind='bar', stacked=True)\n",
      "plt.show()\n",
      "\n",
      "     How long might you be stopped\n",
      "\n",
      "apple\n",
      "date_and_time\n",
      "price\n",
      "volume\n",
      "change\n",
      "\n",
      "change when  change\n",
      "\n",
      "True if the price went up\n",
      "\n",
      "calculate how often the price went up taking the column mean\n",
      "\n",
      "\n",
      "Stefan Jansen\n",
      "https://www.amazon.com/dp/B08D9SP6MB/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1\n",
      "\n",
      "mapping = {'up':True, 'down':False}\n",
      "apple['is_up']=apple.chage.map(mapping)\n",
      "\n",
      "apple.is_up.mean()\n",
      "\n",
      "  >how often searches occur after each violation\n",
      "\n",
      "search_rate=ri.groupby('violation').search_conducted.mean()\n",
      "\n",
      "search_rate.plot(kind='bar')\n",
      "plt.show()\n",
      "\n",
      "search rate is on the y axis\n",
      "the violation is on the x axis\n",
      "\n",
      "\n",
      "search_rate.plot(kind='barh')\n",
      "plt.show()\n",
      "\n",
      "  sample    mapping\n",
      "\n",
      "# Print the unique values in 'stop_duration'\n",
      "print(ri.stop_duration.unique())\n",
      "\n",
      "# Create a dictionary that maps strings to integers\n",
      "mapping = {\n",
      "    '0-15 Min': 8,\n",
      "    '16-30 Min': 23,\n",
      "    '30+ Min': 45\n",
      "    }\n",
      "\n",
      "# Convert the 'stop_duration' strings to integers using the 'mapping'\n",
      "ri['stop_minutes'] = ri.stop_duration.map(mapping)\n",
      "\n",
      "# Print the unique values in 'stop_minutes'\n",
      "print(ri['stop_minutes'].unique())\n",
      "\n",
      "\n",
      "   sample  >  groupby    average  > sort\n",
      "\n",
      "\n",
      "# Calculate the mean 'stop_minutes' for each value in 'violation_raw'\n",
      "print(ri.groupby('violation_raw')['stop_minutes'].mean())\n",
      "\n",
      "# Save the resulting Series as 'stop_length'\n",
      "stop_length = ri.groupby('violation_raw')['stop_minutes'].mean()\n",
      "\n",
      "# Sort 'stop_length' by its values and create a horizontal bar plot\n",
      "stop_length.sort_values().plot(kind='barh')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   Exploring the weather dataset\n",
      "\n",
      "do weather conditions affect police behavior\n",
      "\n",
      "noaa : national centers for environmental information\n",
      "\n",
      "single station in rhode islands to give weather information\n",
      "\n",
      "weather = pd.read_csv('weather.csv')\n",
      "\n",
      "weather.head(3)\n",
      "\n",
      "TAVG, TMIN, TMAX: Temperature\n",
      "AWND. WSF2: Wind Speed (average, fastest wind speed in a 2 minute interval)\n",
      "WT01, WT022: Bad Weather conditions\n",
      "\n",
      "https://mesonet.agron.iastate.edu/request/download.phtml?network=ID_ASOS\n",
      "\n",
      "increased convinced the data is trustworthy\n",
      "\n",
      "\n",
      "station:three or four character site identifier\n",
      "valid:timestamp of the observation\n",
      "tmpf:Air Temperature in Fahrenheit, typically @ 2 meters\n",
      "dwpf:Dew Point Temperature in Fahrenheit, typically @ 2 meters\n",
      "relh:Relative Humidity in %\n",
      "drct:Wind Direction in degrees from north\n",
      "sknt:Wind Speed in knots \n",
      "p01i:One hour precipitation for the period from the observation time to the time of the previous hourly precipitation reset. This varies slightly by site. Values are in inches. This value may or may not contain frozen precipitation melted by some device on the sensor or estimated by some other means. Unfortunately, we do not know of an authoritative database denoting which station has which sensor.\n",
      "alti:Pressure altimeter in inches\n",
      "mslp:Sea Level Pressure in millibar\n",
      "vsby:Visibility in miles\n",
      "gust:Wind Gust in knots\n",
      "skyc1:Sky Level 1 Coverage\n",
      "skyc2:Sky Level 2 Coverage\n",
      "skyc3:Sky Level 3 Coverage\n",
      "skyc4:Sky Level 4 Coverage\n",
      "skyl1:Sky Level 1 Altitude in feet\n",
      "skyl2:Sky Level 2 Altitude in feet\n",
      "skyl3:Sky Level 3 Altitude in feet\n",
      "skyl4:Sky Level 4 Altitude in feet\n",
      "wxcodes:Present Weather Codes (space seperated)\n",
      "feel:Apparent Temperature (Wind Chill or Heat Index) in Fahrenheit\n",
      "ice_accretion_1hr:Ice Accretion over 1 Hour (inches)\n",
      "ice_accretion_3hr:Ice Accretion over 3 Hours (inches)\n",
      "ice_accretion_6hr:Ice Accretion over 6 Hours (inches)\n",
      "peak_wind_gust:Peak Wind Gust (from PK WND METAR remark) (knots)\n",
      "peak_wind_drct:Peak Wind Gust Direction (from PK WND METAR remark) (deg)\n",
      "peak_wind_time:Peak Wind Gust Time (from PK WND METAR remark)\n",
      "metar:unprocessed reported observation in METAR format\n",
      "\n",
      "weather[['AWND','WSF2']].describe()\n",
      "\n",
      "create a box plot\n",
      "weather[['AWND','WSF2']].plot(kind='box')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "take the fast wind speed minus the average wind speed\n",
      "\n",
      "weather['WDIFF']= weather.WSF2-weather.AWND\n",
      "\n",
      "weather.WDIFF.plot(kind='hist', bins =20)\n",
      "plt.show()\n",
      "\n",
      "  Sample   > box plot temperatures\n",
      "\n",
      "# Read 'weather.csv' into a DataFrame named 'weather'\n",
      "df=pd.read_csv('weather.csv')\n",
      "\n",
      "# Describe the temperature columns\n",
      "print(df[['TMIN','TAVG','TMAX']].describe())\n",
      "\n",
      "# Create a box plot of the temperature columns\n",
      "df[['TMIN','TAVG','TMAX']].plot(kind='box')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "  >Sample   histogram to confirm temperature range distribution\n",
      "\n",
      "# Create a 'TDIFF' column that represents temperature difference\n",
      "weather['TDIFF']=weather.TMAX- weather.TMIN\n",
      "\n",
      "# Describe the 'TDIFF' column\n",
      "print(weather['TDIFF'].describe())\n",
      "\n",
      "# Create a histogram with 20 bins to visualize 'TDIFF'\n",
      "weather['TDIFF'].plot(kind='hist',bins=20)\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "            >Categorizing the weather\n",
      "\n",
      "\n",
      "slicing columns of the original dataframe\n",
      "\n",
      "temp=weather.loc[:,'TAVG':'TMAX']\n",
      "\n",
      "\n",
      "temp.sum(axis='columns').head()  \n",
      "\n",
      "this sums all the columns\n",
      "\n",
      "ri.stop_duration.unique()\n",
      "\n",
      "mapping = {\n",
      "    '0-15 Min': 'short',\n",
      "    '16-30 Min': 'medium',\n",
      "    '30+ Min': 'long'\n",
      "    }\n",
      "\n",
      "# Convert the 'stop_duration' strings to integers using the 'mapping'\n",
      "ri['stop_length'] = ri.stop_duration.map(mapping)\n",
      "\n",
      "ri.stop_length.dtype\n",
      "outputs object type because it contains string data\n",
      "\n",
      "ri.stop_length.unique()\n",
      "\n",
      "\n",
      "cats=['short','medium','long']\n",
      "ri.stop_length.astype('category',ordered=True,categories=cats)\n",
      "\n",
      "\n",
      "1. stores more efficiently\n",
      "2. allows a logical order\n",
      "\n",
      "ri.stop_length.memory_usage(deep=True)  #memory used to store the column\n",
      "\n",
      "cats=['short','medium','long']\n",
      "\n",
      "ri[ri.stop_length>'short']\n",
      "output data with categories of medium or long stop_length\n",
      "\n",
      "ri.groupby('stop_length').is_arrested.mean()\n",
      "\n",
      "\n",
      "   Sample  > bad weather conditions\n",
      "\n",
      "# Copy 'WT01' through 'WT22' to a new DataFrame\n",
      "WT = weather.loc[:,'WT01':'WT22']\n",
      "\n",
      "# Calculate the sum of each row in 'WT'\n",
      "weather['bad_conditions'] = WT.sum(axis='columns')\n",
      "\n",
      "# Replace missing values in 'bad_conditions' with '0'\n",
      "weather['bad_conditions'] = weather.bad_conditions.fillna(0).astype('int')\n",
      "\n",
      "# Create a histogram to visualize 'bad_conditions'\n",
      "\n",
      "\n",
      "weather['bad_conditions'].plot(kind='hist')\n",
      "plt.show()\n",
      "# Display the plot\n",
      "\n",
      "   sample   > bad_conditions by category\n",
      "\n",
      "# Count the unique values in 'bad_conditions' and sort the index\n",
      "print(weather.bad_conditions.value_counts().sort_index())\n",
      "\n",
      "# Create a dictionary that maps integers to strings\n",
      "mapping = {0:'good', 1:'bad', 2:'bad', 3:'bad', 4:'bad', 5:'worse', 6:'worse', 7:'worse', 8:'worse', 9:'worse'}\n",
      "\n",
      "# Convert the 'bad_conditions' integers to strings using the 'mapping'\n",
      "weather['rating'] = weather.bad_conditions.map(mapping)\n",
      "\n",
      "# Count the unique values in 'rating'\n",
      "print(weather['rating'].unique())\n",
      "\n",
      "print(weather.rating.value_counts())\n",
      "\n",
      "output:\n",
      "bad      1836\n",
      "good     1749\n",
      "worse     432\n",
      "Name: rating, dtype: int64\n",
      "\n",
      "  Sample   > create a column as a category\n",
      "\n",
      "# Create a list of weather ratings in logical order\n",
      "cats=['good','bad','worse']\n",
      "\n",
      "# Change the data type of 'rating' to category\n",
      "weather['rating'] = weather.rating.astype('category', ordered=True, categories=cats)\n",
      "\n",
      "# Examine the head of 'rating'\n",
      "print(weather['rating'].head())\n",
      "\n",
      "\n",
      "       Merging Datasets\n",
      "\n",
      "reset_index returns the index to an autonumber\n",
      "\n",
      "\n",
      "high=high_low[['DATE','HIGH']]\n",
      "\n",
      "we only need the high column\n",
      "\n",
      "apple_high=pd.merge(left=apple, right=high, left_on='date', right_on='DATE', how=left)\n",
      "\n",
      "apple_high.set_index('date_and_time', inplace=True)\n",
      "\n",
      "\n",
      "\n",
      "<<<<<<<Sample  > reset the index to the autonumber,  extract the DATE and rating columns from the weather dataframe\n",
      "\n",
      "# Reset the index of 'ri'\n",
      "ri.reset_index(inplace=True)\n",
      "\n",
      "# Examine the head of 'ri'\n",
      "print(ri.head())\n",
      "\n",
      "# Create a DataFrame from the 'DATE' and 'rating' columns\n",
      "weather_rating=weather[['DATE','rating']]\n",
      "\n",
      "# Examine the head of 'weather_rating'\n",
      "print(weather_rating.head())\n",
      "\n",
      "\n",
      "   Sample  > merge columns on stop_date and date\n",
      "\n",
      "# Examine the shape of 'ri'\n",
      "print(ri.shape)\n",
      "\n",
      "# Merge 'ri' and 'weather_rating' using a left join\n",
      "ri_weather = pd.merge(left=ri, right=weather_rating, left_on='stop_date', right_on='DATE', how='left')\n",
      "\n",
      "# Examine the shape of 'ri_weather'\n",
      "print(ri_weather.shape)\n",
      "\n",
      "# Set 'stop_datetime' as the index of 'ri_weather'\n",
      "ri_weather.set_index('stop_datetime', inplace=True)\n",
      "\n",
      "\n",
      "https://datatofish.com/multiple-linear-regression-python/\n",
      "\n",
      "\n",
      " > weather and behavior\n",
      "\n",
      "search_rate = ri.groupby(['violation','driver_gender']).search_conducted.mean()\n",
      "\n",
      "multi-index\n",
      "pandas.core.indexes.multi.multiindex (second dimension)\n",
      "\n",
      "level=0\n",
      "level=1\n",
      "\n",
      "search_rate.loc['Equipment'] #level 0\n",
      "search_rate.loc['Equipment','M'] #level 1\n",
      "\n",
      "search_rate.unstack()\n",
      "\n",
      "results in a dataframe\n",
      "\n",
      "ri.pivot_table(index='violation',\n",
      "\tcolumns='driver_gender',\n",
      "\tvalues='search_conducted')\n",
      "\n",
      "  >Sample\n",
      "print(ri_weather.is_arrested.mean())\n",
      "0.0355690117407784\n",
      "\n",
      "overall arrest rate\n",
      "\n",
      "\n",
      "# Calculate the arrest rate for each 'rating'\n",
      "print(ri_weather.groupby('rating').is_arrested.mean())\n",
      "\n",
      "stop_minutes  \n",
      "rating                \n",
      "good     0.033715\n",
      "bad      0.036261\n",
      "worse    0.041667\n",
      "\n",
      "  Sample   > create a multi index series    violation and rating for is_arrested mean\n",
      "\n",
      "# Calculate the arrest rate for each 'violation' and 'rating'\n",
      "print(ri_weather.groupby(['violation','rating']).is_arrested.mean())\n",
      "\n",
      "violation            rating\n",
      "*Equipment            good      0.059007\n",
      "                     bad       0.066311\n",
      "                     worse     0.097357\n",
      "*Moving violation     good      0.056227\n",
      "                     bad       0.058050\n",
      "                     worse     0.065860\n",
      "*Other                good      0.076966\n",
      "                     bad       0.087443\n",
      "                     worse     0.062893\n",
      "*Registration/plates  good      0.081574\n",
      "                     bad       0.098160\n",
      "                     worse     0.115625\n",
      "Seat belt            good      0.028587\n",
      "                     bad       0.022493\n",
      "                     worse     0.000000\n",
      "Speeding             good      0.013405\n",
      "                     bad       0.013314\n",
      "                     worse     0.016886\n",
      "\n",
      "  sample  > slicing the multi-index series\n",
      "\n",
      "# Save the output of the groupby operation from the last exercise\n",
      "arrest_rate = ri_weather.groupby(['violation', 'rating']).is_arrested.mean()\n",
      "\n",
      "# Print the 'arrest_rate' Series\n",
      "print(arrest_rate)\n",
      "\n",
      "# Print the arrest rate for moving violations in bad weather\n",
      "print(arrest_rate.loc['Moving violation','bad'])\n",
      "\n",
      "# Print the arrest rates for speeding violations in all three weather conditions\n",
      "print(arrest_rate.loc['Speeding'])\n",
      "\n",
      "\n",
      "  sample  > unstack and pivot\n",
      "\n",
      "# Unstack the 'arrest_rate' Series into a DataFrame\n",
      "print(arrest_rate.unstack())\n",
      "\n",
      "# Create the same DataFrame using a pivot table\n",
      "print(ri_weather.pivot_table(index='violation', columns='rating', values='is_arrested'))\n",
      "\n",
      "practice answering questions using data\n",
      "\n",
      "https://openpolicing.stanford.edu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\create a datetimeindex.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\create a datetimeindex.txt\n",
      "   >Create a DateTimeIndex\n",
      "\n",
      "1. combine stop_date and stop_time into one column\n",
      "\n",
      "2. convert to datetime format\n",
      "\n",
      "\n",
      "apple.date.str.replace('/','-')\n",
      "\n",
      "combined=apple.date.str.cat(apple.time, sep=' ')\n",
      "#concatenate and separate with a space\n",
      "\n",
      "appled['date_and_time']=pd.to_datetime(combined)\n",
      "\n",
      "stored in a more standard way\n",
      "\n",
      "setting the index\n",
      "\n",
      "\n",
      "apple.set_index('date_and_time', inplace=True)\n",
      "\n",
      "when a column becomes an index, it is not longer considered a dataframe column\n",
      "\n",
      "  sample  > combining a date and time columns into a datetime column\n",
      "\n",
      "# Concatenate 'stop_date' and 'stop_time' (separated by a space)\n",
      "combined = ri.stop_date.str.cat(ri.stop_time, sep=' ')\n",
      "\n",
      "# Convert 'combined' to datetime format\n",
      "ri['stop_datetime'] = pd.to_datetime(combined)\n",
      "\n",
      "# Examine the data types of the DataFrame\n",
      "print(ri.dtypes)\n",
      "\n",
      "  >sample  > set_index\n",
      "\n",
      "# Set 'stop_datetime' as the index\n",
      "ri.set_index('stop_datetime', inplace=True)\n",
      "\n",
      "# Examine the index\n",
      "print(ri.index)\n",
      "\n",
      "# Examine the columns\n",
      "print(ri.columns)\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\data cleansing and sql data analysis.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\data cleansing and sql data analysis.txt\n",
      "\n",
      "\n",
      "# Print the information of ride_sharing\n",
      "print(ride_sharing.info())\n",
      "\n",
      "# Print summary statistics of user_type column\n",
      "print(ride_sharing['user_type'].describe())\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "# Print the information of ride_sharing\n",
      "print(ride_sharing.info())\n",
      "\n",
      "# Print summary statistics of user_type column\n",
      "print(ride_sharing['user_type'].describe())\n",
      "\n",
      "# Convert user_type from integer to category\n",
      "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
      "\n",
      "# Write an assert statement confirming the change\n",
      "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
      "\n",
      "# Print new summary statistics \n",
      "print(ride_sharing['user_type_cat'].describe())\n",
      "\n",
      "print(ride_sharing['user_type_cat'])\n",
      "\n",
      " Sample\n",
      "\n",
      "# Strip duration of minutes\n",
      "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes') \n",
      "\n",
      "# Convert duration to integer\n",
      "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
      "\n",
      "# Write an assert statement making sure of conversion\n",
      "assert ride_sharing['duration_time'].dtype == 'int'\n",
      "\n",
      "# Print formed columns and calculate average ride duration \n",
      "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
      "print(ride_sharing['duration_time'].mean())\n",
      "\n",
      "\n",
      " >Data range constraints\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.hist(movies['avg_rating'])\n",
      "plt.title('Average rating of movies (1-5)')\n",
      "\n",
      "or signups in the future\n",
      "\n",
      "import datetime as dt\n",
      "today_date=dt.date.today()\n",
      "user_signups[user_signups['subscription_date']> today_date]\n",
      "\n",
      " >dealing with out of range data\n",
      "1. drop the data\n",
      "2. set custom minimums and maximums\n",
      "3. treat a s missing and impute\n",
      "4. Set custom value depending on business assumptions\n",
      "\n",
      "\n",
      " > dropping data\n",
      "\n",
      "movies.drop(movies[movies['avg_rating']>5].index, inplace=True)\n",
      "\n",
      "assert movies['avg_rating'].max()<=5\n",
      "\n",
      "  Setting to a hard limit\n",
      "\n",
      "movie.loc[movies['avg_rating']>5, 'avg rating']=5\n",
      "\n",
      "\n",
      "user_signups.dtypes\n",
      "\n",
      "ouput: subscription_date object\n",
      "\n",
      "user_signups['subscription_date']= pd.to_datetime(user_signups['subscription_date'])\n",
      "\n",
      "assert user_signups['subscriptions_date'].dtype == 'datetime64[ns]'\n",
      "\n",
      "today_date=dt.date.today()\n",
      "\n",
      "assert user_signups.subscription_date.max().date() <= today_date\n",
      "\n",
      "\n",
      " >Sample   > convert to categorical\n",
      "\n",
      "# Convert tire_sizes to integer\n",
      "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
      "\n",
      "# Set all values above 27 to 27\n",
      "ride_sharing.loc[ride_sharing.tire_sizes > 27,'tire_sizes'] = 27\n",
      "\n",
      "# Reconvert tire_sizes back to categorical\n",
      "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
      "\n",
      "print(ride_sharing['tire_sizes'])\n",
      "# Print tire size description\n",
      "print(ride_sharing['tire_sizes'].describe())\n",
      "\n",
      " >Sample   > convert to datetime\n",
      "\n",
      "# Convert ride_date to datetime\n",
      "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])\n",
      "\n",
      "# Save today's date\n",
      "today = dt.date.today()\n",
      "\n",
      "# Set all in the future to today's date\n",
      "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
      "\n",
      "# Print maximum of ride_dt column\n",
      "print(ride_sharing['ride_dt'].max())\n",
      "\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "#Correct! Subsetting on metadata and keeping all duplicate records gives you a better bird-eye's view over your data and how to duplicate it!\n",
      "\n",
      "\n",
      "# Find duplicates\n",
      "duplicates = ride_sharing.duplicated('ride_id', keep=False)\n",
      "print(duplicates)\n",
      "\n",
      "# Sort your duplicated rides\n",
      "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
      "\n",
      "# Print relevant columns of duplicated_rides\n",
      "print(duplicated_rides[['ride_id','duration','user_birth_year']])\n",
      "\n",
      " Sample dropping duplicates\n",
      "\n",
      "# Drop complete duplicates from ride_sharing\n",
      "ride_dup = ride_sharing.drop_duplicates()\n",
      "\n",
      "# Create statistics dictionary for aggregation function\n",
      "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
      "\n",
      "# Group by ride_id and compute new statistics\n",
      "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
      "\n",
      "# Find duplicated values again\n",
      "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
      "duplicated_rides = ride_unique[duplicates == True]\n",
      "\n",
      "# Assert duplicates are processed\n",
      "assert duplicated_rides.shape[0] == 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\data cleansing text and categorical data problems.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\data cleansing text and categorical data problems.txt\n",
      "categories\n",
      "1. Predefined finite set of categories\n",
      "2. Text categories are converted to numeric representations\n",
      "\n",
      "how to treat the problems\n",
      "1. drop rows of data\n",
      "2. remap the categories\n",
      "3. infer the category\n",
      "\n",
      "study data\n",
      "name, birthday, blood_type\n",
      "\n",
      "awesome-public datasets\n",
      "https://github.com/awesomedata/awesome-public-datasets\n",
      "\n",
      "\n",
      "study_data= pd.read_csv('study.csv')\n",
      "\n",
      "\n",
      "anti joins (left join)\n",
      "what is a and not in b\n",
      "\n",
      "inner join\n",
      "what is both a and b\n",
      "\n",
      "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
      "\n",
      "print(inconsistent_categories)\n",
      "\n",
      "outputs {'Z+'}\n",
      "\n",
      "inconsistent_rows=study_data['blood_type'].isin(inconsistent_categories)\n",
      "\n",
      "#returns a boolean of true for inconsistent rows\n",
      "\n",
      "study_data[inconsistent_rows]\n",
      "\n",
      "consistent_data=study_data[~inconsistent_rows]\n",
      "\n",
      "  Sample   using unique to find categories\n",
      "\n",
      "# Print categories DataFrame\n",
      "print(categories)\n",
      "\n",
      "# Print unique values of survey columns in airlines\n",
      "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
      "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
      "print('Satisfaction: ', airlines['satisfaction'].unique(),\"\\n\")\n",
      "\n",
      " >Sample  > finding inconsistencies in the categories\n",
      "\n",
      "print(airlines['cleanliness'])\n",
      "print(categories.columns)\n",
      "inconsistent_categories1=set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
      "inconsistent_categories2=set(airlines['safety']).difference(categories['safety'])\n",
      "inconsistent_categories3=set(airlines['satisfaction']).difference(categories['satisfaction'])\n",
      "print(len(inconsistent_categories1),len(inconsistent_categories2),len(inconsistent_categories3))\n",
      "\n",
      "  Sample  > finding the rows with the inconsistent data\n",
      "\n",
      "# Find the cleanliness category in airlines not in categories\n",
      "inconsistent_categories1=set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
      "\n",
      "# Find rows with that category\n",
      "cat_clean_rows = airlines['cleanliness'].isin(inconsistent_categories1)\n",
      "\n",
      "# Print rows with inconsistent category\n",
      "print(airlines[cat_clean_rows])\n",
      "\n",
      "# Print rows with consistent categories\n",
      "print(airlines[~cat_clean_rows])\n",
      "\n",
      "       >What type of errors could we have\n",
      "\n",
      "1. inconsistent fields\n",
      "2. trailing white spaces\n",
      "\n",
      "Collapsing too many categories to few\n",
      "1. create new groups (0-20k) (20-40k) from contineous household income data\n",
      "2. mapping groups to new ones\n",
      "\n",
      "Capitalization:\n",
      "married or Married or UNMARRIED or unmarried\n",
      "\n",
      "marriage_status=demographics['marriage_status']\n",
      "marriage_status.value_counts()\n",
      "\n",
      "\n",
      "for a dataframe\n",
      "\n",
      "marriage_status.groupby('marriage_status').count()\n",
      "\n",
      "fix\n",
      "\n",
      "marriage_status['marriage_status']=marriage_status['marriage_status'].str.upper()\n",
      "\n",
      "leading spaces\n",
      "\n",
      "marriage_status['marriage_status']=marriage_status['marriage_status'].str.strip()\n",
      "\n",
      "\n",
      "   Collapsing data into categories\n",
      "\n",
      "ranges=[0, 200000,500000,np.inf]\n",
      "group_names=['0-200k','200k-500k','500k+']\n",
      "\n",
      "demographics['income_group']=pd.cut(demographics['household_income'], bins=ranges, labels=group_names)\n",
      "\n",
      "print(demographics[['income_group','household_income']]\n",
      "\n",
      "\n",
      "  Collapsing data into categories\n",
      "\n",
      "'Microsoft', 'MacOS', 'IOS', 'Android', 'Linus' are collasped into a category called 'operating sytems'\n",
      "\n",
      "mapping={'Microsoft':'DesktopOS','MacOS':'DesktopOS',\n",
      "'Linux':'DesktopOS','IOS':'MobileOS','Android':'MobileOS'}\n",
      "\n",
      "devices['operating_systems]=devices['operating_systems'].replace(mapping)\n",
      "\n",
      "\n",
      "  > Sample    lower and replace\n",
      "\n",
      "# Print unique values of both columns\n",
      "print(airlines['dest_region'].unique())\n",
      "print(airlines['dest_size'].unique())\n",
      "\n",
      "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
      "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
      "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
      "\n",
      "\n",
      "\n",
      "  Sample  > Strip\n",
      "\n",
      "# Print unique values of both columns\n",
      "print(airlines['dest_region'].unique())\n",
      "print(airlines['dest_size'].unique())\n",
      "\n",
      "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
      "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
      "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
      "\n",
      "# Remove white spaces from `dest_size`\n",
      "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
      "airlines['dest_region'] = airlines['dest_region'].str.strip()\n",
      "\n",
      "# Verify changes have been effected\n",
      "print(airlines['dest_region'].unique())\n",
      "print(airlines['dest_size'].unique())\n",
      "\n",
      "\n",
      "  >Sample  > using cut and remapping\n",
      "\n",
      "# Create ranges for categories\n",
      "label_ranges = [0, 60, 180, np.inf]\n",
      "label_names = ['short', 'medium', 'long']\n",
      "\n",
      "# Create wait_type column\n",
      "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
      "                                labels = label_names)\n",
      "\n",
      "# Create mappings and replace\n",
      "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
      "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
      "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
      "\n",
      "airlines['day_week'] = airlines['day'].replace(mappings)\n",
      "\n",
      "print(airlines.head())\n",
      "\n",
      "  Cleaning text data\n",
      "\n",
      "1. leading zeros on the phone number\n",
      "2. phone numbers with the incorrect length\n",
      "\n",
      "all phone numbers begin with 00\n",
      "and incorrect length phone numbers are replaced with nan\n",
      "\n",
      "phones['Phone Number']=phones['Phone Number'].str.replace(\"-\",\"\")\n",
      "\n",
      "digits = phones['Phone Number'].str.len()\n",
      "\n",
      "phones.loc[digits<10,\"Phone Number\"] = np.nan\n",
      "\n",
      "\n",
      "#assert if minimum phone number length is 10\n",
      "sanity_check=phone['Phone number'].str.len()\n",
      "assert sanity_check.min()>=10\n",
      "\n",
      "#any returns any records that are true\n",
      "assert phone['Phone Number'].str.contains(\"+|-\").any()==False\n",
      "\n",
      "\n",
      "  >Regular expressions\n",
      "\n",
      "#replace any character that is not a digit with nothing\n",
      "\n",
      "phones['Phone number']=phones['Phone number'].str.replace(r'\\D+','')\n",
      "\n",
      "  >Sample  > replace pattern with empty\n",
      "\n",
      "# Replace \"Dr.\" with empty string \"\"\n",
      "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
      "\n",
      "# Replace \"Mr.\" with empty string \"\"\n",
      "airlines['full_name'] = airlines['full_name'].str.replace('Mr.','')\n",
      "\n",
      "# Replace \"Miss\" with empty string \"\"\n",
      "airlines['full_name'] = airlines['full_name'].str.replace('Miss.','')\n",
      "\n",
      "# Replace \"Ms.\" with empty string \"\"\n",
      "airlines['full_name'] = airlines['full_name'].str.replace('Ms.','')\n",
      "\n",
      "# Assert that full_name has no honorifics\n",
      "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False\n",
      "\n",
      "\n",
      "  >Sample find strings with len > 40 and assert if not true\n",
      "\n",
      "# Store length of each row in survey_response column\n",
      "resp_length = airlines['survey_response'].str.len()\n",
      "\n",
      "# Find rows in airlines where resp_length > 40\n",
      "airlines_survey = airlines[resp_length > 40]\n",
      "\n",
      "# Assert minimum survey_response length is > 40\n",
      "assert airlines_survey['survey_response'].str.len().min() > 40\n",
      "\n",
      "# Print new survey_response column\n",
      "print(airlines_survey['survey_response'])\n",
      "\n",
      "\n",
      "          Uniformity\n",
      "1. dealing with missing data\n",
      "\n",
      "\n",
      "tempuratures in both celsius and fahrenheit\n",
      "weight in kilograms and stones\n",
      "date is short and long format\n",
      "money in dollars and euros\n",
      "\n",
      "\n",
      " >Fixing fahrenheit and celius\n",
      "\n",
      "plt.scatter(x='Date', y='Temperature', data=temperatures)\n",
      "plt.title('Temperature in Celsius ')\n",
      "plt.xlabel('Dates')\n",
      "plt.ylabel('Temperature in Celsius')\n",
      "plt.show()\n",
      "\n",
      "C=(F-32) x 5/9\n",
      "\n",
      "temp_fah=temperatures.loc[temperatures['Temperature']>40,'Temperature]\n",
      "\n",
      "temp_celsius=(temp_fah-32)*(5/9)\n",
      "\n",
      "temperatures.loc[temperatures['Temperature']>40,'Temperature]=temp_celsius\n",
      "\n",
      "assert temperatures['Temperature'].max()<40\n",
      "\n",
      "\n",
      " >Fixing Dates\n",
      "\n",
      "datetime is used to format dates\n",
      "\n",
      "pandas.to_datetime()\n",
      " \n",
      "%d-%m-%Y  25-12-2019\n",
      "%c December 25th 2019\n",
      "12-25-2019 %m-%d-%Y\n",
      "\n",
      "birthdays['Birthday']=pd.to_datetime(birthdays['Birthday']\n",
      "\t\t,infer_datetime_format=True,\n",
      "\t\terrors='coerce')\n",
      "\n",
      "NaT is Not a Date Time\n",
      "\n",
      "\n",
      "birthdays['Birthday]=birthdays['Birthday].dt.strftime(\"%d-%m-%Y\")\n",
      "\n",
      "\n",
      " >Sample  > converting euros to dollars\n",
      "\n",
      "# Find values of acct_cur that are equal to 'euro'\n",
      "acct_eu = banking['acct_cur'] == 'euro'\n",
      "\n",
      "# Convert acct_amount where it is in euro to dollars\n",
      "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 \n",
      "\n",
      "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
      "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
      "\n",
      "# Assert that only dollar currency remains\n",
      "assert banking['acct_cur'].unique() == 'dollar'\n",
      "\n",
      "\n",
      " >Sample   converting dates\n",
      "\n",
      "# Print the header of account_opened\n",
      "print(banking['account_opened'].head())\n",
      "\n",
      "# Convert account_opened to datetime\n",
      "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
      "                                           # Infer datetime format\n",
      "                                           infer_datetime_format = True,\n",
      "                                           # Return missing value for error\n",
      "                                           errors = 'coerce')\n",
      "\n",
      "# Get year of account opened\n",
      "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
      "\n",
      "# Print acct_year\n",
      "print(banking['acct_year'])\n",
      "\n",
      "   Cross field validation\n",
      "1. The challenge from merging data from different sources is data integrity.\n",
      "\n",
      "2. cross field validation is using multiple fields in a dataset to sanity check data integrity.\n",
      "\n",
      "3. economy_class+business_class+first_class=total_passengers\n",
      "\n",
      "sum_classes=flights[['economy_class','business_class','first_class']].sum(axis=1)\n",
      "\n",
      "passenger_equal=sum_classes==flights['total_passengers']\n",
      "\n",
      "inconsistent_pass=flights[~passenger_equal]\n",
      "consistent_pass=flights[passenger_equal]\n",
      "\n",
      "   birthday check\n",
      "import pandas as pd\n",
      "import datetime as dt\n",
      "\n",
      "users['Birthday']=pd.to_datetime(users['Birthday'])\n",
      "today=dt.date.today()\n",
      "\n",
      "age_manual=today.year - users['Birthday'].dt.year\n",
      "\n",
      "age_equal=age_manual==users['Age']\n",
      "\n",
      "inconsistent_age=users[~age_equal]\n",
      "consistent_age=users[age_equal]\n",
      "\n",
      "inconsistent data can be dropped, set to missing and impute\n",
      "and apply rules from domain knowlege\n",
      "\n",
      " >Sample   sum funds a,b,c,d and compare inv_amount for inconsistencies\n",
      "\n",
      "# Store fund columns to sum against\n",
      "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
      "\n",
      "# Find rows where fund_columns row sum == inv_amount\n",
      "inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']\n",
      "\n",
      "# Store consistent and inconsistent data\n",
      "consistent_inv = banking[inv_equ]\n",
      "inconsistent_inv = banking[~inv_equ]\n",
      "\n",
      "# Store consistent and inconsistent data\n",
      "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])\n",
      "\n",
      "  Sample   check birthday and age consistency\n",
      "\n",
      "# Store today's date and find ages\n",
      "today = dt.date.today()\n",
      "ages_manual = today.year - banking['birth_date'].dt.year\n",
      "\n",
      "# Find rows where age column == ages_manual\n",
      "age_equ = ages_manual == banking['age']\n",
      "\n",
      "# Store consistent and inconsistent data\n",
      "consistent_ages = banking[age_equ]\n",
      "inconsistent_ages = banking[~age_equ]\n",
      "\n",
      "       Completeness\n",
      "1. missing data is represented as na, nan, 0, ., or ...\n",
      "\n",
      "caused from a technical error or human error\n",
      "\n",
      "Temperature and co2\n",
      "\n",
      "#find missing data\n",
      "airquality.isna()\n",
      "\n",
      "airquality.isna().sum()\n",
      "\n",
      "import missingno as msno\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "msno.matrix(airquality)\n",
      "plt.show()\n",
      "\n",
      "missing=airquality[airquality['CO2'].isna()]\n",
      "complete=airquality[~airquality['CO2'].isna()]\n",
      "\n",
      "missing.describe\n",
      "\n",
      "\n",
      "sorted_airquality = airquality.sort_values(by='Temperature')\n",
      "msno.matrix(sorted_airquality)\n",
      "plt.show()\n",
      "\n",
      "co2 are lost for extremely low temperatures\n",
      "\n",
      "  >Missingness types\n",
      "1. Missing Completely at Random: No systematic relationship between a column's missing values and other or own values.\n",
      "\n",
      "2. Missing at Random: There is a systematic relationship between a column's missing values and other observed values.\n",
      "\n",
      "3. Missing not at Random: There is a systematic relationship between a column's missing values and unobserved values.\n",
      "\n",
      "1. Missing completely at Random (no relationship)\n",
      "2. Missing at Random (relationship with features)\n",
      "3. Missing not at random (systemtic relationship causing the missing data)\n",
      "\n",
      " >Dealing with missing data\n",
      "1. drop missing data\n",
      "2. impute with statistical measures (mean, median, mode)\n",
      "\n",
      "drop values\n",
      "airquality.dropna()\n",
      "\n",
      "impute\n",
      "airquality.fillna(mean)\n",
      "\n",
      "feed values if we have enough knowledge of the dataset\n",
      "airquality.fillna(custom)\n",
      "\n",
      "\n",
      "  >Sample   using msno to visual missing values\n",
      "\n",
      "# Print number of missing values in banking\n",
      "print(banking.isna().sum())\n",
      "\n",
      "# Visualize missingness matrix\n",
      "\n",
      "  Sample   sort by age and display the mnso matrix\n",
      "\n",
      "# Print number of missing values in banking\n",
      "print(banking.isna().sum())\n",
      "\n",
      "# Visualize missingness matrix\n",
      "msno.matrix(banking)\n",
      "plt.show()\n",
      "\n",
      "# Isolate missing and non missing values of inv_amount\n",
      "missing_investors = banking[banking['inv_amount'].isna()]\n",
      "investors = banking[~banking['inv_amount'].isna()]\n",
      "\n",
      "# Sort banking by age and visualize\n",
      "banking_sorted = banking.sort_values(by='age')\n",
      "msno.matrix(banking_sorted)\n",
      "plt.show()\n",
      "\n",
      "  >Sample  > imput\n",
      "\n",
      "# Drop missing values of cust_id\n",
      "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
      "\n",
      "# Compute estimated acct_amount\n",
      "acct_imp = banking_fullid['inv_amount']*5\n",
      "# Impute missing acct_amount with corresponding acct_imp\n",
      "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
      "\n",
      "# Print number of missing values\n",
      "print(banking_imputed.isna().sum())\n",
      "\n",
      "msno.matrix(banking)\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample > print missing inv_amount values\n",
      "\n",
      "# Print number of missing values in banking\n",
      "print(banking.isna().sum())\n",
      "\n",
      "# Visualize missingness matrix\n",
      "msno.matrix(banking)\n",
      "plt.show()\n",
      "\n",
      "# Isolate missing and non missing values of inv_amount\n",
      "missing_investors = banking[banking['inv_amount'].isna()]\n",
      "investors = banking[~banking['inv_amount'].isna()]\n",
      "\n",
      "missing.describe()\n",
      "\n",
      "      >Comparing strings\n",
      "\n",
      "from fuzzywuzzy import fuzz\n",
      "\n",
      "fuzz.WRatio('Reeding','Reading')\n",
      "fuzz.WRatio('Houston Rockets','Rockets')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\datetime.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\datetime.txt\n",
      "from datetime import date\n",
      "\n",
      "two_hurricane_dates=[date(2016,10,7), date(2017,6,21)]\n",
      "\n",
      "print (two_hurricane_dates[0].weekday())\n",
      "\n",
      "   sample  >  date\n",
      "\n",
      "# Import date from datetime\n",
      "from datetime import date\n",
      "\n",
      "# Create a date object\n",
      "hurricane_andrew = date(1992, 8, 24)\n",
      "\n",
      "# Which day of the week is the date?\n",
      "print(hurricane_andrew.weekday())\n",
      "\n",
      "\n",
      "  >sample    month\n",
      "\n",
      "# Counter for how many before June 1\n",
      "early_hurricanes = 0\n",
      "\n",
      "# We loop over the dates\n",
      "for hurricane in florida_hurricane_dates:\n",
      "  # Check if the month is before June (month number 6)\n",
      "  if hurricane.month < 6:\n",
      "    early_hurricanes = early_hurricanes + 1\n",
      "    \n",
      "print(early_hurricanes)\n",
      "\n",
      "<<<<<<<<<Math with Dates\n",
      "\n",
      "2017-11-05  and 2017-12-04\n",
      "\n",
      "from datetime import date\n",
      "\n",
      "d1=date(2017,11,5)\n",
      "d2=date(2017,12,4)\n",
      "\n",
      "delta= d2 - d1\n",
      "\n",
      "print(delta.days)\n",
      "\n",
      "output: 29 days elapsed\n",
      "\n",
      "from datetime import timedelta\n",
      "\n",
      "td=timedelta(days=29)\n",
      "print(d1+td)\n",
      "\n",
      "output 2017-12-04\n",
      "\n",
      "\n",
      " >sample   > subtract two dates\n",
      "\n",
      "# Import date\n",
      "from datetime import date\n",
      "\n",
      "# Create a date object for May 9th, 2007\n",
      "start = date(2007,5,9)\n",
      "\n",
      "# Create a date object for December 13th, 2007\n",
      "end = date(2007,12,13)\n",
      "\n",
      "# Subtract the two dates and print the number of days\n",
      "print((end-start).days)\n",
      "\n",
      "\n",
      " >sample  > month as an index into a dictionary\n",
      "\n",
      "# A dictionary to count hurricanes per calendar month\n",
      "hurricanes_each_month = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6:0,\n",
      "\t\t  \t\t\t\t 7: 0, 8:0, 9:0, 10:0, 11:0, 12:0}\n",
      "\n",
      "# Loop over all hurricanes\n",
      "for hurricane in florida_hurricane_dates:\n",
      "  # Pull out the month\n",
      "  month = hurricane.month\n",
      "  # Increment the count in your dictionary by one\n",
      "  hurricanes_each_month[month] +=1\n",
      "  \n",
      "print(hurricanes_each_month)\n",
      "\n",
      "   sample  print the first and last scrambled dates\n",
      "\n",
      " # Print the first and last scrambled dates\n",
      "print(dates_scrambled)\n",
      "print(dates_scrambled[0])\n",
      "print(dates_scrambled[-1])\n",
      "\n",
      "\n",
      "  > sample  > sort dates\n",
      "\n",
      "# Print the first and last scrambled dates\n",
      "print(dates_scrambled[0])\n",
      "print(dates_scrambled[-1])\n",
      "\n",
      "# Put the dates in order\n",
      "dates_ordered =sorted(dates_scrambled)\n",
      "\n",
      "# Print the first and last ordered dates\n",
      "print(dates_ordered[0])\n",
      "print(dates_ordered[-1])\n",
      "\n",
      "   turning dates into strings\n",
      "\n",
      "from datetime import date\n",
      "\n",
      "d+date(2017,11,5)\n",
      "\n",
      "print(d)\n",
      "\n",
      "output:2017-11-05\n",
      "\n",
      "YYYY-MM-DD (ISO 8601 format)\n",
      "\n",
      "some_dates=['2001-01-01','1999-12-31']\n",
      "\n",
      "iso 8601 strings sort correctly\n",
      "\n",
      "\n",
      "print(sorted(some_dates))\n",
      "\n",
      "d.strftime()\n",
      "\n",
      "d=date(2017,1,5)\n",
      "\n",
      "print(d.strftime(\"%Y\"))\n",
      "\n",
      "\n",
      "base = datetime.datetime.today()\n",
      "date_list = [base - datetime.timedelta(days=x) for x in range(numdays)]\n",
      "\n",
      " datetime.datetime.strptime\n",
      "\n",
      "\n",
      " >Sample  > format string\n",
      "\n",
      "import datetime\n",
      "#https://strftime.org/\n",
      "d=datetime.datetime.now()\n",
      "\n",
      "dateList=['2017-12-31','2001-01-01','2010-11-21']\n",
      "\n",
      "for item in dateList:\n",
      "    diff_days=(d-datetime.datetime.strptime(item,'%Y-%m-%d')).days\n",
      "    if (diff_days/365)>5:\n",
      "        print(item,\" found\")\n",
      "\n",
      "\n",
      "  > sample  > isoformat\n",
      "\n",
      "# Assign the earliest date to first_date\n",
      "first_date = min(florida_hurricane_dates)\n",
      "\n",
      "# Convert to ISO and US formats\n",
      "iso = \"Our earliest hurricane date: \" + first_date.isoformat()\n",
      "us = \"Our earliest hurricane date: \" + first_date.strftime(\"%m/%d/%Y\")\n",
      "\n",
      "print(\"ISO: \" + iso)\n",
      "print(\"US: \" + us)\n",
      "\n",
      "    sample  > YYYY-MM\n",
      "\n",
      "# Import date\n",
      "from datetime import date\n",
      "\n",
      "# Create a date object\n",
      "andrew = date(1992, 8, 26)\n",
      "\n",
      "# Print the date in the format 'YYYY-MM'\n",
      "print(andrew.strftime('%Y-%m'))\n",
      "\n",
      "   > sample Month and Year\n",
      "\n",
      "# Import date\n",
      "from datetime import date\n",
      "\n",
      "# Create a date object\n",
      "andrew = date(1992, 8, 26)\n",
      "\n",
      "# Print the date in the format 'MONTH (YYYY)'\n",
      "print(andrew.strftime('%B (%Y)'))\n",
      "\n",
      "\n",
      "     sample Julian day\n",
      "\n",
      "from datetime import date\n",
      "\n",
      "# Create a date object\n",
      "andrew = date(1992, 8, 26)\n",
      "\n",
      "# Print the date in the format 'YYYY-DDD'\n",
      "print(andrew.strftime('%Y-%j'))\n",
      "\n",
      "     > working with dates and times\n",
      "\n",
      "october 1, 2017 3:23:25 PM\n",
      "\n",
      "from datetime import datetime\n",
      "\n",
      "dt=datetime(2017,10,1,15,23,25)\n",
      "\n",
      "\n",
      "dt=datetime(2017,10,1,15,23,25,500000)\n",
      "\n",
      "python breaks down secords to a millionth of second\n",
      "\n",
      "\n",
      "dt=datetime(year=2017,\n",
      "month=10,\n",
      "day=1,\n",
      "hour=15,\n",
      "minute=23,\n",
      "second=25,\n",
      "microsecond=500000)\n",
      "\n",
      " >replace\n",
      "\n",
      "dt_hr = dt.replace(minute=0, second=0, microsecond=0\n",
      "\n",
      "print(dt_hr)\n",
      "\n",
      "output: 2017-10-01 15:00:00\n",
      "\n",
      "    sample  > isoformat()\n",
      "\n",
      "# Import datetime\n",
      "from datetime import datetime\n",
      "\n",
      "# Create a datetime object\n",
      "dt = datetime(2017,10,1,15,26,26)\n",
      "\n",
      "# Print the results in ISO 8601 format\n",
      "print(dt.isoformat())\n",
      "\n",
      "\n",
      "dt = datetime(2017, 12, 31, 15, 19, 13)\n",
      "\n",
      "dt_old = dt.replace(year=1917)\n",
      "\n",
      "   sample using a dictionary to count time occurrences\n",
      "\n",
      "\n",
      "# Create dictionary to hold results\n",
      "trip_counts = {'AM': 0, 'PM': 0}\n",
      "\n",
      "\n",
      "# Loop over all trips\n",
      "for trip in onebike_datetimes:\n",
      "  # Check to see if the trip starts before noon\n",
      "  if trip['start'].hour < 12:\n",
      "    # Increment the counter for before noon\n",
      "    trip_counts['AM'] += 1\n",
      "  else:\n",
      "    # Increment the counter for after noon\n",
      "    trip_counts['PM'] += 1\n",
      "  \n",
      "print(trip_counts)\n",
      "\n",
      "    Printing and parsing datetimes\n",
      "\n",
      "convert a string into datetime\n",
      "\n",
      "dt=datetime(2017,12,30,15,19,13)\n",
      "\n",
      "print(dt.strftime('%Y-%m-%d'))\n",
      "\n",
      "output:2017-12-30\n",
      "\n",
      "print(dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
      "output:2017-12-30 15:19:13\n",
      "\n",
      "print(dt.isoformat())\n",
      "\n",
      "     strptime   > string parse time\n",
      "\n",
      "dt=datetime.strptime(\"12/30/2017 15:19:13\",\"%m/%d/%Y %H:%M:%S)\n",
      "\n",
      "\n",
      "number of seconds from jan 1 1970\n",
      "\n",
      "ts=1514665153.0\n",
      "\n",
      "print(datetime.formattimestamp(ts))\n",
      "\n",
      "\n",
      "  > Sample  > strptime\n",
      "\n",
      "# Import the datetime class\n",
      "from datetime import datetime\n",
      "\n",
      "# Starting string, in YYYY-MM-DD HH:MM:SS format\n",
      "s = '2017-02-03 00:00:01'\n",
      "\n",
      "# Write a format string to parse s\n",
      "fmt = '%Y-%m-%d %H:%M:%S'\n",
      "\n",
      "# Create a datetime object d\n",
      "d = datetime.strptime(s, fmt)\n",
      "\n",
      "# Print d\n",
      "print(d)\n",
      "\n",
      "or\n",
      "\n",
      "\n",
      "# Import the datetime class\n",
      "from datetime import datetime\n",
      "\n",
      "# Starting string, in YYYY-MM-DD format\n",
      "s = '2030-10-15'\n",
      "\n",
      "# Write a format string to parse s\n",
      "fmt = '%Y-%m-%d'\n",
      "\n",
      "# Create a datetime object d\n",
      "d = datetime.strptime(s, fmt)\n",
      "\n",
      "# Print d\n",
      "print(d)\n",
      "\n",
      "or\n",
      "\n",
      "# Import the datetime class\n",
      "from datetime import datetime\n",
      "\n",
      "# Starting string, in MM/DD/YYYY HH:MM:SS format\n",
      "s = '12/15/1986 08:00:00'\n",
      "\n",
      "# Write a format string to parse s\n",
      "fmt = '%m/%d/%Y %H:%M:%S'\n",
      "\n",
      "# Create a datetime object d\n",
      "d = datetime.strptime(s, fmt)\n",
      "\n",
      "# Print d\n",
      "print(d)\n",
      "\n",
      "  > Sample  > cleanup strings to dates\n",
      "\n",
      "# Write down the format string\n",
      "fmt = \"%Y-%m-%d %H:%M:%S\"\n",
      "\n",
      "# Initialize a list for holding the pairs of datetime objects\n",
      "onebike_datetimes = []\n",
      "\n",
      "# Loop over all trips\n",
      "for (start, end) in onebike_datetime_strings:\n",
      "  trip = {'start': datetime.strptime(start, fmt),\n",
      "          'end': datetime.strptime(end, fmt)}\n",
      "  \n",
      "  # Append the trip\n",
      "  onebike_datetimes.append(trip)\n",
      "\n",
      "\n",
      "<<<< sample  > isoformat using strftime\n",
      "\n",
      "\n",
      "# Import datetime\n",
      "from datetime import datetime\n",
      "\n",
      "# Pull out the start of the first trip\n",
      "first_start = onebike_datetimes[0]['start']\n",
      "\n",
      "# Format to feed to strftime()\n",
      "fmt = \"%Y-%m-%dT%H:%M:%S\"\n",
      "\n",
      "# Print out date with .isoformat(), then with .strftime() to compare\n",
      "print(first_start.isoformat())\n",
      "print(datetime.strftime(first_start,fmt))\n",
      "\n",
      "\n",
      "   sample    fromtimestamp\n",
      "\n",
      "# Import datetime\n",
      "from datetime import datetime\n",
      "\n",
      "# Starting timestamps\n",
      "timestamps = [1514665153, 1514664543]\n",
      "\n",
      "# Datetime objects\n",
      "dts = []\n",
      "\n",
      "# Loop\n",
      "for ts in timestamps:\n",
      "  dts.append(datetime.fromtimestamp(ts))\n",
      "  \n",
      "# Print results\n",
      "print(dts)\n",
      "\n",
      "      working with durations\n",
      "\n",
      "working with durations\n",
      "\n",
      "\n",
      "\n",
      "start= datetime(2017,10,8,23,46,47)\n",
      "end=datetime(2017,10,9,0,10,57)\n",
      "\n",
      "duration=end-start\n",
      "\n",
      "timedelta is a duration\n",
      "\n",
      "print(duration.total_seconds())\n",
      "\n",
      "output: 1450\n",
      "\n",
      "from datetime import timedelta\n",
      "\n",
      "delta = timedelta(seconds=1)\n",
      "\n",
      "print(start + delta1)\n",
      "\n",
      "output: 2017-10-08 23:46:48\n",
      "\n",
      "one second later\n",
      "\n",
      "delta2= timedelta(days=1, seconds=1)\n",
      "\n",
      "print(start+delta2)\n",
      "output: 2017-10-09 23:46:48\n",
      "\n",
      "\n",
      "delta3 = timedelta(week=-1)\n",
      "\n",
      "    sample    duration in seconds\n",
      "\n",
      "# Initialize a list for all the trip durations\n",
      "onebike_durations = []\n",
      "\n",
      "for trip in onebike_datetimes:\n",
      "  # Create a timedelta object corresponding to the length of the trip\n",
      "  trip_duration = trip['end'] - trip['start']\n",
      "  \n",
      "  # Get the total elapsed seconds in trip_duration\n",
      "  trip_length_seconds = trip_duration.total_seconds()\n",
      "  \n",
      "  # Append the results to our list\n",
      "  onebike_durations.append(trip_length_seconds)\n",
      "\n",
      "\n",
      "# What was the total duration of all trips?\n",
      "total_elapsed_time = sum(onebike_durations)\n",
      "\n",
      "# What was the total number of trips?\n",
      "number_of_trips = len(onebike_durations)\n",
      "  \n",
      "# Divide the total duration by the number of trips\n",
      "print(total_elapsed_time / number_of_trips)\n",
      "\n",
      "  >sample  > finding min and max durations\n",
      "\n",
      "\n",
      "# Calculate shortest and longest trips\n",
      "shortest_trip = min(onebike_durations)\n",
      "longest_trip = max(onebike_durations)\n",
      "\n",
      "# Print out the results\n",
      "print(\"The shortest trip was \" + str(shortest_trip) + \" seconds\")\n",
      "print(\"The longest trip was \" + str(longest_trip) + \" seconds\")\n",
      "\n",
      "\n",
      "      UTC offsets\n",
      "\n",
      "comparing dates across different parts of the world\n",
      "\n",
      "time zones\n",
      "pacific 3 pm\n",
      "mountain 4 pm\n",
      "central 5 pm\n",
      "eastern 6 pm\n",
      "\n",
      "uk standard time is utc\n",
      "\n",
      "utc-x (North and South America)\n",
      "utc+x (Africa, Russia, China, and Australia)\n",
      "\n",
      "\n",
      "from datetime import datetime, timedelta, timezone\n",
      "\n",
      "ET=timezone(timedelta(hours=-5))\n",
      "\n",
      "dt=datetime(2017,12,30,15,9,3,tzinfo=ET)\n",
      "\n",
      "output: 2017-12-30 15:09:03-05:00\n",
      "\n",
      "includes the utc offset\n",
      "\n",
      "   change the close to India Standard time zone\n",
      "\n",
      "IST= timezone(timedelta(hours=5,minutes=30))\n",
      "\n",
      "print(dt.astimezone(ITS))\n",
      "\n",
      "print(dt.replace(tzinfo=timezone.utc))\n",
      "\n",
      "timezone.utc has 0 timezone offset\n",
      "\n",
      "print(dt.replace(tzinfo=timezone.utc))\n",
      "\n",
      "the clock stays the same, but the utc has shifted\n",
      "\n",
      "\n",
      "   adjusting timezones and changing the tzinfo\n",
      "\n",
      "import os, time\n",
      "time.strftime('%X %x %Z')\n",
      "'12:45:20 08/19/09 CDT'\n",
      "os.environ['TZ'] = 'Europe/London'\n",
      "time.tzset()\n",
      "\n",
      "year = time.strftime('%Y')\n",
      "month = time.strftime('%m')\n",
      "day = time.strftime('%d')\n",
      "hour = time.strftime('%H')\n",
      "minute = time.strftime('%M')\n",
      "\n",
      "astimezone changes the clock and the utc offset\n",
      "\n",
      "   sample  set the tzinfo\n",
      "\n",
      "the datetime object does not supply any concrete subclass of tzinfo.  The tzinfo object reveals the local time from UTC.  the tzinfo object contains the offset, the name of the time zone, and the dst offset.\n",
      "\n",
      "# Import datetime, timezone\n",
      "from datetime import datetime, timezone\n",
      "\n",
      "# October 1, 2017 at 15:26:26, UTC\n",
      "dt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=timezone.utc)\n",
      "\n",
      "# Print results\n",
      "print(dt.isoformat())\n",
      "\n",
      "   sample  > datetime in pacific time zone\n",
      "\n",
      "# Import datetime, timedelta, timezone\n",
      "from datetime import datetime, timedelta, timezone\n",
      "\n",
      "# Create a timezone for Pacific Standard Time, or UTC-8\n",
      "pst = timezone(timedelta(hours=-8))\n",
      "\n",
      "# October 1, 2017 at 15:26:26, UTC-8\n",
      "dt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=pst)\n",
      "\n",
      "# Print results\n",
      "print(dt.isoformat())\n",
      "\n",
      " > sample  > australia time zone\n",
      "\n",
      "# Import datetime, timedelta, timezone\n",
      "from datetime import datetime, timedelta, timezone\n",
      "\n",
      "# Create a timezone for Australian Eastern Daylight Time, or UTC+11\n",
      "aedt = timezone(timedelta(hours=11))\n",
      "\n",
      "# October 1, 2017 at 15:26:26, UTC+11\n",
      "dt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=aedt)\n",
      "\n",
      "# Print results\n",
      "print(dt.isoformat())\n",
      "\n",
      "\n",
      "   sample  > replace the time as utc time\n",
      "\n",
      "# Loop over the trips\n",
      "for trip in onebike_datetimes[:10]:\n",
      "  # Pull out the start\n",
      "  dt = trip['start']\n",
      "  # Move dt to be in UTC\n",
      "  dt = dt.astimezone(timezone.utc)\n",
      "  \n",
      "  # Print the start time in UTC\n",
      "  print('Original:', trip['start'], '| UTC:', dt.isoformat())\n",
      "\n",
      "Original: 2017-10-01 15:23:25-04:00 | UTC: 2017-10-01T15:23:25+00:00\n",
      "    Original: 2017-10-01 15:42:57-04:00 | UTC: 2017-10-01T15:42:57+00:00\n",
      "\n",
      " .replace() just changes the timezone whereas .astimezone() actually moves the hours and days to match.\n",
      "\n",
      "         >Time Zone Database\n",
      "\n",
      "how to align your data to utc\n",
      "\n",
      "from datetime import datetime\n",
      "for dateutil import tz\n",
      "\n",
      "#eastern time zone\n",
      "et=tz.gettz('America/New_York')\n",
      "\n",
      "#continent slash major city\n",
      "\n",
      "'America/New_York'\n",
      "'America/Mexico_City'\n",
      "'Europe/London'\n",
      "'Africa/Accra'\n",
      "\n",
      "\n",
      "last=datetime(2017,12,30.15,9,3,tzinfo=et)\n",
      "\n",
      "\n",
      "in some place the clocks change twice a year\n",
      "\n",
      "  sample  > using dateutil\n",
      "\n",
      "from dateutil import tz\n",
      "\n",
      "# Create a timezone object for Eastern Time\n",
      "et = tz.gettz('America/New_York')\n",
      "\n",
      "# Loop over trips, updating the datetimes to be in Eastern Time\n",
      "for trip in onebike_datetimes[:10]:\n",
      "  # Update trip['start'] and trip['end']\n",
      "  trip['start'] = trip['start'].replace(tzinfo=et)\n",
      "  trip['end'] = trip['end'].replace(tzinfo=et)\n",
      "\n",
      "\n",
      "  sample  > uk time and local time\n",
      "\n",
      "# Create the timezone object\n",
      "uk = tz.gettz('Europe/London')\n",
      "\n",
      "# Pull out the start of the first trip\n",
      "local = onebike_datetimes[0]['start']\n",
      "\n",
      "# What time was it in the UK?\n",
      "notlocal = local.astimezone(uk)\n",
      "\n",
      "# Print them out and see the difference\n",
      "print(local.isoformat())\n",
      "print(notlocal.isoformat())\n",
      "\n",
      "  > sample    asia time vs local time\n",
      "\n",
      "# Create the timezone object\n",
      "ist = tz.gettz('Asia/Kolkata')\n",
      "\n",
      "# Pull out the start of the first trip\n",
      "local = onebike_datetimes[0]['start']\n",
      "\n",
      "# What time was it in India?\n",
      "notlocal = local.astimezone(ist)\n",
      "\n",
      "# Print them out and see the difference\n",
      "print(local.isoformat())\n",
      "print(notlocal.isoformat())\n",
      "\n",
      "  > sample  > pacific time zone vs local\n",
      "\n",
      "# Create the timezone object\n",
      "sm = tz.gettz('Pacific/Apia')\n",
      "\n",
      "# Pull out the start of the first trip\n",
      "local = onebike_datetimes[0]['start']\n",
      "\n",
      "# What time was it in Samoa?\n",
      "notlocal = local.astimezone(sm)\n",
      "\n",
      "# Print them out and see the difference\n",
      "print(local.isoformat())\n",
      "print(notlocal.isoformat())\n",
      "\n",
      "\n",
      "    Daylight saving time\n",
      "\n",
      "forward in the spring\n",
      "back in the fall\n",
      "\n",
      "\n",
      "\n",
      "2017-03-12 01:59:59 springs forward 2017-03-12 03:00:00\n",
      "\n",
      "spring_ahead_159am = datetime(2017,3,12,1,59,59)\n",
      "spring_ahead_159am=isoformat()\n",
      "\n",
      "spring_ahead_3am=datetime(2017,3,12,3,0,0)\n",
      "spring_ahead_3am.isoformat()\n",
      "\n",
      "(spring_ahead_3am - spring_ahead_159am).total_seconds()\n",
      "\n",
      "EST = timezone(timedelta(hours=-5))\n",
      "EDT = timezone(timedelta(hours=-4))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\financials.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\financials.txt\n",
      "Gross Profit=direct sales + cost\n",
      "\n",
      "Cogs = is the cost of selling these goods\n",
      "\n",
      "gross profit = sales - cogs\n",
      "\n",
      "\n",
      "Income Statement:\n",
      "Inc: Sales\n",
      "Inc: Cost of Goods Sold\n",
      "\n",
      "Operating Expense\n",
      "Exp: Selling Admin\n",
      "Exp: R&D\n",
      "Exp: Training\n",
      "\n",
      "Net Profit\n",
      "\n",
      "\n",
      "Gross Profit Margin = Gross Profit / SalesValue\n",
      "\n",
      "\n",
      "   Sample    > gross profit\n",
      "\n",
      "# Set the sales variable to 8000\n",
      "sales = 8000\n",
      "\n",
      "# Set the cost of goods sold (cogs) variable to 5400\n",
      "cogs = 5400\n",
      "\n",
      "# Calculate the gross profit (gross_profit)\n",
      "gross_profit = sales - cogs\n",
      "\n",
      "# Print the gross profit\n",
      "print(\"The gross profit is {}.\".format(gross_profit))\n",
      "\n",
      "# Calculate the gross profit margin\n",
      "gross_profit_margin = gross_profit / sales\n",
      "\n",
      "# Print the gross profit margin\n",
      "print(\"The gross profit margin is {}.\".format(gross_profit_margin))\n",
      "\n",
      "2600\n",
      ".3256\n",
      "\n",
      "The higher margin you have the better of course, and also this margin will then pay for other expenses not directly related to the product, like Marketing\n",
      "\n",
      "\n",
      "  >Sample    calculating net Profit\n",
      "\n",
      "# Create and print the opex list\n",
      "print(admin)\n",
      "opex = [admin, travel, training, marketing, insurance]\n",
      "print(opex)\n",
      "\n",
      "print(sum(opex))\n",
      "# Calculate and print net profit\n",
      "net_profit = gross_profit - sum(opex)\n",
      "print(\"The net profit is {}.\".format(net_profit))\n",
      "\n",
      "\n",
      "      calculating sales and cost of goods sold\n",
      "\n",
      "gross profits= sales - cost of sales \n",
      "\n",
      "\n",
      "Sales (income, revenue, turnover)\n",
      "\n",
      "sales price per unit sp_unit\n",
      "number of units sold\n",
      "\n",
      "complexities:\n",
      "discounts (discounted sales price)\n",
      "credit sales (where the income is not received up front)\n",
      "\n",
      "sales mix\n",
      "\n",
      " >cost of goods sold (cogs)\n",
      "1. total fixed costs\n",
      "2. variable cost per unit\n",
      "3. inventory opening balance\n",
      "4. inventory closing balance\n",
      "\n",
      "what does gross profit tells\n",
      "\n",
      "profit margin %\n",
      "Gross Profit Margin = Gross Profit / SalesValue\n",
      "\n",
      "\n",
      "fixed costs are costs independent of units produced\n",
      "\n",
      "variable costs per unit are costs incurred per unit produced.\n",
      "\n",
      "variable costs are incurred only if the unit is produced\n",
      "\n",
      "gp_margin\n",
      "profit margin(%)\n",
      "\n",
      "analyze the profitability of our core product\n",
      "\n",
      "if your core product is not profitable how will you pay admin or other expenses or taxes or dividends\n",
      "\n",
      "The gross profit can be used to calculate the break even point.\n",
      "\n",
      "        Break even point\n",
      "\n",
      "Break even is where total cost and total revenue are equal.\n",
      "\n",
      "break_even = fixed_costs / (sales price - variable_costs)\n",
      "\n",
      "\n",
      "   sample   > forecasting sales \n",
      "T-Z has launched a new range of T-Shirts linked to a celebrity meme for 40 USD per T-Shirt. They have excess stock of Celebshirt1 on their shelves, and in expectation of the release of Celebshirt2 in February, they have announced a 40% discount on Celebshirt1 in February.\n",
      "For this exercise, prices are in USD, and unit amounts are total units sold in the respective month. The following variables have been defined for you:\n",
      "sales_price = 40\n",
      "units_january = 500\n",
      "units_february = 700\n",
      "The January sales only include sales of Celebshirt1. \n",
      "The February sales include sales of Celebshirt1 and Celebshirt2, at a ratio of 45:55.\n",
      "\n",
      "\n",
      "# Set variables units sold and sales price of the T-shirts (basic and custom)\n",
      "\n",
      "salesprice_basic = 15\n",
      "salesprice_custom = 25\n",
      "\n",
      "# Calculate the combined sales price taking into account the sales mix\n",
      "average_sales_price = (salesprice_basic * 0.6) + (salesprice_custom * .4)\n",
      "\n",
      "# Calculate the total sales for next month\n",
      "sales_USD = forecast_units * average_sales_price\n",
      "\n",
      "# Print the total sales\n",
      "print(\"Next month's forecast sales figure is {:.2f} USD.\".format(sales_USD))\n",
      "\n",
      "   sample  > january and february sales\n",
      "\n",
      "# Forecast the sales of January\n",
      "sales_january = units_january * sales_price\n",
      "\n",
      "# Forecast the discounted price\n",
      "dsales_price = 40*.60\n",
      "\n",
      "# Forecast the sales of February\n",
      "sales_february = (40 * units_february * 0.55) + (dsales_price * .45 * units_february)\n",
      "\n",
      "# Print the forecast sales for January and February\n",
      "print(\"The forecast sales for January and February are {} and {} USD respectively.\".format(sales_january, sales_february))\n",
      "\n",
      "The forecast sales for January and February are 20000 and 20440.0 USD respectively.\n",
      "\n",
      "\n",
      "   Sample    fixed costs\n",
      "Material costs to produce one T-shirt is 8 USD. Labor costs are 2 USD per shirt.\n",
      "\n",
      "The costs to rent a machine that produces these shirts is 1300 USD per month, regardless of the amount of shirts produced\n",
      "\n",
      "# Set the variables for fixed costs and variable costs\n",
      "fixed_costs = 1300 \n",
      "print(variables)\n",
      "variable_costs_per_unit = material_costs_per_unit + labor_costs_per_unit\n",
      "\n",
      "# Calculate the cogs for January and February\n",
      "cogs_jan = (units_jan * variable_costs_per_unit) + fixed_costs\n",
      "cogs_feb = (units_feb * variable_costs_per_unit) + fixed_costs\n",
      "\n",
      "\n",
      "    sample  > calculate cogs per month  >  cost per unit\n",
      "\n",
      "# From previous step\n",
      "fixed_costs = machine_rental \n",
      "variable_costs_per_unit = material_costs_per_unit + labor_costs_per_unit\n",
      "cogs_jan = (units_jan * variable_costs_per_unit) + fixed_costs\n",
      "cogs_feb = (units_feb * variable_costs_per_unit) + fixed_costs\n",
      "\n",
      "# Calculate the unit cost for January and February\n",
      "unit_cost_jan = cogs_jan / units_jan\n",
      "unit_cost_feb = cogs_feb / units_feb\n",
      "\n",
      "# Print the January and February cost per unit\n",
      "print(\"The cost per unit for January and February are {} and {} USD respectively.\".format(unit_cost_jan, unit_cost_feb))\n",
      "\n",
      "  >sample    number of units to break even\n",
      "\n",
      "# Calculate the break-even point (in units) for Wizit\n",
      "break_even = fixed_costs/(sales_price - variable_costs_per_unit)\n",
      "print(variables)\n",
      "# Print the break even point in units\n",
      "print(\"The break even point is {} units.\".format(break_even))\n",
      "\n",
      "# Forecast the gross profit for January and February\n",
      "gross_profit_jan = (sales_price*units_jan) - cogs_jan\n",
      "gross_profit_feb = (sales_price*units_feb) - cogs_feb\n",
      "\n",
      "# Print the gross profit for January and February\n",
      "print(\"The gross profit for January and February are {} and {} USD respectively.\".format(gross_profit_jan, gross_profit_feb))\n",
      "\n",
      "\n",
      "        Working with raw datasets\n",
      "\n",
      "balance sheet\n",
      "\n",
      "revenue\n",
      "cost of revenue\n",
      "gross profit\n",
      "operating expenses\n",
      "reserrhc and development\n",
      "sales, general and administration\n",
      "net income\n",
      "\n",
      "\n",
      "  > filtering for two rows\n",
      "\n",
      "interesting_metrics=['Gross profit','Net income']\n",
      "\n",
      "filter =income_statement_df.metric.isin(interesting_metrics)\n",
      "\n",
      "filtered_income_statement=income_statement_df[filter]\n",
      "\n",
      "print(filtered_income_statement.head())\n",
      "\n",
      "   more of the same\n",
      "\n",
      "# Choose some interesting metrics\n",
      "interesting_metrics = ['Operating income', 'Net income', 'Cost of revenue', 'Gross profit']\n",
      "\n",
      "# Filter for rows containing these metrics\n",
      "filtered_income_statement = income_statement[income_statement.metric.isin(interesting_metrics)]\n",
      "\n",
      "# See the result\n",
      "print(filtered_income_statement)\n",
      "\n",
      "  > sample   > filter a row and insert a column\n",
      "\n",
      "revenue_metric = ['Revenue']\n",
      "\n",
      "# Filter for rows containing the revenue metric\n",
      "filtered_income_statement = income_statement[income_statement.metric.isin(revenue_metric)]\n",
      "\n",
      "# Get the number of columns in filtered_income_statement\n",
      "n_cols = len(filtered_income_statement.columns)\n",
      "\n",
      "# Insert a column in the correct position containing the column 'Forecast'\n",
      "filtered_income_statement.insert(n_cols, 'Forecast',13000) \n",
      "\n",
      "# See the result\n",
      "print(filtered_income_statement)\n",
      "\n",
      "\n",
      "        balance sheet    >\n",
      "\n",
      "Assets\n",
      "Liabilities\n",
      "Capital (Equity)\n",
      "\n",
      "\n",
      "There are four types of financial statements\n",
      "1. Income Statement\n",
      "2. Balance Sheet\n",
      "3. Cash Flow Statement\n",
      "4. Statement of Shareholder's Equity\n",
      "\n",
      "Assets are economic resource and it can be used to make money\n",
      "\n",
      "Liability = economic obligation to pay for something\n",
      "\n",
      "equity = assets - liabilities\n",
      "\n",
      "house_cost =100000\n",
      "down_payment = 20000\n",
      "\n",
      "financing the rest as a mortgage\n",
      "\n",
      "mortgage =80000\n",
      "\n",
      "total_equity = house_cost - mortgage\n",
      "\n",
      "if your house value remains constant\n",
      "\n",
      "owners_equity = assets - liabilities\n",
      "\n",
      "Balance Sheet\n",
      "\n",
      "\tAssets\n",
      "\ta. bank\n",
      "\tb. debtors\n",
      "\n",
      "\tEquity\n",
      "\n",
      "\tLiabilities\n",
      "\ta. loans\n",
      "\tb. creditors\n",
      "\n",
      "\tTotal Equity and liabilities\n",
      "\n",
      "\n",
      "Income Sales are record and also recorded as a credit under the Debtors item in the balance sheet as an accounts receivable.\n",
      "\n",
      "If we sell our credit we can buy our credit as well.  Operating expense are recorded on the income statment and also recorded as a liability creditors on the balance sheet.\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "# Create the list for sales, and empty lists for debtors and credits\n",
      "sales = [500, 350, 700]\n",
      "debtors = [] \n",
      "credits = []\n",
      "\n",
      "# Create the statement to append the calculated figures to the debtors and credits lists\n",
      "for mvalue in sales: \n",
      "    credits.append(mvalue * 0.6)\n",
      "    if month > 0:\n",
      "        debtors.append(credits[month] + credits[month-1]) \n",
      "    else:\n",
      "        debtors.append(credits[month])\n",
      "    month += 1\n",
      "# Print the result\n",
      "print(\"The ‘Debtors’ are {}.\".format(debtors))\n",
      "\n",
      "\n",
      "\n",
      "        bad debts\n",
      "\n",
      "# Calculate the bad debts for February\n",
      "bad_debts_feb = 500*0.3\n",
      "\n",
      "# Calculate the feb debtors amount\n",
      "debtors_feb = (debtors_jan - bad_debts_feb)\n",
      "\n",
      "# Print the debtors for January and the bad debts and the debtors for February\n",
      "print(\"The debtors are {} in January, {} in February. February's bad debts are {} USD.\".format(debtors_jan, debtors_feb, bad_debts_feb))\n",
      "\n",
      "\n",
      "    > calculating accounts payable\n",
      "\n",
      "# Set the cost per unit\n",
      "unit_cost = .25\n",
      "\n",
      "# Create the list for production units and empty list for creditors\n",
      "production = [1000,1200]\n",
      "creditors = []\n",
      "\n",
      "# Calculate the accounts payable for January and February\n",
      "for mvalue in production: \n",
      "    creditors.append(mvalue * unit_cost * 0.5)\n",
      "    \n",
      "# Print the creditors balance for January and February\n",
      "print(\"The creditors balance for January and February are {} and {} USD.\".format(creditors[0],creditors[1]))\n",
      "\n",
      "       Balance sheet efficiency ratios\n",
      "\n",
      "receivables (debtors)\n",
      "payables (creditors)\n",
      "\n",
      "time\n",
      "\n",
      "a company creates a sale but gets paid later.  the company creates an asset called debtors.\n",
      "\n",
      "or\n",
      "\n",
      "a company buys something and does not settle payment now, but will pay later.  A liability is created called creditors.\n",
      "\n",
      "    The debtor days ratio\n",
      "1. calculates on the average the number of days it takes to receive payments from debtors\n",
      "\n",
      "2. The lower the debtor days ratio the better\n",
      "\n",
      "companies rely on savings or debt during the waiting period of time.\n",
      "\n",
      "debtor days = ending balance debtors / sales * days in financial year\n",
      "\n",
      "ddays_ratio = (debtors_end/sales_tot) * 365\n",
      "\n",
      "      days payable outstanding (DPO ratio)\n",
      "\n",
      "1. calculates how many days to pay creditors\n",
      "\n",
      "2. the higher the ratio the better\n",
      "\n",
      "\n",
      "dpo = ending balance creditors/ total cost of Goods Sold * days in financial year\n",
      "\n",
      "dpo = (creditors_end/cogs_tot) * 365\n",
      "\n",
      "\n",
      "   debtors ratio\n",
      "\n",
      "# Create the variables\n",
      "debtors_end = 650\n",
      "sales_tot = 12500\n",
      "\n",
      "# Calculate the debtor days variable\n",
      "ddays_ratio = (debtors_end/sales_tot) * 365\n",
      "\n",
      "# Print the result\n",
      "print(\"The debtor days ratio is {}.\".format(ddays_ratio))\n",
      "\n",
      "\n",
      "    days payable outstanding\n",
      "\n",
      "# Get the variables\n",
      "#total cogs for the period\n",
      "cogs_tot = 4000\n",
      "#closing payables balance\n",
      "creditors_end = 650\n",
      "\n",
      "# Calculate the days payable outstanding\n",
      "dpo = (creditors_end/cogs_tot)*365\n",
      "\n",
      "# Print the days payable outstanding\n",
      "print(\"The days payable outstanding is {}.\".format(dpo))\n",
      "\n",
      "\n",
      "    > balance sheet efficiency ratios\n",
      "\n",
      "Days in inventory\n",
      "\n",
      "how much stock is too little, and how much is too much\n",
      "\n",
      "inventory needs to be sold\n",
      "\n",
      "how many days to sell inventory\n",
      "\n",
      "the consumer could go to a competitor\n",
      "\n",
      "Days in inventory ratio:\n",
      "dii=average inventory/total cost of goods sold *  days in financial year\n",
      "\n",
      "      asset turnover ratio\n",
      "\n",
      "assets needed to make sales\n",
      "\n",
      "assets compared to sales generated\n",
      "\n",
      "asset turnover = sales / total average assets\n",
      "\n",
      "production companies need high levels of assets to generate profit\n",
      "\n",
      "tech startup do not need a high investment of assets\n",
      "\n",
      "\n",
      "   > days in inventory \n",
      "\n",
      "# Calculate the dii ratio \n",
      "dii_ratio = (av_inv/cogs_tot)*365\n",
      "\n",
      "# Print the result\n",
      "print(\"The DII ratio is {}.\".format(dii_ratio))\n",
      "\n",
      "Metric\tVariable\t\t\tValue\n",
      "Total COGS\t\tcogs_tot\t4000\n",
      "Average Inventory\tav_inv\t\t1900\n",
      "Total Sales\t\tsales_tot\t10000\n",
      "Opening balance Assets\tob_assets\t2000\n",
      "Closing balance Assets\tcb_assets\t7000\n",
      "\n",
      "\n",
      "    Asset turn over ratio\n",
      "\n",
      "# Calculate the DII Ratio\n",
      "dii_ratio = (av_inv/cogs_tot)*365\n",
      "\n",
      "# Print the result\n",
      "print(\"The DII ratio is {}.\".format(dii_ratio))\n",
      "\n",
      "# Calculate the Average Assets\n",
      "av_assets = (ob_assets + cb_assets)/2\n",
      "\n",
      "# Calculate the Asset Turnover Ratio\n",
      "at_ratio = sales_tot/av_assets\n",
      "\n",
      "# Print the Asset Turnover Ratio\n",
      "print(\"The asset turnover ratio is {}.\".format(at_ratio))\n",
      "\n",
      "Metric\tVariable\tValue\n",
      "Total COGS\tcogs_tot\t4000\n",
      "Average Inventory\tav_inv\t1900\n",
      "Total Sales\tsales_tot\t10000\n",
      "Opening balance Assets\tob_assets\t2000\n",
      "Closing balance Assets\tcb_assets\t7000\n",
      "\n",
      "     debtor days\n",
      "\n",
      "debtor days=ending balance debtors/sales * days in financial year\n",
      "\n",
      "\n",
      "   > receivables\n",
      "\n",
      "# Create the filter metric for Receivables\n",
      "receivables_metric = ['Receivables']\n",
      "\n",
      "# Create a boolean series with your metric\n",
      "receivables_filter = balance_sheet.metric.isin(receivables_metric)\n",
      "\n",
      "# Use the series to filter the dataset\n",
      "filtered_balance_sheet = balance_sheet[receivables_filter]\n",
      "\n",
      "print(filtered_balance_sheet)\n",
      "\n",
      " metric  2013-12  2014-12   2015-12  2016-12  2017-12\n",
      "6  Receivables  87309.0  92819.0  101975.0  57368.0  62809.0\n",
      "\n",
      "     debtor days\n",
      "\n",
      "# From previous step\n",
      "receivables_metric = ['Receivables']\n",
      "receivables_filter = balance_sheet.metric.isin(receivables_metric)\n",
      "filtered_balance_sheet = balance_sheet[receivables_filter]\n",
      "\n",
      "# Extract the zeroth value from the last time period (2017-12)\n",
      "debtors_end = filtered_balance_sheet['2017-12'].iloc[0]\n",
      "\n",
      "# Calculate the debtor days ratio\n",
      "ddays = (debtors_end / sales) * 365\n",
      "\n",
      "# Print the debtor days ratio\n",
      "print(\"The debtor day ratio is {:.0f}. A higher debtors days ratio means it takes longer to collect cash from debtors.\".format(ddays))\n",
      "\n",
      "output:\n",
      "The debtor day ratio is 146. A higher debtors days ratio means it takes longer to collect cash from debtors.\n",
      "In [1]:\n",
      "\n",
      "    forecast debtors end\n",
      "\n",
      "# Calculate the forecasted sales \n",
      "f_sales = sales * 1.10\n",
      "\n",
      "# Solve for the forecasted debtors' ending balance\n",
      "f_debtors_end = ddays * f_sales / 365\n",
      "\n",
      "print(\"If sales rise by 10% and the debtor days decrease to {:.0f} then the forecasted closing balance for debtors will be {:.0f}.\".format(ddays, f_debtors_end))\n",
      "\n",
      "\n",
      "    add a column to the dataframe\n",
      "\n",
      "# From previous step\n",
      "f_sales = sales * 1.10\n",
      "f_debtors_end = ddays * f_sales / 365\n",
      "\n",
      "# Get the number of columns in the filtered balance sheet\n",
      "n_cols = len(filtered_balance_sheet.columns)\n",
      "\n",
      "# Append a Forecast column of the forecasted debtors' end balance\n",
      "filtered_balance_sheet.insert(n_cols, 'Forecast', f_debtors_end)\n",
      "\n",
      "# See the result\n",
      "print(filtered_balance_sheet)\n",
      "\n",
      "       financial periods\n",
      "\n",
      "1. The financial year is the full reporting year for financial numbers\n",
      "2. Can start and end at any month of the year, it does not need to be a calendar year\n",
      "\n",
      "3. quarters are three months at a time\n",
      "4. the year is based on the financial year end\n",
      "\n",
      "\n",
      "   >quarters to month amounts\n",
      "\n",
      "\n",
      "# Create a list for quarters and initialize an empty list qrtlist\n",
      "quarters = [700, 650]\n",
      "qrtlist = []\n",
      "\n",
      "# Create a for loop to split the quarters into months and add to qrtlist\n",
      "for qrt in quarters:\n",
      " month = round(qrt / 3, 2)\n",
      " qrtlist = qrtlist + [month, month, month]\n",
      " \n",
      "# Print the result\n",
      "print(\"The values per month for the first two quarters are {}.\".format(qrtlist))\n",
      "\n",
      "\n",
      "The values per month for the first two quarters are [233.33, 233.33, 233.33, 216.67, 216.67, 216.67].\n",
      "\n",
      "    sum totals per quarter\n",
      "\n",
      "# Create a months list, as well as an index, and set the quarter to 0\n",
      "months = [100, 100, 150, 250, 300, 10, 20]\n",
      "quarter = 0\n",
      "quarters = []\n",
      "index = 1\n",
      "\n",
      "# Create for loop for quarter, print result, and increment the index\n",
      "for sales in months:\n",
      "    quarter += sales\n",
      "    if index % 3 == 0 or index == len(months):\n",
      "        quarters.append(quarter)\n",
      "        quarter = 0\n",
      "    index = index + 1\n",
      "    \n",
      "print(\"The quarter totals are Q1: {}, Q2: {}, Q3: {}\".format(quarters[0], quarters[1], quarters[2]))\n",
      "\n",
      "The quarter totals are Q1: 350, Q2: 560, Q3: 20\n",
      "\n",
      "      >The datetime library\n",
      "\n",
      "09/10/2018\n",
      "\n",
      "regional differences\n",
      "* day-month-year\n",
      "* month-day-year\n",
      "\n",
      "dd-mm-yy\n",
      "dd/mm/yyyy\n",
      "\n",
      "from datetime import datetime\n",
      "\n",
      "datetime.strptime(date_string, format)\n",
      "\n",
      "%d Day of the Month\n",
      "%b Month as locales abbreviated name\n",
      "%B Month as locales full name\n",
      "%m Month as zero padded decimal\n",
      "%y Year 2 digit\n",
      "%Y Year with century and decimal number\n",
      "\n",
      "split() function\n",
      "\n",
      "date=\"14/02/2018\"\n",
      "\n",
      "day,month,year = date.split('/')\n",
      "\n",
      "     sample strptime\n",
      "\n",
      "# Import the datetime python library\n",
      "from datetime import datetime\n",
      "\n",
      "# Create a dt_object to convert the first date and print the month result\n",
      "dt_object1 = datetime.strptime('14/02/2018', '%d/%m/%Y')\n",
      "print(dt_object1)\n",
      "\n",
      "# Create a dt_object to convert the second date and print the month result\n",
      "dt_object2 = datetime.strptime('2 March 2018', '%d %B %Y')\n",
      "print(dt_object2 )\n",
      "\n",
      "\n",
      "2018-02-14 00:00:00\n",
      "2018-03-02 00:00:00\n",
      "\n",
      "<<<<<<<< sample   > split\n",
      "\n",
      "# Set the variable for the datetime to convert\n",
      "dt = '14/02/2018'\n",
      "\n",
      "# Create the dictionary for the month values\n",
      "mm = {'01': 'January', '02': 'February', '03': 'March'}\n",
      "\n",
      "# Split the dt string into the different parts\n",
      "day,month, year = dt.split('/')\n",
      "\n",
      "# Print the concatenated date string\n",
      "print(day + ' ' + mm[month] + ' ' + year)\n",
      "\n",
      "\n",
      "14 February 2018\n",
      "\n",
      "     European dates\n",
      "\n",
      "09-08-2018 in the us means 8th of September\n",
      "however in EU it is 9th of August\n",
      "\n",
      "can cause challenges in interpreting and combining datasets\n",
      "\n",
      "('19-02-2018','%d-%m-%Y')\n",
      "\n",
      "iteritems() function\n",
      "\n",
      "wordFrequency=\n",
      "{\n",
      "\"Hello\":7,\n",
      "\"hi\":10,\n",
      "\"there\":45,\n",
      "\"at\":23,\n",
      "\"this\":77\n",
      "}\n",
      "\n",
      "\n",
      "for key in wordFrequency:\n",
      "\tvalue=wordFrequency[key]\n",
      "\tprint(key,\" :: \",value)\n",
      "\n",
      "\n",
      "\n",
      "    sample \n",
      "\n",
      "# Set the index to start at 0\n",
      "index = 0\n",
      "\n",
      "# Create the dictionary for the months\n",
      "tt = {'Jan': 0, 'Feb': 2, 'Mar': 3}\n",
      "\n",
      "# Create a for loop that will iterate the date and amount values in the dataset\n",
      "for date, amount in df.iteritems():\n",
      "    # Create the if statement to split the day and month, then add it to the new tt variable\n",
      "    if index > 0: \n",
      "        day, month = date.split('-')\n",
      "        tt[month] +=float(amount[0])\n",
      "    index += 1\n",
      "\n",
      "print(tt)\n",
      "\n",
      "output:\n",
      "{'Jan': 0, 'Feb': 3000.0, 'Mar': 2700.0}\n",
      "\n",
      "      sample use a dictionary to hold totals\n",
      "\n",
      "totals = {'Jan': 0, 'Feb': 0, 'Mar': 0}\n",
      "calendar = {'01': 'Jan', '02': 'Feb', '03': 'Mar'}\n",
      "\n",
      "for date, amount in df1.iteritems():\n",
      "        day, month, year = date.split('-')\n",
      "        totals[month] +=float(amount[0]) \n",
      "\n",
      "\n",
      "for date, amount in df2.iteritems():\n",
      "        day, month, year = date.split('/')\n",
      "        totals[calendar[month]] += float(amount[0])\n",
      "\n",
      "print(totals)\n",
      "\n",
      "{'Jan': 1000.0, 'Feb': 4200.0, 'Mar': 1200.0}\n",
      "\n",
      "rf = pd.DataFrame.from_dict(tt, orient=\"index\"); rf.to_csv(\"LocalDrive\\\\Sales.csv\")\n",
      "\n",
      "      Building sensitive forecast models\n",
      "\n",
      "how to make our forecast smarter\n",
      "\n",
      "correctly interpret data\n",
      "account for changes in data\n",
      "account for interlinked variables\n",
      "1. dependencies\n",
      "2. sensitivities\n",
      "set assumptions\n",
      "1. best guess\n",
      "2. set at the beginning fo the forecast process\n",
      "3. use to drive forecasting\n",
      "4. can be directly controlled\n",
      "5. can be indirectly controlled, outside the control of the company\n",
      "\n",
      "different types of assumptions\n",
      "1. probablity\n",
      "2. weighted\n",
      "3. market sentiment\n",
      "\n",
      "outcome   probability %\n",
      "1 \t\t30\n",
      "2\t\t20\n",
      "3\t\t50\n",
      "\n",
      "\n",
      "outcome_probability=['1|0.3','2|0.2','3|0.5']\n",
      "\n",
      "\n",
      "def assumption1()\n",
      "\tif marketsentiment=0.3:\n",
      "\t\tsales+=sales * 0.1\t\n",
      "\telse\n",
      "\t\tsales\n",
      "\n",
      "\n",
      "<<<<<<<<sample weighted probability\n",
      "\n",
      "# Create the combined list for sales and probability\n",
      "sales_probability = ['0|0.05', '200|.1', '300|.4', '500|.2', '800|.25'] \n",
      "weighted_probability = 0\n",
      "\n",
      "# Create a for loop to calculate the weighted probability\n",
      "for pair in sales_probability:\n",
      "    parts = pair.split('|')\n",
      "    weighted_probability += float(parts[0]) * float(parts[1])\n",
      "\n",
      "# Print the weighted probability result\n",
      "print(\"The weighted probability is {}.\".format(weighted_probability))\n",
      "\n",
      "The weighted probability is 440.0.\n",
      "\n",
      "The weighted probability is a technique to manage the uncertainty in Txs Tools sales forecasting, and can give a more balanced view on expected sales numbers as opposed to just going for the lowest or highest number.\n",
      "\n",
      "\n",
      "  > sample  > effect of sentiment\n",
      "\n",
      "\n",
      "# Create the computevariance function\n",
      "def computevariance(amount, sentiment):\n",
      " if (sentiment < 0.6):\n",
      "  res = amount + (amount * 0.02)\n",
      " elif (sentiment > 0.8):\n",
      "  res = amount + (amount * 0.07)\n",
      " else:\n",
      "  res = amount + (amount * 0.05)\n",
      " return res\n",
      "\n",
      "# Compute the variance for jan, feb and mar\n",
      "jan = computevariance(500, 0.5)\n",
      "feb = computevariance(500, 0.65)\n",
      "mar = computevariance(500, 0.85)\n",
      "\n",
      "print(\"The forecast sales considering variance due to market sentiment is {} for Jan, {} for Feb, and {} for Mar.\".format(jan,feb,mar))\n",
      "\n",
      "The forecast sales considering variance due to market sentiment is 510.0 for Jan, 525.0 for Feb, and 535.0 for Mar.\n",
      "\n",
      "\n",
      "      Dependencies and sensitivity\n",
      "\n",
      "1. interlinked variables\n",
      "2. changing one variable has a knock-on-effect on other variables\n",
      "\n",
      "if x=0:\n",
      "\tx_costs + y_costs\n",
      "else\n",
      "\tx_costs\n",
      "\n",
      "during december we expect rush orders pushing up the delivery costs by 10%\n",
      "\n",
      "if month = December:\n",
      "\tdelivery_costs + delivery_cost*0.1\n",
      "else\n",
      "\tdelivery_costs\n",
      "\n",
      "   sample   > cost dependency\n",
      "\n",
      "# Set the Sales Dependency\n",
      "if sales >= 350:\n",
      "    sales_dep = (350 * base_sales_price) + ((sales - 350) * (base_sales_price - 1))\n",
      "else:\n",
      "    sales_dep = sales * base_sales_price\n",
      "\n",
      "# Print the results\n",
      "print(\"The sales dependency is {} USD.\".format(sales_dep))\n",
      "\n",
      "500 sales are calculated at the base_cost_price\n",
      "base sales price 15 and any additional sales are calculated with the increased production costs.\n",
      "\n",
      "base sales price 15\n",
      "The sales dependency is 10850 USD.\n",
      "\n",
      "\n",
      "     sample\n",
      "The first 500 sales are calculated at the base_cost_price, and any additional sales are calculated with the increased production costs.\n",
      "\n",
      "\n",
      "# Set the Cost Dependency\n",
      "if sales >= 500:\n",
      "    cost_dep = (500 * base_cost_price) + ((sales - 500) * (base_cost_price + 2))\n",
      "else:\n",
      "    cost_dep = sales * base_cost_price\n",
      "    \n",
      "# Print the results\n",
      "print(\"The cost dependency is {} USD.\".format(cost_dep))\n",
      "\n",
      "The cost dependency is 5750 USD.\n",
      "\n",
      "    > sales - usd list\n",
      "\n",
      "# Create the sales_usd list\n",
      "sales_usd = [700,350,650]\n",
      "\n",
      "\n",
      "  >  calculating gross profit\n",
      "base_cost_price=7\n",
      "base_sales_price=15\n",
      "\n",
      "# Create the sales_usd list\n",
      "sales_usd = [700, 350, 650]\n",
      "\n",
      "# Create the if statement to calculate the forecast_gross_profit\n",
      "for sales in sales_usd:\n",
      "    if sales > 350:\n",
      "        sales_dep = (350 * base_sales_price) + ((sales - 350) * (base_sales_price - 1))\n",
      "    else:\n",
      "        sales_dep = sales * base_sales_price\n",
      "    if sales > 500:\n",
      "        cost_dep = (500 * base_cost_price) + ((sales - 500) * (base_cost_price + 2))\n",
      "    else:\n",
      "        cost_dep = sales * base_cost_price\n",
      "    forecast_gross_profit = sales_dep - cost_dep\n",
      "\n",
      "    # Print the result\n",
      "    print(\"The gross profit forecast for a sale unit value of {} is {} USD.\".format(sales, forecast_gross_profit))\n",
      "\n",
      "The gross profit forecast for a sale unit value of 700 is 4850 USD.\n",
      "The gross profit forecast for a sale unit value of 350 is 2800 USD.\n",
      "The gross profit forecast for a sale unit value of 650 is 4600 USD.\n",
      "\n",
      "     Admin dependency\n",
      "\n",
      "# Set the admin dependency\n",
      "if emp_leave > 0:\n",
      "    admin_dep = emp_leave * 80\n",
      "\n",
      "# Print the results\n",
      "print(\"The admin dependency for August is {} USD.\".format(admin_dep))\n",
      "\n",
      "The admin dependency for August is 480 USD.\n",
      "\n",
      "  > sample forecast net profit\n",
      "\n",
      "admin_usd = [1500, 1500, 1500]\n",
      "emp_leave = [6, 6, 0]\n",
      "forecast_gross_profit = [4850, 2800, 4600]\n",
      "index = 0\n",
      "\n",
      "for admin in admin_usd:\n",
      "    temp = emp_leave[index]\n",
      "    if temp > 0:\n",
      "        admin_dep = emp_leave[index] * 80 + admin\n",
      "    else: \n",
      "         admin_dep = admin \n",
      "    forecast_net_profit = forecast_gross_profit[index] - admin_dep\n",
      "    print(forecast_net_profit)\n",
      "    index += 1\n",
      "print(\"The forecast net profit is:\")\n",
      "\n",
      " 2870\n",
      "    820\n",
      "    3100\n",
      "    The forecast net profit is:\n",
      "\n",
      "\n",
      "   >variances\n",
      "\n",
      "it is difficult to predict the future\n",
      "\n",
      "it failure\n",
      "crash in the market\n",
      "\n",
      "\n",
      "  >gap analysis\n",
      "\n",
      "rollingforecast1=1200\n",
      "sales = 300\n",
      "\n",
      "what caused this gap\n",
      "\n",
      "ones supplier was expected to buy 120 units but they only bought 30\n",
      "\n",
      "dependency1=120\n",
      "units=30\n",
      "expected_unit=45 #in the next six months\n",
      "\n",
      "dependency2=units+expected_units\n",
      "\n",
      "\n",
      "  > sample  > cost and sales price and total sales\n",
      "\n",
      "def dependencies(base_cost_price, base_sales_price, sales_usd):\n",
      "    res = []\n",
      "    for sales in sales_usd:\n",
      "        if sales >= 350:\n",
      "            sales_dep = (350 * base_sales_price) + ((sales - 350) * (base_sales_price - 1))\n",
      "        else:\n",
      "            sales_dep = sales * base_sales_price\n",
      "        if sales >= 500:\n",
      "            cost_dep = (500 * base_cost_price) + ((sales - 500) * (base_cost_price + 2))\n",
      "        else:\n",
      "            cost_dep = sales * base_cost_price\n",
      "        res.append(sales_dep - cost_dep)\n",
      "    return res\n",
      "\n",
      "\n",
      "forecast1 = dependencies(7, 15, [700, 350, 650])\n",
      "forecast2 = dependencies(7, 15, [700,220,520])\n",
      "\n",
      "print(\"The original forecast scenario is {}:\".format(forecast1))\n",
      "print(\"The alternative forecast scenario is {}:\".format(forecast2))\n",
      "\n",
      "gross profit\n",
      "The original forecast scenario is [4850, 2800, 4600]:\n",
      "The alternative forecast scenario is [4850, 1760, 3950]:\n",
      "\n",
      "\n",
      "   > sample change in gross profit\n",
      "\n",
      "# Set the two results\n",
      "forecast1 = dependencies(7, 15, [700, 350, 650])\n",
      "forecast2 = dependencies(7,15,[700,220,520])\n",
      "\n",
      "# Create an index and the gap analysis for the forecast\n",
      "index = 0\n",
      "for value in forecast2:\n",
      "    print(\"The gap between forecasts is {}\".format(value - forecast1[index]))\n",
      "    index += 1\n",
      "\n",
      "The gap between forecasts is 0\n",
      "The gap between forecasts is -1040\n",
      "The gap between forecasts is -650\n",
      "\n",
      "\n",
      "    get the sales row and the 2019 fc column\n",
      "\n",
      "# Create a filter to select the sales row from the netflix_f_is dataset\n",
      "print(netflix_f_is)\n",
      "sales_metric = ['Sales']\n",
      "\n",
      "# Filter for rows containing the Sales metric\n",
      "filtered_netflix_f_is =netflix_f_is[netflix_f_is.metric.isin(sales_metric)]\n",
      "\n",
      "# Extract the 2019 Sales forecast value\n",
      "forecast1 = netflix_f_is[\"2019 fc\"].iloc[0]\n",
      "\n",
      "# Print the resulting forecast\n",
      "print(\"The sales forecast is {}.\".format(forecast1))\n",
      "\n",
      " metric  2014 act  2015 act  2016 act  2017 fc  2018 fc  2019 fc\n",
      "0                    Sales      5505      6780      8831    11688    14979    17994\n",
      "1                   EBITDA       528       493       611     1088     1899     2943\n",
      "2  Operating profit (EBIT)       403       306       380      837     1660     2702\n",
      "3               Net income       267       123       187      559     1024     1721\n",
      "The sales forecast is 17994.\n",
      "\n",
      "    pct success\n",
      "\n",
      "n_subscribers_per_pp=500\n",
      "# Set the success percentage to 78%\n",
      "pct_success = 0.78\n",
      "\n",
      "print(n_subscribers_per_pp)\n",
      "# Calculate the dependency for the subscriber base\n",
      "n_subscribers = n_subscribers_per_pp * pct_success\n",
      "\n",
      "# See the result\n",
      "print(\"The dependency for the subscriber base is {}.\".format(n_subscribers))\n",
      "The dependency for the subscriber base is 390.0.\n",
      "\n",
      "    > subscribers to sales and amount per subscriber\n",
      "\n",
      "# From previous steps\n",
      "sales_metric = ['Sales']\n",
      "filtered_netflix_f_is = netflix_f_is[netflix_f_is.metric.isin(sales_metric)]\n",
      "forecast1 = filtered_netflix_f_is[\"2019 fc\"].iloc[0]\n",
      "pct_success = 0.78\n",
      "n_subscribers = n_subscribers_per_pp * pct_success\n",
      "\n",
      "# Calculate the ratio between forecast sales and subscribers\n",
      "sales_subs_ratio = forecast1 / n_subscribers\n",
      "\n",
      "# See the result\n",
      "print(\"The ratio between subscribers and sales is 1 subscriber equals ${:.2f}.\".format(sales_subs_ratio))\n",
      "\n",
      "\n",
      "The ratio between subscribers and sales is 1 subscriber equals $46.14.\n",
      "\n",
      "\n",
      "     forecast2\n",
      "\n",
      "# Set the proportion of successes to 65%\n",
      "pct_success2 = 65\n",
      "\n",
      "# Calculate the number of subscribers\n",
      "n_subscribers2 = n_subscribers_per_pp * pct_success2\n",
      "# Calculate the new forecast\n",
      "forecast2 = n_subscribers2* sales_subs_ratio\n",
      "print(forecast2)\n",
      "\n",
      "output:\n",
      "\n",
      "14995.0\n",
      "\n",
      "        gap analysis\n",
      "\n",
      "# From previous step\n",
      "pct_success2 = 65\n",
      "n_subscribers2 = n_subscribers_per_pp * pct_success2\n",
      "forecast2 = n_subscribers2 * sales_subs_ratio\n",
      "\n",
      "# Insert a column named AltForecast, containing forecast2\n",
      "filtered_netflix_f_is.insert(len(filtered_netflix_f_is.columns), 'AltForecast', forecast2)\n",
      "\n",
      "# Insert a column named Gap, containing the difference\n",
      "filtered_netflix_f_is.insert(len(filtered_netflix_f_is.columns), 'Gap', forecast1-forecast2)\n",
      "# See the result\n",
      "print(filtered_netflix_f_is)\n",
      "\n",
      "metric  2014 act  2015 act  2016 act  2017 fc  2018 fc  2019 fc  AltForecast     Gap\n",
      "0  Sales      5505      6780      8831    11688    14979    17994      14995.0  2999.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\ingesting data.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\ingesting data.txt\n",
      "pandas ingest data into data frames\n",
      "structure is rows and columns\n",
      "\n",
      "import pandas as pd\n",
      "df=pd.read_csv('us_tax_data_2016.csv')\n",
      "\n",
      "df=pd.read_csv('us_tax_data_2016.tsv',sep='\\t')\n",
      "\n",
      "sample\n",
      "\n",
      "# Import pandas with the alias pd\n",
      "import pandas as pd\n",
      "\n",
      "# Load TSV using the sep keyword argument to set delimiter\n",
      "data = pd.read_csv('vt_tax_data_2016.tsv', '\\t')\n",
      "\n",
      "# Plot the total number of tax returns by income group\n",
      "counts = data.groupby(\"agi_stub\").N1.sum()\n",
      "counts.plot.bar()\n",
      "plt.show()\n",
      "\n",
      "    modifying flat file imports\n",
      "\n",
      "df.shape\n",
      "\n",
      "usecols: choose columns to load\n",
      "\n",
      "col_names=['STATEFIPS','STATE','zipcode','agi_stub','N1']\n",
      "col_nums=[0,1,2,3,4]\n",
      "\n",
      "tax_data_v1=pd.read_csv('us_tax_data_2016.csv',usecols=col_names)\n",
      "\n",
      "\n",
      "limiting rows with nrows\n",
      "\n",
      "tax_data_v1=pd.read_csv('us_tax_data_2016.csv',nrows=1000)\n",
      "\n",
      "nrows and skiprows are useful to process a file in chunks\n",
      "\n",
      "if you skip the row with the headers then specify header=None\n",
      "\n",
      "tax_data_next500 = pd.read_csv('us_tax_data_2016.csv',\n",
      "\tnrows=500, skiprows=1000, header=None,names=col_names)\n",
      "\n",
      "names become the column header names\n",
      "\n",
      "\n",
      "sample  > column names are case sensitive\n",
      "\n",
      "# Create list of columns to use\n",
      "cols = ['zipcode','agi_stub','mars1','MARS2','NUMDEP']\n",
      "\n",
      "# Create data frame from csv using only selected columns\n",
      "data = pd.read_csv(\"vt_tax_data_2016.csv\", usecols=cols)\n",
      "\n",
      "# View counts of dependents and tax returns by income level\n",
      "print(data.groupby(\"agi_stub\").sum())\n",
      "\n",
      "\n",
      "sample  > nrows and skiprows and names\n",
      "\n",
      "# Create data frame of next 500 rows with labeled columns\n",
      "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
      "                       \t\t  nrows=500,\n",
      "                       \t\t  skiprows=500,\n",
      "                       \t\t  header=None,\n",
      "                       \t\t  names=list(vt_data_first500))\n",
      "\n",
      "# View the Vermont data frames to confirm they're different\n",
      "print(vt_data_first500.head())\n",
      "print(vt_data_next500.head())\n",
      "\n",
      "\n",
      "     handling errors and missing data\n",
      "\n",
      " common flat file import issues\n",
      "1. column data types are wrong\n",
      "2. values are missing\n",
      "3. record that cannot be ready by pandas\n",
      "\n",
      "print(df.dtypes)\n",
      "\n",
      "read_csv dtype argument takes a dictionary of column names and data types.\n",
      "\n",
      "dtype={'zipcode':str}\n",
      "\n",
      "pandas automatically intrepets some values as missing (null) or na\n",
      "\n",
      "\n",
      "   customizing missing data values\n",
      "\n",
      "na_values to setup custom missing value.  you can pass a single value, list, or dictionary of columns and values.\n",
      "\n",
      "tax_data=pd.read_csv('us_tax_data_2016.csv', na_values={'zipcode':0})\n",
      "\n",
      "print(tax_data[tax_data.zipcode.isna()])\n",
      "\n",
      "  Lines with errors\n",
      "\n",
      "error_bad_lines=False to skip unparseable records\n",
      "\n",
      "warn_bad_lines=True to see messages when records are skipped\n",
      "\n",
      "tax_data=pd.read_csv('us_tax_data_2016.csv', \n",
      "\terror_bad_lines=False,\n",
      "\twarn_bad_lines=True)\n",
      "\n",
      "sample  > dtype using a dictionary\n",
      "\n",
      "# Create dict specifying data types for agi_stub and zipcode\n",
      "data_types = {'agi_stub':'category',\n",
      "\t\t\t  'zipcode':str}\n",
      "\n",
      "# Load csv using dtype to set correct data types\n",
      "data = pd.read_csv(\"vt_tax_data_2016.csv\", dtype=data_types)\n",
      "\n",
      "# Print data types of resulting frame\n",
      "print(data.dtypes.head())\n",
      "\n",
      "sample  > na_values\n",
      "\n",
      "# Create dict specifying that 0s in zipcode are NA values\n",
      "null_values = {'zipcode':0}\n",
      "\n",
      "# Load csv using na_values keyword argument\n",
      "data = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
      "                   na_values=null_values)\n",
      "\n",
      "# View rows with NA ZIP codes\n",
      "print(data[data.zipcode.isna()])\n",
      "\n",
      "\n",
      "sample  > error_bad_lines and warn_bad_lines\n",
      "\n",
      "try:\n",
      "  # Set warn_bad_lines to issue warnings about bad records\n",
      "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
      "                     error_bad_lines=False, \n",
      "                     warn_bad_lines=True)\n",
      "  \n",
      "  # View first 5 records\n",
      "  print(data.head())\n",
      "  \n",
      "except pd.io.common.CParserError:\n",
      "    print(\"Your data contained rows that could not be parsed.\")\n",
      "\n",
      "\n",
      "   > introduction to spreadsheets\n",
      "\n",
      "survey_data=pd.read_excel('fcc_survey.xlsx')\n",
      "\n",
      "print(survey_data.head())\n",
      "\n",
      "nrows: limit number of rows to load\n",
      "skiprows: specify number of rows to skip\n",
      "\n",
      "usecols: choose columns by name, positional number or letter(\"A:P\")\n",
      "\n",
      "\n",
      "survey_data=pd.read_excel('fcc_survey.xlsx',\n",
      "skiprows=2,\n",
      "usecols=\"W:AB, AR\"\n",
      ")\n",
      "\n",
      "sample\n",
      "\n",
      "# Load pandas as pd\n",
      "import pandas as pd\n",
      "\n",
      "# Read spreadsheet and assign it to survey_responses\n",
      "survey_responses = pd.read_excel('fcc_survey.xlsx')\n",
      "\n",
      "survey_responses = pd.read_excel('fcc_survey.xlsx',sheet_name=None)\n",
      "\n",
      "reads all sheets in a workbook\n",
      "\n",
      "for key, value in survey_responses.items():\n",
      "\tprint(key, type(value))\n",
      "\n",
      "the value is a dataframe\n",
      "\n",
      "\n",
      "# View the head of the data frame\n",
      "print(survey_responses.head())\n",
      "\n",
      "sample    skiprows and column ranges\n",
      "\n",
      "# Create string of lettered columns to load\n",
      "col_string = 'AD,AW:BA'\n",
      "\n",
      "# Load data with skiprows and usecols set\n",
      "survey_responses = pd.read_excel(\"fcc_survey_headers.xlsx\", \n",
      "                        skiprows=2, \n",
      "                        usecols=col_string)\n",
      "\n",
      "# View the names of the columns selected\n",
      "print(survey_responses.columns)\n",
      "\n",
      "    fixing decimals\n",
      "\n",
      "def fix_decimal(num):\n",
      "### convert numeric value with comma as decimal separator to float\n",
      "  print(num)\n",
      "  return float(num.replace(',', '.')) if num else 0\n",
      "  \n",
      "tmp = pd.read_excel(\"test.xlsx\", converters={0: fix_decimal} )\n",
      "\n",
      "\n",
      "   > getting data from multiple worksheets\n",
      "\n",
      "specifying sheets to get data\n",
      "\n",
      "read_excel loads the first sheet in an excel file by default\n",
      "\n",
      "sheet_name keyword argument to load other sheets or a list of name/numbers to load more than one sheet at a time\n",
      "\n",
      "sheets are 0 indexed\n",
      "\n",
      "survey_data_sheet2 = pd.read_excel('fcc_survey.xlsx',sheet_name=1)\n",
      "\n",
      "survey_data_responses = pd.read_excel('fcc_survey.xlsx',sheet_name=None)\n",
      "\n",
      "for key, value in survey_data_responses.items():\n",
      "\tprint(key, type(value))\n",
      "\n",
      "all_responses=pd.DataFrame()\n",
      "\n",
      "for sheet_name, frame in survey_data_responses.items():\n",
      "\tframe['Year']=sheet_name\n",
      "\n",
      "\tall_responses=all_responses.append(frame)\n",
      "\n",
      "print(all_response.Year.unique())\n",
      "\n",
      "\n",
      "sample   read and plot\n",
      "\n",
      "# Create df from second worksheet by referencing its position\n",
      "responses_2017 = pd.read_excel(\"fcc_survey.xlsx\",\n",
      "                               sheet_name=1)\n",
      "\n",
      "# Graph where people would like to get a developer job\n",
      "job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
      "job_prefs.plot.barh()\n",
      "plt.show()\n",
      "\n",
      " > sample\n",
      "\n",
      "# Load all sheets in the Excel file\n",
      "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
      "                                sheet_name=[0,'2017'])\n",
      "\n",
      "# View the sheet names in all_survey_data\n",
      "print(all_survey_data.keys())c\n",
      "\n",
      "\n",
      "   sample load all dataframes\n",
      "\n",
      "# Create an empty data frame\n",
      "all_responses = pd.DataFrame()\n",
      "\n",
      "# Set up for loop to iterate through values in responses\n",
      "for df in responses.values():\n",
      "  # Print the number of rows being added\n",
      "  print(\"Adding {} rows\".format(df.shape[0]))\n",
      "  # Append df to all_responses, assign result\n",
      "  all_responses = all_responses.append(df)\n",
      "\n",
      "# Graph employment statuses in sample\n",
      "counts = all_responses.groupby(\"EmploymentStatus\").EmploymentStatus.count()\n",
      "counts.plot.barh()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "          >importing boolean data\n",
      "\n",
      "print(bootcamp_data.sum())\n",
      "print(bootcamp_data.isna().sum())\n",
      "\n",
      "\n",
      "NA and missing values in boolan columns are changed to True\n",
      "\n",
      "true_values=True\n",
      "false_values=False\n",
      "\n",
      "each takes a list of values to treat as True/False, respectively.\n",
      "\n",
      "bool_data=pd.read_excel(\"fcc_survey.xlsx\",\n",
      "\tdtype={\n",
      "\t\"AttendedBootcamp\":bool,\n",
      "\t\"LoanYesNo\":bool,\n",
      "\t\"LoanTF\":bool\n",
      "\t},\n",
      "       true_values=['Yes'],\n",
      "       false_values=['No'])\n",
      "\n",
      "\n",
      "na as coded as true\n",
      "\n",
      "are there missing values, or could there be in the future\n",
      "how will this column be used in analysis\n",
      "what could happen if a value wee incorrectly coded as true\n",
      "\n",
      "sample\n",
      "\n",
      "# Set dtype to load appropriate column(s) as Boolean data\n",
      "survey_data = pd.read_excel(\"fcc_survey_subset.xlsx\",\n",
      "                            dtype={'HasDebt':bool})\n",
      "\n",
      "# View financial burdens by Boolean group\n",
      "print(survey_data.groupby('HasDebt').sum())\n",
      "\n",
      "\n",
      "sample  true_values false_values\n",
      "\n",
      "# Load file with Yes as a True value and No as a False value\n",
      "survey_subset = pd.read_excel(\"fcc_survey_yn_data.xlsx\",\n",
      "                              dtype={\"HasDebt\": bool,\n",
      "                              \"AttendedBootCampYesNo\": bool},\n",
      "                              true_values=['Yes'],\n",
      "                              false_values=['No'])\n",
      "\n",
      "# View the data\n",
      "print(survey_subset.head())\n",
      "\n",
      "\n",
      "   > parse dates\n",
      "\n",
      "dates and times have their own data type and internal representation\n",
      "\n",
      "by default pandas loads datetime columns as strings\n",
      "\n",
      "use parse_dates parameter to parse datetime columns\n",
      "\n",
      "parse_dates can accept:\n",
      "1. a list of column names or numbers to parse\n",
      "2. a list containing lists of columns to combine and parse\n",
      "\n",
      "\n",
      "date_cols=['Part1StartTime','Part1EndTime']\n",
      "\n",
      "survey_df= pd.read_excel('fcc_survey.xlsx',\n",
      "\tparse_dates=date_cols)\n",
      "\n",
      "\n",
      "date_cols=['Part1StartTime','Part1EndTime',\n",
      "[['Part2StartDate','Part2StartTime']]\n",
      "]\n",
      "\n",
      "survey_df= pd.read_excel('fcc_survey.xlsx',\n",
      "\tparse_dates=date_cols)\n",
      "\n",
      "\n",
      "date_cols={\n",
      "\"Part1Start\":\"Part1StartTime\",\n",
      "\"Part1End\":\"Part1EndTime\",\n",
      "\"Part2Start\":[\"Part2StartDate\",\"Part2StartTime\"]\n",
      "}\n",
      "\n",
      "survey_df= pd.read_excel('fcc_survey.xlsx',\n",
      "\tparse_dates=date_cols)\n",
      "\n",
      "\n",
      "use pd.to_datetime() after loading data if parse_dates does not work\n",
      "\n",
      "\n",
      "see strftime.org for all for the format \n",
      "\n",
      "%Y Year(4-digit)\n",
      "%m Month padded\n",
      "%d Day padded\n",
      "%H Hour 24 hour clock\n",
      "%M Minute padded\n",
      "\n",
      "format_string=\"%m%d%Y %H:%M:%S\"\n",
      "\n",
      "survey_df[\"Part2EndTime\"]=pd.to_datetime(survey_df['Part2EndTime'],format=format_string)\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Load file, with Part1StartTime parsed as datetime data\n",
      "survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
      "                            parse_dates=['Part1StartTime'])\n",
      "\n",
      "# Print first few values of Part1StartTime\n",
      "print(survey_data.Part1StartTime.head())\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "\n",
      "# Create dict of columns to combine into new datetime column\n",
      "datetime_cols = {\"Part2Start\": ['Part2StartDate', 'Part2StartTime']}\n",
      "\n",
      "\n",
      "# Load file, supplying the dict to parse_dates\n",
      "survey_data = pd.read_excel(\"fcc_survey_dts.xlsx\",\n",
      "                            parse_dates=datetime_cols)\n",
      "\n",
      "# View summary statistics about Part2Start\n",
      "print(survey_data.Part2Start.describe())\n",
      "\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Parse datetimes and assign result back to Part2EndTime\n",
      "survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"], \n",
      "                                             format=\"%m%d%Y %H:%M:%S\")\n",
      "\n",
      "# Print first few values of Part2EndTime\n",
      "print(survey_data[\"Part2EndTime\"])\n",
      "\n",
      "\n",
      "       >Connecting to a database\n",
      "\n",
      "SQL alchemy\n",
      "create_engine() makes an engine to handle database connections\n",
      "1. need string url of database to connect to\n",
      "SQLite url format: sqlite:///filename.db\n",
      "\n",
      "pd.read_sql(query, engine)\n",
      "\n",
      "engine is a way to connect to the database\n",
      "\n",
      "\n",
      "from sqlachemy import create_engine\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Import sqlalchemy's create_engine() function\n",
      "from sqlalchemy import create_engine\n",
      "\n",
      "# Create the database engine\n",
      "engine = create_engine(\"sqlite:///data.db\")\n",
      "\n",
      "# View the tables in the database\n",
      "print(engine.table_names())\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Load libraries\n",
      "import pandas as pd\n",
      "from sqlalchemy import create_engine\n",
      "\n",
      "# Create the database engine\n",
      "engine = create_engine('sqlite:///data.db')\n",
      "\n",
      "# Load hpd311calls without any SQL\n",
      "hpd_calls = pd.read_sql(\"hpd311calls\", engine)\n",
      "\n",
      "# View the first few rows of data\n",
      "print(hpd_calls.head())\n",
      "\n",
      "sample\n",
      "\n",
      "# Create the database engine\n",
      "engine = create_engine(\"sqlite:///data.db\")\n",
      "\n",
      "# Create a SQL query to load the entire weather table\n",
      "query = \"\"\"\n",
      "SELECT * \n",
      "  FROM weather;\n",
      "\"\"\"\n",
      "\n",
      "# Load weather with the SQL query\n",
      "weather = pd.read_sql(query, engine)\n",
      "\n",
      "# View the first few rows of data\n",
      "print(weather.head())\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Create database engine for data.db\n",
      "engine = create_engine(\"sqlite:///data.db\")\n",
      "\n",
      "# Write query to get date, tmax, and tmin from weather\n",
      "query = \"\"\"\n",
      "SELECT date, \n",
      "       tmax, \n",
      "       tmin\n",
      "  FROM weather;\n",
      "\"\"\"\n",
      "\n",
      "# Make a data frame by passing query and engine to read_sql()\n",
      "temperatures = pd.read_sql(query,engine)\n",
      "\n",
      "# View the resulting data frame\n",
      "print(temperatures)\n",
      "\n",
      "sample\n",
      "\n",
      "# Create query to get hpd311calls records about safety\n",
      "query = \"\"\"\n",
      "select *\n",
      "from hpd311calls\n",
      "where complaint_type='SAFETY';\n",
      "\"\"\"\n",
      "\n",
      "# Query the database and assign result to safety_calls\n",
      "safety_calls = pd.read_sql(query,engine)\n",
      "\n",
      "# Graph the number of safety calls by borough\n",
      "call_counts = safety_calls.groupby('borough').unique_key.count()\n",
      "call_counts.plot.barh()\n",
      "plt.show()\n",
      "\n",
      "sample\n",
      "\n",
      "# Create query for records with max temps <= 32 or snow >= 1\n",
      "query = \"\"\"\n",
      "SELECT *\n",
      "  FROM weather\n",
      "  where tmax<32 or snow>=1;\n",
      "\"\"\"\n",
      "\n",
      "# Query database and assign result to wintry_days\n",
      "wintry_days = pd.read_sql(query,engine)\n",
      "\n",
      "# View summary stats about the temperatures\n",
      "print(wintry_days.describe())\n",
      "\n",
      "   querie\n",
      "\n",
      "select distinct column_name from table\n",
      "select distinct * from table\n",
      "\n",
      "select avg(tmax) from weather\n",
      "select count(*) from table\n",
      "select count(distinct column_name) from table\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Create query for unique combinations of borough and complaint_type\n",
      "query = \"\"\"\n",
      "SELECT distinct borough, \n",
      "      complaint_type\n",
      "  from hpd311calls;\n",
      "\"\"\"\n",
      "\n",
      "# Load results of query to a data frame\n",
      "issues_and_boros = pd.read_sql(query,engine)\n",
      "\n",
      "# Check assumption about issues and boroughs\n",
      "print(issues_and_boros)\n",
      "\n",
      "sample\n",
      "\n",
      "# Create query to get call counts by complaint_type\n",
      "query = \"\"\"\n",
      "select complaint_type, \n",
      "     count(*)\n",
      "  FROM hpd311calls\n",
      "  group by complaint_type;\n",
      "\"\"\"\n",
      "\n",
      "# Create data frame of call counts by issue\n",
      "calls_by_issue = pd.read_sql(query, engine)\n",
      "\n",
      "# Graph the number of calls for each housing issue\n",
      "calls_by_issue.plot.barh(x=\"complaint_type\")\n",
      "plt.show()\n",
      "\n",
      "# Create a query to get month and max tmax by month\n",
      "query = \"\"\"\n",
      "SELECT max(tmax), \n",
      "       month\n",
      "  FROM weather\n",
      "  group by month;\"\"\"\n",
      "\n",
      "# Get data frame of monthly weather stats\n",
      "weather_by_month = pd.read_sql(query, engine)\n",
      "\n",
      "# View weather stats by month\n",
      "print(weather_by_month)\n",
      "\n",
      "SELECT month, \n",
      "       MAX(tmax), \n",
      "       MIN(tmin)\n",
      "  FROM weather \n",
      " GROUP BY month;\n",
      "\n",
      "\n",
      "  > join\n",
      "select * from hpd311calls join weather on hpd311calls.created_date = weather. date\n",
      "\n",
      "select hpd311calls.borough,\n",
      "\tcount(*),\n",
      "boro_census.total_population,\n",
      "boro_census.housing_units\n",
      "from hpd311calls\n",
      "join boro_census\n",
      "\ton hpd311calls.borough=boro_census.borough\n",
      "group by hpd311calls.borough\n",
      "\n",
      "\n",
      "# Query to get water leak calls and daily precipitation\n",
      "query = \"\"\"\n",
      "SELECT hpd311calls.*, weather.prcp\n",
      "  FROM hpd311calls\n",
      "  JOIN weather\n",
      "    ON hpd311calls.created_date = weather.date\n",
      "  where hpd311calls.complaint_type = 'WATER LEAK';\"\"\"\n",
      "\n",
      "# Load query results into the leak_calls data frame\n",
      "leak_calls = pd.read_sql(query, engine)\n",
      "\n",
      "# View the data frame\n",
      "print(leak_calls.head())\n",
      "\n",
      "# Modify query to join tmax and tmin from weather by date\n",
      "query = \"\"\"\n",
      "SELECT hpd311calls.created_date, \n",
      "\t   COUNT(*), \n",
      "       weather.tmax,\n",
      "       weather.tmin\n",
      "  FROM hpd311calls \n",
      "       join weather\n",
      "       on hpd311calls.created_date = weather.date\n",
      " WHERE hpd311calls.complaint_type = 'HEAT/HOT WATER' \n",
      " GROUP BY hpd311calls.created_date;\n",
      " \"\"\"\n",
      "\n",
      "# Query database and save results as df\n",
      "df = pd.read_sql(query, engine)\n",
      "\n",
      "# View first 5 records\n",
      "print(df.head())\n",
      "\n",
      "\n",
      "   > introduction to json\n",
      "\n",
      "based on javascript\n",
      "data is organized into collections of objects\n",
      "attribute value pairs\n",
      "\n",
      "\n",
      "read_json()\n",
      "\n",
      "oriented-record\n",
      "1. list of records\n",
      "\n",
      "oriented-column\n",
      "1. array of values for a column name\n",
      "\n",
      "{ 'age_adjusted_death_rate\":{\n",
      "\t'0': '7.6',\n",
      "\t'1': '8.1'\n",
      "     }\n",
      "}\n",
      "\n",
      "oriented - split\n",
      "\n",
      "{\n",
      "columns:['age_adjusted_death_rate',\n",
      "\t'death_rate',\n",
      "\t'deaths'\n",
      "\t'leading_cause',\n",
      "\t'race_ethnicity',\n",
      "\t'sex',\n",
      "\t'year'\n",
      "],\n",
      "\n",
      "'index':[]\n",
      "'data':[]\n",
      "}\n",
      "\n",
      "sample\n",
      "\n",
      "# Load pandas as pd\n",
      "import pandas as pd\n",
      "\n",
      "# Load the daily report to a data frame\n",
      "pop_in_shelters = pd.read_json('dhs_daily_report.json')\n",
      "\n",
      "# View summary stats about pop_in_shelters\n",
      "print(pop_in_shelters.describe())\n",
      "\n",
      "sample  > orient split\n",
      "\n",
      "try:\n",
      "    # Load the JSON with orient specified\n",
      "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
      "                      orient='split')    \n",
      "                      \n",
      "    # Plot total population in shelters over time\n",
      "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
      "    df.plot(x=\"date_of_census\", \n",
      "            y=\"total_individuals_in_shelter\")\n",
      "    plt.show()\n",
      "    \n",
      "except ValueError:\n",
      "    print(\"pandas could not parse the JSON.\")\n",
      "\n",
      "\n",
      "   > api\n",
      "\n",
      "requests\n",
      "send and get data an url\n",
      "requests.get(url_string)\n",
      "params: dictionary\n",
      "headers: users authentication key\n",
      "\n",
      "response object containing data and meta data\n",
      "response.json()\n",
      "\n",
      "import requests\n",
      "https://api.yelp.com/v3/businesses/search\n",
      "\n",
      "params={\n",
      "\"term\":\"bookstore\",\n",
      "\"location\":\"San Francisco\"\n",
      "}\n",
      "\n",
      "headers={'Authorization':'Bear {}'.format(token) }\n",
      "\n",
      "response = requests.get(url,params=params,headers=headers)\n",
      "data=response.json()\n",
      "print(data)\n",
      "\n",
      "\n",
      "sample  > query yelp\n",
      "\n",
      "# Create dictionary to query API for cafes in NYC\n",
      "parameters = {\"term\":\"cafe\",\n",
      "          \t  \"location\":\"NYC\"}\n",
      "\n",
      "# Query the Yelp API with headers and params set\n",
      "response = requests.get(api_url,\n",
      "                params=parameters,\n",
      "                headers=headers)\n",
      "\n",
      "# Extract JSON data from response\n",
      "data = response.json()\n",
      "\n",
      "# Load \"businesses\" values to a data frame and print head\n",
      "cafes = pd.DataFrame(data['businesses'])\n",
      "print(cafes.head())\n",
      "\n",
      "\n",
      "sample   authorization token\n",
      "\n",
      "# Create dictionary that passes Authorization and key string\n",
      "headers = {'Authorization': \"Bearer {}\".format(api_key)}\n",
      "\n",
      "# Query the Yelp API with headers and params set\n",
      "response = requests.get(api_url,params=params,headers=headers)\n",
      "\n",
      "\n",
      "\n",
      "# Extract JSON data from response\n",
      "data = response.json()\n",
      "\n",
      "# Load \"businesses\" values to a data frame and print names\n",
      "cafes = pd.DataFrame(data['businesses'])\n",
      "print(cafes.name)\n",
      "\n",
      "\n",
      "\n",
      "    nested json data\n",
      "\n",
      "json has attribute-value\n",
      "\n",
      "\n",
      "pandas.io.json  submodule for reading and writing json\n",
      "\n",
      "json_normalize to flatten nested data\n",
      "returns a flatten dataframe\n",
      "\n",
      "import pandas as pd\n",
      "import requests\n",
      "from pandas.io.json import json_normalize\n",
      "\n",
      "\n",
      "# Create dictionary that passes Authorization and key string\n",
      "headers = {'Authorization': \"Bearer {}\".format(api_key)}\n",
      "\n",
      "parameters = {\"term\":\"cafe\",\n",
      "          \t  \"location\":\"NYC\"}\n",
      "\n",
      "# Query the Yelp API with headers and params set\n",
      "response = requests.get(api_url,params=params,headers=headers)\n",
      "\n",
      "# Extract JSON data from response\n",
      "data = response.json()\n",
      "\n",
      "bookstores= json_normalize(data['businesses'],sep='_')\n",
      "print(list(bookstores))\n",
      "\n",
      "json_normalize()\n",
      "1.record_path: string/list of string attributes to nested data\n",
      "2. meta: list of other attributes to load to data frame\n",
      "3. meta_prefix: string to prefix to meta column names\n",
      "\n",
      "\n",
      "df=json_normalize(data['businesses'],\n",
      "\tsep=\"_\",\n",
      "\trecord_path=\"categories\",\n",
      "\tmeta=['name','alias','rating',['coordinates','lattitude'],\n",
      "['coordinates','lattitude']],\n",
      "meta_prefix='biz_')\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Load json_normalize()\n",
      "from pandas.io.json import json_normalize\n",
      "\n",
      "# Isolate the JSON data from the API response\n",
      "data = response.json()\n",
      "\n",
      "# Flatten business data into a data frame, replace separator\n",
      "cafes = json_normalize(data[\"businesses\"],\n",
      "             sep=\"_\")\n",
      "\n",
      "# View data\n",
      "print(cafes.head())\n",
      "\n",
      "sample\n",
      "\n",
      "# Specify record path to get categories data\n",
      "flat_cafes = json_normalize(data[\"businesses\"],\n",
      "                            sep=\"_\",\n",
      "                    \t\trecord_path=\"categories\")\n",
      "\n",
      "# View the data\n",
      "print(flat_cafes.head())\n",
      "\n",
      "sample\n",
      "\n",
      "# Load other business attributes and set meta prefix\n",
      "flat_cafes = json_normalize(data[\"businesses\"],\n",
      "                            sep=\"_\",\n",
      "                    \t\trecord_path=\"categories\",\n",
      "                    \t\tmeta=[\"name\", \n",
      "                                  \"alias\",  \n",
      "                                  \"rating\",\n",
      "                          \t\t  [\"coordinates\", \"latitude\"], \n",
      "                          \t\t  [\"coordinates\", \"longitude\"]],\n",
      "                    \t\tmeta_prefix=\"biz_\")\n",
      "\n",
      "# View the data\n",
      "print(flat_cafes.head())\n",
      "\n",
      "   > combining multiple datasets\n",
      "\n",
      "df1.append(d2)\n",
      "\n",
      "\n",
      "bookstores=first_20_bookstores.append(next_20_bookstores,ignore_index=True)\n",
      "\n",
      "set ignore_index=True to renumber rows\n",
      "\n",
      "\n",
      "  > merging\n",
      "\n",
      "merge: is both a pandas function and a data frame method\n",
      "1. second data frame to merge\n",
      "2. column to merge on, left_on, right_on\n",
      "\n",
      "merged= call_counts.merge(weather, left_on='created_date',right_on='date')\n",
      "print(merged.head())\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "# Add an offset parameter to get cafes 51-100\n",
      "params = {\"term\": \"cafe\", \n",
      "          \"location\": \"NYC\",\n",
      "          \"sort_by\": \"rating\", \n",
      "          \"limit\": 50,\n",
      "          \"offset\": 50}\n",
      "\n",
      "result = requests.get(api_url, headers=headers, params=params)\n",
      "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
      "\n",
      "# Append the results, setting ignore_index to renumber rows\n",
      "cafes = top_50_cafes.append(next_50_cafes, ignore_index=True)\n",
      "\n",
      "# Print shape of cafes\n",
      "print(cafes.shape)\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Merge crosswalk into cafes on their zip code fields\n",
      "cafes_with_pumas = cafes.merge(crosswalk,left_on='location_zip_code',\n",
      "right_on='zipcode')\n",
      "\n",
      "print(cafes.info())\n",
      "print(crosswalk.info())\n",
      "\n",
      "# Merge pop_data into cafes_with_pumas on puma field\n",
      "cafes_with_pop = cafes_with_pumas.merge(pop_data,on='puma')\n",
      "\n",
      "# View the data\n",
      "print(cafes_with_pop.head())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\kaggle.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\kaggle.txt\n",
      "1. problem\n",
      "2. data\n",
      "3. model\n",
      "4. submission\n",
      "5. leaderboard\n",
      "\n",
      "download the data\n",
      "build your models\n",
      "\n",
      "\n",
      "taxi_train=pd.read_csv('taxi_train.csv')\n",
      "\n",
      "taxi_train.columns.to_list()\n",
      "\n",
      "taxi_test=pd.read_csv('taxi_test.csv')\n",
      "\n",
      "taxi_test.columns.to_list()\n",
      "\n",
      "submission file:\n",
      "\n",
      "taxi_sample_submission.csv\n",
      "\n",
      "taxi_sample_sub=pd.read_csv('taxi_sample_submission.csv')\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Read train data\n",
      "train = pd.read_csv('train.csv')\n",
      "\n",
      "# Look at the shape of the data\n",
      "print('Train shape:', train.shape)\n",
      "\n",
      "# Look at the head() of the data\n",
      "print(train.head())\n",
      "\n",
      " Read the test data\n",
      "test = pd.read_csv('test.csv')\n",
      "\n",
      "# Print train and test columns\n",
      "print('Train columns:', train.columns.tolist())\n",
      "print('Test columns:', test.columns.tolist())\n",
      "\n",
      "output:\n",
      "Train columns: ['id', 'date', 'store', 'item', 'sales']\n",
      "Test columns: ['id', 'date', 'store', 'item']\n",
      "\n",
      "\n",
      "# Read the sample submission file\n",
      "sample_submission = pd.read_csv('sample_submission.csv')\n",
      "\n",
      "# Look at the head() of the sample submission\n",
      "print(sample_submission.head())\n",
      "\n",
      "\n",
      "            >Prepare your first submission\n",
      "\n",
      "taxi_train=pd.read_csv('taxi_train.csv')\n",
      "taxi_train.columns.to_list()\n",
      "\n",
      "what is the problem type (regression, classification)\n",
      "\n",
      "\n",
      "taxi_train.fare_amount.hist(bins=30, alpha=0.5)\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lr=LinearRegression()\n",
      "\n",
      "lr.fit(X=taxi_train[['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']],\n",
      "\ty=taxi_train['fare_amount'])\n",
      "\n",
      "features=['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']\n",
      "\n",
      "taxi_test['fare_amount']=lr.predict(taxi_test[features])\n",
      "\n",
      "  submission file\n",
      "\n",
      "key\n",
      "fare_amount\n",
      "\n",
      "taxi_submission=taxi_test[['key','fare_amount']]\n",
      "\n",
      "taxi_submission.to_csv('first_sub.csv',index=False)\n",
      "\n",
      "\n",
      "   sample submission\n",
      "\n",
      "features=['id', 'date', 'store', 'item', 'sales']\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "# Read the train data\n",
      "train = pd.read_csv('train.csv')\n",
      "\n",
      "print(train.columns.to_list())\n",
      "\n",
      "# Create a Random Forest object\n",
      "rf = RandomForestRegressor()\n",
      "\n",
      "# Train a model\n",
      "rf.fit(X=train[['store', 'item']], y=train['sales'])\n",
      "\n",
      "# Read test and sample submission data\n",
      "test = pd.read_csv('test.csv')\n",
      "sample_submission = pd.read_csv('sample_submission.csv')\n",
      "\n",
      "# Show the head() of the sample_submission\n",
      "print(sample_submission.head())\n",
      "\n",
      "# Get predictions for the test set\n",
      "test['sales'] = rf.predict(test[['store', 'item']])\n",
      "\n",
      "# Write test predictions using the sample_submission format\n",
      "test[['id', 'sales']].to_csv('kaggle_submission.csv', index=False)\n",
      "\n",
      "\n",
      "  > public vs private leaderboard\n",
      "\n",
      "evaluation metric\n",
      "\n",
      "1. area under the roc (auc) (classification)\n",
      "2. f1 score (classification)\n",
      "3. mean log loss (logloss) (classification)\n",
      "4. mean absolute error (mae) (regression)\n",
      "5. mean squared error (mse) (regression)\n",
      "6. mean average precision Ranking (ranking)\n",
      "\n",
      "test split\n",
      "1. public test\n",
      "2. private test\n",
      "\n",
      "\n",
      "submission[['id','target']].to_csv('submission_1.csv',index=False)\n",
      "\n",
      "\n",
      "as model complexity increases the train data error goes down but the test data error goes up\n",
      "\n",
      "private LB\n",
      "public LB\n",
      "\n",
      "shake-up\n",
      "\n",
      "   > sample  > xgboost  > max_depth=2\n",
      "\n",
      "import xgboost as xgb\n",
      "\n",
      "# Create DMatrix on train data\n",
      "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
      "                     label=train['sales'])\n",
      "\n",
      "# Define xgboost parameters\n",
      "params = {'objective': 'reg:linear',\n",
      "          'max_depth': 2,\n",
      "          'silent': 1}\n",
      "\n",
      "# Train xgboost model\n",
      "xg_depth_2 = xgb.train(params=params, dtrain=dtrain)\n",
      "\n",
      "\n",
      "     sample  > xgboost  > max_depth=8\n",
      "\n",
      "import xgboost as xgb\n",
      "\n",
      "# Create DMatrix on train data\n",
      "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
      "                     label=train['sales'])\n",
      "\n",
      "# Define xgboost parameters\n",
      "params = {'objective': 'reg:linear',\n",
      "          'max_depth': 8,\n",
      "          'silent': 1}\n",
      "\n",
      "# Train xgboost model\n",
      "xg_depth_8 = xgb.train(params=params, dtrain=dtrain)\n",
      "\n",
      "dtrain = xgb.DMatrix(data=train[['store', 'item']])\n",
      "dtest = xgb.DMatrix(data=test[['store', 'item']])\n",
      "\n",
      "# For each of 3 trained models\n",
      "for model in [xg_depth_2, xg_depth_8, xg_depth_15]:\n",
      "    # Make predictions\n",
      "    train_pred = model.predict(dtrain)     \n",
      "    test_pred = model.predict(dtest)          \n",
      "    \n",
      "    # Calculate metrics\n",
      "    mse_train = mse(train['sales'], train_pred)                  \n",
      "    mse_test =mse(test['sales'], test_pred)\n",
      "    print('MSE Train: {:.3f}. MSE Test: {:.3f}'.format(mse_train, mse_test))\n",
      "\n",
      "output:\n",
      "\n",
      "MSE Train: 631.275. MSE Test: 558.522\n",
      "MSE Train: 183.771. MSE Test: 337.337\n",
      "MSE Train: 134.984. MSE Test: 355.534\n",
      "\n",
      "       Understand the problem\n",
      "\n",
      "solution workflow\n",
      "1. understand the problem\n",
      "2. eda - explore data analysis\n",
      "3. local validation\n",
      "4. modeling\n",
      "\n",
      "\n",
      "data type: tabular, time series, image, text\n",
      "\n",
      "problem type: classification, regression, ranking\n",
      "\n",
      "evaluation metric: roc au, f1 score, mae, mse\n",
      "\n",
      "from sklearn.metrics import roc_auc_score, f1_score, mean_squared_error\n",
      "\n",
      "def rmsl(y_true, y_pred):\n",
      "\tdiffs=np.log(y_true+1) - np.log(y_pred+1)\n",
      "\tsquares=np.power(diffs,2)\n",
      "\terr=np.sqrt(np.mean(squares))\n",
      "\treturn err\n",
      "\n",
      "\n",
      "def own_mse(y_true, y_pred):\n",
      "  \t# Raise differences to the power of 2\n",
      "    squares = np.power(y_true - y_pred, 2)\n",
      "    # Find mean over all observations\n",
      "    err = np.mean(squares)\n",
      "    return err\n",
      "\n",
      "\n",
      "   sample  > log loss\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "# Import log_loss from sklearn\n",
      "from sklearn.metrics import log_loss\n",
      "\n",
      "# Define your own LogLoss function\n",
      "def own_logloss(y_true, prob_pred):\n",
      "  \t# Find loss for each observation\n",
      "    terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1 - prob_pred)\n",
      "    # Find mean over all observations\n",
      "    err = np.mean(terms) \n",
      "    return -err\n",
      "\n",
      "print('Sklearn LogLoss: {:.5f}'.format(log_loss(y_classification_true, y_classification_pred)))\n",
      "print('Your LogLoss: {:.5f}'.format(own_logloss(y_classification_true, y_classification_pred)))\n",
      "\n",
      "\n",
      "\n",
      "        Initial EDA\n",
      "\n",
      "Exploratory Data Analysis\n",
      "\n",
      "1. size of the data\n",
      "2. properties of the target variable\n",
      "3. properties of the features\n",
      "4. generate ideas for feature engineering\n",
      "\n",
      "predict the popularity of an apartment rental listing\n",
      "\n",
      "target_variable\n",
      "1. interest_level\n",
      "\n",
      "two sigma connect\n",
      "\n",
      "id\n",
      "bathrooms\n",
      "bedrooms\n",
      "building_id\n",
      "latitude\n",
      "longitude\n",
      "manager_id\n",
      "price\n",
      "interest_level\n",
      "\n",
      "df.interest_level.value_counts()\n",
      "\n",
      "df.describe()\n",
      "1. count\n",
      "2. std\n",
      "3. min\n",
      "4. 25%\n",
      "5. 50%\n",
      "6. 75%\n",
      "7. max\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.style.use('ggplot')\n",
      "\n",
      "prices = df.groupby('interest_level', as_index=False)['price'].median()\n",
      "\n",
      "fig=plt.figure(figsize=(7,5))\n",
      "\n",
      "plt.bar(prices.interest_level, prices.price, width=0.5, alpha=0.8)\n",
      "\n",
      "plt.xlabel('Interest level')\n",
      "plt.ylabel('Median price')\n",
      "plt.title('Median listing price across interest level')\n",
      "\n",
      "plt.show()\n",
      "\n",
      " > sample \n",
      "\n",
      "# Shapes of train and test data\n",
      "print('Train shape:', train.shape)\n",
      "print('Test shape:', test.shape)\n",
      "\n",
      "# Train head()\n",
      "print(train.head())\n",
      "\n",
      "# Describe the target variable\n",
      "print(train.fare_amount.describe())\n",
      "\n",
      "# Train distribution of passengers within rides\n",
      "print(train.passenger_count.value_counts())\n",
      "\n",
      "\n",
      "# Calculate the ride distance\n",
      "train['distance_km'] = haversine_distance(train)\n",
      "\n",
      "# Draw a scatterplot\n",
      "plt.scatter(x=train['fare_amount'], y=train['distance_km'], alpha=0.5)\n",
      "plt.xlabel('Fare amount')\n",
      "plt.ylabel('Distance, km')\n",
      "plt.title('Fare amount based on the distance')\n",
      "\n",
      "# Limit on the distance\n",
      "plt.ylim(0, 50)\n",
      "plt.show()\n",
      "\n",
      "  > sample fare amount on day time\n",
      "\n",
      "# Create hour feature\n",
      "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n",
      "train['hour'] = train.pickup_datetime.dt.hour\n",
      "\n",
      "# Find median fare_amount for each hour\n",
      "hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()\n",
      "\n",
      "# Plot the line plot\n",
      "plt.plot(hour_price['hour'], hour_price['fare_amount'], marker='o')\n",
      "plt.xlabel('Hour of the day')\n",
      "plt.ylabel('Median fare amount')\n",
      "plt.title('Fare amount based on day time')\n",
      "plt.xticks(range(24))\n",
      "plt.show()\n",
      "\n",
      "     >Local validation\n",
      "\n",
      "private LB overfitting\n",
      "\n",
      "holdout set\n",
      "\n",
      "train data-> train set and holdout set\n",
      "\n",
      "train set -> training ->model ->predicting -> holdout set   (assess model quality)\n",
      "\n",
      "       cross validation using k-folding\n",
      "\n",
      "the test on each fold is on data the model has never seen before\n",
      "\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "kdf=KFold(n_splits=5, shuffle=True, random_state=123)\n",
      "\n",
      "for train_index, test_index = kdf.split(train):\n",
      "\tcv_train,cv_test=train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "str_kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
      "\n",
      "for train_index, test_index = str_kf.split(train):\n",
      "\tcv_train,cv_test=train.iloc[train_index], train.iloc[test_index]\n",
      "\t\n",
      "\t\n",
      "\n",
      "    sample    KFold\n",
      "\n",
      "# Import KFold\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "# Create a KFold object\n",
      "kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
      "\n",
      "# Loop through each split\n",
      "fold = 0\n",
      "for train_index, test_index in kf.split(train):\n",
      "    # Obtain training and testing folds\n",
      "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "    print('Fold: {}'.format(fold))\n",
      "    print('CV train shape: {}'.format(cv_train.shape))\n",
      "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
      "    fold += 1\n",
      "\n",
      "   > sample    stratified Fold\n",
      "\n",
      "# Import StratifiedKFold\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "# Create a StratifiedKFold object\n",
      "str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
      "\n",
      "# Loop through each split\n",
      "fold = 0\n",
      "for train_index, test_index in str_kf.split(train, train['interest_level']):\n",
      "    # Obtain training and testing folds\n",
      "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "    print('Fold: {}'.format(fold))\n",
      "    print('CV train shape: {}'.format(cv_train.shape))\n",
      "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
      "    fold += 1\n",
      "\n",
      "\n",
      "       >Validation Usage\n",
      "\n",
      "data leakage\n",
      "1. leak in the features: using data that will not be available in the real setting\n",
      "\n",
      "2. leak in validation strategy - validation strategy differs from the real-world situation\n",
      "\n",
      "\n",
      "    Time K-fold cross validation\n",
      "\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "\n",
      "time_kfold=TimeSeriesSpit(n_splits=5)\n",
      "\n",
      "train=train.sort_values('date')\n",
      "\n",
      "from train_index, test_index in time_kfold.split(train):\n",
      "\t cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "\n",
      "    validation pipeline CV_STRATEGY\n",
      "\n",
      "#list for results\n",
      "\n",
      "fold_metrics=[]\n",
      "\n",
      "for train_index, test_index in CV_STRATEGY.split(train):\n",
      "\t cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "\tmodel.fit(cv_train)\n",
      "\tpredictions=model.predict(cv_test)\n",
      "\t\n",
      "\tmetric=evaluate(cv_test, predictions)\n",
      "\n",
      "\tfold_metrics.append(metric)\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "mean_score = np.mean(fold_metrics)\n",
      "\n",
      "overall_score_minimizing = no.mean(fold_metrics)+ np.std(fold_metrics)\n",
      "\n",
      "overall_score_maximizing = no.mean(fold_metrics)- np.std(fold_metrics)\n",
      "\n",
      "\n",
      "  > sample  > timeseries fold\n",
      "\n",
      "# Create TimeSeriesSplit object\n",
      "time_kfold = TimeSeriesSplit(n_splits=3)\n",
      "\n",
      "# Sort train data by date\n",
      "train = train.sort_values('date')\n",
      "\n",
      "# Iterate through each split\n",
      "fold = 0\n",
      "for train_index, test_index in time_kfold.split(train):\n",
      "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "\n",
      "    \n",
      "    print('Fold :', fold)\n",
      "    print('Train date range: from {} to {}'.format(cv_train.date.min(), cv_train.date.max()))\n",
      "    print('Test date range: from {} to {}\\n'.format(cv_test.date.min(), cv_test.date.max()))\n",
      "    fold += 1\n",
      "\n",
      "\n",
      "\n",
      "Fold : 0\n",
      "Train date range: from 2017-12-01 to 2017-12-08\n",
      "Test date range: from 2017-12-08 to 2017-12-16\n",
      "\n",
      "Fold : 1\n",
      "Train date range: from 2017-12-01 to 2017-12-16\n",
      "Test date range: from 2017-12-16 to 2017-12-24\n",
      "\n",
      "Fold : 2\n",
      "Train date range: from 2017-12-01 to 2017-12-24\n",
      "Test date range: from 2017-12-24 to 2017-12-31\n",
      "\n",
      "\n",
      "the test date ranges do not overlap\n",
      "\n",
      "\n",
      "  > sample get the mean validation mse:\n",
      "\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "import numpy as np\n",
      "\n",
      "# Sort train data by date\n",
      "train = train.sort_values('date')\n",
      "\n",
      "# Initialize 3-fold time cross-validation\n",
      "kf = TimeSeriesSplit(n_splits=3)\n",
      "\n",
      "# Get MSE scores for each cross-validation split\n",
      "mse_scores = get_fold_mse(train, kf)\n",
      "\n",
      "print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))\n",
      "\n",
      "print('MSE by fold: {}'.format(mse_scores))\n",
      "\n",
      "print('Overall validation MSE: {:.5f}'.format(np.mean(mse_scores) + np.std(mse_scores)))\n",
      "\n",
      "output:\n",
      "Mean validation MSE: 955.49186\n",
      "MSE by fold: [890.30336, 961.65797, 1014.51424]\n",
      "Overall validation MSE: 1006.38784\n",
      "\n",
      "\n",
      "        feature engineering\n",
      "\n",
      "modeling\n",
      "\n",
      "1. create new features\n",
      "2. improve models\n",
      "3. apply tricks\n",
      "4. preprocess data\n",
      "\n",
      "feature engineering is creating new features\n",
      "1. numerical\n",
      "2. categorical\n",
      "3. datetime\n",
      "4. coordinates\n",
      "5. text\n",
      "6. images\n",
      "\n",
      "data = pd.concat([train,test])\n",
      "\n",
      "train=data[data.id.isin(train_id)]\n",
      "test=data[data.id.isin(test_id)]\n",
      "\n",
      "dem['date']=pd.to_datetime(dem['date'])\n",
      "\n",
      "dem['year']=dem['date'].dt.year\n",
      "dem['month']=dem['date'].dt.month\n",
      "dem['day']=dem['date'].dt.day\n",
      "dem['dayofweek']=dem['date'].dt.dayofweek\n",
      "\n",
      "\n",
      "  sample    create a new feature\n",
      "\n",
      "# Look at the initial RMSE\n",
      "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
      "\n",
      "# Find the total area of the house\n",
      "train['TotalArea'] = train[\"TotalBsmtSF\"] + train[\"FirstFlrSF\"] + train[\"SecondFlrSF\"]\n",
      "\n",
      "# Look at the updated RMSE\n",
      "print('RMSE with total area:', get_kfold_rmse(train))\n",
      "\n",
      "# Find the area of the garden\n",
      "train['GardenArea'] = train[\"LotArea\"] - train[\"FirstFlrSF\"]\n",
      "print('RMSE with garden area:', get_kfold_rmse(train))\n",
      "\n",
      "RMSE before feature engineering: 36029.39\n",
      "RMSE with total area: 35073.2\n",
      "RMSE with garden area: 34413.55\n",
      "\n",
      "# Find total number of bathrooms\n",
      "train['TotalBath'] = train['FullBath']+train['HalfBath']\n",
      "print('RMSE with number of bathrooms:', get_kfold_rmse(train))\n",
      "\n",
      "RMSE with number of bathrooms: 34506.78\n",
      "\n",
      "Here you see that house area improved the RMSE by almost $1,000. Adding garden area improved the RMSE by another $600. However, with the total number of bathrooms, the RMSE has increased. It means that you keep the new area features, but do not add \"TotalBath\" as a new feature. Let's now work with the datetime features!\n",
      "\n",
      "\n",
      "    sample  > new datetime feature\n",
      "\n",
      "# Concatenate train and test together\n",
      "taxi = pd.concat([train, test])\n",
      "\n",
      "# Convert pickup date to datetime object\n",
      "taxi['pickup_datetime'] = pd.to_datetime(taxi['pickup_datetime'])\n",
      "\n",
      "# Create a day of week feature\n",
      "taxi['dayofweek'] = taxi['pickup_datetime'].dt.dayofweek\n",
      "\n",
      "# Create an hour feature\n",
      "taxi['hour'] = taxi['pickup_datetime'].dt.hour\n",
      "\n",
      "# Split back into train and test\n",
      "new_train = taxi[taxi['id'].isin(train['id'])]\n",
      "new_test = taxi[taxi['id'].isin(test['id'])]\n",
      "\n",
      "\n",
      "        categorical features\n",
      "\n",
      "label encoding\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "4 a\n",
      "\n",
      "encoded\n",
      "1 0\n",
      "2 1\n",
      "3 2\n",
      "4 0\n",
      "\n",
      "\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "le = LabelEncoder()\n",
      "\n",
      "df['cat_encoded'] = le.fit_transform(df['cat'])\n",
      "\n",
      "to overcome the independency between categories, one hot encoding was developed\n",
      "\n",
      "ohe=pd.get_dummies(df['cat'], prefix='ohe_cat')\n",
      "\n",
      "df.drop('cat',axis=1,inplace=True)\n",
      "df=pd.concat([df,ohe],axis=1)\n",
      "\n",
      "  > binary features\n",
      "yes or no\n",
      "\n",
      "le=LabelEncoder()\n",
      "\n",
      "binary_feature['binary_encoded']=le.fit_transform(binary_feature['binary_feat'])\n",
      "\n",
      " > other encoders\n",
      "\n",
      "target encoder\n",
      "\n",
      "  > sample  > labelEncoder\n",
      "\n",
      "# Concatenate train and test together\n",
      "houses = pd.concat([train, test])\n",
      "\n",
      "# Label encoder\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "le = LabelEncoder()\n",
      "\n",
      "# Create new features\n",
      "houses['RoofStyle_enc'] = le.fit_transform(houses[\"RoofStyle\"])\n",
      "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
      "\n",
      "# Look at new features\n",
      "print(houses[['RoofStyle', 'RoofStyle_enc', 'CentralAir', 'CentralAir_enc']].head())\n",
      "\n",
      "\n",
      "\n",
      "    problem with label encoding\n",
      "\n",
      "The problem with label encoding is that it implicitly assumes that there is a ranking dependency between the categories.\n",
      "\n",
      "\n",
      "  > sample  value counts between roof style and central air\n",
      "\n",
      "# Concatenate train and test together\n",
      "houses = pd.concat([train, test])\n",
      "\n",
      "# Look at feature distributions\n",
      "print(houses['RoofStyle'].value_counts(), '\\n')\n",
      "print(houses['CentralAir'].value_counts())\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "Name: RoofStyle, dtype: int64 \n",
      "\n",
      "Y    2723\n",
      "N     196\n",
      "\n",
      "Name: CentralAir, dtype: int64\n",
      "\n",
      "  > encode CentralAir as binary 0 or 1\n",
      "\n",
      "# Concatenate train and test together\n",
      "houses = pd.concat([train, test])\n",
      "\n",
      "# Label encode binary 'CentralAir' feature\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "le = LabelEncoder()\n",
      "houses['CentralAir_enc'] = le.fit_transform(houses[\"CentralAir\"])\n",
      "\n",
      " >one hot encode\n",
      "\n",
      "# Create One-Hot encoded features\n",
      "ohe = pd.get_dummies(houses['RoofStyle'], prefix='RoofStyle')\n",
      "\n",
      "# Concatenate OHE features to houses\n",
      "houses = pd.concat([houses, ohe], axis=1)\n",
      "\n",
      "# Look at OHE features\n",
      "print(houses[[col for col in houses.columns if 'RoofStyle' in col]].head(3))\n",
      "\n",
      "\n",
      "     >Target encoding\n",
      "\n",
      "1. label encoder provides distinct number for each category\n",
      "\n",
      "2. one-hot encoder creates new features for each category value\n",
      "\n",
      "target encoding creates a single column\n",
      "\n",
      "1. calculate mean on the train, apply to the test\n",
      "2. split train into K folds.  calculate mean on k-1 folds, apply to the k-th fold.  this prevents overfitting\n",
      "3. add mean target encoded feature to the model\n",
      "\n",
      "\n",
      "  > sample    categorical  target\n",
      "def train_mean_target_encoding(train, target, categorical, alpha=5):\n",
      "    # Create 5-fold cross-validation\n",
      "    kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
      "    train_feature = pd.Series(index=train.index)\n",
      "\n",
      "    # For each folds split\n",
      "    for train_index, test_index in kf.split(train):\n",
      "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "        # Calculate out-of-fold statistics and apply to cv_test\n",
      "        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)\n",
      "        # Save new feature for this particular fold\n",
      "        train_feature.iloc[test_index] = cv_test_feature       \n",
      "    return train_feature.values\n",
      "\n",
      "def test_mean_target_encoding(train, test, target, categorical, alpha=5):\n",
      "    # Calculate global mean on the train data\n",
      "    global_mean = train[target].mean()\n",
      "    \n",
      "    # Group by the categorical feature and calculate its properties\n",
      "    train_groups = train.groupby(categorical)\n",
      "    category_sum = train_groups[target].sum()\n",
      "    category_size = train_groups.size()\n",
      "    \n",
      "    # Calculate smoothed mean target statistics\n",
      "    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)\n",
      "    \n",
      "    # Apply statistics to the test data and fill new categories\n",
      "    test_feature = test[categorical].map(train_statistics).fillna(global_mean)\n",
      "    return test_feature.values\n",
      "\n",
      "\n",
      "def mean_target_encoding(train, test, target, categorical, alpha=5):\n",
      "  \n",
      "    # Get the train feature\n",
      "    train_feature = train_mean_target_encoding(train, target, categorical, alpha)\n",
      "  \n",
      "    # Get the test feature\n",
      "    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)\n",
      "    \n",
      "    # Return new features to add to the model\n",
      "    return train_features, test_features\n",
      "\n",
      "\n",
      " code\n",
      "\n",
      "# For each folds split\n",
      "    for train_index, test_index in kf.split(train):\n",
      "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
      "      \n",
      "        # Calculate out-of-fold statistics and apply to cv_test\n",
      "\n",
      "        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)\n",
      "        \n",
      "        # Save new feature for this particular fold\n",
      "        train_feature.iloc[test_index] = cv_test_feature       \n",
      "\n",
      "    return train_feature.values\n",
      "\n",
      "\n",
      "  > target categorical\n",
      "\n",
      "# Create 5-fold cross-validation\n",
      "kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
      "\n",
      "# For each folds split\n",
      "for train_index, test_index in kf.split(bryant_shots):\n",
      "    cv_train, cv_test = bryant_shots.iloc[train_index], bryant_shots.iloc[test_index]\n",
      "\n",
      "    # Create mean target encoded feature\n",
      "    cv_train['game_id_enc'], cv_test['game_id_enc'] = mean_target_encoding(train=cv_train,\n",
      "                                                                           test=cv_test,\n",
      "                                                                           target='shot_made_flag',\n",
      "                                                                           categorical='game_id',\n",
      "                                                                           alpha=5)\n",
      "    # Look at the encoding\n",
      "    print(cv_train[['game_id', 'shot_made_flag', 'game_id_enc']].sample(n=1))\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "<script.py> output:\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    7106  20500532             0.0     0.361914\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    5084  20301100             0.0     0.568395\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    6687  20500228             0.0      0.48131\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    5046  20301075             0.0     0.252103\n",
      "           game_id  shot_made_flag  game_id_enc\n",
      "    4662  20300515             1.0     0.452637\n",
      "\n",
      "\n",
      "The main conclusion you should make: while using local cross-validation, you need to repeat mean target encoding procedure inside each folds split separately. Go on to try other problem types beyond binary classification!\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "# Create mean target encoded feature\n",
      "train['RoofStyle_enc'], test['RoofStyle_enc'] = mean_target_encoding(train=train,\n",
      "                                                                     test=test,\n",
      "                                                                     target='SalePrice',\n",
      "                                                                     categorical='RoofStyle',\n",
      "                                                                     alpha=10)\n",
      "\n",
      "# Look at the encoding\n",
      "print(test[['RoofStyle', 'RoofStyle_enc']].drop_duplicates())\n",
      "\n",
      "\n",
      "\n",
      "output:\n",
      "<script.py> output:\n",
      "         RoofStyle  RoofStyle_enc\n",
      "    0        Gable  171565.947836\n",
      "    1          Hip  217594.645131\n",
      "    98     Gambrel  164152.950424\n",
      "    133       Flat  188703.563431\n",
      "    362    Mansard  180775.938759\n",
      "    1053      Shed  188267.663242\n",
      "\n",
      "\n",
      "So, you observe that houses with the Hip roof are the most pricy, while houses with the Gambrel roof are the cheapest.\n",
      "\n",
      "\n",
      "   > Missing data\n",
      "\n",
      "Mean/Median imputation\n",
      "\n",
      "categorical missing data replaced with the most frequent value\n",
      "\n",
      "new category imputation\n",
      "\n",
      "df.isnull().head(1)\n",
      "\n",
      "\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "mean_imputer = SimpleImputer(strategy='mean')\n",
      "\n",
      "constant_imputer = SimpleImputer(strategy='constant', fill_value=-999)\n",
      "\n",
      "\n",
      "df[['num']] = mean_imputer.fit_transform(df[['num']])\n",
      "\n",
      "\n",
      "constant_imputer = SimpleImputer(strategy='constant', fill_value='MISS')\n",
      "\n",
      "   sample  > find columns with missing data\n",
      "\n",
      "# Read dataframe\n",
      "twosigma = pd.read_csv(\"twosigma_train.csv\")\n",
      "\n",
      "# Find the number of missing values in each column\n",
      "print(twosigma.isnull().sum())\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "id                 0\n",
      "bathrooms          0\n",
      "bedrooms           0\n",
      "building_id       13\n",
      "latitude           0\n",
      "longitude          0\n",
      "manager_id         0\n",
      "price             32\n",
      "interest_level     0\n",
      "dtype: int64\n",
      "\n",
      " # Look at the columns with the missing values\n",
      "print(twosigma[['building_id', 'price']].head())\n",
      "\n",
      "\n",
      " > sample  > imputer mean\n",
      "\n",
      "# Import SimpleImputer\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "# Create mean imputer\n",
      "mean_imputer = SimpleImputer(strategy='mean')\n",
      "\n",
      "# Price imputation\n",
      "rental_listings[['price']] = mean_imputer.fit_transform(rental_listings[['price']])\n",
      "\n",
      "   sample  > simple imputer constant\n",
      "\n",
      "# Import SimpleImputer\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "# Create constant imputer\n",
      "constant_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
      "\n",
      "# building_id imputation\n",
      "rental_listings[['building_id']] = constant_imputer.fit_transform(rental_listings[['building_id']])\n",
      "\n",
      "     >baseline model\n",
      "\n",
      "taxi_train=pd.read_csv('taxi_train.csv')\n",
      "taxi_test=pd.read_csv('taxi_test.csv')\n",
      "\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "validation_train, validation_test=train_test_split(taxi_train, test_size=0.3, random_state=123)\n",
      "\n",
      "taxi_test['fare_amount']=np.mean(taxi_train.fair_amount)\n",
      "\n",
      "#### mean_sub\n",
      "taxi_test[['id','fare_amount']].to_csv('mean_sub.csv',index=False)\n",
      "\n",
      "naive_prediction_groups=taxi_train.groupby('passenger_count').fare_amount.mean()\n",
      "\n",
      "taxi_test['fare_amount']=taxi_test.passenger_count.map(naive_prediction_groups)\n",
      "\n",
      "#map- Used for substituting each value in a Series with another value\n",
      "\n",
      "### mean group sub\n",
      "\n",
      "taxi_test[['id','fare_amount']].to_csv('mean_group_sub.csv', index=False)\n",
      "\n",
      "#select only numeric features\n",
      "\n",
      "features['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']\n",
      "\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "gb=GradientBoostingRegressor()\n",
      "\n",
      "gb.fit(taxi_train[features], taxi_train.fare_amount)\n",
      "\n",
      "taxi_test['fare_amount']=gb.predict(taxi_test[features])\n",
      "\n",
      "### gradient boost\n",
      "\n",
      "taxi_test[['id','fare_amount']].to_csv('gb_sub.csv',index=False)\n",
      "\n",
      "\n",
      "model\t\tvalidation RMSE  public LB RMSE\n",
      "\n",
      "simple mean\t9.986\t9.409\n",
      "group mean\t9.978\t9.407\n",
      "gradient boost\t5.996\t4.595\n",
      "\n",
      "\n",
      "   sample  > hold out\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from math import sqrt\n",
      "\n",
      "# Calculate the mean fare_amount on the validation_train data\n",
      "naive_prediction = np.mean(validation_train['fare_amount'])\n",
      "\n",
      "# Assign naive prediction to all the holdout observations\n",
      "validation_test['pred'] = naive_prediction\n",
      "\n",
      "# Measure the local RMSE\n",
      "rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))\n",
      "print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))\n",
      "\n",
      "Validation RMSE for Baseline I model: 9.986\n",
      "\n",
      "\n",
      "   sample Group by hour\n",
      "\n",
      "# Get pickup hour from the pickup_datetime column\n",
      "train['hour'] = train['pickup_datetime'].dt.hour\n",
      "test['hour'] = test['pickup_datetime'].dt.hour\n",
      "\n",
      "# Calculate average fare_amount grouped by pickup hour \n",
      "hour_groups = train.groupby('hour')['fare_amount'].mean()\n",
      "\n",
      "# Make predictions on the test set\n",
      "test['fare_amount'] = test.hour.map(hour_groups)\n",
      "\n",
      "# Write predictions\n",
      "test[['id','fare_amount']].to_csv('hour_mean_sub.csv', index=False)\n",
      "\n",
      "\n",
      "   hyperparameter tuning\n",
      "\n",
      "add hour feature: validation rmse 5.553\n",
      "\n",
      "add distance feature: validation rmse 5.268\n",
      "\n",
      "deep learning does not require feature engineering\n",
      "\n",
      "least squares linear regression\n",
      "1. loss = (y_i-yhat_i)**2 -> min\n",
      "\n",
      "ridge regression\n",
      "loss =  (y_i-yhat_i)**2 + alpha * weights**2\n",
      "\n",
      "popular approachs\n",
      "1. Grid Search\n",
      "2. Random Grid Search\n",
      "3. Bayesian optimization\n",
      "\n",
      "\n",
      "     >Grid search\n",
      "\n",
      "alpha_grid=[0.01,0.1,1,10]\n",
      "\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "results={}\n",
      "\n",
      "for candidate_alpha in alpha_grid:\n",
      "\tridge_regression=Ridge(alpha=candidate_alpha)\n",
      "\n",
      "\tresults[candidate_alpha]=validation_score\n",
      "\n",
      "  > sample max depth\n",
      "\n",
      "# Possible max depth values\n",
      "max_depth_grid = [3,6,9,12,15]\n",
      "results = {}\n",
      "\n",
      "# For each value in the grid\n",
      "for max_depth_candidate in max_depth_grid:\n",
      "    # Specify parameters for the model\n",
      "    params = {'max_depth': max_depth_candidate}\n",
      "\n",
      "    # Calculate validation score for a particular hyperparameter\n",
      "    validation_score = get_cv_score(train, params)\n",
      "\n",
      "    # Save the results for each max depth value\n",
      "    results[max_depth_candidate] = validation_score   \n",
      "print(results)\n",
      "\n",
      "output:\n",
      "\n",
      "{3: 6.50509, 6: 6.52138, 9: 6.64181, 12: 6.8819, 15: 6.99156}\n",
      "\n",
      "\n",
      "\n",
      "The drawback of tuning each hyperparameter independently is a potential dependency between different hyperparameters. The better approach is to try all the possible hyperparameter combinations.\n",
      "\n",
      "\n",
      "       Sample Product with parameters\n",
      "\n",
      "import itertools\n",
      "\n",
      "# Hyperparameter grids\n",
      "max_depth_grid = [3, 5, 7]\n",
      "subsample_grid = [0.8, 0.9, 1.0]\n",
      "results = {}\n",
      "\n",
      "# For each couple in the grid\n",
      "for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid):\n",
      "    params = {'max_depth': max_depth_candidate,\n",
      "              'subsample': subsample_candidate}\n",
      "    validation_score = get_cv_score(train, params)\n",
      "    # Save the results for each couple\n",
      "    results[(max_depth_candidate, subsample_candidate)] = validation_score   \n",
      "print(results)\n",
      "\n",
      "\n",
      "{(3, 0.8): 6.33917, (3, 0.9): 6.43642, (3, 1.0): 6.50509, (5, 0.8): 6.26977, (5, 0.9): 6.35116, (5, 1.0): 6.45468, (7, 0.8): 6.1635, (7, 0.9): 6.34018, (7, 1.0): 6.48436}\n",
      "\n",
      "With max_depth equal to 7 and subsample equal to 0.8, the best RMSE is now $6.16.\n",
      "\n",
      "(grid_df_class.cv_results_)\n",
      "\n",
      "applies only to classification\n",
      "\n",
      "    Model Ensembling\n",
      "\n",
      "input data:\n",
      "1. categorical\n",
      "2. one hote encoded categorical\n",
      "3. numerics\n",
      "\n",
      "inputs into\n",
      "500 modesl\n",
      "inputs into \n",
      "125 xgboost models\n",
      "\n",
      "different subsets\n",
      "40 models and 60 models\n",
      "\n",
      "input into \n",
      "5 models of keras\n",
      "\n",
      "weighted Rank Average\n",
      "\n",
      "\n",
      "  > Regression problem\n",
      "\n",
      "regression classifier\n",
      "\n",
      "train two models a and b\n",
      "\n",
      "\n",
      "  > model stacking\n",
      "\n",
      "1. split train data into two parts\n",
      "2. train multiple models on part 1\n",
      "3. make predictions on part 2\n",
      "4. make predictions on the test data\n",
      "\n",
      "5. train a new model on part 2 using predictions as features\n",
      "\n",
      "6. make predictions on the test data using the 2nd level model.\n",
      "\n",
      "train models A, b, c on part 1\n",
      "\n",
      "trainid   feature1 feature2 Target A_pred, B_pred, c_pred\n",
      "\n",
      "4\t.10\t2.87\t1\t0.71\t0.52\t.098\n",
      "\n",
      "make predictions of the test data as well\n",
      "\n",
      "testid   feature1 feature2 Target A_pred, B_pred, c_pred\n",
      "\n",
      "\n",
      "train 2nd level model on part 2 using the train and test data set from part 1\n",
      "\n",
      "resulting in a stacking prediction\n",
      "\n",
      "    sample add new feature gb and rf part 1\n",
      "\n",
      "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
      "\n",
      "# Train a Gradient Boosting model\n",
      "gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\n",
      "\n",
      "# Train a Random Forest model\n",
      "rf = RandomForestRegressor().fit(train[features], train.fare_amount)\n",
      "\n",
      "# Make predictions on the test data\n",
      "test['gb_pred'] = gb.predict(test[features])\n",
      "test['rf_pred'] = rf.predict(test[features])\n",
      "\n",
      "# Find mean of model predictions\n",
      "test['blend'] = (test['gb_pred'] + test['rf_pred']) / 2\n",
      "print(test[['gb_pred', 'rf_pred', 'blend']].head(3))\n",
      "\n",
      "\n",
      "# Split train data into two parts\n",
      "part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n",
      "\n",
      "# Train a Gradient Boosting model on Part 1\n",
      "gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n",
      "\n",
      "# Train a Random Forest model on Part 1\n",
      "rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)\n",
      "\n",
      "# Make predictions on the Part 2 data\n",
      "part_2['gb_pred'] = gb.predict(part_2[features])\n",
      "part_2['rf_pred'] = rf.predict(part_2[features])\n",
      "\n",
      "# Make predictions on the test data\n",
      "test['gb_pred'] = gb.predict(test[features])\n",
      "test['rf_pred'] = rf.predict(test[features])\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Create linear regression model without the intercept\n",
      "lr = LinearRegression(fit_intercept=False)\n",
      "\n",
      "# Train 2nd level model on the Part 2 data\n",
      "lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\n",
      "\n",
      "# Make stacking predictions on the test data\n",
      "test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\n",
      "\n",
      "# Look at the model coefficients\n",
      "print(lr.coef_)\n",
      "\n",
      "output:\n",
      "[0.72504358 0.27647395]\n",
      "\n",
      "Looking at the coefficients, it's clear that 2nd level model has more trust to the Gradient Boosting: 0.7 versus 0.3 for the Random Forest model. \n",
      "\n",
      "        >.Save Information\n",
      "\n",
      "1. save folds to the disk\n",
      "2. save model runs\n",
      "3. save model predictions to the disk\n",
      "4. save performance results\n",
      "\n",
      "\n",
      "forums\n",
      "\n",
      "Competition discussion by the participants\n",
      "\n",
      "Kaggle kernels\n",
      "scripts and notebooks shared by the participants\n",
      "\n",
      "cloud computational environment\n",
      "\n",
      "competitions last 2 to 3 months\n",
      "\n",
      "  > sample drop column and score\n",
      "\n",
      "# Drop passenger_count column\n",
      "new_train_1 = train.drop('passenger_count', axis=1)\n",
      "\n",
      "# Compare validation scores\n",
      "initial_score = get_cv_score(train)\n",
      "new_score = get_cv_score(new_train_1)\n",
      "\n",
      "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))\n",
      "\n",
      "\n",
      " Initial score is 6.50509 and the new score is 6.41902\n",
      "\n",
      "\n",
      "# Create copy of the initial train DataFrame\n",
      "new_train_2 = train.copy()\n",
      "\n",
      "# Find sum of pickup latitude and ride distance\n",
      "new_train_2['weird_feature'] = new_train_2['pickup_latitude'] + new_train_2['distance_km']\n",
      "\n",
      "# Compare validation scores\n",
      "initial_score = get_cv_score(train)\n",
      "new_score = get_cv_score(new_train_2)\n",
      "\n",
      "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))\n",
      "\n",
      "Initial score is 6.50509 and the new score is 6.5121\n",
      "\n",
      " In this particular case, dropping the \"passenger_count\" feature helped, while finding the sum of pickup latitude and ride distance did not. \n",
      "\n",
      "Machine learning models\n",
      "\n",
      "1. talk to business.  Define the problem\n",
      "2. collect the data\n",
      "3. select the metric\n",
      "4. make train and test split\n",
      "5. create the model\n",
      "6. move the model to production\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\kpi.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\kpi.txt\n",
      "import pandas as pd\n",
      "\n",
      "customer_demographics=pd.read_csv('customer_demographics.csv')\n",
      "\n",
      "uid\n",
      "reg_date\n",
      "device\n",
      "gender\n",
      "country\n",
      "age\n",
      "\n",
      "\n",
      "#customer actions\n",
      "customer_subscriptions=pd.read_csv('customer_subscriptions.csv')\n",
      "\n",
      "print(customer_subscriptions.head())\n",
      "\n",
      "uid\n",
      "lapse_date\n",
      "subscription_date\n",
      "price\n",
      "\n",
      "KPI : conversion rate\n",
      "\n",
      "importance across different user groups\n",
      "\n",
      "sub_data_demo=customer_demographics.merge(\n",
      "\tcustomer_subscriptions,\n",
      "\thow='inner',\n",
      "\ton=['uid']\n",
      "\t)\n",
      "\n",
      "\n",
      "\n",
      "   > sample\n",
      "\n",
      "# Import pandas \n",
      "import pandas as pd\n",
      "\n",
      "# Load the customer_data\n",
      "customer_data = pd.read_csv('customer_data.csv')\n",
      "\n",
      "# Load the app_purchases\n",
      "app_purchases = pd.read_csv('inapp_purchases.csv')\n",
      "\n",
      "# Print the columns of customer data\n",
      "print(customer_data.columns)\n",
      "\n",
      "# Print the columns of app_purchases\n",
      "print(app_purchases.columns)\n",
      "\n",
      "\n",
      "Index(['uid', 'reg_date', 'device', 'gender', 'country', 'age'], dtype='object')\n",
      "\n",
      "Index(['date', 'uid', 'sku', 'price'], dtype='object')\n",
      "\n",
      "# Merge on the 'uid' field\n",
      "uid_combined_data = app_purchases.merge(customer_data, on=['uid'], how='inner')\n",
      "\n",
      "# Examine the results \n",
      "print(uid_combined_data.head())\n",
      "print(len(uid_combined_data))\n",
      "\n",
      "\n",
      "date_x       uid            sku  price      date_y device gender country  age\n",
      "0  2017-07-10  41195147  sku_three_499    499  2017-06-26    and      M     BRA   17\n",
      "1  2017-07-15  41195147  sku_three_499    499  2017-06-26    and      M     BRA   17\n",
      "2  2017-11-12  41195147   sku_four_599    599  2017-06-26    and      M     BRA   17\n",
      "3  2017-09-26  91591874    sku_two_299    299  2017-01-05    and      M     TUR   17\n",
      "4  2017-12-01  91591874   sku_four_599    599  2017-01-05    and      M     TUR   17\n",
      "9006\n",
      "In [1]:\n",
      "\n",
      "\n",
      "# Merge on the 'uid' and 'date' field\n",
      "uid_date_combined_data = app_purchases.merge(customer_data, on=['uid', 'date'], how='inner')\n",
      "\n",
      "# Examine the results \n",
      "print(uid_date_combined_data.head())\n",
      "print(len(uid_date_combined_data))\n",
      "\n",
      "\n",
      " uid             sku  price device gender country  age\n",
      "0  2016-03-30  94055095    sku_four_599    599    iOS      F     BRA   16\n",
      "1  2015-10-28  69627745     sku_one_199    199    and      F     BRA   18\n",
      "2  2017-02-02  11604973  sku_seven_1499    499    and      F     USA   16\n",
      "3  2016-06-05  22495315    sku_four_599    599    and      F     USA   19\n",
      "4  2018-02-17  51365662     sku_two_299    299    iOS      M     TUR   16\n",
      "\n",
      "      . exploratory analysis of kpi\n",
      "\n",
      "1. most companies will have many kpis\n",
      "2. each serves a different purpose\n",
      "\n",
      "#axis=0 is columns\n",
      "#as_index will use group labels as index\n",
      "\n",
      "sub_data_grp=sub_data_deep.groupby(by=['country','device'], axis=0, as_index=False)\n",
      "\n",
      "sub_data_grp.mean()\n",
      "or\n",
      "sub_data_grp.agg('mean')\n",
      "or\n",
      "sub_data_grp.agg(['mean','median'])\n",
      "or\n",
      "sub_data_grp.agg({'price':['mean','median','max'],\n",
      "\t'age':['mean','median','max']\n",
      "\t})\n",
      "\n",
      "def truncate_mean(data):\n",
      "\ttop_val=data.quantile(.9)\n",
      "\tbot_val=data.quantile(.1)\n",
      "\ttrunc_data=data[(data<=top_val) & (data>=bot_val)]\n",
      "\tmean=trunc_data.mean()\n",
      "\treturn (mean)\n",
      "\n",
      "\n",
      "sub_data_grp.agg({'age':[truncated_mean]})\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "# Calculate the mean and median purchase price \n",
      "purchase_price_summary = purchase_data.price.agg(['mean', 'median'])\n",
      "\n",
      "# Examine the output \n",
      "print(purchase_price_summary)\n",
      "\n",
      "mean      406.772596\n",
      "median    299.000000\n",
      "\n",
      "# Calculate the mean and median of price and age\n",
      "purchase_summary = purchase_data.agg({'price': ['mean', 'median'], 'age': ['mean', 'median']})\n",
      "\n",
      "# Examine the output \n",
      "print(purchase_summary)\n",
      "\n",
      "             price        age\n",
      "mean    406.772596  23.922274\n",
      "median  299.000000  21.000000\n",
      "\n",
      "\n",
      "Notice how the mean is higher than the median? This suggests that we have some users who are making a lot of purchases!\n",
      "\n",
      "# Group the data \n",
      "grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])\n",
      "\n",
      "# Aggregate the data\n",
      "purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})\n",
      "\n",
      "# Examine the results\n",
      "print(purchase_summary)\n",
      "\n",
      "\n",
      "price                   \n",
      "                     mean median         std\n",
      "device gender                               \n",
      "and    F       400.747504    299  179.984378\n",
      "       M       416.237308    499  195.001520\n",
      "iOS    F       404.435330    299  181.524952\n",
      "       M       405.272401    299  196.843197\n",
      "\n",
      "       calculating a conversion rate\n",
      "\n",
      "import pandas as pd\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "current_date=pd.to_datetime('2018-03-17')\n",
      "\n",
      "#what is the maximum lapse date in our dataset\n",
      "\n",
      "max_lapse_date = current_date - timedelta(days=7)\n",
      "\n",
      "conv_sub_data=sub_data_demo[(sub_data_demo.lapse_date<max_lapse_date)]\n",
      "\n",
      "\n",
      "total_users_count=conv_sub_data.price.count()\n",
      "print(total_users_count)\n",
      "\n",
      "max_sub_date=conv_sub_data.lapse_date+timedelta(days=7)\n",
      "\n",
      "total_subs=conv_sub_data[\n",
      "(conv_sub_data.price>0) &\n",
      "(conv_sub_data.subscription_data<=max_sub_data)\n",
      "]\n",
      "\n",
      "total_sub_count=total_sub.price.count()\n",
      "print(total_subs_count)\n",
      "\n",
      "conversion rate = Total subscribers/potential subscribers\n",
      "\n",
      "conversion_rate = total_subs_count / total_users_count\n",
      "print(conversion_rate)\n",
      "\n",
      "\n",
      "      cohort conversion rate\n",
      "\n",
      "conv_sub_data = conv_sub_data.copy()\n",
      "\n",
      "#keep users who lapsed prior to the last 14 days\n",
      "\n",
      "max_lapse_date = current_date - timedelta(days=14)\n",
      "\n",
      "conv_sub_data = sub_data_demo[\n",
      " (sub_data_demo.lapse_date <=max_lapse_date)\n",
      "]\n",
      "\n",
      "sub time is the number of days been the lapse date and the subscription date\n",
      "\n",
      "np.where receives a number to return a true and one to return a false\n",
      "\n",
      "sub_time = np. where(\n",
      "\tconv_sub_data.subscription_date.notnull(),\n",
      "\t#then find how many days since their lapse\n",
      "\t(conv_sub_data.scription_date - conv_sub_data.lapse_date).dt.days,\n",
      "\t#else set the value to pd.NaT\n",
      "\tpd.NaT)\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "\n",
      "find the conversion rate gcr7() and gcr14()\n",
      "\n",
      "purchase_cohorts=conv_sub_data.groupby(by=['gender','device'],as_index=False)\n",
      "\n",
      "#find the conversion rate for each cohort using gcr7 and gcr14\n",
      "\n",
      "purchase_cohorts.agg({sub_time:[gcr7,gcr14]})\n",
      "\n",
      "     How to choose KPI metrics\n",
      "\n",
      "how long does it take to gain insight on a metric\n",
      "\n",
      "what is an actionable time scale\n",
      "\n",
      "monthly conversion rate = 1 month wait time\n",
      "\n",
      "leverage exploratory data analysis\n",
      "* reveals relationships between metrics and key results\n",
      "\n",
      "KPI should measure strong growth\n",
      "* potential early warning sign of problems\n",
      "* senstive to changes in the overall ecosystem\n",
      "\n",
      "       sample\n",
      "\n",
      "# Compute max_purchase_date \n",
      "max_purchase_date = current_date - timedelta(days=28)\n",
      "\n",
      "# Filter to only include users who registered before our max date\n",
      "purchase_data_filt = purchase_data[purchase_data.reg_date < max_purchase_date]\n",
      "\n",
      "# Filter to contain only purchases within the first 28 days of registration\n",
      "purchase_data_filt = purchase_data_filt[(purchase_data_filt.date <=\n",
      "                                         purchase_data_filt.reg_date + \n",
      "                                         timedelta(days=28))]\n",
      "\n",
      "# Output the mean price paid per purchase\n",
      "print(purchase_data_filt.price.mean())\n",
      "\n",
      "414.4237288135593\n",
      "\n",
      "\n",
      "      find a 1 month of data\n",
      "\n",
      "# Set the max registration date to be one month before today\n",
      "max_reg_date = current_date - timedelta(days=28)\n",
      "\n",
      "# Find the month 1 values:\n",
      "month1 = np.where((purchase_data.reg_date < max_reg_date) &\n",
      "                    (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),\n",
      "                  purchase_data.price, \n",
      "                  np.NaN)\n",
      "                 \n",
      "# Update the value in the DataFrame \n",
      "purchase_data['month1'] = month1\n",
      "\n",
      "print(month1)\n",
      "\n",
      "# Group the data by gender and device \n",
      "purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False)\n",
      "\n",
      "# Aggregate the month1 and price data \n",
      "purchase_summary = purchase_data_upd.agg(\n",
      "                        {'month1': ['mean', 'median'],\n",
      "                        'price': ['mean', 'median']})\n",
      "\n",
      "# Examine the results \n",
      "print(purchase_summary)\n",
      "\n",
      "gender device      month1              price       \n",
      "                       mean median        mean median\n",
      "0      F    and  388.204545  299.0  400.747504    299\n",
      "1      F    iOS  432.587786  499.0  404.435330    299\n",
      "2      M    and  413.705882  399.0  416.237308    499\n",
      "3      M    iOS  433.313725  499.0  405.272401    299\n",
      "\n",
      "\n",
      "\n",
      "      >.Working with time series\n",
      "\n",
      "exploratory data analysis\n",
      "\n",
      "2nd week subscribers\n",
      "\n",
      "exclude customers who have not been on the platform for two weeks\n",
      "\n",
      "\n",
      "current_date=pd.to_datetime('2018-03-17')\n",
      "max_lapse_date=current_date - timedelta(days=14)\n",
      "\n",
      "#find all uses who have not been on the platform in the last two weeks\n",
      "\n",
      "conv_sub_data= sub_data_demo[\n",
      "  sub_data_demo.lapse_date < max_lapse_date\n",
      "]\n",
      "\n",
      "#how many days before the user subscribed\n",
      "\n",
      "sub_time = conv_sub_data.subscription_date -\n",
      "conv_sub_data.lapse_date\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "or\n",
      "conv_sub_data['sub_time']= conv_sub_data.sub_time.dt.days\n",
      "\n",
      "#find the users who have not subscribed in week one but have been on the platform for two or more weeks\n",
      "\n",
      "conv_base=conv_sub_data[\n",
      " (conv_sub_data.sub_time.notnull()) |\n",
      "\t(conv_sub_data.sub_time > 7)]\n",
      "\n",
      "total_users=len(conv_base)\n",
      "\n",
      "total_sub=np.where(conv_sub_data.sub_time.notnull()) &\n",
      "\t(conv_sub_data.sub_time <= 14,1,0)\n",
      "\n",
      "total_subs=sum(total_subs)\n",
      "\n",
      "conversion_rate = total_subs/total_users\n",
      "\n",
      "output\n",
      "0.009\n",
      "\n",
      "    pandas date parser on read_csv\n",
      "\n",
      "pandas.read_csv(\n",
      "\n",
      "\tparse_dates=False\n",
      "\tinfer_datetime_format=False\n",
      "\tkeep_date_col=False\n",
      "\tdate_parser=None\n",
      "\tdayFirst=False\n",
      "\t)\n",
      "\n",
      "strftime\n",
      "\"%Y-%m-%d\"\n",
      "\"%H:%M:%S\"\n",
      "\n",
      "\"%B %d, %Y\"\n",
      "\n",
      "to_datetime\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_one = pd.to_datetime(date_data_one, format=\"%A %B %d, %Y\")\n",
      "print(date_data_one)\n",
      "\n",
      "output:\n",
      "DatetimeIndex(['2017-01-27', '2017-12-02'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_two = pd.to_datetime(date_data_two, format=\"%Y-%m-%d\")\n",
      "print(date_data_two)\n",
      "\n",
      "output:\n",
      "'2017-01-01', '2016-05-03']\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_three = pd.to_datetime(date_data_three, format=\"%m/%d/%Y\")\n",
      "print(date_data_three)\n",
      "\n",
      "output:\n",
      "'1978-08-17', '1976-01-07'\n",
      "\n",
      "# Provide the correct format for the date\n",
      "date_data_four = pd.to_datetime(date_data_four, format=\"%Y %B %d %H:%M\")\n",
      "print(date_data_four)\n",
      "\n",
      "output:\n",
      "2016-03-01 01:56:00', '2016-01-04 02:16:00'\n",
      "\n",
      "    Creating time series graphs with matplotlib\n",
      "\n",
      "#find all uses who have not been on the platform in the last two weeks\n",
      "\n",
      "conv_sub_data= sub_data_demo[\n",
      "  sub_data_demo.lapse_date < max_lapse_date\n",
      "]\n",
      "\n",
      "#how many days before the user subscribed\n",
      "\n",
      "sub_time = conv_sub_data.subscription_date -\n",
      "conv_sub_data.lapse_date\n",
      "\n",
      "conv_sub_data['sub_time']=sub_time\n",
      "\n",
      "or\n",
      "conv_sub_data['sub_time']= conv_sub_data.sub_time.dt.days\n",
      "\n",
      "#find the users who have not subscribed in week one but have been on the platform for two or more weeks\n",
      "\n",
      "conv_base=conv_sub_data[\n",
      " (conv_sub_data.sub_time.notnull()) |\n",
      "\t(conv_sub_data.sub_time > 7)]\n",
      "\n",
      "total_users=len(conv_base)\n",
      "\n",
      "total_sub=np.where(conv_sub_data.sub_time.notnull()) &\n",
      "\t(conv_sub_data.sub_time <= 14,1,0)\n",
      "\n",
      "total_subs=sum(total_subs)\n",
      "\n",
      "conversion_rate = total_subs/total_users\n",
      "\n",
      "   new stuff\n",
      "conversion_data = conv_sub_data.groupby(\n",
      "\tby=['lapse_date'], as_index=False\n",
      ").agg('sub_time': [gc7]})\n",
      "\n",
      "#produces the week one conversion rate by conversion date.\n",
      "\n",
      "\n",
      "conversion_data.plot(x='lapse_date',y='sub_time')\n",
      "\n",
      "* compare users of different genders\n",
      "* evaluate the impact of a change across regions\n",
      "* see the impact for different devices\n",
      "\n",
      "reformatted_cntry_data=pd.pivot_table(\n",
      "\tconversion_data,\n",
      "\tvalues=['sub_time'],\n",
      "\tcolumns=['country'],\n",
      "\tindex=['reg_data'],\n",
      "\tfill_value=0\n",
      ")\n",
      "\n",
      "reformat_cntry_data.plot(\n",
      "\tx='reg_date',\n",
      "\ty=['BRA','FRA','DEU','TUR','USA','CAN']\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >sample   graph reg_date by first_week_purchases\n",
      "\n",
      "# Group the data and aggregate first_week_purchases\n",
      "\n",
      "user_purchases columns: 'reg_date', 'first_week_purchases'\n",
      "\n",
      "user_purchases = user_purchases.groupby(by=['reg_date', 'uid']).agg({'first_week_purchases': ['sum']})\n",
      "\n",
      "# Reset the indexes\n",
      "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
      "user_purchases.reset_index(inplace=True)\n",
      "\n",
      "# Find the average number of purchases per day by first-week users\n",
      "user_purchases = user_purchases.groupby(by=['reg_date']).agg({'first_week_purchases': ['mean']})\n",
      "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
      "user_purchases.reset_index(inplace=True)\n",
      "\n",
      "# Plot the results\n",
      "user_purchases.plot(x='reg_date', y='first_week_purchases')\n",
      "plt.show()\n",
      "\t\n",
      "\n",
      "   sample pivot table on the first_week_purchases by country\n",
      "\n",
      "# Pivot the data\n",
      "country_pivot = pd.pivot_table(user_purchases_country, values=['first_week_purchases'], columns=['country'], index=['reg_date'])\n",
      "print(country_pivot.head())\n",
      "\n",
      "\n",
      "# Pivot the data\n",
      "device_pivot = pd.pivot_table(user_purchases_device, values=['first_week_purchases'], columns=['device'], index=['reg_date'])\n",
      "print(device_pivot.head())\n",
      "\n",
      "\n",
      "          first_week_purchases          \n",
      "device                      and       iOS\n",
      "reg_date                                 \n",
      "2017-06-01             0.714286  1.000000\n",
      "2017-06-02             1.400000  1.285714\n",
      "2017-06-03             1.545455  1.000000\n",
      "2017-06-04             1.600000  1.833333\n",
      "2017-06-05             1.625000  2.000000\n",
      "\n",
      "\n",
      "# Plot the average first week purchases for each country by registration date\n",
      "country_pivot.plot(x='reg_date', y=['USA', 'CAN', 'FRA', 'BRA', 'TUR', 'DEU'])\n",
      "plt.show()\n",
      "\n",
      "# Plot the average first week purchases for each device by registration date\n",
      "device_pivot.plot(x='reg_date', y=['and', 'iOS'])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     understanding and visualizing trends in customer data\n",
      "\n",
      "usa_subscriptions['sub_day']=(usa_subscriptions.sub_date - usa_subscriptions.lapse_date).dt.days\n",
      "\n",
      "\n",
      "usa_subscriptions = usa_subscriptions[usa_subscriptions.sub_day <=7]\n",
      "\n",
      "usa_subscriptions = usa_subscriptions.groupby(\n",
      "\tby=['sub_date'],as_index=False\n",
      ").agg({'subs':['sum']})\n",
      "\n",
      "\n",
      "     >looking for seasonal change in buying movement\n",
      "\n",
      "Trailing average smoothing technique that averages over a lagging window\n",
      "1. reveal hidden trends by smoothing out seasonality\n",
      "2. average across the period of seasonality\n",
      "3. 7-day window to smooth weekly seasonality\n",
      "4. average out day level effects to produce the average week effect\n",
      "\n",
      "calculate the rolling average over the usa subscribers data with .rolling()\n",
      "\n",
      "rolling_subs = usa_subscriptions.subs.rolling(\n",
      "\twindow=7,\n",
      "\t#specify to average backwards\n",
      "\tcenter=False\n",
      ")\n",
      "\n",
      "usa_subscriptions['rolling_subs']\n",
      "\t=rolling_subs.mean()\n",
      "usa_subscriptions.tail()\n",
      "\n",
      "high_sku_purchases = pd.read_csv(\n",
      "\t'high_sku_purchases.csv',\n",
      "\tparse_dates=True,\n",
      "\tinfer_datetime_format=True\n",
      ")\n",
      "\n",
      "high_sku_purchases.plot(x='date', y='purchases')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "       exponential moving average\n",
      "\n",
      "1. weighted moving (rolling) average\n",
      "\n",
      "* weights more recent items in the window more\n",
      "* applies weights according to an exponential distribution\n",
      "* average back to a central trend without masking any recent movements\n",
      "\n",
      ".ewm() : exponential weighting function\n",
      "\n",
      "\n",
      "window to apply weights over\n",
      "\n",
      "exp_mean=high_sku_purchases.purchases.ewm(span=30)\n",
      "\n",
      "high_sku_purchases['exp_mean'] = exp_mean.mean()\n",
      "\n",
      "\n",
      "   >  sample  > rolling window 7, 28, 365\n",
      "\n",
      "# Compute 7_day_rev\n",
      "daily_revenue['7_day_rev'] = daily_revenue.revenue.rolling(window=7,center=False).mean()\n",
      "\n",
      "# Compute 28_day_rev\n",
      "daily_revenue['28_day_rev'] = daily_revenue.revenue.rolling(window=28,center=False).mean()\n",
      "    \n",
      "# Compute 365_day_rev\n",
      "daily_revenue['365_day_rev'] = daily_revenue.revenue.rolling(window=365,center=False).mean()\n",
      "    \n",
      "# Plot date, and revenue, along with the 3 rolling functions (in order)    \n",
      "daily_revenue.plot(x='date', y=['revenue', '7_day_rev', '28_day_rev', '365_day_rev', ])\n",
      "plt.show()\n",
      "\n",
      "   > sample ewm\n",
      "\n",
      "# Calculate 'small_scale'\n",
      "daily_revenue['small_scale'] = daily_revenue.revenue.ewm(span=10).mean()\n",
      "\n",
      "# Calculate 'medium_scale'\n",
      "daily_revenue['medium_scale'] = daily_revenue.revenue.ewm(span=100).mean()\n",
      "\n",
      "# Calculate 'large_scale'\n",
      "daily_revenue['large_scale'] = daily_revenue.revenue.ewm(span=500).mean()\n",
      "\n",
      "# Plot 'date' on the x-axis and, our three averages and 'revenue'\n",
      "# on the y-axis\n",
      "daily_revenue.plot(x = 'date', y =['revenue', 'small_scale', 'medium_scale', 'large_scale'])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >Events and releases\n",
      "\n",
      "discover the cause of an issue\n",
      "\n",
      "visualizing the drop in conversion rate (3 years)\n",
      "\n",
      "we notice a dip in new user retention\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "conv_sub_data = sub_data_demo(\n",
      "\tsub_data_demo.lapse_date <= max_lapse_date]\n",
      "\n",
      "sub_time = (conv_sub_data.subscription_date -\n",
      "\tconv_sub_data.lapse_date).dt.days\n",
      "\n",
      "conv_sub_date['sub_time']=sub_time\n",
      "\n",
      "conversion_data = conv_sub_data.groupby(\n",
      "\tby=['lapse_data'], as_index=False)\n",
      ".agg({sub_time':[gc7]})\n",
      "\n",
      "conversion_data.plot()\n",
      "plt.show()\n",
      "\n",
      "    >look at the recent six months\n",
      "\n",
      "current_date = pd.to_date('2018-03-17')\n",
      "\n",
      "start_date=current_date - timedelta(days=(6*28))\n",
      "\n",
      "conv_filter=(\n",
      "\tconversion_data.lapse_date >= start_date)\n",
      "\t& (conversion_data.lapse_date <= current_date)\n",
      ")\n",
      "\n",
      "con_data_filt=conversion_data[conv_filter]\n",
      "\n",
      "conv_data_filt.plot(x='lapse_date', y='sub_time')\n",
      "plt.show()\n",
      "\n",
      "* is this drop impacting all users or just specific cohort\n",
      "\n",
      "* this could provide clues on what the issue may be\n",
      "\n",
      "* ecosystems within our data\n",
      "1. distinct countries\n",
      "2. specific device (android or ios)\n",
      "\n",
      "\n",
      "\n",
      "#pivot the results to have one column per country\n",
      "\n",
      "conv_data_cntry = pd.pivot_table(\n",
      "\tconv_data_cntry, values=['sub_time'],\n",
      "\tcolumns=['country'], index=['lapse_date'], fill_value=0\n",
      ")\n",
      "\n",
      "#pivot the results to have one column per device\n",
      "\n",
      "\n",
      "conv_data_device = pd.pivot_table(\n",
      "\tconv_data_cntry, values=['sub_time'],\n",
      "\tcolumns=['device'], index=['lapse_date'], fill_value=0\n",
      ")\n",
      "\n",
      "* all countries experience the drop\n",
      "\n",
      "* most pronounced in Brazil & Turkey\n",
      "\n",
      "* breaking out by device\n",
      "1 the drop only appears on android devices\n",
      "\n",
      "events: holidays and events impacting user behavior\n",
      "\n",
      "events=pd.read_csv('events.csv')\n",
      "1. Date\n",
      "2. Event\n",
      "\n",
      "releases: ios and android software releases\n",
      "\n",
      "releases = pd.read_csv('releases.csv')\n",
      "\n",
      "     >Plot the conversion rate trend per device\n",
      "\n",
      "conv_data_dev.plot(\n",
      "\tx=['lapse_date'], y=['iOS','and']\n",
      ")\n",
      "\n",
      "events.Date = pd.to_datetime(events.Date)\n",
      "\n",
      "#iterate through events and plot each one\n",
      "\n",
      "for row in events.iterrows():\n",
      "\ttmp=row[1]\n",
      "\tplt.axvline(\n",
      "\tx=tmp.Date, color='k', linestyle='---'\n",
      ")\n",
      "\n",
      "\n",
      "#iterate through the releases and plot each one\n",
      "\n",
      "releases.Date = pd.to_datetime(releases.Date)\n",
      "\n",
      "\n",
      "for row in releases.iterrows():\n",
      "\ttmp=row[1]\n",
      "\tif tmp.Event== 'iOS Release':\n",
      "\n",
      "\t\tplt.axvline(\n",
      "\t\tx=tmp.Date, color='b', linestyle='---'\n",
      ")\n",
      "\telse:\n",
      "\t\tplt.axvline(\n",
      "\t\tx=tmp.Date, color='r', linestyle='---'\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "There was an android release in feb/mar aligns with our dip in conversion rate\n",
      "\n",
      "\n",
      "visualizing data over time to uncover hidden trends\n",
      "\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "user_revenue:\n",
      "1. device\n",
      "2. gender\n",
      "3. country\n",
      "4. date \n",
      "5. revenue\n",
      "6. month\n",
      "\n",
      "\n",
      "# Pivot user_revenue\n",
      "pivoted_data = pd.pivot_table(user_revenue, values ='revenue', columns=['device', 'gender'], index='month')\n",
      "pivoted_data = pivoted_data[1:(len(pivoted_data) -1 )]\n",
      "\n",
      "# Create and show the plot\n",
      "pivoted_data.plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "more female bought ios devices\n",
      "\n",
      "      Introduction to A/B testing\n",
      "\n",
      "discoverying causal relationships\n",
      "\n",
      "test two or more variants against each other\n",
      "\n",
      "to evaluate which one performs best\n",
      "\n",
      "in context of a randomized experiment\n",
      "\n",
      "testing two more ideas against each other\n",
      "\n",
      "control: the current state of your product\n",
      "\n",
      "treatment: the variant that you want to test\n",
      "\n",
      "current paywall: I hope you enjoyed your free-trial please consider subscribing\n",
      "\n",
      "proposed paywall: your free-trial has ended, don't miss out, subscribe today\n",
      "\n",
      "randomly select a subset of users and show one set the control and on e the treatment\n",
      "\n",
      "monitor the conversion rates of each group to see which is better\n",
      "\n",
      "by randomly assigning the user we isolate the impact of the change and reduce the potential impact of confounding variables\n",
      "\n",
      "using an assignment criteria may introduce confounders\n",
      "\n",
      "A/B testing can be used to \n",
      "1. improve sales within a mobile application\n",
      "2. increase user interactions with a website\n",
      "3. identify the impact of a medical treatment\n",
      "4. optimize an assembly lines efficiency\n",
      "\n",
      "good problems for ab testing\n",
      "1. where users are being impacted individually\n",
      "2. testing changes that can directly impact their behavior\n",
      "\n",
      "bad problems for ab testing\n",
      "1. challenging to segment the users into groups\n",
      "2. difficult to untangle the impact of the test\n",
      "\n",
      "\n",
      "     >initial ab test design\n",
      "\n",
      "increasing our apps revenue with a/b testing\n",
      "\n",
      "1. test change to our consumable purchase paywall\n",
      "2. increase revenue by increasing the purchase rate\n",
      "\n",
      "general concepts\n",
      "1. a/b testing techniques transfer across a variety of context\n",
      "2. keep in mind how you would apply these techniques\n",
      "\n",
      "    paywall views & demographics data\n",
      "\n",
      "demographics_data = pd.read_csv('user_demographics.csv')\n",
      "demographics_data.head(n=2)\n",
      "\n",
      "1.uid\n",
      "2.reg_date\n",
      "3.device\n",
      "4.gender\n",
      "5.country\n",
      "6.age\n",
      "\n",
      "\n",
      "paywall_views = pd.read_csv('paywall_views.csv')\n",
      "\n",
      "1.uid\n",
      "2.date\n",
      "3.purchase\n",
      "4.sku\n",
      "5.price\n",
      "\n",
      "\n",
      "   >Response variable\n",
      "1. A response variable is used to measure the impact of your change\n",
      "2. should either be a kpi or directly related to a kpi\n",
      "3. something that is easy to measure\n",
      "\n",
      "factors:\n",
      "1. the paywall color\n",
      "\n",
      "variants:\n",
      "1. particular changes you are testing\n",
      "\n",
      "Experimental unit of our test\n",
      "1. the smallest unit you are measuring the change over\n",
      "2. Individual users make a convenient experimental unit\n",
      "\n",
      "purchase_data = demographics_data.merge(\n",
      "\tpaywall_views, how='left', on=['uid'])\n",
      "\n",
      "#find the total purchases for each user\n",
      "\n",
      "total_purchases = purchase_data.groupby(\n",
      "\tby=['uid'], as_index=False).purchase.sum()\n",
      "\n",
      "#find the mean number of purchase per user\n",
      "total_purchases.purchase.mean()\n",
      "\n",
      "print('total purchases average does not make alot of sense, instead try min and max')\n",
      "\n",
      "\n",
      "#find the min and max number of purchases per users in the time period\n",
      "\n",
      "total_purchases.purchase.min()\n",
      "total_purchases.purchase.max()\n",
      "\n",
      "    user days\n",
      "\n",
      "user interactions on a given day\n",
      "1. more convenient than users by itself\n",
      "2. not required to track users actions across time\n",
      "3. can treat simpler actions as responses to the test\n",
      "\n",
      "\n",
      "total_purchases = purchase_data.groupby(\n",
      "\tby=['uid','date'], as_index=False).purchase.sum()\n",
      "\n",
      "total_purchases.purchase.mean()\n",
      "users in the time period\n",
      "total_purchases.purchase.min()\n",
      "total_purchases.purchase.max()\n",
      "\n",
      "\n",
      "   Randomize by user\n",
      "1. best to randomize by individuals regardless of our experimental unit\n",
      "2. otherwise users can have inconsistent experience\n",
      "\n",
      "important to build intuition about your users and data overall\n",
      "\n",
      "\n",
      "   sample  > calculate the user average purchase per day\n",
      "\n",
      "# Extract the 'day'; value from the timestamp\n",
      "purchase_data.date = purchase_data.date.dt.floor('d')\n",
      "\n",
      "# Replace the NaN price values with 0 \n",
      "purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)\n",
      "\n",
      "# Aggregate the data by 'uid' & 'date'\n",
      "purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)\n",
      "revenue_user_day = purchase_data_agg.sum()\n",
      "\n",
      "# Calculate the final average\n",
      "revenue_user_day = revenue_user_day.price.mean()\n",
      "print(revenue_user_day) \n",
      "\n",
      "\n",
      "output:\n",
      "407.33800579385104\n",
      "\n",
      "\n",
      "    Preparing to run an ab test\n",
      "\n",
      "current paywall: \"I hope you are enjoying the relaxing benefits of our app.  Consider making a purchase\"\n",
      "\n",
      "proposed Paywall: \"don't miss out! try one of our new products!\"\n",
      "\n",
      "Questions:\n",
      "Will updating the paywall text impact our revenue\n",
      "How do our three different consumable prices impact this?\n",
      "\n",
      "Considerations in test design\n",
      "1. can our test be run well in practice\n",
      "2. will we be able to derive meaningful results from it\n",
      "\n",
      "Test sensitivity\n",
      "1. What size of impact is meaningful to detect\n",
      "\n",
      "smaller changes are more difficult to detect and can be hidden by randomness\n",
      "\n",
      "Sensitivity is the minimum level of change we want to be able to detect in our tests\n",
      "\n",
      "     Calculating the revenue per user\n",
      "\n",
      "purchase_data = demographics_data.merge(\n",
      "\tpaywall_views, how='left', on=['uid'])\n",
      "\n",
      "total_revenue = purchase_data.groupby(by=['uid'], as_index=False).price.sum()\n",
      "\n",
      "total_revenue.price = np.where(\n",
      "\tnp.isnan(total_revenue.price),0, total_revenue.price)\n",
      "\n",
      "#calculate the average revenue per user\n",
      "\n",
      "avg_revenue = total_revenue.price.mean()\n",
      "\n",
      "print(avg_revenue)\n",
      "16\n",
      "\n",
      "#find the 1% 10% and 20% change in revenue\n",
      "\n",
      "avg_revenue *1 1.01\n",
      "16.32\n",
      "avg_revenue *1 1.10\n",
      "17.77\n",
      "avg_revenue *1 1.20\n",
      "19.39\n",
      "\n",
      "    Data variability\n",
      "1. important to understand the variability in the data\n",
      "2. does the amount spent vary alot among users\n",
      "a. if it does not then it will be easier to detect a change\n",
      "\n",
      "\n",
      "#calculate the standard deviation of revenue per user\n",
      "\n",
      "revenue_variation = total_revenue.price.std()\n",
      "\n",
      "print(revenue_variation)\n",
      "\n",
      "17.520\n",
      "\n",
      "notice the standard deviation is roughly 100% of what the mean average of 16 is.\n",
      "\n",
      "revenue_variation/avg_revenue\n",
      "1.084\n",
      "\n",
      "\n",
      "#find the average number of purchases per user\n",
      "avg_purchases = total_purchases.purchase.mean()\n",
      "3.15\n",
      "\n",
      "purchase_variation = total_purchases.purchase.std()\n",
      "2.68\n",
      "\n",
      "purchase_variation/avg_purchases\n",
      "0.850\n",
      "\n",
      "Primary goal is the increase revenue\n",
      "1. paywall view to purchase conversion rate\n",
      "a. more granular than overall revenue\n",
      "b. directly related to our test\n",
      "\n",
      "Experimental unit: paywall views\n",
      "1. simplest to work with\n",
      "2. assuming these interactions are independent\n",
      "\n",
      "\n",
      "     finding the baseline conversion rate\n",
      "\n",
      "purchase_data = demographic_data.merge(\n",
      "\tpaywall_views, how='inner', on=['uid']\n",
      ")\n",
      "\n",
      "conversion_rate = (sum(purchase_data.purchase) /\n",
      "\tpurchase_data.purchase.count())\n",
      "\n",
      "print(conversion_rate)\n",
      "\n",
      "0.347\n",
      "\n",
      "      sample get the sum and count\n",
      "\n",
      "# Merge and group the datasets\n",
      "purchase_data = demographics_data.merge(paywall_views,  how='left', on=['uid'])\n",
      "purchase_data.date = purchase_data.date.dt.floor('d')\n",
      "\n",
      "# Group and aggregate our combined dataset \n",
      "daily_purchase_data = purchase_data.groupby(by=['uid'], as_index=False)\n",
      "daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})\n",
      "\n",
      "# Find the mean of each field and then multiply by 1000 to scale the result\n",
      "daily_purchases = daily_purchase_data.purchase['sum'].mean()\n",
      "daily_paywall_views = daily_purchase_data.purchase['count'].mean()\n",
      "daily_purchases = daily_purchases * 1000\n",
      "daily_paywall_views = daily_paywall_views * 1000\n",
      "\n",
      "print(daily_purchases)\n",
      "print(daily_paywall_views)\n",
      "\n",
      "3150.0 (purchases)\n",
      "90814.54545454546 (number of views)\n",
      "\n",
      "\n",
      "        calculating lift dependent upon sensitivity\n",
      "\n",
      "small_sensitivity = 0.1 \n",
      "\n",
      "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
      "small_conversion_rate = conversion_rate * (1 + small_sensitivity) \n",
      "\n",
      "# Apply the new conversion rate to find how many more users per day that translates to\n",
      "small_purchasers = daily_paywall_views * small_conversion_rate\n",
      "\n",
      "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
      "purchaser_lift = small_purchasers - daily_purchases\n",
      "\n",
      "print(small_conversion_rate)\n",
      "print(small_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "\n",
      "0.03814800000000001 (small conversion rate)\n",
      "3499.384706400001 (small purchasers)\n",
      "317.58470640000087 (lift)\n",
      "\n",
      "  > medium sensitivity\n",
      "\n",
      "medium_sensitivity = 0.2\n",
      "\n",
      "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
      "medium_conversion_rate = conversion_rate * (1 + medium_sensitivity) \n",
      "\n",
      "# Apply the new conversion rate to find how many more users per day that translates to\n",
      "medium_purchasers = daily_paywall_views * medium_conversion_rate\n",
      "\n",
      "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
      "purchaser_lift = medium_purchasers - daily_purchases\n",
      "\n",
      "print(medium_conversion_rate)\n",
      "print(medium_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "0.041616 (4% conversion rate)\n",
      "3817.5105888000003 (purchasers)\n",
      "635.7105888000001 (lift)\n",
      "\n",
      "     large sensitivity\n",
      "\n",
      "large_sensitivity = 0.5\n",
      "\n",
      "# Find the conversion rate lift with the sensitivity above\n",
      "large_conversion_rate = conversion_rate * (1 + large_sensitivity)\n",
      "\n",
      "# Find how many more users per day that translates to\n",
      "large_purchasers = daily_paywall_views * large_conversion_rate\n",
      "purchaser_lift = large_purchasers - daily_purchases\n",
      "\n",
      "print(large_conversion_rate)\n",
      "print(large_purchasers)\n",
      "print(purchaser_lift)\n",
      "\n",
      "0.052020000000000004\n",
      "4771.888236000001\n",
      "1590.0882360000005\n",
      "\n",
      "Awesome! While it seems that a 50% increase may be too drastic and unreasonable to expect, the small and medium sensitivities both seem very reasonable.\n",
      "\n",
      "\n",
      "       standard error\n",
      "\n",
      "\n",
      "# Find the n & v quantities\n",
      "n = purchase_data.purchase.count()\n",
      "\n",
      "# Calculate the quantity \"v\"\n",
      "v = conversion_rate * (1 - conversion_rate) \n",
      "\n",
      "# Calculate the variance and standard error of the estimate\n",
      "var = v / n \n",
      "se = var**0.5\n",
      "\n",
      "print(var)\n",
      "print(se)\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "3.351780834114284e-07\n",
      "0.0005789456653360731\n",
      "\n",
      "     >calculating sample size\n",
      "\n",
      "what is the null hypothesis\n",
      "\n",
      "1. hypothesis that control and treatment have the same impact on response\n",
      "a. updated paywall does not improve conversion rate\n",
      "b. any observed difference is due to randomness\n",
      "\n",
      "rejecting the null hypothesis\n",
      "a. determine their is a difference between the treatment and control\n",
      "b. we say the test has statistical significances\n",
      "\n",
      "\n",
      "\n",
      "Null hypothesis\n",
      "\n",
      "     \ttrue   \t\tfalse\n",
      "accept\tcorrect\t\ttype II error\n",
      "reject\ttype I error\tcorrect\n",
      "\n",
      "types of error & confidence level\n",
      "1. probablilty of not making type 1 error\n",
      "2. higher this value, larger the test sample needed\n",
      "\n",
      "common values is 0.95\n",
      "\n",
      "     >Statistical power\n",
      "\n",
      "statistical power is the probability of finding a statistically siginificant result when the null hypothesis is false\n",
      "\n",
      "confidence level\n",
      "standard error\n",
      "statistical power\n",
      "test sensitivity\n",
      "\n",
      "\n",
      "as the sample size increases so does our power increase\n",
      "\n",
      "\n",
      "    calculating our needed sample size\n",
      "\n",
      "baseline conversion rate 0.3468\n",
      "confidence level: 0.95\n",
      "desired power: 0.80\n",
      "sensitivity=0.1\n",
      "\n",
      "sample_size_group=get_sample(size(0.8, conversion_rate *1.1, 0.95)\n",
      "\n",
      "print(sample_size_per_group)\n",
      "\n",
      "output:\n",
      "45788\n",
      "\n",
      "\n",
      "      >generality of this function\n",
      "\n",
      "function shown specific to conversion rate calculations\n",
      "\n",
      "different response variables have different buy analogous formulas\n",
      "\n",
      "\n",
      "  > decreasing the need sample size\n",
      "\n",
      "* choose a unit of observation with lower variability\n",
      "\n",
      "* excluding users irrelevant to the process/change\n",
      "\n",
      "* think through how different factors relate to the sample size\n",
      "\n",
      "\n",
      "\n",
      "       increase the confidence level\n",
      "\n",
      "# Look at the impact of sample size increase on power\n",
      "n_param_one = get_power(n=1000, p1=p1, p2=p2, cl=cl)\n",
      "n_param_two = get_power(n=2000, p1=p1, p2=p2, cl=cl)\n",
      "\n",
      "# Look at the impact of confidence level increase on power\n",
      "alpha_param_one = get_power(n=n1, p1=p1, p2=p2, cl=0.8)\n",
      "alpha_param_two = get_power(n=n1, p1=p1, p2=p2, cl=0.95)\n",
      "    \n",
      "# Compare the ratios\n",
      "print(n_param_two / n_param_one)\n",
      "print(alpha_param_one / alpha_param_two)\n",
      "\n",
      "\n",
      "1.7596440001351992  (change sample size)\n",
      "1.8857367092232278  (change confidence levels)\n",
      "\n",
      "\n",
      "\n",
      "With these particular values it looks like decreasing our confidence level has a slightly larger impact on the power than increasing our sample size\n",
      "\n",
      "      calculate the conversion rate\n",
      "\n",
      "# Merge the demographics and purchase data to only include paywall views\n",
      "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
      "                            \n",
      "# Find the conversion rate\n",
      "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
      "            \n",
      "print(conversion_rate)\n",
      "\n",
      "0.03468607351645712\n",
      "\n",
      "    > calculate sample size\n",
      "\n",
      "# Merge the demographics and purchase data to only include paywall views\n",
      "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
      "                            \n",
      "# Find the conversion rate\n",
      "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
      "\n",
      "# Desired Power: 0.8\n",
      "# CL: 0.90\n",
      "# Percent Lift: 0.1\n",
      "p2 = conversion_rate * (1 + 0.1)\n",
      "sample_size = get_sample_size(0.8, conversion_rate, p2, 0.90)\n",
      "print(sample_size)\n",
      "\n",
      "36101\n",
      "\n",
      "\n",
      "# Desired Power: 0.95\n",
      "# CL 0.90\n",
      "# Percent Lift: 0.1\n",
      "p2 = conversion_rate * (1 + 0.1)\n",
      "sample_size = get_sample_size(0.95, conversion_rate, p2, 0.90)\n",
      "print(sample_size)\n",
      "\n",
      "63201\n",
      "\n",
      "\n",
      "      analyzing the ab test results\n",
      "\n",
      "compare the two groups purchase rates\n",
      "\n",
      "test_demographics = pd.read_csv('test_demographics.csv')\n",
      "\n",
      "#results for our ab test\n",
      "#group column c for control | v for variant\n",
      "\n",
      "test_results=pd.read_csv('ab_test_results.csv')\n",
      "test_results.head()\n",
      "\n",
      "uid\n",
      "date\n",
      "purchase\n",
      "sku\n",
      "price\n",
      "group\n",
      "\n",
      "\n",
      "    confirming our test results\n",
      "\n",
      "does the data look reasonable\n",
      "\n",
      "\n",
      "test_results_grpd = test_results.groupby(\n",
      "\tby=['group'], as_index=False)\n",
      "\n",
      "test_results_grpd.uid.count()\n",
      "\n",
      "48236\n",
      "49867\n",
      "\n",
      "\n",
      "test_results_demo = test_results.merge(\n",
      "\ttest_demo, how='inner', on='uid')\n",
      "\n",
      "test_results_grpd = test_results_demo.groupby(\n",
      "\tby=['country','gender','device','group'],\n",
      "as_index=False)\n",
      "\n",
      "test_results_grd.uid.count()\n",
      "\n",
      "\n",
      "    > find the mean conversion\n",
      "\n",
      "test_results_summary= test_results_demo.groupby(\n",
      "\tby=['group'], as_index=False\n",
      ").agg({'purchase':['count','sum']})\n",
      "\n",
      "test_results_summary['conv'] = (test_results_summary.purchase['sum']/\n",
      "\ttest_results_summary.purchase['count'])\n",
      "\n",
      "test_results_summary\n",
      "\n",
      "grp  sum   count   conversion\n",
      "c    48236 1657    0.034351\n",
      "v    49867 2094    0.041984\n",
      "\n",
      "Is the result statistically significant\n",
      "1. are the conversion rates different enough\n",
      "2. if yes then reject the null hypothesis\n",
      "3. conclude that the paywalls have different effects\n",
      "4. if no then it may just be randomness\n",
      "\n",
      "    p -value\n",
      "\n",
      "probability if the null hypothesis is true\n",
      "\n",
      "of observing a value as or more extreme\n",
      " \n",
      "what does a low p-value mean\n",
      "1. the power is low\n",
      "2. the observation is unlikely to happen due to randomness\n",
      "\n",
      "\n",
      "<0.01 very strong evidence against the null hypothesis\n",
      "\n",
      "0.01-0.5 strong evidence against the null hypothesis\n",
      "0.05-1. very weak evidence against the null hypothesis\n",
      ">0.1 small or no evidence against the null hypothesis\n",
      "\n",
      "\n",
      "?     sample test the null hypothesis\n",
      "\n",
      "# Compute and print the results\n",
      "results = ab_test_results.groupby('group').agg({'uid':pd.Series.nunique}) \n",
      "print(results)\n",
      "\n",
      "\n",
      "   uid\n",
      "group        \n",
      "C      2825.0\n",
      "V      2834.0\n",
      "\n",
      "\n",
      "# Find the unique users in each group \n",
      "results = ab_test_results.groupby('group').agg({'uid': pd.Series.nunique}) \n",
      "\n",
      "# Find the overall number of unique users using \"len\" and \"unique\"\n",
      "unique_users = len(ab_test_results.uid.unique()) \n",
      "\n",
      "# Find the percentage in each group\n",
      "results = results / unique_users * 100\n",
      "print(results)\n",
      "\n",
      "     uid\n",
      "group           \n",
      "C      49.920481\n",
      "V      50.079519\n",
      "\n",
      "\n",
      "   find the number of users in group device and gender\n",
      "\n",
      "# Find the unique users in each group, by device and gender\n",
      "results = ab_test_results.groupby(by=['group', 'device', 'gender']).agg({'uid': pd.Series.nunique}) \n",
      "\n",
      "# Find the overall number of unique users using \"len\" and \"unique\"\n",
      "unique_users = len(ab_test_results.uid.unique())\n",
      "\n",
      "# Find the percentage in each group\n",
      "results = results / unique_users * 100\n",
      "print(results)\n",
      "\n",
      "uid\n",
      "group device gender           \n",
      "C     and    F       14.896625\n",
      "             M       13.518289\n",
      "      iOS    F       11.309419\n",
      "             M       10.196148\n",
      "V     and    F       14.861283\n",
      "             M       13.659657\n",
      "      iOS    F       10.920657\n",
      "             M       10.637922\n",
      "\n",
      "\n",
      "     understanding statistical significance\n",
      "\n",
      "distribution of expected difference between control and test groups _if_ the null hypothesis is true\n",
      "\n",
      "The red line is the observed difference in the conversion rates from our tests\n",
      "\n",
      "p-value: probability of being as or more extreme than the red line on either side of the distribution.\n",
      "\n",
      "\n",
      "def get_pvalue ( con_conv, test_conv, con_size, test_size):\n",
      "\n",
      "\tlift= - abs(test_conv - con_conv)\n",
      "\tscale_one = con_conv * (1-con_conv) * (1/con_size)\n",
      "\tscale_two= test_conv * (1-test_conv) * (1/test_size)\n",
      "\tscale_val = (scale_one + scale_two) **0.5\n",
      "\tp_value=2*stats.norm.cdf(lift, loc=0, scale=scale_val)\n",
      "\treturn p_value\n",
      "\n",
      "\n",
      "con_conv=0.034351\n",
      "test_conv=0.041984\n",
      "con_size=48236\n",
      "test_size=49867\n",
      "\n",
      "p_value=get_pvalue(con_conv,test_conv,con_size,test_size)\n",
      "print(p_value)\n",
      "\n",
      "4.2572974 e-10  (extremely small p-value)\n",
      "\n",
      "accept the null hypothesis\n",
      "\n",
      "\n",
      "    find the power of the test\n",
      "\n",
      "def get_power(n, p1, p2, cl):\n",
      "    alpha = 1 - cl\n",
      "    qu = stats.norm.ppf(1 - alpha/2)\n",
      "    diff = abs(p2-p1)\n",
      "    bp = (p1+p2) / 2\n",
      "    \n",
      "    v1 = p1 * (1-p1)\n",
      "    v2 = p2 * (1-p2)\n",
      "    bv = bp * (1-bp)\n",
      "    \n",
      "    power_part_one = stats.norm.cdf((n**0.5 * diff - qu * (2 * bv)**0.5) / (v1+v2) ** 0.5)\n",
      "    power_part_two = 1 - stats.norm.cdf((n**0.5 * diff + qu * (2 * bv)**0.5) / (v1+v2) ** 0.5)\n",
      "    \n",
      "    power = power_part_one + power_part_two\n",
      "    \n",
      "    return (power)\n",
      "\n",
      "\n",
      "power= get_power (test_size, con_conv, test_conv, 0.95)\n",
      "print(power)\n",
      "0.9999925941372282\n",
      "\n",
      "\n",
      "small p-value and nearly perfect power\n",
      "\n",
      "        confidence interval\n",
      "\n",
      "ranges of values for our estimation rather than a single number\n",
      "\n",
      "provides context for our estimation process\n",
      "\n",
      "series of repeated experiments\n",
      "1. the calculated intervals will contain the true parameter x% of the time\n",
      "2. the true conversion rate is fixed quantity, it is the interval that is random not the conversion rate.\n",
      "\n",
      "\n",
      "The estimated parameter or difference in conversion rate follows a normal distribution\n",
      "\n",
      "1. we can estimate the standard deviation\n",
      "2. the mean of this distribution\n",
      "\n",
      "alpha is the desired confidence interval width\n",
      "\n",
      "bounds containing X% of hte probabilty around the mean (95%) of that distribution\n",
      "\n",
      "\n",
      "from scipy import stats\n",
      "\n",
      "def get_ci(test_conv, con_conv, test_size, con_size, ci):\n",
      "\n",
      "\tsd=((test_conv * (1-test_conv))/test_size+\n",
      "\t(con_conv * (1-con_conv)) / con_size)**0.5\n",
      "\n",
      "\tlift=test_conv - con_conv\n",
      "\n",
      "\tval=stats.norm.isf((1-ci)/2)\n",
      "\tlwr_bnd=lift - val *sd\n",
      "\tupr_bnd=lift+ val*sd\n",
      "\treturn ((lwr_bnd,upr_bnd))\n",
      "\n",
      "\n",
      "    get p-value\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=0.1, test_conv=0.17, con_size=1000, test_size=1000)\n",
      "print(p_value)\n",
      "\n",
      "4.131297741047306e-06\n",
      "\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=.1, test_conv=.15, con_size=100, test_size=100)\n",
      "print(p_value) \n",
      "\n",
      "0.28366948940702086\n",
      "\n",
      "\n",
      "# Get the p-value\n",
      "p_value = get_pvalue(con_conv=.48, test_conv=.5, con_size=1000, test_size=1000)\n",
      "print(p_value)\n",
      "\n",
      "0.370901935824383\n",
      "\n",
      "\n",
      "To recap we observed that a large lift makes us confident in our observed result, while a small sample size makes us less so, and ultimately high variance can lead to a high p-value!\n",
      "\n",
      "\n",
      "    check for statistically signficant\n",
      "\n",
      "\n",
      "cont_conv=0.09096495570387314 \n",
      "test_conv=0.1020053238686779 \n",
      "con_size=5329 \n",
      "test_size=5748\n",
      "\n",
      "# Compute the p-value\n",
      "p_value = get_pvalue(con_conv=cont_conv, test_conv=test_conv, con_size=cont_size, test_size=test_size)\n",
      "print(p_value)\n",
      "\n",
      "# Check for statistical significance\n",
      "if p_value >= 0.05:\n",
      "    print(\"Not Significant\")\n",
      "else:\n",
      "    print(\"Significant Result\")\n",
      "\n",
      "\n",
      "  > confidence interval\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, 0.975, 0.5)\n",
      "print(confidence_interval)\n",
      "\n",
      "(0.9755040421682947, 1.0244959578317054)\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, .95, 2)\n",
      "print(confidence_interval)\n",
      "\n",
      "2 standard deviations\n",
      "\n",
      "(0.6690506448818785, 1.3309493551181215)\n",
      "\n",
      "\n",
      "# Compute and print the confidence interval\n",
      "confidence_interval  = get_ci(1, 0.95, .001)\n",
      "print(confidence_interval)\n",
      "\n",
      "(1.0, 1.0)\n",
      "\n",
      "\n",
      "As our standard deviation decreases so too does the width of our confidence interval. Great work!\n",
      "\n",
      "\n",
      "con_conv=0.034351\n",
      "test_conv=0.041984\n",
      "con_size=48236\n",
      "test_size=49867\n",
      "ci=.95\n",
      "\n",
      "# Calculate the mean of our lift distribution \n",
      "lift_mean = test_conv -cont_conv\n",
      "\n",
      "# Calculate variance and standard deviation \n",
      "lift_variance = (1 - test_conv) * test_conv /test_size + (1 - cont_conv) * cont_conv / cont_size\n",
      "lift_sd = lift_variance**0.5\n",
      "\n",
      "# Find the confidence intervals with cl = 0.95\n",
      "confidence_interval = get_ci(lift_mean, 0.95,lift_sd)\n",
      "print(confidence_interval)\n",
      "\n",
      "confidence interval:\n",
      "(0.011039999822042502, 0.011040000177957487)\n",
      "\n",
      "Notice that our interval is very narrow thanks to our substantial lift and large sample size.\n",
      "\n",
      "      interpreting your results\n",
      "\n",
      "report \n",
      "\t\tTest Group\tControl Group\n",
      "1. Sample size  \t7030\t6970\n",
      "2. run time\t2 weeks\t\t2 weeks\n",
      "3. mean\t\t3.12\t\t2.69\n",
      "4. variance\t3.20\t\t2.64\n",
      "5. est lift\t0.56\n",
      "6. conf level\t0.56 += 0.4\n",
      "\n",
      "* significant at the 0.05 level\n",
      "\n",
      "visualization\n",
      "\n",
      "histograms: bucketed counts of observations across values\n",
      "\n",
      "user data rolled up to group and user level\n",
      "uid\n",
      "group\n",
      "purchase\n",
      "\n",
      "var=results[results.group=='V']\n",
      "con=results[results.group=='C']\n",
      "\n",
      "plt.hist(var['purchase'],color='yellow',\n",
      "\talpha=0.8, bins=50, label='Test')\n",
      "plt.hist(con['purchase'], color='blue',\n",
      "\talpha=0.8, bins=50, label='Control')\n",
      "plt.legend(loc='upper right')\n",
      "\n",
      "\n",
      "plt.axvline(x= np.mean(results.purchase),\n",
      "\tcolor='red')\n",
      "plt.axvline(x=np.mean(results.purchase),\n",
      "\tcolor='green')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >plotting a distribution\n",
      "\n",
      "mean_con=0.090965\n",
      "mean_test=0.102005\n",
      "var_con=(mean_con * (1-mean_con))/58583\n",
      "var_test=(mean_test *(1-mean_test))/56350\n",
      "\n",
      "con_line = np.linspace(-3*var_con**0.5+mean_con, 3*var_con**0.5+mean_con, 100)\n",
      "test_line=np.linspace(-3*var_test**0.5+mean_test, 3*var_test**0.5+mean_test, 100)\n",
      "\n",
      "\n",
      "\n",
      "#plot the probabilities across the distribution of conversion rates\n",
      "\n",
      "plt.plot(con_line, norm.pdf(\n",
      "    con_line, mean_con, var_con**0.5)\n",
      ")\n",
      "\n",
      "plt.plot(test_line, norm.pdf(\n",
      "    test_line, mean_test, var_test**0.5)\n",
      ")\n",
      "plt.show()\n",
      "\n",
      "mlab.normpdf(): converts values to probablities from Normal Distribution\n",
      "\n",
      "   plotting the difference of conversion rates\n",
      "\n",
      "lift= mean_test - mean_control\n",
      "var = var_test + var_control\n",
      "\n",
      "variance is the sum of variances\n",
      "\n",
      "diff_line = np.linspace(-3*var**0.5 + lift,\n",
      "3*var**0.5 + lift, 100)\n",
      "\n",
      "plt.plot(diff_line, mlab.normpdf(\n",
      "\tdiff_line, lift, var**0.5)\n",
      ")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     plotting the confidence interval\n",
      "\n",
      "section = np.arange(0.007624, 0.01445,1/10000)\n",
      "\n",
      "\n",
      "#fill in between these boundaries\n",
      "\n",
      "plt.fill_between(\n",
      "\tsection,\n",
      "\tmlab.normpdf(section, lift, var**0.5)\n",
      ")\n",
      "\n",
      "    sample   > show the control and test distributions do not intersect\n",
      "\n",
      "# Compute the standard deviations\n",
      "control_sd = cont_var**0.5\n",
      "test_sd = test_var**0.5\n",
      "\n",
      "# Create the range of x values \n",
      "control_line = np.linspace( cont_conv - 3 * control_sd, cont_conv + 3 * control_sd , 100)\n",
      "test_line = np.linspace( test_conv - 3 * test_sd,  test_conv + 3 * test_sd , 100)\n",
      "\n",
      "# Plot the distribution \n",
      "plt.plot(control_line, mlab.normpdf(control_line, cont_conv, control_sd))\n",
      "plt.plot(test_line, mlab.normpdf(test_line,test_conv, test_sd))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "We see no overlap, which intuitively implies that our test and control conversion rates are significantly distinct.\n",
      "\n",
      "     sample show  > show the confidence intervals and show the lift mean\n",
      "\n",
      "# Find the lift mean and standard deviation\n",
      "lift_mean = np.mean(test_conv-con_conv)\n",
      "lift_sd = (var_test + var_con) ** 0.5\n",
      "\n",
      "# Generate the range of x-values\n",
      "lift_line = np.linspace(lift_mean - 3 * lift_sd, lift_mean + 3 * lift_sd, 100)\n",
      "\n",
      "# Plot the lift distribution\n",
      "plt.plot(lift_line, norm.pdf(lift_line, lift_mean, lift_sd))\n",
      "\n",
      "ci_lower,ci_upper = get_ci(test_conv, con_conv, test_size, con_size,ci)\n",
      "\n",
      "# Add the annotation lines\n",
      "plt.axvline(x = lift_mean, color = 'green')\n",
      "plt.axvline(x = ci_lower, color = 'red')\n",
      "plt.axvline(x = ci_upper, color = 'red')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\object oriented programming constructors.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\object oriented programming constructors.txt\n",
      "class Employee:\n",
      "\tdef __init__(self, name, salary):\n",
      "\t\tself.name=name\n",
      "\t\tself.salary=salary\n",
      "\n",
      "emp1 = Employee(\"Teo Mille\",50000)\n",
      "emp2 = Employee(\"Marta Popov\", 65000)\n",
      "\n",
      "how to share data among all the instances of the class?\n",
      "\n",
      "class attribute\n",
      "\n",
      "class MyClass\n",
      "\tCLASS_ATTR_NAME = attr_value\n",
      "\n",
      "serves as a global variable in the class\n",
      "\n",
      "class Employee:\n",
      "\tMIN_SALARY=30000\n",
      "\n",
      "\tdef __init__(self, name,salary):\n",
      "\t\tself.name=name\n",
      "\n",
      "\n",
      "\t\tif salary>=Employee.MIN_SALARY:\n",
      "\t\t\tself.salary=salary\n",
      "\t\telse\n",
      "\t\t\tself.salary=Employee.MIN_SALARY\n",
      "\n",
      "\n",
      "\tdef my_awesome_method(cls, args)\n",
      "\t\tprint(cls)\n",
      "\n",
      "class methods can be called statically\n",
      "\n",
      "Employee.my_awesome_method(cls, args)\n",
      "\n",
      "why?\n",
      "alternative constructors\n",
      "\n",
      "an instantiated class can only have one constructor\n",
      "\n",
      "\n",
      "\tdef from_file(cls, filename):\n",
      "\t\twith open(filename, \"r\") as f:\n",
      "\t\t\tname=f.readline()\n",
      "\n",
      "\t\treturn cls(name)\n",
      "\n",
      "\n",
      "emp = Employee.from_file(\"employee_data.txt\")\n",
      "\n",
      "type(emp)\n",
      "\n",
      "creates the employee instance without calling the constructor\n",
      "\n",
      "\n",
      " > sample  > print the shared variable MAX_POSITION from the player class\n",
      "\n",
      "# Create a Player class\n",
      "class Player:\n",
      "        MAX_POSITION=30\n",
      "        def __init__(self):\n",
      "            self.position=0\n",
      "\n",
      "\n",
      "# Print Player.MAX_POSITION       \n",
      "print(Player.MAX_POSITION)\n",
      "\n",
      "# Create a player p and print its MAX_POSITITON\n",
      "p=Player\n",
      "print(p.MAX_POSITION)\n",
      "\n",
      "   sample appling an where clause using the shared variable\n",
      "\n",
      "class Player:\n",
      "    MAX_POSITION = 10\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.position = 0\n",
      "\n",
      "    # Add a move() method with steps parameter\n",
      "    def move(self,steps):\n",
      "            if steps<Player.MAX_POSITION:\n",
      "                self.position+=steps\n",
      "            else:\n",
      "                self.position=Player.MAX_POSITION\n",
      "    \n",
      "\n",
      "       \n",
      "    # This method provides a rudimentary visualization in the console    \n",
      "    def draw(self):\n",
      "        drawing = \"-\" * self.position + \"|\" +\"-\"*(Player.MAX_POSITION - self.position)\n",
      "        print(drawing)\n",
      "\n",
      "p = Player(); p.draw()\n",
      "p.move(4); p.draw()\n",
      "p.move(5); p.draw()\n",
      "p.move(3); p.draw()\n",
      "\n",
      "\n",
      "   > sample  assigning to a shared variable by instance and by class\n",
      "\n",
      "# Create Players p1 and p2\n",
      "p1=Player()\n",
      "p2=Player()\n",
      "\n",
      "print(\"MAX_SPEED of p1 and p2 before assignment:\")\n",
      "# Print p1.MAX_SPEED and p2.MAX_SPEED\n",
      "print(p1.MAX_SPEED)\n",
      "print(p2.MAX_SPEED)\n",
      "\n",
      "# Assign 7 to p1.MAX_SPEED\n",
      "p1.MAX_SPEED=7\n",
      "\n",
      "print(\"MAX_SPEED of p1 and p2 after assignment:\")\n",
      "# Print p1.MAX_SPEED and p2.MAX_SPEED\n",
      "print(p1.MAX_SPEED)\n",
      "print(p2.MAX_SPEED)\n",
      "\n",
      "print(\"MAX_SPEED of Player:\")\n",
      "# Print Player.MAX_SPEED\n",
      "print(Player.MAX_SPEED)\n",
      "\n",
      "output:\n",
      "\n",
      "MAX_SPEED of p1 and p2 before assignment:\n",
      "3\n",
      "3\n",
      "MAX_SPEED of p1 and p2 after assignment:\n",
      "7\n",
      "3\n",
      "MAX_SPEED of Player:\n",
      "3\n",
      "\n",
      "\n",
      "You shouldn't be able to change the data in all the instances of the class through a single instance.\n",
      "\n",
      "   sample  > using a decorator with a method\n",
      "\n",
      "class BetterDate:    \n",
      "    # Constructor\n",
      "    def __init__(self, year, month, day):\n",
      "      # Recall that Python allows multiple variable assignments in one line\n",
      "      self.year, self.month, self.day = year, month, day\n",
      "    \n",
      "    # Define a class method from_str\n",
      "    @classmethod\n",
      "    def from_str(cls, datestr):\n",
      "        # Split the string at \"-\" and convert each part to integer\n",
      "        parts = datestr.split(\"-\")\n",
      "        year, month, day = int(parts[0]), int(parts[1]), int(parts[2])\n",
      "        # Return the class instance\n",
      "        return BetterDate(year,month,day)\n",
      "        \n",
      "bd = BetterDate.from_str('2020-04-30')   \n",
      "print(bd.year)\n",
      "print(bd.month)\n",
      "print(bd.day)\n",
      "\n",
      "    sample return a class using a from_datetime method call of a class\n",
      "\n",
      "# import datetime from datetime\n",
      "from datetime import datetime\n",
      "\n",
      "class BetterDate:\n",
      "    def __init__(self, year, month, day):\n",
      "      self.year, self.month, self.day = year, month, day\n",
      "      \n",
      "    @classmethod\n",
      "    def from_str(cls, datestr):\n",
      "        year, month, day = map(int, datestr.split(\"-\"))\n",
      "        return cls(year, month, day)\n",
      "      \n",
      "    # Define a class method from_datetime accepting a datetime object\n",
      "    def from_datetime(datetime):\n",
      "      return BetterDate(datetime.year,datetime.month, datetime.day)\n",
      "\n",
      "# You should be able to run the code below with no errors: \n",
      "today = datetime.today()     \n",
      "bd = BetterDate.from_datetime(today)   \n",
      "print(bd.year)\n",
      "print(bd.month)\n",
      "print(bd.day)\n",
      "\n",
      "\n",
      "        >Class inheritance\n",
      "\n",
      "some one may have solved the problem\n",
      "\n",
      "1. modules are great for fixed functionality\n",
      "\n",
      "BankAccount\n",
      "a. balance\n",
      "b. interest\n",
      "\n",
      "SavingAccount\n",
      "1. withdraw()\n",
      "2. compute_interest()\n",
      "\n",
      "CheckingAccount\n",
      "a. balance\n",
      "b. limit\n",
      "1. withdraw()\n",
      "2. deposit()\n",
      "\n",
      "class BankAccount:\n",
      "\tdef __init__(self, balance):\n",
      "\t\tself.balance=balance\n",
      "\tdef withdraw(self,amount):\n",
      "\t\tself.balance-=amount\n",
      "\n",
      "class SavingsAccount(BankAccount):\n",
      "\tpass\n",
      "\n",
      "BankAccount functionality is being extended or inherited\n",
      "\n",
      "SavingsAccount will inherit the functionality and add more of its own functionality\n",
      "\n",
      "savings_account = SavingsAccount(1000)\n",
      "type(savings_acct)\n",
      "\n",
      "savings_acct.balance\n",
      "\n",
      "savings_acct.withdraw(300)\n",
      "\n",
      "savingAccount is a BankAccount\n",
      "\n",
      "isinstance(savings_acct, SavingsAccount)\n",
      "\n",
      "output: True\n",
      "\n",
      "bank_acct=BankAccount(300)\n",
      "\n",
      "isinstance(bank_acct,SavingsAccount)\n",
      "\n",
      "output: False\n",
      "\n",
      "\n",
      " > sample  > inheritance\n",
      "\n",
      "class Employee:\n",
      "  MIN_SALARY = 30000    \n",
      "\n",
      "  def __init__(self, name, salary=MIN_SALARY):\n",
      "      self.name = name\n",
      "      if salary >= Employee.MIN_SALARY:\n",
      "        self.salary = salary\n",
      "      else:\n",
      "        self.salary = Employee.MIN_SALARY\n",
      "        \n",
      "  def give_raise(self, amount):\n",
      "      self.salary += amount      \n",
      "        \n",
      "# Define a new class Manager inheriting from Employee\n",
      "class Manager(Employee):\n",
      "  pass\n",
      "\n",
      "  def display(self):\n",
      "      return(\"Manager \"+self.name)\n",
      "\n",
      "# Define a Manager object\n",
      "mng = Manager(\"Debbie Lashko\",86500)\n",
      "\n",
      "# Print mng's name\n",
      "print(mng.name)\n",
      "\n",
      "# Call mng.display()\n",
      "mng.display()\n",
      "\n",
      "output:\n",
      "\n",
      "Debbie Lashko\n",
      "'Manager Debbie Lashko'\n",
      "\n",
      "      >Customizing functionality via inheritance\n",
      "\n",
      "class SavingsAccount(BankAccount):\n",
      "\n",
      "\tdef __init__(self, balance, interest_rate):\n",
      "\n",
      "\t\tBankAccount.__init(self, balance)\n",
      "\t\tself.interest_rate= interest_rate\n",
      "\n",
      "\n",
      "\tdef compute_interest(self, n_periods=1):\n",
      "\t\treturn self.balance *((1+self.interest_rate)**n_periods-1)\n",
      "\n",
      "\n",
      "acct=SavingsAccount(1000,0.03)\n",
      "\n",
      "acct.interest_rate\n",
      "\n",
      "output: 0.03\n",
      "\n",
      "Adding Functionality:\n",
      "1. add methods as usual\n",
      "2. use data from the parent and the child class\n",
      "\n",
      "CheckingAccount\n",
      "1. balance\n",
      "2. limit\n",
      "\n",
      "withdraw()\n",
      "deposit()\n",
      "\n",
      "\n",
      "\n",
      "class CheckingAccount(BankAccount):\n",
      "\tdef __init__(self, balance, limit):\n",
      "\t\tBankAccount.__init__(self, content)\n",
      "\t\tself.limit=limit\n",
      "\n",
      "\tdef deposit(self, amount):\n",
      "\t\tself.balance+=amount\n",
      "\n",
      "\tdef withdraw(self,amount, fee=0):\n",
      "\t\tif fee<=self.limit:\n",
      "\t\t\tBankAccount.withdraw(self, amount-fee)\n",
      "\t\telse:\n",
      "\t\t\tBankAccount.withdraw(self, amount-self.limit)\n",
      "\n",
      "  > sample Employee and Manager\n",
      "\n",
      "class Employee:\n",
      "    def __init__(self, name, salary=30000):\n",
      "        self.name = name\n",
      "        self.salary = salary\n",
      "\n",
      "    def give_raise(self, amount):\n",
      "        self.salary += amount\n",
      "\n",
      "        \n",
      "class Manager(Employee):\n",
      "    def display(self):\n",
      "\tprint(\"Manager \", self.name)\n",
      "\n",
      "  # Add a constructor\n",
      "    def __init__(self, name, salarysalary=50000, project=None):\n",
      "\n",
      "        # Call the parent's constructor   \n",
      "        Employee.__init__(self, name, salary)\n",
      "\n",
      "        # Assign project attribute\n",
      "        self.project=project\n",
      "\n",
      "  \t# Add a give_raise method\n",
      "\tdef give_raise(self, amount,bonus=1.05):\n",
      "        new_amount=amount * bonus\n",
      "        Employee.give_raise(self,new_amount)\n",
      "    \n",
      "    \n",
      "mngr=Manager(\"Ashta Dunbar\", 78500)\n",
      "mngr.give_raise(1000)\n",
      "print(mngr.salary)\n",
      "mngr.give_raise(2000, bonus=1.03)\n",
      "print(mngr.salary)\n",
      "\n",
      "\n",
      "output:\n",
      "78500\n",
      "79550.0\n",
      "81610.0\n",
      "\n",
      "     > sample    racer\n",
      "\n",
      "# Create a Racer class and set MAX_SPEED to 5\n",
      "class Racer(Player):\n",
      "    MAX_SPEED=5\n",
      "# Create a Player and a Racer objects\n",
      "p = Player()\n",
      "r = Racer()\n",
      "\n",
      "print(\"p.MAX_SPEED = \", p.MAX_SPEED)\n",
      "print(\"r.MAX_SPEED = \", r.MAX_SPEED)\n",
      "\n",
      "print(\"p.MAX_POSITION = \", p.MAX_POSITION)\n",
      "print(\"r.MAX_POSITION = \", r.MAX_POSITION)\n",
      "\n",
      "p.MAX_SPEED =  3\n",
      "r.MAX_SPEED =  5\n",
      "p.MAX_POSITION =  10\n",
      "r.MAX_POSITION =  10\n",
      "\n",
      "\n",
      "    sample inheriting from pd.DataFrame and adding the attribute created_at\n",
      "\n",
      "# Import pandas as pd\n",
      "import pandas as pd\n",
      "\n",
      "# Define LoggedDF inherited from pd.DataFrame and add the constructor\n",
      "class LoggedDF(pd.DataFrame):\n",
      "        def __init__(self,*args,**kwargs):\n",
      "            pd.DataFrame.__init__(self,*args,**kwargs)\n",
      "            self.created_at = datetime.today()\n",
      "\n",
      "def to_csv(self, *args, **kwargs):\n",
      "    # Copy self to a temporary DataFrame\n",
      "    temp = self.copy()\n",
      "    \n",
      "    # Create a new column filled with self.created at\n",
      "    temp[\"created_at\"] = self.created_at\n",
      "    \n",
      "    # Call pd.DataFrame.to_csv on temp with *args and **kwargs\n",
      "        pd.DataFrame.to_csv(temp, *args,**kwargs)\n",
      "\n",
      "\n",
      "Using *args and **kwargs allows you to not worry about keeping the signature of your customized method compatible\n",
      "\n",
      "\n",
      "      >overloading comparison\n",
      "\n",
      "class Customer:\n",
      "\tdef __init__(self, id, name):\n",
      "\t\tself.id,self.name=id,name\n",
      "\n",
      "\tdef __eq__(self, other):\n",
      "\t\treturn(self.id==other.id and\n",
      "\t\t\tself.name==other.name)\n",
      "\n",
      "eq == should return a boolean value\n",
      "\n",
      "comparison operations\n",
      "\n",
      "== __eq__()\n",
      "!= __ne__()\n",
      ">= __ge__()\n",
      "<= __le__()\n",
      ">  __gt__()\n",
      "<  __lt__()\n",
      "\n",
      "objects as dictionaries __hash__()\n",
      "\n",
      "  > sample   > override __eq__\n",
      "\n",
      "class BankAccount:\n",
      "   # MODIFY to initialize a number attribute\n",
      "    def __init__(self, number, balance=0):\n",
      "        self.number=number\n",
      "        self.balance = balance\n",
      "      \n",
      "    def withdraw(self, amount):\n",
      "        self.balance -= amount \n",
      "    \n",
      "    # Define __eq__ that returns True if the number attributes are equal \n",
      "    def __eq__(self, other):\n",
      "        return self.number == other.number\n",
      "\n",
      "# Create accounts and compare them       \n",
      "acct1 = BankAccount(123, 1000)\n",
      "acct2 = BankAccount(123, 1000)\n",
      "acct3 = BankAccount(456, 1000)\n",
      "print(acct1 == acct2)\n",
      "print(acct1 == acct3)\n",
      "\n",
      "\n",
      "   sample  > override __eq__ and check type of the object\n",
      "\n",
      "class BankAccount:\n",
      "    def __init__(self, number, balance=0):\n",
      "        self.number, self.balance = number, balance\n",
      "      \n",
      "    def withdraw(self, amount):\n",
      "        self.balance -= amount \n",
      "\n",
      "    # MODIFY to add a check for the type()\n",
      "    def __eq__(self, other):\n",
      "        return ((self.number == other.number) \\\n",
      "            & (type(self) == type(other))\n",
      "        )\n",
      "\n",
      "acct = BankAccount(873555333)\n",
      "pn = Phone(873555333)\n",
      "print(acct == pn)\n",
      "\n",
      "\n",
      "Python always calls the child's __eq__() method when comparing a child object to a parent\n",
      " \n",
      "\n",
      "# Import pandas as pd\n",
      "import pandas as pd\n",
      "\n",
      "# Define LoggedDF inherited from pd.DataFrame and add the constructor\n",
      "class LoggedDF(pd.DataFrame):\n",
      "  \n",
      "  def __init__(self, *args, **kwargs):\n",
      "    pd.DataFrame.__init__(self, *args, **kwargs)\n",
      "    self.created_at = datetime.today()\n",
      "    \n",
      "    \n",
      "ldf = LoggedDF({\"col1\": [1,2], \"col2\": [3,4]})\n",
      "print(ldf.values)\n",
      "print(ldf.created_at)\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<< overloading\n",
      "\n",
      "class Customer:\n",
      "\tdef __init__(self, name, balance):\n",
      "\t\tself.name,self.balance=name,balance\n",
      "\n",
      "\tdef __str__(self):\n",
      "\tcust_str=\"\"\"\n",
      "\t\tCustomer:\n",
      "\t\t\tname:{name}\n",
      "\t\t\tbalance:{balance}\n",
      "\t\t\t\"\"\".format(name=self.name, balance=self.balance)\n",
      "\treturn cust_str\n",
      "\n",
      "\tdef __repr__(self):\n",
      "\treturn \"Customer('{name}',{balance})\".format(name=self.name,balance=self.balance)\n",
      "\n",
      "\n",
      "cust=Customer(\"Maryam Azur\",3000)\n",
      "\n",
      "print(cust)\n",
      "\n",
      "output: see an user friendly\n",
      "   getting a printable representation of an object\n",
      "\n",
      "__str__()\n",
      "__repr__()\n",
      "\n",
      "print(obj), str(obj)\n",
      "\n",
      "repr(obj)\n",
      "\n",
      "\n",
      "   sample  > __str__\n",
      "\n",
      "class Employee:\n",
      "    def __init__(self, name, salary=30000):\n",
      "        self.name, self.salary = name, salary\n",
      "            \n",
      "    # Add the __str__() method\n",
      "    def __str__(self):\n",
      "        s = \"Employee name: {name}\\nEmployee salary: {salary}\".format(name=self.name, salary=self.salary)      \n",
      "        return s\n",
      "\n",
      "emp1 = Employee(\"Amar Howard\", 30000)\n",
      "print(emp1)\n",
      "emp2 = Employee(\"Carolyn Ramirez\", 35000)\n",
      "print(emp2)\n",
      "\n",
      "   sample  > __repr__\n",
      "\n",
      "class Employee:\n",
      "    def __init__(self, name, salary=30000):\n",
      "        self.name, self.salary = name, salary\n",
      "      \n",
      "\n",
      "    def __str__(self):\n",
      "        s = \"Employee name: {name}\\nEmployee salary: {salary}\".format(name=self.name, salary=self.salary)      \n",
      "        return s\n",
      "      \n",
      "    # Add the __repr__method  \n",
      "    def __repr__(self):\n",
      "        s = \"Employee(\\\"{name}\\\", {salary})\".format(name=self.name, salary=self.salary)      \n",
      "        return s       \n",
      "\n",
      "emp1 = Employee(\"Amar Howard\", 30000)\n",
      "print(repr(emp1))\n",
      "emp2 = Employee(\"Carolyn Ramirez\", 35000)\n",
      "print(repr(emp2))\n",
      "\n",
      "\n",
      "     > sample    exceptions\n",
      "\n",
      "\n",
      "exceptions will stop the execution of the program.\n",
      "\n",
      "try except finally\n",
      "\n",
      "\n",
      "try:\n",
      " #try running some code\n",
      "except ExceptionNameHere:\n",
      " #run this code if exceptionNamehere happens\n",
      "except AnotherExceptionHere:\n",
      " #run this code if anotherexceptionhere happens\n",
      "\n",
      "finally:\n",
      " #run this code no matter what\n",
      "\n",
      "\n",
      "raising exceptions:\n",
      "\n",
      "def make_list_of_ones(length):\n",
      "\tif length<=0:\n",
      "\t\traise ValueError(\"Invalid length!\")\n",
      "\treturn[1]*length\n",
      "\n",
      "make_list_of_ones(-1)\n",
      "\n",
      "\n",
      "exceptions are classes\n",
      "\n",
      "BaseException\n",
      "\tException\n",
      "\t\tArithemticError\n",
      "\t\t\tFloatingPointError\n",
      "\t\t\tOverflowError\n",
      "\t\t\tZeroDivisionError\n",
      "\t\tTypeError\n",
      "\t\tValueError\n",
      "\t\t\tUnicodeError\n",
      "\t\t\t\tUnicodeDecodeError\n",
      "\t\t\tUnicodeEncodeError\n",
      "\t\t\tUnicodeTranslateError\n",
      "\n",
      "\t\tRuntimeError\n",
      "\n",
      "\tSystemExit\n",
      "\n",
      "\n",
      "\n",
      "class BalanceError(Exception): pass\n",
      "\n",
      "\n",
      "class Customer:\n",
      "\tdef __init__(self,name, balance):\n",
      "\tif balance < 0:\n",
      "\t\traise BalanceError(\"Balance has to be non-negative!\")\n",
      "\telse:\n",
      "\tself.name, self.balance=name,balance\n",
      "\n",
      "cust=Customer(\"Larry Torres\",-100)\n",
      "\n",
      "\n",
      "    sample adding try except to a function\n",
      "\n",
      "# MODIFY the function to catch exceptions\n",
      "def invert_at_index(x, ind):\n",
      "  try:\n",
      "    return 1/x[ind]\n",
      "  except ZeroDivisionError:\n",
      "    print(\"Cannot divide by zero!\")\n",
      "  except IndexError:\n",
      "    print(\"Index out of range!\")\n",
      " \n",
      "a = [5,6,0,7]\n",
      "\n",
      "# Works okay\n",
      "print(invert_at_index(a, 1))\n",
      "\n",
      "# Potential ZeroDivisionError\n",
      "print(invert_at_index(a, 2))\n",
      "\n",
      "# Potential IndexError\n",
      "print(invert_at_index(a, 5))\n",
      "\n",
      "\n",
      " > sample   > inherit ValueError\n",
      "\n",
      "class SalaryError(ValueError): pass\n",
      "class BonusError(SalaryError): pass\n",
      "\n",
      "class Employee:\n",
      "  MIN_SALARY = 30000\n",
      "  MAX_RAISE = 5000\n",
      "\n",
      "  def __init__(self, name, salary = 30000):\n",
      "    self.name = name\n",
      "    \n",
      "    # If salary is too low\n",
      "    if  salary < Employee.MIN_SALARY:\n",
      "      # Raise a SalaryError exception\n",
      "      raise SalaryError(\"Salary is too low\")\n",
      "      \n",
      "    self.salary = salary\n",
      "\n",
      "emp=Employee(\"bob\",100)  \n",
      "\n",
      "\n",
      "   sample  > BonusError\n",
      "\n",
      "class SalaryError(ValueError): pass\n",
      "class BonusError(SalaryError): pass\n",
      "\n",
      "class Employee:\n",
      "  MIN_SALARY = 30000\n",
      "  MAX_BONUS = 5000\n",
      "\n",
      "  def __init__(self, name, salary = 30000):\n",
      "    self.name = name    \n",
      "    if salary < Employee.MIN_SALARY:\n",
      "      raise SalaryError(\"Salary is too low!\")      \n",
      "    self.salary = salary\n",
      "    \n",
      "  # Rewrite using exceptions  \n",
      "  def give_bonus(self, amount):\n",
      "    if amount > Employee.MAX_BONUS:\n",
      "      raise BonusError(\"The bonus amount is too high!\")\n",
      "        \n",
      "    elif self.salary + amount <  Employee.MIN_SALARY:\n",
      "       raise SalaryError(\"The salary after bonus is too low!\")\n",
      "      \n",
      "    else:  \n",
      "      self.salary += amount\n",
      "\n",
      "\n",
      "\n",
      "t's better to list the except blocks in the increasing order of specificity, i.e. children before parents, otherwise the child exception will be called in the parent except block.\n",
      "\n",
      "\n",
      "       >Polymorphism\n",
      "\n",
      "polymorphism: using an unified interface to operate on objects of different classes\n",
      "\n",
      "def batch_withdraw(list_of_accounts, amount):\n",
      "\tfor acct in list_of_accounts:\n",
      "\t\tacct.withdraw(amount)\n",
      "\n",
      "b,c,s= BankAccount(1000), CheckingAccount(2000), SavingsAccount(3000)\n",
      "\n",
      "batch_withdraw([b,c,s])\n",
      "\n",
      "liskov substitution principle\n",
      "\n",
      "base class should be interchangeable with any of its subclasses without altering any properties of the program\n",
      "\n",
      "\n",
      "Wherever BankAccount works, CheckingAccount should work as well\n",
      "\n",
      "syntactically\n",
      "* function signatures are compatible arguments and returned values\n",
      "\n",
      "semantically\n",
      "* the state of the object and the program remains consistent\n",
      "* subclass method doesn't strengthen input conditions\n",
      "* subclass method doesn't weaken output conditions\n",
      "* no additional exceptions\n",
      "\n",
      "violation of lsp\n",
      "\n",
      "BankAccount.withdraw() requires 1 parameter, but CheckingAccount.withdraw() requires 2\n",
      "\n",
      "violation subclass strengthening input conditions\n",
      "\n",
      "BankAccount.withdraw() accepts any amount, but CheckingAccount.withdraw() assumes that the amount is limited\n",
      "\n",
      "violation subclass weakening output conditions\n",
      "\n",
      "Bankaccount.withdraw() can only leave a positive balance or cause an error,\n",
      "\n",
      "CheckingAccount.withdraw() can leave balance negative\n",
      "\n",
      "violating lsp\n",
      "1. changing additional attributes in subclass's method\n",
      "2. throwing additional exceptions in subclasses method\n",
      "\n",
      "if the inheritance violates the lsp principle, you should not be using inheritance\n",
      "\n",
      "  > sample\n",
      "\n",
      "class Parent:\n",
      "    def talk(self):\n",
      "        print(\"Parent talking!\")     \n",
      "\n",
      "class Child(Parent):\n",
      "    def talk(self):\n",
      "        print(\"Child talking!\")          \n",
      "\n",
      "class TalkativeChild(Parent):\n",
      "    def talk(self):\n",
      "        print(\"TalkativeChild talking!\")\n",
      "        Parent.talk(self)\n",
      "\n",
      "\n",
      "p, c, tc = Parent(), Child(), TalkativeChild()\n",
      "\n",
      "for obj in (p, c, tc):\n",
      "    obj.talk()\n",
      "\n",
      "\n",
      "  > sample rectangle and square\n",
      "\n",
      "# Define a Rectangle class\n",
      "class Rectangle():\n",
      "    def __init__(self,h,w):\n",
      "        self.h,self.w=h,w\n",
      "\n",
      "# Define a Square class\n",
      "class Square(Rectangle):\n",
      "    def __init__(self,w):\n",
      "        self.h,self.w=w,w\n",
      "\n",
      "A Square inherited from a Rectangle will always have both the h and w attributes, but we can't allow them to change independently of each other.\n",
      "\n",
      "  > sample   square violates the liskov principle\n",
      "\n",
      "  preserve consistent state\n",
      "\n",
      "class Rectangle:\n",
      "    def __init__(self, w,h):\n",
      "      self.w, self.h = w,h\n",
      "\n",
      "# Define set_h to set h      \n",
      "    def set_h(self, h):\n",
      "      self.h = h\n",
      "      \n",
      "# Define set_w to set w          \n",
      "    def set_w(self, w):\n",
      "      self.w=w\n",
      "      \n",
      "      \n",
      "class Square(Rectangle):\n",
      "    def __init__(self, w):\n",
      "      self.w, self.h = w, w \n",
      "\n",
      "# Define set_h to set w and h\n",
      "    def set_h(self, h):\n",
      "      self.h = h\n",
      "      self.w = h\n",
      "\n",
      "# Define set_w to set w and h      \n",
      "    def set_w(self,w):\n",
      "      self.h = w\n",
      "      self.w = w\n",
      "\n",
      "\n",
      "Remember that the substitution principle requires the substitution to preserve the oversall state of the program. An example of a program that would fail when this substitution is made is a unit test for a setter functions in Rectangle class.\n",
      "\n",
      "    data access  > private attributes\n",
      "\n",
      "1. all class data is public\n",
      "2. restricting access\n",
      "\n",
      "use @property to customize access\n",
      "\n",
      "override __getattr__() and __setattr__()\n",
      "\n",
      "start with a sing _ > \"internal\"\n",
      "not a part of the public api\n",
      "\n",
      "if the variable starts with __ then it is private\n",
      "\n",
      "name mangling obj.__attr_name is intrepreted as obj._MyClass__attr_name\n",
      "\n",
      "Use to prevent name clashes in inherited classes\n",
      "\n",
      "   sample create an internal method called _is_valid\n",
      "\n",
      "# MODIFY to add class attributes for max number of days and months\n",
      "class BetterDate:\n",
      "    _MAX_DAYS = 30\n",
      "    _MAX_MONTHS = 12\n",
      "    \n",
      "    def __init__(self, year, month, day):\n",
      "      self.year, self.month, self.day = year, month, day\n",
      "      \n",
      "    @classmethod\n",
      "    def from_str(cls, datestr):\n",
      "        year, month, day = map(int, datestr.split(\"-\"))\n",
      "        return cls(year, month, day)\n",
      "    \n",
      "    # Add _is_valid() checking day and month values\n",
      "    def _is_valid(self):\n",
      "        return (self.day <= BetterDate._MAX_DAYS) and \\\n",
      "               (self.month <= BetterDate._MAX_MONTHS)\n",
      "         \n",
      "bd1 = BetterDate(2020, 4, 30)\n",
      "print(bd1._is_valid())\n",
      "\n",
      "bd2 = BetterDate(2020, 6, 45)\n",
      "print(bd2._is_valid())\n",
      "\n",
      "\n",
      "The single underscore naming convention is purely a convention, and Python doesn't do anything special with such attributes and methods behind the scenes\n",
      "\n",
      " That convention is widely followed, though, so if you see an attribute name with one leading underscore in someone's class - don't use it\n",
      "\n",
      "\n",
      "    >Properties\n",
      "\n",
      "\n",
      "df.shape\n",
      "\n",
      "attribute can not be changed\n",
      "\n",
      "\n",
      "class Employer:\n",
      "\tdef __init__(self,name, new_salary):\n",
      "\t\tself._salary=new_salary\n",
      "\n",
      "\n",
      "\t@property\n",
      "\tdef salary(self):\n",
      "\t\treturn self._salary\n",
      "\n",
      "\t@salary.setter\n",
      "\tdef salary(self,new_salary):\n",
      "\t\tif new_salary<0:\n",
      "\t\t\traise ValueError(\"Invalid salary\")\n",
      "\t\tself._salary=new_salary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emp.salary=60000\n",
      "\n",
      "without a setter, the property will be read only\n",
      "\n",
      "read only\n",
      "@salary.getter (read only)\n",
      "\n",
      "@salary.deleter\n",
      "\n",
      "\n",
      "    sample customer\n",
      "\n",
      "# Create a Customer class\n",
      "class Customer():\n",
      "    def __init__(self,name,new_bal):\n",
      "            self._name=name\n",
      "          if new_bal<0:\n",
      "                raise ValueError(\"Invalid new balance\")\n",
      "            else:\n",
      "                self._new_bal=new_bal\n",
      "\n",
      "    @property\n",
      "    def balance(self):\n",
      "        return self._balance\n",
      "\n",
      "       @balance.setter\n",
      "    def balance(self,balance):\n",
      "        if balance<0:\n",
      "            raise ValueError(\"Invalid new balance\")\n",
      "        else:\n",
      "            self._balance=balance\n",
      "            print(\"Setter method is called\")\n",
      "\n",
      "# Create a Customer        \n",
      "cust = Customer(\"Belinda Lutz\",2000)\n",
      "# Assign 3000 to the balance property\n",
      "cust.balance=3000\n",
      "\n",
      "# Print the balance property\n",
      "print(cust.balance)\n",
      "\n",
      "\n",
      "   > sample   read only property\n",
      "\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "\n",
      "# MODIFY the class to turn created_at into a read-only property\n",
      "class LoggedDF(pd.DataFrame):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        pd.DataFrame.__init__(self, *args, **kwargs)\n",
      "        self._created_at = datetime.today()\n",
      "\n",
      "    def to_csv(self, *args, **kwargs):\n",
      "        temp = self.copy()\n",
      "        temp[\"created_at\"] = self.created_at\n",
      "        pd.DataFrame.to_csv(temp, *args, **kwargs) \n",
      "\n",
      "    @property  \n",
      "    def created_at(self):\n",
      "        return self._created_at\n",
      "\n",
      "ldf = LoggedDF({\"col1\": [1,2], \"col2\":[3,4]}) \n",
      "\n",
      "# Put into try-except block to catch AtributeError and print a message\n",
      "try:\n",
      "    ldf.created_at = '2035-07-13'\n",
      "except AttributeError:\n",
      "    print(\"Could not set attribute\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\object oriented programming.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\object oriented programming.txt\n",
      "think object oriented\n",
      "\n",
      "start thinking of patterns of behavior\n",
      "\n",
      "code as interactions of objects\n",
      "code is more usable and maintainable\n",
      "\n",
      "object= state+ behavior  (encapsulation)\n",
      "\n",
      "classes are blueprints for objects outlining possible states and behaviors\n",
      "\n",
      "in python everything is an object\n",
      "every object has a class\n",
      "\n",
      "type() to find the class\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "a=np.array([1,2,3,4])\n",
      "a.shape\n",
      "a.reshape(2,2)\n",
      "\n",
      " >Class\n",
      "\n",
      "class Customer:\n",
      "\tpass #blank template\n",
      "\n",
      "c1=Customer()\n",
      "\n",
      "you can create instances of a template class\n",
      "\n",
      "\n",
      "class Customer:\n",
      "\n",
      "\tdef identify(self, name):\n",
      "\t\tprint(\"I am a customer \"+name)\n",
      "\n",
      "cust = Customer()\n",
      "\n",
      "cust.identify(\"Laura\")\n",
      "\n",
      "self is a stand-in for a particular object used in class definition\n",
      "\n",
      "attributes customer name should be name\n",
      "attributes are created by assignment\n",
      "\n",
      "\n",
      "class Customer:\n",
      "\n",
      "\tdef set_name(self, new_name):\n",
      "\t\tself.name=new_name\n",
      "\n",
      "\tdef identify(self):\n",
      "\t\tprint(\"I am Customer\"+self.name)\n",
      "\n",
      "\n",
      "self reference the object and name comes into existence by assignment\n",
      "\n",
      "customer=Customer()\n",
      "customer.set_name(\"Laura\")\n",
      "print(customer.name)\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "class Employee:\n",
      "  \n",
      "  def set_name(self, new_name):\n",
      "    self.name = new_name\n",
      "  \n",
      "  # Add set_salary() method\n",
      "  def set_salary(self,new_salary):\n",
      "    self.salary=new_salary\n",
      "  \n",
      "  \n",
      "# Create an object emp of class Employee  \n",
      "emp = Employee()\n",
      "\n",
      "# Use set_name to set the name of emp to 'Korel Rossi'\n",
      "emp.set_name('Korel Rossi')\n",
      "\n",
      "# Set the salary of emp to 50000\n",
      "emp.set_salary(50000)\n",
      "\n",
      "print(emp.name,emp.salary)\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "class Employee:\n",
      "    def set_name(self, new_name):\n",
      "        self.name = new_name\n",
      "\n",
      "    def set_salary(self, new_salary):\n",
      "        self.salary = new_salary \n",
      "\n",
      "    # Add a give_raise() method with raise amount as a parameter\n",
      "    def give_raise(self, new_raise):\n",
      "        self.salary += new_raise\n",
      "\n",
      "\n",
      "emp = Employee()\n",
      "emp.set_name('Korel Rossi')\n",
      "emp.set_salary(50000)\n",
      "\n",
      "print(emp.salary)\n",
      "emp.give_raise(1500)\n",
      "print(emp.salary)\n",
      "\n",
      " >Sample\n",
      "\n",
      "class Employee:\n",
      "    def set_name(self, new_name):\n",
      "        self.name = new_name\n",
      "\n",
      "    def set_salary(self, new_salary):\n",
      "        self.salary = new_salary \n",
      "\n",
      "    def give_raise(self, amount):\n",
      "        self.salary = self.salary + amount\n",
      "\n",
      "    # Add monthly_salary method that returns 1/12th of salary attribute\n",
      "    def monthly_salary(self):\n",
      "        return self.salary/12\n",
      "\n",
      "    \n",
      "emp = Employee()\n",
      "emp.set_name('Korel Rossi')\n",
      "emp.set_salary(50000)\n",
      "\n",
      "# Get monthly salary of emp and assign to mon_sal\n",
      "mon_sal = emp.monthly_salary()\n",
      "\n",
      "# Print mon_sal\n",
      "print(mon_sal)\n",
      "\n",
      "\n",
      " >init constructor\n",
      "\n",
      "class Customer:\n",
      "\tdef __init__(self, name, balance):\n",
      "\t\tself.name=name\n",
      "\t\tself.balance=balance\n",
      "\t\tprint(\" the constructor method was called\n",
      "\n",
      "customer = Customer(\"Bob\", 1000)\n",
      "print(customer.name, customer.balance)\n",
      "\n",
      "1. try defining attribute in the constructor\n",
      "2. defining attributes in the constructor puts all the attributes in one place\n",
      "3. work to create usable and maintainable code\n",
      "4. name your classes using camelcase low case for functions and attributes\n",
      "5. \"\"\"Doc strings are comments in python\"\"\"\n",
      "\n",
      " >Sample\n",
      "\n",
      "class Employee:\n",
      "    # Create __init__() method\n",
      "    def __init__(self, name, salary=0):\n",
      "        # Create the name and salary attributes\n",
      "        self.name = name\n",
      "        self.salary = salary\n",
      "    \n",
      "    # From the previous lesson\n",
      "    def give_raise(self, amount):\n",
      "        self.salary += amount\n",
      "\n",
      "    def monthly_salary(self):\n",
      "        return self.salary/12\n",
      "        \n",
      "emp = Employee(\"Korel Rossi\")\n",
      "\n",
      " >Sample\n",
      "\n",
      "class Employee:\n",
      "  \n",
      "    def __init__(self, name, salary=0):\n",
      "        self.name = name\n",
      "        # Modify code below to check if salary is positive\n",
      "        if salary>0:\n",
      "            self.salary = salary \n",
      "        else:\n",
      "            self.salary=0\n",
      "   \n",
      "   # ...Other methods omitted for brevity ...\n",
      "      \n",
      "emp = Employee(\"Korel Rossi\", -1000)\n",
      "print(emp.name)\n",
      "print(emp.salary)\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Import datetime from datetime\n",
      "from datetime import datetime\n",
      "\n",
      "class Employee:\n",
      "    \n",
      "    def __init__(self, name, salary=0):\n",
      "        self.name = name\n",
      "        if salary > 0:\n",
      "          self.salary = salary\n",
      "        else:\n",
      "          self.salary = 0\n",
      "          print(\"Invalid salary!\")\n",
      "          \n",
      "        # Add the hire_date attribute and set it to today's date\n",
      "        self.hire_date=datetime.today()\n",
      "        \n",
      "   # ...Other methods omitted for brevity ...\n",
      "      \n",
      "emp = Employee(\"Korel Rossi\", -1000)\n",
      "print(emp.name)\n",
      "print(emp.salary)\n",
      "\n",
      " >Sample\n",
      "\n",
      "import numpy as np\n",
      "class Point:\n",
      "    def __init__(self,x=0.0,y=0.0):\n",
      "        self.x=x\n",
      "        self.y=y\n",
      "        \n",
      "    def reflect(self,axis):\n",
      "        if axis=='x':\n",
      "            self.y=self.y*-1\n",
      "        elif axis=='y':\n",
      "            self.x=self.x*-1\n",
      "\n",
      "    def distance_to_origin(self):\n",
      "        return np.sqrt(self.x**2+self.y**2)\n",
      "        \n",
      "pt = Point(x=3.0)\n",
      "pt.reflect(\"y\")\n",
      "print((pt.x, pt.y))\n",
      "pt.y = 4.0\n",
      "print(pt.distance_to_origin())\n",
      "\n",
      " How to define inheritance in python\n",
      "https://www.w3schools.com/python/python_inheritance.asp\n",
      "\n",
      "inheritance\n",
      "\n",
      "class Person:\n",
      "  def __init__(self, fname, lname):\n",
      "    self.firstname = fname\n",
      "    self.lastname = lname\n",
      "\n",
      "  def printname(self):\n",
      "    print(self.firstname, self.lastname)\n",
      "\n",
      "class Student(Person):\n",
      "  def __init__(self, fname, lname):\n",
      "    super().__init__(fname, lname)\n",
      "    self.graduationyear = 2019\n",
      "\n",
      "x = Student(\"Mike\", \"Olsen\")\n",
      "print(x.graduationyear)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pandas time series.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pandas time series.txt\n",
      "How to plot data without a date\n",
      "\n",
      "data={'key':[0,1,2,3],'data_values':[100,150,400,200]}\n",
      "data=pd.DataFrame(data)\n",
      "data2={'key':[0,1,2,3],'data_values':[45,98,200,300]}\n",
      "data2=pd.DataFrame(data2)\n",
      "data.set_index('key')\n",
      "data2.set_index('key')\n",
      "print(data)\n",
      "\n",
      "fig, axs = plt.subplots(2,1, figsize=(5,10))\n",
      "data.iloc[:,1].plot(y='data_values',ax=axs[0])\n",
      "data2.iloc[:,1].plot(y='data_values',ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "\n",
      " adding an x time stamp\n",
      "\n",
      "data={'time':['2020-04-01','2020-04-02','2020-04-03','2020-04-04'],'data_values':[100,150,400,200]}\n",
      "data=pd.DataFrame(data)\n",
      "data2={'time':['2020-04-01','2020-04-02','2020-04-03','2020-04-04'],'data_values':[45,98,200,300]}\n",
      "\n",
      "data2=pd.DataFrame(data2)\n",
      "data.set_index('time')\n",
      "data2.set_index('time')\n",
      "print(data)\n",
      "\n",
      "fig, axs = plt.subplots(2,1, figsize=(5,10))\n",
      "data.plot(x='time',y='data_values',ax=axs[0])\n",
      "data2.plot(x='time', y='data_values',ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "\n",
      " linear regression\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "# Prepare input and output DataFrames\n",
      "X = boston[['AGE']]\n",
      "y = boston[['RM']]\n",
      "\n",
      "# Fit the model\n",
      "model = linear_model.LinearRegression()\n",
      "model.fit(X, y)\n",
      "\n",
      "print(new_inputs.reshape(-1,1))\n",
      "predictions = model.predict(new_inputs.reshape(-1,1))\n",
      "\n",
      "# Visualize the inputs and predicted values\n",
      "plt.scatter(new_inputs, predictions, color='r', s=3)\n",
      "plt.xlabel('inputs')\n",
      "plt.ylabel('predictions')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " generating time\n",
      "\n",
      "generates 11 numbers \n",
      "\n",
      "indices=np.arange(0,10)  \n",
      "print(indices)\n",
      "\n",
      "creates 10 evenly spaced numbers starting with 1 and ending with 10\n",
      "print(np.linspace(1,10,10))\n",
      "\n",
      "\n",
      "import librosa as lr\n",
      "from glob import glob\n",
      "\n",
      "# List all the wav files in the folder\n",
      "audio_files = glob(data_dir + '/*.wav')\n",
      "\n",
      "# Read in the first audio file, create the time array\n",
      "audio, sfreq = lr.load(audio_files[0])\n",
      "time = np.arange(0, len(audio)) / sfreq\n",
      "\n",
      "\n",
      "# Plot audio over time\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(time, audio)\n",
      "ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      " >read in stock prices per year\n",
      "\n",
      "# Read in the data\n",
      "data = pd.read_csv('prices.csv', index_col=0)\n",
      "\n",
      "# Convert the index of the DataFrame to datetime\n",
      "data.index = pd.to_datetime(data.index)\n",
      "print(data.head())\n",
      "\n",
      "# Loop through each column, plot its values over time\n",
      "fig, ax = plt.subplots()\n",
      "for column in data:\n",
      "    data[column].plot(ax=ax, label=column)\n",
      "ax.legend()\n",
      "plt.show()\n",
      "\n",
      " classification and feature engineering\n",
      "\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "X=np.column_stack([means,maxs,stds])\n",
      "y=labels.reshape([-1,1])\n",
      "model=LinearSVC()\n",
      "model.fit(X,y)\n",
      "\n",
      "predictions=model.predict(X_test)\n",
      "\n",
      "percent_score= sum(predictions==labels_test)/len(labels_test)\n",
      "percent_score= accuracy_score(labels_test,predictions)\n",
      "\n",
      "\n",
      " heartbeat analysis\n",
      "\n",
      "fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True)\n",
      "\n",
      "# Calculate the time array\n",
      "time = np.arange(normal.shape[0]) / sfreq\n",
      "\n",
      "# Stack the normal/abnormal audio so you can loop and plot\n",
      "stacked_audio = np.hstack([normal, abnormal]).T\n",
      "\n",
      "# Loop through each audio file / ax object and plot\n",
      "# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
      "for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
      "    ax.plot(time, iaudio)\n",
      "show_plot_and_make_titles()\n",
      "\n",
      " using the mean to smooth the noise\n",
      "\n",
      "mean_normal = np.mean(normal, axis=1)\n",
      "mean_abnormal = np.mean(abnormal, axis=1)\n",
      "\n",
      "# Plot each average over time\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
      "ax1.plot(time, mean_normal)\n",
      "ax1.set(title=\"Normal Data\")\n",
      "ax2.plot(time, mean_abnormal)\n",
      "ax2.set(title=\"Abnormal Data\")\n",
      "plt.show()\n",
      "\n",
      " Test the data\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "# Initialize and fit the model\n",
      "model = LinearSVC()\n",
      "model.fit(X_train,y_train)\n",
      "\n",
      "# Generate predictions and score them manually\n",
      "predictions = model.predict(X_test)\n",
      "print(sum(predictions == y_test.squeeze()) / len(y_test))\n",
      "\n",
      " Smoothing signal\n",
      "\n",
      "# Rectify the audio signal\n",
      "audio_rectified = audio.apply(np.abs)\n",
      "\n",
      "# Plot the result\n",
      "# figsize parameter 1 is the width and parameter 2 is the height\n",
      "audio_rectified.plot(figsize=(10, 5))\n",
      "plt.show() \n",
      "\n",
      " Rolling and mean\n",
      "\n",
      "# Smooth by applying a rolling mean\n",
      "audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
      "\n",
      "# Plot the result\n",
      "audio_rectified_smooth.plot(figsize=(10, 5))\n",
      "plt.show()\n",
      "\n",
      "\n",
      " cross_val_score\n",
      "\n",
      "means = np.mean(audio_rectified_smooth, axis=0)\n",
      "stds = np.std(audio_rectified_smooth, axis=0)\n",
      "maxs = np.max(audio_rectified_smooth, axis=0)\n",
      "\n",
      "# Create the X and y arrays\n",
      "X = np.column_stack([means, stds, maxs])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "from sklearn.model_selection import cross_val_score\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "print(percent_score)\n",
      "\n",
      " tempo\n",
      "\n",
      "tempos = []\n",
      "for col, i_audio in audio.items():\n",
      "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
      "\n",
      "# Convert the list to an array so you can manipulate it more easily\n",
      "tempos = np.array(tempos)\n",
      "\n",
      "# Calculate statistics of each tempo\n",
      "tempos_mean = tempos.mean(axis=-1)\n",
      "tempos_std = tempos.std(axis=-1)\n",
      "tempos_max = tempos.max(axis=-1)\n",
      "\n",
      "  column_stack\n",
      "\n",
      "# Create the X and y arrays\n",
      "X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "\n",
      "\n",
      "\n",
      "  converting date index to a datetime\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "  grid=True\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "# Plot the entire time series diet and show gridlines\n",
      "diet.plot(grid=True)\n",
      "plt.show()\n",
      "\n",
      " filter on index\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "# Slice the dataset to keep only 2012\n",
      "diet2012 = diet['2012']\n",
      "\n",
      "# Plot 2012 data\n",
      "diet2012.plot(grid=True)\n",
      "plt.show()\n",
      "\n",
      " Differences between two sets of dates\n",
      "\n",
      "set_stock_dates = set(stocks.index)\n",
      "set_bond_dates = set(bonds.index)\n",
      "\n",
      "\n",
      "differences=set_stock_dates - set_bond_dates\n",
      "# Take the difference between the sets and print\n",
      "print(differences)\n",
      "\n",
      "# Merge stocks and bonds DataFrames using join()\n",
      "stocks_and_bonds = stocks.join(bonds, how='inner')\n",
      "\n",
      "\n",
      " Correlation between two variables\n",
      "\n",
      "# Compute percent change using pct_change()\n",
      "returns = stocks_and_bonds.pct_change()\n",
      "\n",
      "# Compute correlation using corr()\n",
      "correlation = returns['SP500'].corr(returns['US10Y'])\n",
      "print(\"Correlation of stocks and interest rates: \", correlation)\n",
      "\n",
      "# Make scatter plot\n",
      "plt.scatter(returns['SP500'],returns['US10Y'])\n",
      "plt.show()\n",
      "\n",
      "        Spectrogram\n",
      "\n",
      "fourier transforms\n",
      "fast or slow moving waves\n",
      "fft show a series of fast and slow wave osciliations in a time series.\n",
      "\n",
      "short-time fourier transform is calculating a fft over a time frame then sliding the window over by one\n",
      "\n",
      "The spectrogram is the square of each sfft\n",
      "\n",
      "we can calculate the stft with librosa\n",
      "\n",
      "the sound frequencies are converted in to decibels which normalizes the average values of all the frequencies\n",
      "\n",
      "we can visualize it with specshow() function\n",
      "\n",
      "from librosa.core import stft, amplitude_to_db\n",
      "from librosa.display import specshow\n",
      "\n",
      "HOP_LENGTH = 2**4\n",
      "SIZE_WINDOW= 2**7\n",
      "\n",
      "#calculate the short fast fourier transform\n",
      "\n",
      "audio_spec = stft(audio, hop_length=HOP_LENGTH, n_fft=SIZE_WINDOW)\n",
      "\n",
      "#convert into decibels\n",
      "spec_db=amplitude_to_db(audio_spec)\n",
      "\n",
      "#visualize\n",
      "specshow(spec_db, sr=sfreq, x_axis='time',\n",
      "\ty_axis='hz', hop_length=HOP_LENGTH)\n",
      "\n",
      "bandwidths=lr.feature.spectral_bandwidth(S=spec)[0]\n",
      "centroids=lr.feature.spectral_centroid(S=spec)[0]\n",
      "\n",
      "ax= spectshow(spec, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "ax.plot(times_spec, centroids)\n",
      "ax.fill_between(times_spec, centroids-bandwidths/2, centroids+bandwidths/2, alpha=0.5)\n",
      "\n",
      "\n",
      "each spectral has different patterns\n",
      "we can use these patterns to distinquish spectrals from one another\n",
      "\n",
      "for example spectral bandwidth and spectral centroids describe where most of the energy is at each moment in time.\n",
      "\n",
      "centroids_all=[]\n",
      "bandwidths_all=[]\n",
      "\n",
      "for spec in spectrograms:\n",
      "\tbandwidths=lr.feature.spectral_bandwidth(S=lr.db_to_amplitude(spec))\n",
      "\tcentroids=lr.feature.spectral_centroid(S=lr.db_to_amplitude(spec))\n",
      "\tbandwidths_all.append(np.mean(bandwidths))\n",
      "\tcentroids_all.appen(np.mean(centroids))\n",
      "\n",
      "#input matrix\n",
      "\n",
      "X= np.column_stack([means,stds,maxs,tempo_mean,tempo_max,tempo_std, bandwidths_all, centroids_all])\n",
      "\n",
      "\n",
      "\n",
      "   sample   >  short term fourier transform heart beat audio\n",
      "#Spectral engineering is one of the most common techniques in machine learning for time series data\n",
      "\n",
      "# Import the stft function\n",
      "from librosa.core import stft\n",
      "\n",
      "# Prepare the STFT\n",
      "HOP_LENGTH = 2**4\n",
      "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n",
      "\n",
      " > sample  > convert the spectral to decibals\n",
      "\n",
      "# Convert into decibels\n",
      "spec_db = amplitude_to_db(spec)\n",
      "\n",
      "# Compare the raw audio to the spectrogram of the audio\n",
      "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
      "axs[0].plot(time, audio)\n",
      "\n",
      "specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "plt.show()\n",
      "\n",
      "#the heartbeats come in pairs as seen by the vertical lines in the spectrogram\n",
      "\n",
      "\n",
      "   >sample  > calculate the bandwidths and centroids\n",
      "\n",
      "# By computing the spectral features, you have a much better idea of what's going on. \t\n",
      "\n",
      "import librosa as lr\n",
      "from librosa.core import amplitude_to_db\n",
      "from librosa.display import specshow\n",
      "\n",
      "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
      "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
      "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
      "\n",
      "\n",
      "# Convert spectrogram to decibels for visualization\n",
      "spec_db = amplitude_to_db(spec)\n",
      "\n",
      "# Display these features on top of the spectrogram\n",
      "fig, ax = plt.subplots(figsize=(10, 5))\n",
      "ax = specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "ax.plot(times_spec, centroids)\n",
      "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
      "ax.set(ylim=[None, 6000])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#You've spent this lesson engineering many features from the audio data - some contain information about how the audio changes in time\n",
      "\n",
      "#Combine all of them into an array that can be fed into the classifier, and see how it does.\n",
      "\n",
      " > sample build the final array for the classifier\n",
      "\n",
      "# Loop through each spectrogram\n",
      "bandwidths = []\n",
      "centroids = []\n",
      "\n",
      "for spec in spectrograms:\n",
      "    # Calculate the mean spectral bandwidth\n",
      "    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
      "    # Calculate the mean spectral centroid\n",
      "    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
      "    # Collect the values\n",
      "    bandwidths.append(this_mean_bandwidth)  \n",
      "    centroids.append(this_mean_centroid)\n",
      "\n",
      "# Create X and y arrays\n",
      "X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "\n",
      "output .48\n",
      "\n",
      "#To improve the accuracy, you want to find the right features that provide relevant information and also build models on much larger data\n",
      "\n",
      "   >Regression\n",
      "\n",
      "regression model predict continueous models\n",
      "\n",
      "regression: a process that results in a formal model of the data\n",
      "correlation: a statistic that describe the data.  how two features correlate between each other.\n",
      "\n",
      "down or up together or an inverse relationship\n",
      "\n",
      "timeseries often have patterns that change over time\n",
      "\n",
      "two timeseries that seem correlated at one moment may not remain so over time\n",
      "\n",
      "fig, axs=plt.subplots(1,2)\n",
      "\n",
      "axs[0].plot(x,c='k',lw=3,alpha=.2)\n",
      "axs[0].plot(y)\n",
      "axs[0].set(xlabel='time',title='X values=time')\n",
      "\n",
      "#encode time as acolor in a scatterplot\n",
      "\n",
      "axs[1].scatter(x_long,y_long, c=np.arange(len(x_long)),cmap='viridis')\n",
      "axs[1].set(xlabel='x',ylabel='y',title='Color=time')\n",
      "\n",
      "  >regression models\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "model=LinearRegression()\n",
      "model.fit(X,y)\n",
      "model.predict(X)\n",
      "\n",
      "  >Ridge (see introductory course on skilearn)\n",
      "\n",
      "alphas=[.1,1e2,1e3]\n",
      "\n",
      "ax.plot(y_test,color='k', alpha=.3,lw=3)\n",
      "\n",
      "for ii, alpha in enumerate(alphas):\n",
      "\ty_predicted=Ridge(alpha=alpha).fit(X_train,y_train).predict(X_test)\n",
      "\tax.plot(y_predict, c=cmap(ii/len(alphas)))\n",
      "\n",
      "ax.legend(['True values','Model 1', 'Model 2', 'Model 3'])\n",
      "ax.set(xlabel='Time')\n",
      "\n",
      "  >Scoring a regression model\n",
      "\n",
      "Correlation (r)\n",
      "Coefficient of Determination(R2)\n",
      "\n",
      "\n",
      "Coefficient of Determination R2\n",
      "1- error(model)/variance(testdata)\n",
      "\n",
      "Error is actual - predicted of the model\n",
      "\n",
      "Variance is the mean squared distance of the data from their mean\n",
      "(x-x_mean) ** 2 / n\n",
      "\n",
      "or\n",
      "\n",
      "np.var(versicolor_petal_length)\n",
      "\n",
      "\n",
      "deviations = np.mean(y_data) - y_data\n",
      "\n",
      "VAR = np.sum(np.square(deviations))\n",
      "\n",
      "R-Squared : what fraction of variation is linear\n",
      "\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "print(r2_score(y_predicted, y_test))\n",
      "\n",
      "\n",
      "   Sample  > plot the prices for Ebay and Yhoo over time\n",
      "\n",
      "# Plot the raw values over time\n",
      "print(prices.columns)\n",
      "prices.plot()\n",
      "plt.show()\n",
      "\n",
      "   SAMPLE  > plot a scatter plot for ebay and yahoo prices\n",
      "\n",
      "# Scatterplot with one company per axis\n",
      "prices.plot.scatter('EBAY', 'YHOO')\n",
      "plt.show()\n",
      "\n",
      "   sample  > plot the scatter plot with color showing the price index\n",
      "# Scatterplot with color relating to time\n",
      "prices.plot.scatter('EBAY', 'YHOO', c=prices.index, \n",
      "                    cmap=plt.cm.viridis, colorbar=False)\n",
      "plt.show()\n",
      "\n",
      "The prices.index are dates.   The color changes over time.\n",
      "\n",
      "<<<<<sample  > add a ridge regressor and do 3 fold cross validation to check the accuracy.\n",
      "\n",
      "\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "# Use stock symbols to extract training data\n",
      "X = all_prices[['EBAY','NVDA','YHOO']]\n",
      "y = all_prices[['AAPL']]\n",
      "\n",
      "# Fit and score the model with cross-validation\n",
      "scores = cross_val_score(Ridge(), X, y, cv=3)\n",
      "print(scores)\n",
      "\n",
      "If Measure of fit of the model is a small value that means model is well fit to the data.\n",
      "\n",
      "If Measure of magnitude of coefficient is a small value that means model is not overfit.\n",
      "\n",
      "  >sample  > calculating the R2 factor\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "# Split our data into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, \n",
      "                                                    train_size=.8, shuffle=False, random_state=1)\n",
      "\n",
      "# Fit our model and generate predictions\n",
      "model = Ridge()\n",
      "model.fit(X_train,y_train)\n",
      "predictions = model.predict(X_test)\n",
      "score = r2_score(y_test, predictions)\n",
      "print(score)\n",
      "\n",
      "output: -5.70939901949\n",
      "\n",
      "ebay, nvda, yhoo are not linear predicters for apple prices\n",
      "\n",
      "  >sample    y_test is falling then predicted apple price is climbing\n",
      "\n",
      "# Visualize our predictions along with the \"true\" values, and print the score\n",
      "fig, ax = plt.subplots(figsize=(15, 5))\n",
      "ax.plot(y_test, color='k', lw=3)\n",
      "ax.plot(predictions, color='r', lw=2)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "The poor r2 score reflects a deviation between the predicted and true time series values.\n",
      "\n",
      "\n",
      "       cleaning and improving the data\n",
      "\n",
      "time data errors often happens because of human error, machine sensor malfunctions, and database failures.\n",
      "\n",
      "\n",
      "interpolation is using time to fill in missing data\n",
      "\n",
      "you can use time to assist in interpolation\n",
      "\n",
      "interpolation means using known values on either side of a gap in the data to make assumptions about what missing\n",
      "\n",
      "#create a boolean mask to find where the missing values are\n",
      "\n",
      "missing= prices.isna()\n",
      "\n",
      "#create a list of interpolated missing values\n",
      "prices_interp = prices.interpolate('linear')\n",
      "\n",
      "ax=prices_interp.plot(c='r')\n",
      "prices.plot(c='k',ax=ax, lw=2)\n",
      "\n",
      "    another way to fix missing data is to transform it so it more better behaved\n",
      "\n",
      "1. smooth the data\n",
      "2. use more complex transformations\n",
      "\n",
      "a common transformation is to standardize the mean and variance over time\n",
      "\n",
      "1. convert the dataset so each point represents the % change over a previous window\n",
      "2. this makes timepoints more comparable to one another if absolute values of data change a lot\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def percent_change(values):\n",
      "\t\"\"\"Calculates the % change between the last value and the mean of previous values\"\"\"\n",
      "\n",
      "\tprevious_values=values[:-1]\n",
      "\tlast_value=values[-1]\n",
      "\tpercent_change=(last_value-np.mean(previous_values))/np.mean(previous_values)\n",
      "\treturn percent_change\n",
      "\n",
      "\n",
      " > pass the percent_change function as a input to rolling prices\n",
      "\n",
      "fig,axs=plt.subplots(1,2,figsize=(10,5))\n",
      "ax=prices.plot(ax=axs[0])\n",
      "\n",
      "ax=prices.rolling(window=20).aggregate(percent_change).plot(ax=axs[1])\n",
      "ax.legend_.set_visible(False)\n",
      "\n",
      "#periods of high or low changes are easier to spot.\n",
      "\n",
      "    Outliers\n",
      "\n",
      "outliers are datapoints that are significantly statistically different from the dataset\n",
      "\n",
      "they have negative effects on the predictive power of your model, biasing it away from its true value\n",
      "\n",
      "One solution: remove or replace outliers with a more representative vlaue\n",
      "\n",
      "** there can be legitimate extreme values\n",
      "\n",
      "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
      "\n",
      "for data, ax in zip([prices,prices_perc_change],axs):\n",
      "\tthis_mean=data.mean()\n",
      "\tthis.std=data.std()\n",
      "\n",
      "\tdata.plot(ax=ax)\n",
      "\tax.axhline(this_mean+this_std*3, ls='--',c='r')\n",
      "\tax.axhline(this_mean-this_std*3, ls='--',c='r')\n",
      "\n",
      "\n",
      "find price outside of 3 std range from the mean\n",
      "\n",
      "#replace the outliers with the median\n",
      "\n",
      "prices_outlier_centered=prices_outlier_perc - prices_outlier_perc.mean()\n",
      "\n",
      "std=prices_outlier_perc.std()\n",
      "\n",
      "outliers=np.abs(prices_outlier_centered)>(std*3)\n",
      "\n",
      "prices_outlier_fixed=prices_outlier_centered.copy()\n",
      "prices_outlier_fixed[outliers]=np.nanmedian(prices_outlier_fixed)\n",
      "\n",
      "fig,axs = plt.subplots(1,2,figsize=(10,5))\n",
      "prices_outlier_centered.plot(ax=axs[0])\n",
      "prices_outlier_fixed.plot(ax=axs[1])\n",
      "\n",
      "\n",
      "   >sample      visualize missing values in continueous data\n",
      "\n",
      "# Visualize the dataset\n",
      "prices.plot(legend=False)\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "# Count the missing values of each time series\n",
      "missing_values = prices.isna().sum()\n",
      "print(missing_values)\n",
      "\n",
      "   sample  > interpolate\n",
      "\n",
      "# Create a function we'll use to interpolate and plot\n",
      "def interpolate_and_plot(prices, interpolation_type):\n",
      "\n",
      "    # Create a boolean mask for missing values\n",
      "    missing_values = prices.isna()\n",
      "\n",
      "    # Interpolate the missing values\n",
      "    prices_interp = prices.interpolate(interpolation_type)\n",
      "\n",
      "    # Plot the results, highlighting the interpolated values in black\n",
      "    fig, ax = plt.subplots(figsize=(10, 5))\n",
      "    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
      "    \n",
      "    # Now plot the interpolated values on top in red\n",
      "    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "# Interpolate using the latest non-missing value\n",
      "interpolation_type = 'zero' #'linear' or 'quadratic'\n",
      "interpolate_and_plot(prices, interpolation_type)\n",
      "\n",
      "\n",
      "  > sample  > rolling percent change\n",
      "\n",
      "# Your custom function\n",
      "def percent_change(series):\n",
      "    # Collect all *but* the last value of this window, then the final value\n",
      "    previous_values = series[:-1]\n",
      "    last_value = series[-1]\n",
      "\n",
      "    # Calculate the % difference between the last value and the mean of earlier values\n",
      "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
      "    return percent_change\n",
      "\n",
      "# Apply your custom function and plot\n",
      "prices_perc = prices.rolling(20).aggregate(percent_change)\n",
      "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
      "plt.show()\n",
      "\n",
      "    sample  > apply  replace_outliers\n",
      "\n",
      "def replace_outliers(series):\n",
      "    # Calculate the absolute difference of each timepoint from the series mean\n",
      "    absolute_differences_from_mean = np.abs(series - np.mean(series))\n",
      "\n",
      "    # Calculate a mask for the differences that are > 3 standard deviations from the mean\n",
      "    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n",
      "    \n",
      "    # Replace these values with the median accross the data\n",
      "    series[this_mask] = np.nanmedian(series)\n",
      "    return series\n",
      "\n",
      "# Apply your preprocessing function to the timeseries and plot the results\n",
      "prices_perc = prices_perc.apply(replace_outliers)\n",
      "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "          creating features over time\n",
      "\n",
      "extract features as they change over time\n",
      "\n",
      "feats=prices.rolling(20).aggregate([np.std,np.max]).dropna()\n",
      "print(feats.head())\n",
      "\n",
      "\n",
      "\n",
      "rolling is a rolling window\n",
      "\n",
      "AIG: std, amax\n",
      "ABt: std, amax\n",
      "\n",
      "always plot properties of your features\n",
      "it will help you spot noise data and outliers\n",
      "\n",
      "    >partial function\n",
      "\n",
      "lets you define a new function with parts of the old one\n",
      "\n",
      "\n",
      "from functools import partial\n",
      "\n",
      "mean_over_first_axis = partial(np.mean, axis=0)\n",
      "\n",
      "print(mean_over_first_axis(a))\n",
      "\n",
      "#the mean function always operates on the first axis\n",
      "\n",
      "percentiles give fine grained summaries of your data\n",
      "\n",
      "print(np.percentile(np.linespace(0,200),q=20))\n",
      "\n",
      "percentiles first input is an array\n",
      "q, the second input is an integer between 0 and 100\n",
      "\n",
      "returns the values of the first input as a percentile of the second input\n",
      "\n",
      "\n",
      "data = np.linspace(0,100)\n",
      "\n",
      "percentile_funcs= [partial(np.percentile, q=ii) for ii in [20,40,60]]\n",
      "\n",
      "percentiles = [i_func(data) for i_func in percentile_funcs]\n",
      "print(percentiles)\n",
      "\n",
      "output: [20,40,60]\n",
      "\n",
      "data.rolling(20).aggregrate(percentiles)\n",
      "\n",
      "         Calculating date-based features      \n",
      "\n",
      "statistical features: are numerical features like mean and standard deviation.\n",
      "human features like days of the week, holidays\n",
      "these features can span multiple years.\n",
      "\n",
      "\n",
      "#ensure index is datetime\n",
      "prices.index=pd.to_datetime(prices.index)\n",
      "\n",
      "#extract datetime features\n",
      "\n",
      "day_of_week_num = prices.index.weekday\n",
      "print(day_of_week_num[:10])\n",
      "output:[0 1 2 3 4 0 1 2 3 4]\n",
      "\n",
      "day_of_week = prices.index.weekday_name\n",
      "print(day_of_week[:10])\n",
      "output:['Monday','Tuesday'...'Friday']\n",
      "\n",
      "\n",
      "   >sample  > rolling window   > visualize min, max,mean, std for ebay\n",
      "\n",
      "# Define a rolling window with Pandas, excluding the right-most datapoint of the window\n",
      "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
      "\n",
      "# Define the features you'll calculate for each window\n",
      "features_to_calculate = [np.min, np.max, np.mean, np.std]\n",
      "\n",
      "# Calculate these features for your rolling window object\n",
      "features = prices_perc_rolling.aggregate(features_to_calculate)\n",
      "\n",
      "# Plot the results\n",
      "ax = features.loc[:\"2011-01\"].plot()\n",
      "prices_perc.loc[:\"2011-01\"].plot(ax=ax, color='k', alpha=.2, lw=3)\n",
      "ax.legend(loc=(1.01, .6))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   percentiles and partial functions\n",
      "\n",
      "# Import partial from functools\n",
      "from functools import partial\n",
      "percentiles = [1, 10, 25, 50, 75, 90, 99]\n",
      "\n",
      "# Use a list comprehension to create a partial function for each quantile\n",
      "percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n",
      "\n",
      "# Calculate each of these quantiles on the data using a rolling window\n",
      "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
      "features_percentiles = prices_perc_rolling.aggregate(percentile_functions)\n",
      "\n",
      "# Plot a subset of the result\n",
      "ax = features_percentiles.loc[:\"2011-01\"].plot(cmap=plt.cm.viridis)\n",
      "ax.legend(percentiles, loc=(1.01, .5))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Extract date features from the data, add them as columns\n",
      "prices_perc['day_of_week'] = prices_perc.index.dayofweek\n",
      "prices_perc['week_of_year'] = prices_perc.index.weekofyear\n",
      "prices_perc['month_of_year'] = prices_perc.index.month\n",
      "\n",
      "# Print prices_perc\n",
      "print(prices_perc)\n",
      "\n",
      "\n",
      "       >Feature extraction\n",
      "\n",
      "time series has a linear flow with relationships between the data\n",
      "\n",
      "information in the past can help predict what happens in the future\n",
      "\n",
      "often the features best-suited to predict a timeseries are previous values of the same timeseries\n",
      "\n",
      "the smoothness of the data help determine how correlated a timepoint is with its neighboring timepoints\n",
      "\n",
      "the amount of auto-correlation in data will impact your models\n",
      "\n",
      "data= pd.Series()\n",
      "\n",
      "shifts=[0,1,2,3,4,5,6,7]\n",
      "\n",
      "many_shifts={'lag_{}'.format(ii): data.shift(ii) for ii in shifts}\n",
      "\n",
      "many_shifts=pd.DataFrame(many_shifts)\n",
      "\n",
      "model=Ridge()\n",
      "model.fit(many_shifts,data)\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots()\n",
      "ax.bar(many_shifts.columns, model_coef_)\n",
      "ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
      "\n",
      "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "\n",
      "    Sample  > time shifted features\n",
      "In machine learning for time series, it's common to use information about previous time points to predict a subsequent time point.\n",
      "\n",
      "# These are the \"time lags\"\n",
      "shifts = np.arange(1, 11).astype(int)\n",
      "\n",
      "print(prices_perc)\n",
      "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
      "shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
      "\n",
      "# Convert into a DataFrame for subsequent use\n",
      "prices_perc_shifted = pd.DataFrame(shifted_data)\n",
      "\n",
      "# Plot the first 100 samples of each\n",
      "ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis)\n",
      "prices_perc.iloc[:100].plot(color='r', lw=2)\n",
      "ax.legend(loc='best')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Replace missing values with the median for each column\n",
      "X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
      "y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
      "\n",
      "# Fit the model\n",
      "model = Ridge()\n",
      "model.fit(X, y)\n",
      "\n",
      "def visualize_coefficients(coefs, names, ax):\n",
      "    # Make a bar plot for the coefficients, including their names on the x-axis\n",
      "    ax.bar(names, coefs)\n",
      "    ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
      "    \n",
      "    # Set formatting so it looks nice\n",
      "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "    return ax\n",
      "\n",
      "# Visualize the output data up to \"2011-01\"\n",
      "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
      "y.loc[:'2011-01'].plot(ax=axs[0])\n",
      "\n",
      "# Run the function to visualize model's coefficients\n",
      "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "Increase the data window from 20 to 40\n",
      "\n",
      "As you can see here, by transforming your data with a larger window, you've also changed the relationship between each timepoint and the ones that come just before it. This model's coefficients gradually go down to zero, which means that the signal itself is smoother over time.\n",
      "\n",
      "\n",
      "     >Cross validating time series data\n",
      "\n",
      "KFold is the most common cross validation\n",
      "\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "cv=KFold(n_splits=5)\n",
      "for tr,tt in cv.split(X,y):\n",
      "\n",
      "always visualize your models behavior during cross validation\n",
      "\n",
      "fig, axs = plt.subplots(2,1)\n",
      "\n",
      "axs[0].scatter(tt,[0]*len(tt),marker='_',s=2,lw=40)\n",
      "axs[0].set(ylim=[-.1,.1],title='Test set indices (color=CV loop)', xlabel='Index of raw data')\n",
      "\n",
      "axs[1].plot(model.predict(X[tt]))\n",
      "axs[1].set(title='Test set predictions on each CV loop', xlabel('Prediction index')\n",
      "\n",
      "\n",
      "from sklearn.model_selection import ShuffleSplit\n",
      "\n",
      "cv=ShuffleSplit(n_splits=3)\n",
      "for tr,tt in cv.split(X,y):\n",
      "\n",
      "\n",
      "    >time series cv iterator  (use only the past to validate)\n",
      "\n",
      "1. generally you should not use datapoints in the future to predict data in the past\n",
      "\n",
      "2. Always use training data from the past to predict the future\n",
      "\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "cv=TimeSeriesSplit(n_splits=10)\n",
      "\n",
      "fig,ax=plt.subplots(figsize=(10,5))\n",
      "\n",
      "for ii,(tr,tt) in enumerate(cv.split(X,y)):\n",
      "\tl1=ax.scatter(tr,[ii]*len(tr), c=[plt.cm.coolwarm(.1)],marker='_',lw=6)\n",
      "\n",
      "\tl2=ax.scatter(tt,[ii]*len(tt), c=[plt.cm.coolwarm(.9)],marker='_',lw=6)\n",
      "\n",
      "\tax.set(ylim[10,-1],title='TimeSeriesSplit behavior', xlabel='data index', ylabel='CV iteration')\n",
      "\tax.legend([l1,l2],['Training','Validation'])\n",
      "\n",
      "\n",
      "only the past is use to validate the data\n",
      "\n",
      "def myfunction(estimator, X,y):\n",
      "\ty_pred=estimator.predict(X)\n",
      "\tmy_custom_score=my_custom_function(y_pred,y)\n",
      "\treturn my_custom_score\n",
      "\n",
      "def my_pearsonr(est,X,y):\n",
      "\ty_pred=est.predict(X).squeeze()\n",
      "\tmy_corrcoef_matrix=np.corrcoef(y_pred,y.squeeze())\n",
      "\tmy_corrcoef = my_corrcoef[1,0]\n",
      "\treturn my_corrcoef\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Sample     Shufflesplit    > visualization by time\n",
      "\n",
      "# Import ShuffleSplit and create the cross-validation object\n",
      "from sklearn.model_selection import ShuffleSplit\n",
      "cv = ShuffleSplit(n_splits=3, random_state=1)\n",
      "\n",
      "# Iterate through CV splits\n",
      "results = []\n",
      "\n",
      "for tr, tt in cv.split(X, y):\n",
      "    # Fit the model on training data\n",
      "    model.fit(X[tr], y[tr])\n",
      "    \n",
      "    # Generate predictions on the test data, score the predictions, and collect\n",
      "    prediction = model.predict(X[tt])\n",
      "    score = r2_score(y[tt], prediction)\n",
      "    results.append((prediction, score, tt))\n",
      "\n",
      "# Custom function to quickly visualize predictions\n",
      "visualize_predictions(results)\n",
      "\n",
      "https://goodboychan.github.io/chans_jupyter/python/datacamp/time_series_analysis/machine_learning/2020/06/18/02-Validating-and-Inspecting-Time-Series-Models.html\n",
      "\n",
      "def visualize_predictions(results):\n",
      "    fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
      "\n",
      "    # Loop through our model results to visualize them\n",
      "    for ii, (prediction, score, indices) in enumerate(results):\n",
      "        # Plot the predictions of the model in the order they were generated\n",
      "        offset = len(prediction) * ii\n",
      "        axs[0].scatter(np.arange(len(prediction)) + offset, prediction, \n",
      "                       label='Iteration {}'.format(ii))\n",
      "\n",
      "        # Plot the predictions of the model according to how time was ordered\n",
      "        axs[1].scatter(indices, prediction)\n",
      "    axs[0].legend(loc=\"best\")\n",
      "    axs[0].set(xlabel=\"Test prediction number\", title=\"Predictions ordered by test prediction number\")\n",
      "    axs[1].set(xlabel=\"Time\", title=\"Predictions ordered by time\")\n",
      "\n",
      "\n",
      "            Sample    using KFold\n",
      "\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "# Create KFold cross-validation object\n",
      "from sklearn.model_selection import KFold\n",
      "cv = KFold(n_splits=10, shuffle=False, random_state=1)\n",
      "\n",
      "# Iterate through CV splits\n",
      "results = []\n",
      "for tr, tt in cv.split(X, y):\n",
      "    # Fit the model on training data\n",
      "    model.fit(X[tr],y[tr])\n",
      "    \n",
      "    # Generate predictions on the test data and collect\n",
      "    prediction = model.predict(X[tt])\n",
      "    results.append((prediction, tt))\n",
      "    \n",
      "# Custom function to quickly visualize predictions\n",
      "visualize_predictions(results)\n",
      "\n",
      "#This time, the predictions generated within each CV loop look 'smoother' than they were before - they look more like a real time series because you didn't shuffle the data\n",
      "\n",
      "\n",
      "     Sample   > Timeseries Split\n",
      "\n",
      "# Import TimeSeriesSplit\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "\n",
      "# Create time-series cross-validation object\n",
      "cv = TimeSeriesSplit(n_splits=10)\n",
      "\n",
      "# Iterate through CV splits\n",
      "fig, ax = plt.subplots()\n",
      "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
      "    # Plot the training data on each iteration, to see the behavior of the CV\n",
      "    ax.plot(tr, ii + y[tr])\n",
      "\n",
      "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
      "plt.show()\n",
      "\n",
      "#Note that the size of the training set grew each time when you used the time series cross-validation object\n",
      "\n",
      "\n",
      "      >Stationary and stability\n",
      "\n",
      "a stationary time series is one that does not change their statistical properties over time.\n",
      "\n",
      "most time series are non-stationary to some extent\n",
      "\n",
      "it has the same mean, standard deviation, and trends\n",
      "\n",
      "cross validation to quantify parameter stability\n",
      "\n",
      "calculate model parameter on each iteration\n",
      "\n",
      "assess parameter stability across all cv split\n",
      "\n",
      "bootstrapping is a way to estimate the confidence using the mean of a group of numbers\n",
      "\n",
      "1. take a random sample of data with replacement\n",
      "2. calculate the mean of the sample\n",
      "3. repeat the process many times\n",
      "4. caclulate the percentiles of the result\n",
      "\n",
      "the result is a 95% confidence interval of the mean of each coefficent\n",
      "\n",
      "from sklearn.utils import resample\n",
      "\n",
      "n_boots=100\n",
      "\n",
      "bootstrap_means=np.zeros(n_boots, n_coefficients)\n",
      "for ii in range(n_boots):\n",
      "\trandom_sample=resample(cv_coefficients)\n",
      "\tbootstrap_means[ii]=random_sample.mean(axis=0)\n",
      "\n",
      "percentiles=np.percentiles(bootstrap_means,(2.5,97.5),axis=0)\n",
      "\n",
      "fig,ax=plt.subplots()\n",
      "\n",
      "ax.scatter(many_shifts.columns,percentiles[0], marker='_',s=200)\n",
      "ax.scatter(many_shifts.columns,percentiles[1], marker='_',s=200)\n",
      "\n",
      "this gives an idea of the variability of the mean across all cross validation iterations\n",
      "\n",
      "      Assessing model performance stability\n",
      "\n",
      "if your using the TimeSeriesSplit, you can plot the models score over time.\n",
      "\n",
      "This is helpful to find certain regions of time that hurt the score\n",
      "\n",
      "it is also import to find non-stationary signals\n",
      "\n",
      "\n",
      "def my_corrcoef(est,X,y):\n",
      "\t\"\"\"return the correlation coefficient between model predictions and a validation set\"\"\"\n",
      "\treturn np.corrcoef(y,est,predict(X))[1,0]\n",
      "\n",
      "first_indices=[data.index[tt[0]] for tr,tt in cv.split(X,y)]\n",
      "\n",
      "cv_scores=cross_val_score(model, X,y,cv=cv, scoring=my_corrcoef)\n",
      "cv_scores=pd.Series(cv_scores, index=first_indices)\n",
      "\n",
      "\n",
      "cv.split rturns a ndarray train set indices and test ndarray set indices\n",
      "\n",
      "find the beginning of each validation block using a list comprehension\n",
      "\n",
      "collect the score and convert them into a pandas series\n",
      "\n",
      "visualize the results as a timeseries\n",
      "\n",
      "fig, axs=plt.subplots(2,1, figsize=(10,5), sharex=True)\n",
      "\n",
      "cv_scores_mean=cv_scores.rolling(10, min_periods=1).mean()\n",
      "\n",
      "cv_scores.plot(ax=axs[0])\n",
      "axs[0].set(title='Validation scores (correlation)', ylim=[0,1])\n",
      "\n",
      "data.plot(ax=axs[1])\n",
      "axs[1].set(title='Validation data')\n",
      "\n",
      "   >restrict to the latest time points to be used in training\n",
      "\n",
      "window=100\n",
      "\n",
      "cv=TimeSeries(n_splits=10, max_train_size=window)\n",
      "\n",
      "\n",
      "      >Sample  > boot strap the data    build the function\n",
      "\n",
      "from sklearn.utils import resample\n",
      "\n",
      "def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
      "    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
      "    # Create our empty array to fill the results\n",
      "    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
      "    for ii in range(n_boots):\n",
      "        # Generate random indices for our data *with* replacement, then take the sample mean\n",
      "        random_sample = resample(data)\n",
      "        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
      "        \n",
      "    # Compute the percentiles of choice for the bootstrapped means\n",
      "    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
      "    return percentiles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pyspark pipelines.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pyspark pipelines.txt\n",
      "components of a data platform\n",
      "\n",
      "ingest using singer\n",
      "\n",
      "deploy spark transformation pipelines\n",
      "\n",
      "test your code automatically\n",
      "\n",
      "apply common data cleaning operations\n",
      "\n",
      "gain insights by combining data with pyspark\n",
      "\n",
      "operational systems:\n",
      "1. messaging services\n",
      "2. payment systems\n",
      "3. google analytics\n",
      "4. crm\n",
      "5. clickstream\n",
      "6. location services\n",
      "\n",
      "data lake\n",
      "1. organized by zones\n",
      "(operational data is the landing zone)\n",
      "2. services to clean (clean zone)\n",
      "3. business zone\n",
      "\n",
      "extract transform and load\n",
      "\n",
      "      Singer\n",
      "\n",
      "1. connecting to multiple data sources\n",
      "2. open source\n",
      "\n",
      "communicates using json\n",
      "\n",
      "tap and targets\n",
      "\n",
      "1. schema\n",
      "2. state\n",
      "3. record\n",
      "\n",
      "import singer\n",
      "singer.write_schema(schema=json_schema,\n",
      "\tstream_name='DC_employees',\n",
      "\tkey_properties=[\"id\"]\n",
      "\n",
      "import json\n",
      "\n",
      "json.dumps(json_schema['properties']['age'])\n",
      "\n",
      "with open('foo.json', mode='w') as fh:\n",
      "\tjson.dump(obj=json_schema, fp=fh)\n",
      "\n",
      "#writes the json-serialized object to the open file handle\n",
      "\n",
      "  > sample  > json.dump\n",
      "\n",
      "# Import json\n",
      "import json\n",
      "\n",
      "database_address = {\n",
      "  \"host\": \"10.0.0.5\",\n",
      "  \"port\": 8456\n",
      "}\n",
      "\n",
      "# Open the configuration file in writable mode\n",
      "with open(\"database_config.json\", mode='w') as fh:\n",
      "  # Serialize the object in this file handle\n",
      "  json.dump(obj=database_address, fp=fh) \n",
      "\n",
      "\n",
      "    > sample singer write_schema\n",
      "\n",
      "# Complete the JSON schema\n",
      "schema = {'properties': {\n",
      "    'brand': {'type': 'string'},\n",
      "    'model': {'type': 'string'},\n",
      "    'price': {'type': 'number'},\n",
      "    'currency': {'type': 'string'},\n",
      "    'quantity': {'type': 'integer', 'minimum': 1},  \n",
      "    'date': {'type': 'string', 'format': 'date'},\n",
      "    'countrycode': {'type': 'string', 'pattern': \"^[A-Z]{2}$\"}, \n",
      "    'store_name': {'type': 'string'}}}\n",
      "\n",
      "\n",
      "# Write the schema\n",
      "singer.write_schema(stream_name='products', schema=schema, key_properties=[])\n",
      "\n",
      "\n",
      "     >ingestion pipeline with Singer\n",
      "\n",
      "columns=(\"id\",\"name\",\"age\",\"has_children\")\n",
      "\n",
      "users={\n",
      "(1,\"adrian\",32,False),\n",
      "(2,\"ruanne\",28,True)\n",
      "}\n",
      "\n",
      "\n",
      "singer.write_record(stream_name=\"DC_employees\",\n",
      "record=dict(zip(columns,users.pop())))\n",
      "\n",
      "fixed_dict = {\"type\":\"RECORD\",\"stream\":\"DC_employees\"}\n",
      "\n",
      "record_msg={**fixed_dict,\"record\":dict(zip(columns, users.pop()))}\n",
      "\n",
      "print(json.dumps(record_msg))\n",
      "\n",
      "** unpacks the dictionary\n",
      "\n",
      "\n",
      "import singer\n",
      "\n",
      "singer.write_schema(stream_name=\"foo\", schema=...)\n",
      "\n",
      "singer.write_records(stream_name=\"foo\", records=...)\n",
      "\n",
      "     introduction to pipes\n",
      "\n",
      "python my_tap.py | target-csv\n",
      "\n",
      "python my_tap.py | target-csv --config userconfig.cfg\n",
      "\n",
      "      >keeping track with state messages\n",
      "\n",
      "last_update_on\n",
      "\n",
      "extract after last_update_on\n",
      "\n",
      "update after tap\n",
      "\n",
      "singer.write_state(value={\"max-last-updated-on\": some_variable})\n",
      "\n",
      "\n",
      "You’re running a Singer tap daily at midnight, to synchronize changes between databases. Your tap, called tap-mydelta, extracts only the records that were updated in this database since your last retrieval. To do so, your tap keeps state: it keeps track of the last record it reported on, which can be derived from the table’s last_updated_on field.\n",
      "\n",
      "   sample   get from local host rest end point\n",
      "\n",
      "endpoint = \"http://localhost:5000\"\n",
      "\n",
      "# Fill in the correct API key\n",
      "api_key = \"scientist007\"\n",
      "\n",
      "# Create the web API’s URL\n",
      "authenticated_endpoint = \"{}/{}\".format(\"http://localhost:5000\", api_key)\n",
      "\n",
      "print(authenticated_endpoint)\n",
      "\n",
      "# Get the web API’s reply to the endpoint\n",
      "api_response = requests.get(authenticated_endpoint).json()\n",
      "pprint.pprint(api_response)\n",
      "\n",
      "output:\n",
      "\n",
      "<script.py> output:\n",
      "    http://localhost:5000/scientist007\n",
      "    {'apis': [{'description': 'list the shops available',\n",
      "               'url': '<api_key>/diaper/api/v1.0/shops'},\n",
      "              {'description': 'list the items available in shop',\n",
      "               'url': '<api_key>/diaper/api/v1.0/items/<shop_name>'}]}\n",
      "    {'apis': [{'url': '<api_key>/diaper/api/v1.0/shops', 'description': 'list the shops available'}, {'url': '<api_key>/diaper/api/v1.0/items/<shop_name>', 'description': 'list the items available in shop'}]}\n",
      "\n",
      "/shops\n",
      "items/<shop_name>\n",
      "\n",
      "\n",
      "   sample get a list of shops\n",
      "\n",
      "# Create the API’s endpoint for the shops\n",
      "shops_endpoint = \"{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"shops\")\n",
      "\n",
      "shops = requests.get(shops_endpoint).json()\n",
      "print(shops)\n",
      "\n",
      "{'shops': ['Aldi', 'Kruidvat', 'Carrefour', 'Tesco', 'DM']}\n",
      "\n",
      "items_endpoint = \"{}/{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"items\",\"Aldi\")\n",
      "items = requests.get(items_endpoint).json()\n",
      "\n",
      "\n",
      "{'items': [{'countrycode': 'BE', 'brand': 'Diapers-R-Us', 'model': '6months', 'price': 6.8, 'currency': 'EUR', 'quantity': 40, 'date': '2019-02-03'}]}\n",
      "\n",
      "\n",
      "\n",
      "dm\n",
      "\n",
      "{'items': [{'brand': 'Huggies',\n",
      "            'countrycode': 'DE',\n",
      "            'currency': 'EUR',\n",
      "            'date': '2019-02-01',\n",
      "            'model': 'newborn',\n",
      "            'price': 6.8,\n",
      "            'quantity': 40},\n",
      "           {'brand': 'Huggies',\n",
      "            'countrycode': 'AT',\n",
      "            'currency': 'EUR',\n",
      "            'date': '2019-02-01',\n",
      "            'model': 'newborn',\n",
      "            'price': 7.2,\n",
      "            'quantity': 40}]}\n",
      "\n",
      "\n",
      "    sample   > extract the schema\n",
      "\n",
      "\n",
      "# Use the convenience function to query the API\n",
      "tesco_items = retrieve_products(\"Tesco\")\n",
      "\n",
      "singer.write_schema(stream_name=\"products\", schema=schema,\n",
      "                    key_properties=[])\n",
      "\n",
      "\n",
      "# Write a single record to the stream, that adheres to the schema\n",
      "singer.write_record(stream_name=\"products\", \n",
      "            record={**tesco_items[0], 'store_name': \"Tesco\"})\n",
      "\n",
      "\n",
      "      >sample  write the store_name to the database\n",
      "\n",
      "\n",
      "# Use the convenience function to query the API\n",
      "tesco_items = retrieve_products(\"Tesco\")\n",
      "\n",
      "singer.write_schema(stream_name=\"products\", schema=schema,\n",
      "                    key_properties=[])\n",
      "\n",
      "# Write a single record to the stream, that adheres to the schema\n",
      "singer.write_record(stream_name=\"products\", \n",
      "                    record={**tesco_items[0], \"store_name\": \"Tesco\"})\n",
      "\n",
      "for shop in requests.get(SHOPS_URL).json()[\"shops\"]:\n",
      "    # Write all of the records that you retrieve from the API\n",
      "    singer.write_records(\n",
      "      stream_name=\"products\", # Use the same stream name that you used in the schema\n",
      "      records=({**tesco_items[0], \"store_name\": shop}\n",
      "               for item in retrieve_products(shop))\n",
      "    )  \n",
      "\n",
      "    tap to drain\n",
      "\n",
      "tap-marketing-api | target-csv --config data_lake.conf\n",
      "\n",
      "\n",
      "     > spark \n",
      "\n",
      "1. spark sql\n",
      "2. spark streaming\n",
      "3. MLlib\n",
      "4. GraphX\n",
      "\n",
      "api is pyspark\n",
      "\n",
      "data processing at scale\n",
      "interactive analytics\n",
      "\n",
      "validate hypothesis\n",
      "machine learning and score models\n",
      "\n",
      "spark is not used for little data\n",
      "\n",
      "prices.csv\n",
      "\n",
      "ratings.csv\n",
      "\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.getOrCreate()\n",
      "\n",
      "prices-spark.read.\n",
      "options(header=\"true\").\n",
      "csv('mnt/data_lake/landing/prices.csv\")\n",
      "\n",
      "prices.show()\n",
      "\n",
      "\n",
      "from pprint import pprint\n",
      "\n",
      "pprint(prices.dtypes)\n",
      "\n",
      "\n",
      "schema=StructType(\n",
      "StructField(\"store\",StringType(),nullable=False),\n",
      "\n",
      "StructField(\"price\",FloatType(), nullable=False)\n",
      "\n",
      "StructField(\"date\",DateType(),nullable=False\n",
      ")\n",
      "\n",
      "\n",
      "prices-spark.read.\n",
      "options(header=\"true\").\n",
      "schema(schema).\n",
      "csv('mnt/data_lake/landing/prices.csv\")\n",
      "\n",
      "\n",
      "\n",
      "# Read a csv file and set the headers\n",
      "df = (spark.read\n",
      "      .options(header=True)\n",
      "      .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
      "\n",
      "df.show()\n",
      "\n",
      "brand\n",
      "model\n",
      "absorption_rate\n",
      "comfort\n",
      "\n",
      "ham or spam csv\n",
      "\n",
      "https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/sms_spam.csv\n",
      "\n",
      "\n",
      "        read.csv\n",
      "\n",
      "# Read a csv file and set the headers\n",
      "df = (spark.read\n",
      "      .options(header=True)\n",
      "      .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
      "\n",
      "df.show()\n",
      "\n",
      "  >schema\n",
      "\n",
      "# Define the schema\n",
      "schema = StructType([\n",
      "  StructField(\"brand\", StringType(), nullable=False),\n",
      "  StructField(\"model\", StringType(), nullable=False),\n",
      "  StructField(\"absorption_rate\", ByteType(), nullable=True),\n",
      "  StructField(\"comfort\", ByteType(), nullable=True)\n",
      "])\n",
      "\n",
      "better_df = (spark\n",
      "             .read\n",
      "             .options(header=\"true\")\n",
      "             # Pass the predefined schema to the Reader\n",
      "             .schema(schema)\n",
      "             .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
      "pprint(better_df.dtypes)\n",
      "\n",
      "\n",
      "     >cleaning data\n",
      "\n",
      "handling invalid rows\n",
      "\n",
      "prices=(spark.read\n",
      "\t.options(header='true',mode'DROPMALFORMED')\n",
      "\t.csv('landing/prices.csv'))\n",
      "\n",
      "the significance of null\n",
      "1. keep the row\n",
      "2. fill the blanks with null\n",
      "\n",
      "prices.fillna(25, subset=['quantity']).show()\n",
      "\n",
      "\n",
      "   >badly chosen placeholders\n",
      "\n",
      "employees= spark.read.options(header='true').\n",
      "schema(schema).csv('employees.csv')\n",
      "\n",
      "   replace with condition in the when function\n",
      "\n",
      "from pyspark.sql.functions import col, when\n",
      "from datetime import date, timedelta\n",
      "\n",
      "one_year_from_now = date.today().replace(year=date.today().year+1)\n",
      "better_frame = employees.withColumn('end_date',\n",
      "\twhen(col('end_date')> one_year_from_now,None).otherwise(col('end_date')))\n",
      "\n",
      "better_frame.show()\n",
      "\n",
      "none is translated to null\n",
      "\n",
      "bytetype range is -128 to 127\n",
      "\n",
      "\n",
      " > fillna to replace missing values\n",
      "\n",
      "print(\"BEFORE\")\n",
      "ratings.show()\n",
      "\n",
      "print(\"AFTER\")\n",
      "# Replace nulls with arbitrary value on column subset\n",
      "ratings = ratings.fillna(4, subset=[\"comfort\"])\n",
      "ratings.show()\n",
      "\n",
      "   > drop invalid rows\n",
      "PERMISSIVE is the default mode\n",
      "\n",
      "# Specify the option to drop invalid rows\n",
      "ratings = (spark\n",
      "           .read\n",
      "           .options(header=True, mode='DROPMALFORMED')\n",
      "           .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings_with_invalid_rows.csv\"))\n",
      "ratings.show()\n",
      "\n",
      "  >  conditionally replacing data\n",
      "\n",
      "from pyspark.sql.functions import col, when\n",
      "\n",
      "# Add/relabel the column\n",
      "categorized_ratings = ratings.withColumn(\n",
      "    \"comfort\",\n",
      "    # Express the condition in terms of column operations\n",
      "    when(col(\"comfort\") > 3, \"sufficient\").otherwise(\"insufficient\"))\n",
      "\n",
      "categorized_ratings.show()\n",
      "\n",
      "\n",
      "           >Transforming data with spark\n",
      "deriving insights\n",
      "\n",
      "standardizing names and normalizing numerical data\n",
      "\n",
      "filtering rows\n",
      "selecting and renaming columns\n",
      "grouping and aggregation\n",
      "ordering results\n",
      "\n",
      "prices=spark.read.options(header='true').schema(schema).csv('landing/prices.csv')\n",
      "\n",
      "filter is passed boolean values\n",
      "\n",
      "prices_in_beligium = prices.filter(col('countrycode')=='BE).orderBy(col('date'))\n",
      "\n",
      "col creates column objects\n",
      "\n",
      "prices.select(\n",
      "\n",
      "\tcol('store'),\n",
      "\tcol('brand').alias('brandname')\n",
      ").distinct()\n",
      "\n",
      "  > grouping and aggregating\n",
      "\n",
      "(prices\n",
      "\t.groupBy(col('brand'))\n",
      "\t.mean('price')\n",
      ").show()\n",
      "\n",
      "(prices\n",
      "\t.groupBy(col('brand'))\n",
      "\t.agg(\n",
      "\t\tavg('price').alias('average_price')\n",
      "\t\tcount('brand').alias('number_of_times')\n",
      "\t)\n",
      ")\n",
      "\n",
      "\n",
      "  > joining data\n",
      "\n",
      "ratings_with_prices=ratings.join(prices,['brand','model'])\n",
      "\n",
      "     sample    select distinct columns\n",
      "\n",
      "from pyspark.sql.functions import col\n",
      "\n",
      "# Select the columns and rename the \"absorption_rate\" column\n",
      "result = ratings.select([col(\"brand\"),\n",
      "                       col(\"model\"),\n",
      "                       col(\"absorption_rate\").alias('absorbency')])\n",
      "\n",
      "# Show only unique values\n",
      "result.distinct().show()\n",
      "\n",
      "\n",
      "    sample  > agg\n",
      "\n",
      "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
      "\n",
      "aggregated = (purchased\n",
      "              # Group rows by 'Country'\n",
      "              .groupBy(col('Country'))\n",
      "              .agg(\n",
      "                # Calculate the average salary per group and rename\n",
      "                avg('Salary').alias('average_salary'),\n",
      "                # Calculate the standard deviation per group\n",
      "                stddev_samp('Salary'),\n",
      "                # Retain the highest salary per group and rename\n",
      "                sfmax('Salary').alias('highest_salary')\n",
      "              )\n",
      "             )\n",
      "\n",
      "aggregated.show()\n",
      "\n",
      "\n",
      "+-------+--------------+-------------------+--------------+\n",
      "    |Country|average_salary|stddev_samp(Salary)|highest_salary|\n",
      "    +-------+--------------+-------------------+--------------+\n",
      "    |Germany|       63000.0|                NaN|         63000|\n",
      "    | France|       48000.0|                NaN|         48000|\n",
      "    |  Spain|       62000.0| 12727.922061357855|         71000|\n",
      "    +-------+--------------+-------------------+--------------+\n",
      "    \n",
      "\n",
      "Note that the standard deviation column has returned NaN in a few cases. That’s because there weren’t enough data points for these countries (only one record, so you can’t compute a meaningful sample standard deviation), as we’re only loading a small file in this exercise\n",
      "\n",
      "\n",
      "    >Packaging your application\n",
      "\n",
      "\n",
      "python my_pyspark_data_pipeline.py\n",
      "\n",
      "spark-submit\n",
      "\n",
      "1. sets up launch environment for use with the cluster manager and the selected deploy mode\n",
      "\n",
      "spark-submit\n",
      "\n",
      "\t--master \"local[*]\" \\\n",
      "\t--py-files PY_FILES \\\n",
      "\tMAIN_PYTHON_FILE \\\n",
      "\tapp_arguments\n",
      "\n",
      "zip files\n",
      "\tzip \\\n",
      "\t\t--recurse-path\\\n",
      "\t\tdependencies.zip\n",
      "\t\tpydiaper\n",
      "\n",
      "spark-submit \\\n",
      "\t--py-files dependencies.zip \\\n",
      "\tpydiaper/cleaning/clean_prices.py\n",
      "\n",
      "\n",
      "        >importance of tests\n",
      "\n",
      "1. new functionality desired\n",
      "2. bugs need to get squashed\n",
      "\n",
      "written expectations of the code\n",
      "\n",
      "raises confidence that the code is correct now\n",
      "\n",
      "tests are the most up-to-date form of documentation\n",
      "\n",
      "testing takes time\n",
      "testing have a high return on investment\n",
      "\n",
      "unit tests\n",
      "service tests\n",
      "ui test (end to end tests)\n",
      "\n",
      "   writing unit tests\n",
      "\n",
      "1. Extract\n",
      "2. Transform\n",
      "3. Load\n",
      "\n",
      "transformation is where we add the business logic\n",
      "\n",
      "\n",
      "prices_with_ratings=spark.read.csv()\n",
      "exchange_rates=spark.read.csv()\n",
      "\n",
      "unit_prices_with_ratings = (prices_with_ratings.join() #transform\n",
      ".withColumn())\n",
      "\n",
      "transformations operate on dataframes\n",
      "\n",
      "     >dataframes in memory\n",
      "\n",
      "from pyspark.sql import Row\n",
      "\n",
      "purchase=Row(\"price\",\n",
      "\t\"quantity\",\n",
      "\t\"product\")\n",
      "\n",
      "record=purchase(12.99,1,\"cake\")\n",
      "\n",
      "df=spark.createDataFrame((record,))\n",
      "\n",
      "unit_prices_with_ratings=(prices_with_ratings\n",
      "\t.join(exchange_rates,['currency','date'])\n",
      "\t.withColumn('unit_price_in_euro',\n",
      "\tcol('price')/col('quantity')\n",
      "\t*col('exchange_rate_to_euro'))\n",
      "\n",
      "\n",
      "     create reusable well name functions\n",
      "\n",
      "def link_with_exchange_rates(prices,rates):\n",
      "\treturn prices.join(rates,['currency','date'])\n",
      "\n",
      "def calculate_unit_price_in_euro(df):\n",
      "\treturn\n",
      "\tdf.withColumn('unit_price_in_euro',\n",
      "\tcol('price')/col('quantity')\n",
      "\t*col('exchange_rate_to_euro'))\n",
      "\n",
      "\n",
      "unit_price_with_ratings=(\n",
      "\tcalculate_unit_price_in_euro(\n",
      "\tlink_with_exchange_rates(prices,exchange_rates)\n",
      "\t)\n",
      ")\n",
      "\n",
      "***each transformation can be tested and reduced\n",
      "\n",
      "def test_calculate_unit_price_in_euro():\n",
      "\trecord=dict(price=10,\n",
      "\t\tquantity=5,\n",
      "\t\texchange_rate_to_euro=2.)\n",
      "\n",
      "\tdf=spark.createDataFrame([Row(**record)])\n",
      "\n",
      "\tresult=calculate_unit_price_in_euro(df)\n",
      "\n",
      "\texpected_record=Row(**record, unit_price_in_euro=4.)\n",
      "\texpected=spark.createDateFrame([expected_record])\n",
      "\tassertDataFrameEqual(result,expected)\n",
      "\n",
      "testing framework: pytest\n",
      "\n",
      "create in-memory dataframes makes testing easier because the data is in plain sight\n",
      "focus is on a small number of examples\n",
      "\n",
      "    sample  > in memory dataframe\n",
      "\n",
      "\n",
      "from datetime import date\n",
      "from pyspark.sql import Row\n",
      "\n",
      "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
      "\n",
      "# Create a tuple of records\n",
      "data = (\n",
      "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
      "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
      "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
      ")\n",
      "\n",
      "# Create a DataFrame from these records\n",
      "frame = spark.createDataFrame(data)\n",
      "frame.show()\n",
      "\n",
      "\n",
      "\n",
      "script.py\n",
      "\n",
      "\n",
      "\n",
      "Light mode\n",
      "\n",
      "\n",
      "\n",
      "from datetime import date\n",
      "from pyspark.sql import Row\n",
      " \n",
      "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
      " \n",
      "# Create a tuple of records\n",
      "data = (\n",
      "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
      "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
      "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
      ")\n",
      " \n",
      "# Create a DataFrame from these records\n",
      "frame = spark.createDataFrame(data)\n",
      "frame.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Run Code\n",
      "Submit Answer\n",
      "\n",
      "\n",
      "IPython Shell\n",
      "\n",
      "\n",
      "Slides\n",
      "\n",
      "\n",
      "\n",
      "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
      "\n",
      "aggregated = (purchased\n",
      "              # Group rows by 'Country'\n",
      "              .groupBy(col('Country'))\n",
      "              .agg(\n",
      "                # Calculate the average salary per group and rename\n",
      "                avg('Salary').alias('average_salary'),\n",
      "                # Calculate the standard deviation per group\n",
      "                stddev_samp('Salary'),\n",
      "                # Retain the highest salary per group and rename\n",
      "                sfmax('Salary').alias('highest_salary')\n",
      "              )\n",
      "             )\n",
      "\n",
      "aggregated.show()\n",
      "from datetime import date\n",
      "from pyspark.sql import Row\n",
      "\n",
      "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
      "\n",
      "# Create a tuple of records\n",
      "data = (\n",
      "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
      "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
      "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
      ")\n",
      "\n",
      "# Create a DataFrame from these records\n",
      "frame = spark.createDataFrame(data)\n",
      "frame.show()\n",
      "+-------+------------+------------------+----------+----------+\n",
      "|country|utm_campaign|airtime_in_minutes|start_date|  end_date|\n",
      "+-------+------------+------------------+----------+----------+\n",
      "|    USA|DiapersFirst|                28|2017-01-20|2017-01-27|\n",
      "|Germany|  WindelKind|                31|2017-01-25|      null|\n",
      "|  India|CloseToCloth|                32|2017-01-25|2017-02-02|\n",
      "+-------+------------+------------------+----------+----------+\n",
      "\n",
      "    sample\t\n",
      "\n",
      "pipenv run pytest\n",
      "\n",
      "\n",
      "from .chinese_provinces_improved import \\\n",
      "    aggregate_inhabitants_by_province\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.types import StructType, \\\n",
      "    StructField, StringType, LongType, BooleanType\n",
      "\n",
      "\n",
      "def test_aggregate_inhabitants_by_province():\n",
      "    \"\"\"The number of inhabitants per province should be aggregated,\n",
      "    regardless of their distinctive features.\n",
      "    \"\"\"\n",
      "\n",
      "    spark = SparkSession.builder.getOrCreate()\n",
      "\n",
      "    fields = [\n",
      "        StructField(\"country\", StringType(), True),\n",
      "        StructField(\"province\", StringType(), True),\n",
      "        StructField(\"inhabitants\", LongType(), True),\n",
      "        StructField(\"foo\", BooleanType(), True),  # distinctive features\n",
      "    ]\n",
      "\n",
      "    frame = spark.createDataFrame({\n",
      "        (\"China\", \"A\", 3, False),\n",
      "        (\"China\", \"A\", 2, True),\n",
      "        (\"China\", \"B\", 14, False),\n",
      "        (\"US\", \"A\", 4, False)},\n",
      "        schema=StructType(fields)\n",
      "    )\n",
      "    actual = aggregate_inhabitants_by_province(frame).cache()\n",
      "\n",
      "    # In the older implementation, the data was first filtered for a specific\n",
      "    # country, after which you'd aggregate by province. The same province\n",
      "    # name could occur in multiple countries though.\n",
      "    # This test is expecting the data to be grouped by country,\n",
      "    # then province from aggregate_inhabitants_by_province()\n",
      "    expected = spark.createDataFrame(\n",
      "        {(\"China\", \"A\", 5), (\"China\", \"B\", 14), (\"US\", \"A\", 4)},\n",
      "        schema=StructType(fields[:3])\n",
      "    ).cache()\n",
      "\n",
      "    assert actual.schema == expected.schema, \"schemas don't match up\"\n",
      "    assert sorted(actual.collect()) == sorted(expected.collect()),\\\n",
      "        \"data isn't equal\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " > improvements\n",
      "\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import col, lower, sum\n",
      "\n",
      "from .catalog import catalog\n",
      "\n",
      "\n",
      "def extract_demographics(sparksession, catalog):\n",
      "    return sparksession.read.parquet(catalog[\"clean/demographics\"])\n",
      "\n",
      "\n",
      "def store_chinese_demographics(frame, catalog):\n",
      "    frame.write.parquet(catalog[\"business/chinese_demographics\"])\n",
      "\n",
      "\n",
      "# Improved aggregation function, grouped by country and province\n",
      "def aggregate_inhabitants_by_province(frame):\n",
      "    return (frame\n",
      "            .groupBy(\"province\")\n",
      "            .agg(sum(col(\"inhabitants\")).alias(\"inhabitants\"))\n",
      "            )\n",
      "\n",
      "\n",
      "def main():\n",
      "    spark = SparkSession.builder.getOrCreate()\n",
      "    frame = extract_demographics(spark, catalog)\n",
      "    chinese_demographics = frame.filter(lower(col(\"country\")) == \"china\")\n",
      "    aggregated_demographics = aggregate_inhabitants_by_province(\n",
      "        chinese_demographics\n",
      "    )\n",
      "    store_chinese_demographics(aggregated_demographics, catalog)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "\n",
      "      continuous testing     \n",
      "\n",
      "\n",
      "unittest\n",
      "pytest\n",
      "doctest\n",
      "nose\n",
      "\n",
      "assert or raise\n",
      "\n",
      "assert compute == expected\n",
      "\n",
      "report which test passed and which ones failed\n",
      "\n",
      "automation is one of the objectives of a data engineer\n",
      "\n",
      "ci/cd pipeline\n",
      "\n",
      "continuous integration\n",
      "1. get code changes integrated with the master branch regularly\n",
      "\n",
      "continuous delivery\n",
      "1. Create artifacts (deliverables like documentation, but also programs) that can be deployed into production without breaking things\n",
      "\n",
      "cicleci - run tests automatically for you\n",
      "\n",
      "circleci looks for .circleci/config.yml\n",
      "1. has a section called jobs\n",
      "\n",
      "jobs:\n",
      "\ttest:\n",
      "\t\tdocker:\n",
      "\t\t\t-image:circleci/python:3.6.4\n",
      "\t\tsteps:\n",
      "\t\t\t-checkout\n",
      "\t\t\t-run: pip install -r requirements.txt\n",
      "\t\t\t-run: pytest\n",
      "\n",
      "\n",
      "cicleci\n",
      "1. checkout code\n",
      "2. install test & build requirements\n",
      "3. run tests\n",
      "\n",
      "\n",
      "order\n",
      "1. check out your application from version control\n",
      "2. install your python application dependencies\n",
      "3. run the test suite of your application\n",
      "4. create artifacts\n",
      "5. save the artifacts to location accessible by your company's compute infrastructure\n",
      "\n",
      "\n",
      "\n",
      "Add flake8 to the development section in the Pipfile, which is in the project’s root folder. This file serves a similar purpose as the requirements.txt files you might have seen in other Python projects. It solves some problems with those though. To add flake8 correctly, look at the line that mentions pytest.\n",
      "\n",
      "\n",
      "      Modern day workflow management\n",
      "\n",
      "\n",
      "sequence of tasks scheduled to be run\n",
      "a task can be trigger by a sequence of event\n",
      "\n",
      "schedule or triggered\n",
      "\n",
      "scheduled with cron\n",
      "\n",
      "reads crontab files\n",
      "\n",
      "#Minutes hours Days Months Day of the week Command\n",
      "\n",
      "*/15 9-17 * * 1-3,5 log_my_activity\n",
      "1. one task per line\n",
      "2. launch my process, log my activity at a specific time\n",
      "3. every fifteen minutes between normal office hours, ever day of the month, for every month, \n",
      "Mon, tues, wednesday, and fridays\n",
      "\n",
      "your can add comments\n",
      "\n",
      "other tools\n",
      "1. luigi\n",
      "2. azkaban\n",
      "3. airflow\n",
      "\n",
      "apache airflow fulfills modern engineering needs\n",
      "1. create and visualize complex workflows\n",
      "2. monitor and log workflows\n",
      "3. scales horizontally (work with other machines)\n",
      "\n",
      "      >The directed Acyclic Graph (DAG)\n",
      "\n",
      "1. nodes are connected by edges\n",
      "2. the edge denote a sense of direction on the nodes\n",
      "3. Acyclic means there is no way to circle back to the same node\n",
      "4. The nodes are operators\n",
      "\n",
      "from airflow import DAG\n",
      "\n",
      "my_dag = DAG(\n",
      "\tdag_id=\"publish_logs\",\n",
      "\tschedule_interval=\"* * * * *\",\n",
      "\tstate_date=datetime(2010,1,1)\n",
      "\n",
      ")\n",
      "\n",
      "BashOperator (bash script)\n",
      "Pythonoperator (python script)\n",
      "SparkSubmitOperator\n",
      "\n",
      "    > defining dependencies between task is established using set_downstream and set_upstream operators\n",
      "\n",
      "task1.set_downstream(task2)\n",
      "task3.set_upstream(task2)\n",
      "\n",
      "\n",
      "\n",
      "    Dag schedule job\n",
      "\n",
      "schedule interval: * default\n",
      "\n",
      "minute\n",
      "hour\n",
      "day of the month\n",
      "day of the week\n",
      "\n",
      "\n",
      "from datetime import datetime\n",
      "from airflow import DAG\n",
      "\n",
      "reporting_dag = DAG(\n",
      "    dag_id=\"publish_EMEA_sales_report\", \n",
      "    # Insert the cron expression\n",
      "    schedule_interval=\"0 7 * * 1\",\n",
      "    start_date=datetime(2019, 11, 24),\n",
      "    default_args={\"owner\": \"sales\"}\n",
      ")\n",
      "\n",
      "# Specify direction using verbose method\n",
      "prepare_crust.set_downstream(apply_tomato_sauce)\n",
      "\n",
      "tasks_with_tomato_sauce_parent = [add_cheese, add_ham, add_olives, add_mushroom]\n",
      "\n",
      "for task in tasks_with_tomato_sauce_parent:\n",
      "    # Specify direction using verbose method on relevant task\n",
      "    apply_tomato_sauce.set_downstream(task)\n",
      "\n",
      "# Specify direction using bitshift operator\n",
      "tasks_with_tomato_sauce_parent   bake_pizza\n",
      "\n",
      "# Specify direction using verbose method\n",
      "bake_pizza.set_upstream(prepare_oven)\n",
      "\n",
      "a.set_downstream(b) means b must be executed after a.\n",
      "a   b also means b must be executed after a.\n",
      "b.set_upstream(a) means a must be executed before b.\n",
      "b << a also means a must be executed before b.\n",
      "\n",
      "Set prepare_crust to precede apply_tomato_sauce using the appropriate method.\n",
      "\n",
      "Set apply_tomato_sauceto precede each of tasks in tasks_with_tomato_sauce_parent using the appropriate method.\n",
      "Set the tasks_with_tomato_sauce_parent list to precede bake_pizza using either the bitshift operator   or <<.\n",
      "Set bake_pizza to succeed prepare_oven using the appropriate method.\n",
      "\n",
      "   >sample\n",
      "\n",
      "# Specify direction using verbose method\n",
      "prepare_crust.set_downstream(apply_tomato_sauce)\n",
      "\n",
      "tasks_with_tomato_sauce_parent = [add_cheese, add_ham, add_olives, add_mushroom]\n",
      "for task in tasks_with_tomato_sauce_parent:\n",
      "    # Specify direction using verbose method on relevant task\n",
      "    apply_tomato_sauce.set_downstream(task)\n",
      "\n",
      "# Specify direction using bitshift operator\n",
      "tasks_with_tomato_sauce_parent   bake_pizza\n",
      "\n",
      "# Specify direction using verbose method\n",
      "bake_pizza.set_upstream(prepare_oven)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\sql intermediate.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\sql intermediate.txt\n",
      "-- Calculate the average, minimum and maximum\n",
      "SELECT avg(DurationSeconds) AS Average, \n",
      "       min(DurationSeconds) AS Minimum, \n",
      "       max(DurationSeconds) AS Maximum\n",
      "FROM Incidents\n",
      "\n",
      "-- Calculate the aggregations by Shape\n",
      "SELECT Shape,\n",
      "       AVG(DurationSeconds) AS Average, \n",
      "       MIN(DurationSeconds) AS Minimum, \n",
      "       MAX(DurationSeconds) AS Maximum\n",
      "FROM Incidents\n",
      "Group by Shape\n",
      "-- Return records where minimum of DurationSeconds is greater than 1\n",
      "having min(DurationSeconds) > 1\n",
      "\n",
      "\n",
      "      finding missing data\n",
      "\n",
      "select Country, InternetUse, Year from EconomicIndicators\n",
      "where InternetUse is not null\n",
      "\n",
      "select Country, InternetUse, Year from EconomicIndicators\n",
      "where len(InternetUse)>0\n",
      "\n",
      "select isnull(TradeGDPPercent,'unknown') as new_percent\n",
      "from EconomicIndicators\n",
      "\n",
      "select coalesce(TradeGDPPercent,ImportGoodPercent,'N/A') as new_percent\n",
      "from EconomicIndicators\n",
      "\n",
      "coalesce returns the first non-missing value\n",
      "\n",
      "  >\n",
      "\n",
      "-- Return the specified columns\n",
      "select IncidentDateTime, IncidentState\n",
      "FROM Incidents\n",
      "-- Exclude all the missing values from IncidentState  \n",
      "WHERE IncidentState is not null\n",
      "\n",
      "\n",
      "-- Check the IncidentState column for missing values and replace them with the City column\n",
      "SELECT IncidentState, IsNull(IncidentState,City) AS Location\n",
      "FROM Incidents\n",
      "-- Filter to only return missing values from IncidentState\n",
      "WHERE IncidentState Is Null\n",
      "\n",
      "-- Replace missing values \n",
      "SELECT Country, coalesce(IncidentState, City, 'N/A') AS Location\n",
      "FROM Incidents\n",
      "WHERE Country IS NULL\n",
      "\n",
      "\n",
      "      >binning data with case\n",
      "\n",
      "case when then end\n",
      "\n",
      "select Continent\n",
      "case when Continent='Europe' or Continent='Asia' then 'Eurasia' else 'Other' end as NewContinent\n",
      "from EconomicIndicators\n",
      "\n",
      "case statement is a good way to range data into groups\n",
      "\n",
      "select country, LifeExp,\n",
      "case when LifeExp < 30 then 1\n",
      "when LifeExp > 29 and LifeExp < 40 then 2\n",
      "when LifeExp > 39 and LifeExp < 50 then 3\n",
      "when LifeExp > 49 and LifeExp < 60 then 4\n",
      "else 5\n",
      "end as LifeExpGroup\n",
      "from EconomicIndicators\n",
      "where Year=2007\n",
      "\n",
      "SELECT Country, \n",
      "       CASE WHEN Country = 'us'  THEN 'USA'\n",
      "       ELSE 'International'\n",
      "       END AS SourceCountry\n",
      "FROM Incidents\n",
      "\n",
      "-- Complete the syntax for cutting the duration into different cases\n",
      "SELECT DurationSeconds, \n",
      "-- Start with the 2 TSQL keywords, and after the condition a TSQL word and a value\n",
      "      case when (DurationSeconds <= 120) then 1\n",
      "-- The pattern repeats with the same keyword and after the condition the same word and next value          \n",
      "       when (DurationSeconds > 120 AND DurationSeconds <= 600) then 2\n",
      "-- Use the same syntax here             \n",
      "       when (DurationSeconds > 601 AND DurationSeconds <= 1200) then 3\n",
      "-- Use the same syntax here               \n",
      "       when (DurationSeconds > 1201 AND DurationSeconds <= 5000) then 4\n",
      "-- Specify a value      \n",
      "       ELSE 5 \n",
      "       END AS SecondGroup   \n",
      "FROM Incidents\n",
      "\n",
      "        >Counting and Totals\n",
      "\n",
      "select count(*) from incidents\n",
      "\n",
      "count(distinct column_name)\n",
      "\n",
      "select count(distinct Country) as Countries from Incidents\n",
      "\n",
      "select count(distinct Country) as Countries,\n",
      "Count(Distinct City) as Cities\n",
      "from Incidents\n",
      "\n",
      "\n",
      "group by count(), avg(), min(), max()\n",
      "\n",
      "order by asc, desc\n",
      "\n",
      "select count(*) as TotalRowsByCountry, Country\n",
      "from incidents\n",
      "group by Country\n",
      "order by Country asc\n",
      "\n",
      "\n",
      "-- Write a query that returns an aggregation \n",
      "select MixDesc, sum(Quantity) Total\n",
      "FROM Shipments\n",
      "-- Group by the relevant column\n",
      "group by MixDesc\n",
      "\n",
      "\n",
      "-- Count the number of rows by MixDesc\n",
      "SELECT MixDesc, Count(*) Total\n",
      "FROM Shipments\n",
      "GROUP BY MixDesc\n",
      "\n",
      "              Dates  >\n",
      "\n",
      "DatePart\n",
      "1. DD for day\n",
      "2. MM for month\n",
      "3. YY for year\n",
      "4. HH for hour\n",
      "\n",
      "DateAdd(): add or subtract datetime values\n",
      "1. always return a date\n",
      "\n",
      "DateDiff(): obtain the difference between two datetime values\n",
      "1. always return a number\n",
      "\n",
      "\n",
      "dateAdd(DatePart, number, date)\n",
      "\n",
      "select DateAdd(DD,30,'2020-06-21')\n",
      "select DateAdd(DD,-30,'2020-06-21')\n",
      "\n",
      "datediff(datepart,startdate, enddate)\n",
      "\n",
      "select datediff(dd,'2020-05-22','2020-06-21') as difference1\n",
      ",datediff(dd,'2020-07-21','2020-06-21') as difference2\n",
      "\n",
      "  >\n",
      "\n",
      "-- Return the difference in OrderDate and ShipDate\n",
      "SELECT OrderDate, ShipDate, \n",
      "       DATEDIFF(DD, OrderDate, ShipDate) AS Duration\n",
      "FROM Shipments\n",
      "\n",
      "-- Return the DeliveryDate as 5 days after the ShipDate\n",
      "SELECT OrderDate, \n",
      "       DATEADD(DD,5,ShipDate) AS DeliveryDate\n",
      "FROM Shipments\n",
      "\n",
      "     rounding and truncating numbers\n",
      "\n",
      "round(number, length)\n",
      "\n",
      "select DurationSeconds,\n",
      "round(DurationSeconds,0) as RoundToZero,\n",
      "round(DurationSeconds,1) as RoundToOne\n",
      "from Incidents\n",
      "\n",
      "\n",
      "select DurationSeconds,\n",
      "round(DurationSeconds,-1) as RoundToTen,\n",
      "round(DurationSeconds,-2) as RoundToHundred\n",
      "from Incidents\n",
      "\n",
      "Round(DurationSeconds,0,1) as Truncating\n",
      "\n",
      "    \n",
      "\n",
      "-- Round Cost to the nearest dollar\n",
      "SELECT Cost, \n",
      "       Round(Cost,0) AS RoundedCost\n",
      "FROM Shipments\n",
      "\n",
      "-- Truncate cost to whole number\n",
      "SELECT Cost, \n",
      "       Round(Cost,0,1) AS TruncateCost\n",
      "FROM Shipments\n",
      "\n",
      "       > more math functions\n",
      "abs(number)  returns non-negative values\n",
      "\n",
      "sqrt(number ) square root of a number\n",
      "square(number) square of a number\n",
      "log(number) returns the natural logarithm\n",
      "\n",
      "\n",
      "select DurationSeconds, Log(DurationSeconds,10) as LogSeconds from Incidents\n",
      "\n",
      "   \n",
      "\n",
      "-- Return the absolute value of DeliveryWeight\n",
      "SELECT DeliveryWeight,\n",
      "       ABS(DeliveryWeight) AS AbsoluteValue\n",
      "FROM Shipments\n",
      "\n",
      "-- Return the square and square root of WeightValue\n",
      "SELECT WeightValue, \n",
      "       Square(WeightValue) AS WeightSquare, \n",
      "       Sqrt(WeightValue) AS WeightSqrt\n",
      "FROM Shipments\n",
      "\n",
      "\n",
      "   > while loops\n",
      "\n",
      "declare @variablename data_type\n",
      "\n",
      "1. varchar(n) : variable length text field\n",
      "2. int: -2,147,483,647 to 2,147,483,647\n",
      "3. decimal (p,s) numeric(p,s)\n",
      "p total number of digits stored to the left of the decimal\n",
      "s total number of digits stored to the right of the decimal\n",
      "\n",
      "declare @Snack varchar(10)\n",
      "set @Snack='Cookie'\n",
      "select @Snack='Cookie'\n",
      "\n",
      "while condition\n",
      "begin\n",
      "\tbreak\n",
      "\tcontinue\n",
      "end\n",
      "\n",
      "declare @ctr int\n",
      "set @ctr=1\n",
      "\n",
      "while @ctr<10\n",
      "\tbegin\n",
      "\t\tset @ctr=@ctr+1\n",
      "\n",
      "\t\tif @ctr=4\n",
      "\t\t\tbreak\n",
      "\tend\n",
      "\n",
      "select @ctr\n",
      "\n",
      "break will exist the loop\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "-- Declare the variable (a SQL Command, the var name, the datatype)\n",
      "DECLARE @Counter INT\n",
      "\n",
      "-- Set the counter to 20\n",
      "SET @Counter=20\n",
      "\n",
      "-- Select the counter\n",
      "SELECT @counter\n",
      "\n",
      "\n",
      "-- Declare the variable (a SQL Command, the var name, the datatype)\n",
      "DECLARE @counter INT \n",
      "\n",
      "-- Set the counter to 20\n",
      "SET @counter = 20\n",
      "\n",
      "-- Select and increment the counter by one \n",
      "SELECT @counter=@counter+1\n",
      "\n",
      "-- Print the variable\n",
      "SELECT @counter\n",
      "\n",
      "DECLARE @counter INT \n",
      "SET @counter = 20\n",
      "\n",
      "-- Create a loop\n",
      "WHILE @counter<30\n",
      "\n",
      "-- Loop code starting point\n",
      "BEGIN\n",
      "\tSELECT @counter = @counter + 1\n",
      "-- Loop finish\n",
      "END\n",
      "\n",
      "-- Check the value of the variable\n",
      "SELECT @counter\n",
      "\n",
      "\n",
      "      derived tables\n",
      "\n",
      "derived tables are used when you want to break down a query into smaller steps.\n",
      "\n",
      "select a.* from Kidney a\n",
      "join (select avg(Age) as AverageAge from Kidney) b\n",
      "on a.Age=b.AverageAge\n",
      "\n",
      "    \n",
      "\n",
      "SELECT a.RecordId, a.Age, a.BloodGlucoseRandom, \n",
      "-- Select maximum glucose value (use colname from derived table)    \n",
      "       b.MaxGlucose\n",
      "FROM Kidney a\n",
      "-- Join to derived table\n",
      "Join (SELECT Age, MAX(BloodGlucoseRandom) AS MaxGlucose FROM Kidney GROUP BY Age) b\n",
      "-- Join on Age\n",
      "ON a.Age=b.Age\n",
      "\n",
      "\n",
      "SELECT *\n",
      "FROM Kidney a\n",
      "-- Create derived table: select age, max blood pressure from kidney grouped by age\n",
      "JOIN (select Age, Max(BloodPressure) as MaxBloodPressure from Kidney GROUP BY Age) b\n",
      "-- JOIN on BloodPressure equal to MaxBloodPressure\n",
      "ON a.BloodPressure=b.MaxBloodPressure\n",
      "-- Join on Age\n",
      "AND a.Age=b.Age\n",
      "\n",
      "       common type table\n",
      "\n",
      "with CTEName(Col1,Col2)\n",
      "as\n",
      "(select Col1, Col2 from TableName\n",
      ")\n",
      "\n",
      "\n",
      "WITH BloodPressureAge(Age, MaxBloodPressure)\n",
      "as\n",
      "(\n",
      "select Age, Max(BloodPressure) as MaxBloodPressure from Kidney GROUP BY Age)\n",
      "\n",
      "\n",
      "select a.Age, Min(a.BloodPressure), b.MaxBloodPresure\n",
      "from Kidney a\n",
      "join BloodpressureAge b\n",
      "on a.Age=b.Age\n",
      "group by a.Age, b.MaxBloodPressure\n",
      "\n",
      "    \n",
      "\n",
      "-- Specify the keyowrds to create the CTE\n",
      "WITH BloodGlucoseRandom (MaxGlucose) \n",
      "AS (SELECT MAX(BloodGlucoseRandom) AS MaxGlucose FROM Kidney)\n",
      "\n",
      "SELECT a.Age, b.MaxGlucose\n",
      "FROM Kidney a\n",
      "-- Join the CTE on blood glucose equal to max blood glucose\n",
      "JOIN BloodGlucoseRandom b\n",
      "ON a.BloodGlucoseRandom=b.MaxGlucose\n",
      "\n",
      "\n",
      "-- Create the CTE\n",
      "WITH BloodPressure \n",
      "AS (select Max(BloodPressure) MaxBloodPressure from Kidney)\n",
      "\n",
      "SELECT *\n",
      "FROM Kidney a\n",
      "-- Join the CTE  \n",
      "JOIN BloodPressure b\n",
      "ON a.BloodPressure=b.MaxBloodPressure\n",
      "\n",
      "      window t-sql\n",
      "\n",
      "current row, previous row and the next row\n",
      "\n",
      "OVER (PARTITION by SalesYear ORDER BY SalesYear)\n",
      "\n",
      "select SalesPerson, SalesYear, CurrentQuota,\n",
      "Sum(CurrentQuota) over (Partition by SalesYear) as YearlyTotal,\n",
      "ModifiedDate as ModDate\n",
      "From SaleGoal\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "SELECT OrderID, TerritoryName, \n",
      "       -- Total price for each partition\n",
      "       SUM(OrderPrice) \n",
      "       -- Create the window and partitions\n",
      "       OVER(PARTITION BY TerritoryName) AS TotalPrice\n",
      "FROM Orders\n",
      "\n",
      "SELECT OrderID, TerritoryName, \n",
      "       -- Number of rows per partition\n",
      "       COUNT(*)\n",
      "       -- Create the window and partitions\n",
      "       OVER(PARTITION BY TerritoryName) AS TotalOrders\n",
      "FROM Orders\n",
      "\n",
      "\n",
      "<<<<<<< common window functions\n",
      "\n",
      "lead() next row in the window\n",
      "lag() previous row in the window \n",
      "first_value() returns the first value in a window \n",
      "last_value() returns the last value in a window\n",
      "\n",
      "\n",
      "select SalesPerson, SalesYear, CurrentQuota\n",
      "FIRST_VALUE(CurrentQuota) over (partition by SalesYear Order by ModifiedDate) as StartQuota,\n",
      "LAST_VALUE(CurrentQuota) over (partition by SalesYear Order by ModifiedDate) as EndQuota,\n",
      "ModifiedDate as ModDate\n",
      "from SaleGoal\n",
      "\n",
      "select SalesPerson, SalesYear, CurrentQuota,\n",
      "Lead(CurrentQuota) Over (Partition by SalesYear Order by ModifiedDate) as NextQuota,\n",
      "ModifiedDate as ModDate\n",
      "from SaleGoal\n",
      "\n",
      "select SalesPerson, SalesYear, CurrentQuota,\n",
      "Lag(CurrentQuota) Over (Partition by SalesYear Order by ModifiedDate) as PreviousQuota,\n",
      "ModifiedDate as ModDate\n",
      "from SaleGoal\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "SELECT TerritoryName, OrderDate, \n",
      "       -- Select the first value in each partition\n",
      "       FIRST_VALUE(OrderDate) \n",
      "       -- Create the partitions and arrange the rows\n",
      "       OVER(PARTITION BY TerritoryName ORDER BY OrderDate) AS FirstOrder\n",
      "FROM Orders\n",
      "\n",
      "SELECT TerritoryName, OrderDate, \n",
      "       -- Specify the previous OrderDate in the window\n",
      "       LAG(OrderDate) \n",
      "       -- Over the window, partition by territory & order by order date\n",
      "       OVER(PARTITION BY TerritoryName Order BY OrderDate) AS PreviousOrder,\n",
      "       -- Specify the next OrderDate in the window\n",
      "       LEAD(OrderDate) \n",
      "       -- Create the partitions and arrange the rows\n",
      "       OVER(PARTITION BY TerritoryName Order BY OrderDate) AS NextOrder\n",
      "FROM Orders\n",
      "\n",
      "    > Increasing window complexity\n",
      "\n",
      "\n",
      "select SalesPerson, SalesYear, CurrentQuota,\n",
      "Sum(CurrentQuota)\n",
      "Over (Partition by SalesYear order by ModifiedDate) as RunningTotal,\n",
      "ModifiedDate as ModDate\n",
      "from SaleGoal\n",
      "\n",
      "select SalesPerson, SalesYear, CurrentQuota,\n",
      "ROW_NUMBER()\n",
      "OVER (PARTITION BY SalesPerson Order by SalesYear) as QuotabySalesPerson\n",
      "From SaleGoal\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "SELECT TerritoryName, OrderDate, \n",
      "       -- Create a running total\n",
      "       ROW_NUMBER()\n",
      "       -- Create the partitions and arrange the rows\n",
      "       OVER(PARTITION BY TerritoryName ORDER BY OrderDate) AS TerritoryTotal\t  \n",
      "FROM Orders\n",
      "\n",
      "SELECT TerritoryName, OrderDate, \n",
      "       -- Create a running total\n",
      "       SUM(OrderPrice)\n",
      "       -- Create the partitions and arrange the rows\n",
      "       OVER(PARTITION BY TerritoryName ORDER BY OrderDate) AS TerritoryTotal\t  \n",
      "FROM Orders\n",
      "\n",
      "\n",
      "    > Using windows for statistical functions\n",
      "\n",
      "\n",
      "STDEV() - calculates the standard deviation\n",
      "\n",
      "\n",
      "select SalesPerson, SalesYear, CurrentQuota,\n",
      "STDEV(CurrentQuota)\n",
      "OVER () StandardDev,\n",
      "ModifiedDate as ModDate\n",
      "From SaleGoal\n",
      "\n",
      "one window over the whole table\n",
      "\n",
      "select SalesPerson, SalesYear, CurrentQuota,\n",
      "STDEV(CurrentQuota)\n",
      "OVER (PARTITION BY SalesYear Order by SalesYear) StandardDev,\n",
      "ModifiedDate as ModDate\n",
      "From SaleGoal\n",
      "\n",
      "window by SalesYear\n",
      "\n",
      "Mode is the value which appeas the most often in your data\n",
      "1. create a cte containing an ordered count of values using ROW_NUMBER\n",
      "2. write a query using the CTE to pick the value with the highest row number\n",
      "\n",
      "\n",
      "with QuotaCount as (\n",
      "select SalesPerson, SalesYear, CurrentQuota,\n",
      "ROW_NUMBER()\n",
      "OVER (PARTITION BY CurrentQuota ORDER BY CurrentQuota) as QuotaList\n",
      "from SaleGoal)\n",
      "\n",
      "select * from QuotaCount\n",
      "where QuotaList in (select max(QuotaList) from QuotaCount)\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "SELECT OrderDate, TerritoryName, \n",
      "       -- Calculate the standard deviation\n",
      "\t   STDEV(OrderPrice)\n",
      "       OVER(PARTITION BY TerritoryName ORDER BY OrderDate) AS StdDevPrice\t  \n",
      "FROM Orders\n",
      "\n",
      "\n",
      "-- Create a CTE Called ModePrice which contains two columns\n",
      "WITH ModePrice (OrderPrice, UnitPriceFrequency)\n",
      "as\n",
      "(\n",
      "\tSELECT OrderPrice, \n",
      "\tROW_NUMBER() \n",
      "\tOVER(PARTITION BY OrderPrice ORDER BY OrderPrice) AS UnitPriceFrequency\n",
      "\tFROM Orders \n",
      ")\n",
      "\n",
      "-- Select everything from the CTE\n",
      "select * from ModePrice\n",
      "\n",
      "\n",
      "-- CTE from the previous exercise\n",
      "WITH ModePrice (OrderPrice, UnitPriceFrequency)\n",
      "AS\n",
      "(\n",
      "\tSELECT OrderPrice,\n",
      "\tROW_NUMBER() \n",
      "    OVER (PARTITION BY OrderPrice ORDER BY OrderPrice) AS UnitPriceFrequency\n",
      "\tFROM Orders\n",
      ")\n",
      "\n",
      "-- Select the order price from the CTE\n",
      "SELECT OrderPrice AS ModeOrderPrice\n",
      "FROM ModePrice\n",
      "-- Select the maximum UnitPriceFrequency from the CTE\n",
      "WHERE UnitPriceFrequency IN (SELECT MAX(UnitPriceFrequency) FROM ModePrice)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\sql server functions for manipulating data.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\sql server functions for manipulating data.txt\n",
      "categories of data types\n",
      "1. exact numerics whole numbers(smallint, tinyint, int, bigint) decimals(numeric, decimal, money, smallmoney)\n",
      "decimal types are defined by precision and scale  (precision is the maximum number of digits to be stored) scale is the digit count to the right of the decimal\n",
      "2. approximate numerics  (float, real)\n",
      "3. date and time \n",
      "a. time hh:mm:ss.nnnnnnnn\n",
      "b. date YYYY-MM-DD\n",
      "c. smalldatetime YYYY-MM-DD hh:mm:ss\n",
      "d. datetime YYYY-MM-DD hh:mm:ss.nnn\n",
      "e. datetime2 YYYY-MM-DD hh:mm:ss.nnnnnnn\n",
      "4. character strings\n",
      "a. char\n",
      "b. varchar\n",
      "c. text\n",
      "5. unicode character strings\n",
      "a. nchar\n",
      "b. nvarchar\n",
      "c. ntext\n",
      "6. binary data\n",
      "7. other types\n",
      "a. image\n",
      "b. cursor\n",
      "c. rowversion\n",
      "d. uniqueidentifier\n",
      "e. xml\n",
      "f. spatial geometry/geography types\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "SELECT \n",
      "\tcompany, \n",
      "\tcompany_location, \n",
      "\tbean_origin, \n",
      "\tcocoa_percent, \n",
      "\trating\n",
      "FROM ratings\n",
      "-- Location should be Belgium and the rating should exceed 3.5\n",
      "WHERE company_location = 'Belgium'\n",
      "\tAND rating > 3.5;\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\tbirthdate,\n",
      "\tgender,\n",
      "\temail,\n",
      "\tcountry,\n",
      "\ttotal_votes\n",
      "FROM voters\n",
      "-- Birthdate > 1990-01-01, total_votes > 100 but < 200\n",
      "WHERE Birthdate > '1990-01-01'\n",
      "  AND total_votes > 100\n",
      "  AND total_votes < 200;\n",
      "\n",
      "ALTER TABLE voters\n",
      "ADD last_vote_time time;\n",
      "\n",
      "     how convert data types\n",
      "\n",
      "for comparing two values, they need to be of the same type\n",
      "\n",
      "sql server will try to convert one type to another (implicit)\n",
      "\n",
      "explicit conversion using\n",
      "cast\n",
      "convert\n",
      "\n",
      "data type precedence\n",
      "1. user defined data type\n",
      "2. datetime\n",
      "3. date\n",
      "4. float\n",
      "5. decimal\n",
      "6. int\n",
      "7. bit\n",
      "8. nvarchar\n",
      "9. varchar\n",
      "10. binary\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,     \n",
      "\ttotal_votes\n",
      "FROM voters\n",
      "where total_votes > '120'\n",
      "\n",
      "When comparing decimals to integers, the integer value is automatically converted to a decimal.\n",
      "\n",
      "\n",
      "explicit and implicit conversion of data\n",
      "implicit are performed automatically\n",
      "\n",
      "explicit conversion is done by the user with cast and convert\n",
      "\n",
      "cast(expression as data_type [(length)])\n",
      "\n",
      "select \n",
      "cast(3.14 as int) as decimal_to_int,\n",
      "cast('3.14' as decimal(3,2)) as string_to_decimal,\n",
      "cast(GetDate() as nvarchar(20)) as date_to_string,\n",
      "cast(GetDate() as float) as date_to_float\n",
      "\n",
      "convert(date_type [(length), expression [,style])\n",
      "\n",
      "select \n",
      "convert(int, 3.14) as decimal_to_int,\n",
      "convert(decimal(3,2),'3.14') as string_to_decimal,\n",
      "convert(nvarchar(20),GetDate(),104) as date_to_string,  dd.mm.yyyy\n",
      "convert(float,GetDate()) as date_to_float\n",
      "\n",
      "\n",
      "cast is converted into convert\n",
      "\n",
      "cast is sql standard and portable to different databases\n",
      "\n",
      "SELECT \n",
      "\t-- Transform to int the division of total_votes to 5.5\n",
      "\tCAST(total_votes/5.5 AS int) AS DividedVotes\n",
      "FROM voters;\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\ttotal_votes\n",
      "FROM voters\n",
      "-- Transform the total_votes to char of length 10\n",
      "WHERE cast(total_votes AS nvarchar(10)) LIKE '5%';\n",
      "\n",
      "\n",
      "SELECT \n",
      "\temail,\n",
      "    -- Convert birthdate to varchar show it like: \"Mon dd,yyyy\" \n",
      "    Convert(varchar, birthdate, 107) AS birthdate\n",
      "FROM voters;\n",
      "\n",
      "SELECT \n",
      "\tcompany,\n",
      "    bean_origin,\n",
      "    -- Convert the rating column to an integer\n",
      "    convert(int,rating) AS rating\n",
      "FROM ratings;\n",
      "\n",
      "SELECT \n",
      "\tcompany,\n",
      "    bean_origin,\n",
      "    rating\n",
      "FROM ratings\n",
      "-- Convert the rating to an integer before comparison\n",
      "WHERE convert(int, rating) = 3;\n",
      "\n",
      "SELECT\n",
      "\tfirst_name,\n",
      "    last_name,\n",
      "\t-- Convert birthdate to varchar(10) to show it as yy/mm/dd\n",
      "\tCONVERT(varchar(10), birthdate, 11) AS birthdate,\n",
      "    gender,\n",
      "    country,\n",
      "    -- Convert the total_votes number to nvarchar\n",
      "    'Voted ' + CAST(total_votes AS nvarchar) + ' times.' AS comments\n",
      "FROM voters\n",
      "WHERE country = 'Belgium'\n",
      "    -- Select only the female voters\n",
      "\tAND gender = 'F'\n",
      "    -- Select only people who voted more than 20 times\n",
      "    AND total_votes > 20;\n",
      "\n",
      "\n",
      "    > functions that return system date and time\n",
      "\n",
      "inconsistent date time formats or patterns\n",
      "arithmetic operations\n",
      "issues with time zones\n",
      "\n",
      "\n",
      "Time zones in SQL Server\n",
      "1. local time zone\n",
      "2. UTC time zone (Universal Time Coordinate)\n",
      "\n",
      "sysdatetime()  -> high precision date time function without timezone information\n",
      "sysutcdatetime() -> high precision date time with timezone information\n",
      "sysdatetimeoffset() -> utc offset  2021-08-11 16:49:50.2451186 -06:00\n",
      "\n",
      "less precise functions:\n",
      "GetDate()\n",
      "GetUTCDate()  2021-08-11 22:50:45.090\n",
      "CURRENT_TIMESTAMP ->same as getdate without parameters\n",
      "\n",
      "\n",
      "select convert(time, sysdatetime()) as time\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tSYSDATETIME() AS CurrentDate;\n",
      "\n",
      "SELECT \n",
      "\tSYSUTCDATETIME() UTC_HighPrecision,\n",
      "\tGETUTCDATE() AS UTC_LowPrecision;\n",
      "\n",
      "SELECT \n",
      "\tSYSDATETIMEOFFSET() AS Timezone;  -> LOCAL time and the timezone\n",
      "\n",
      "\n",
      "    UTC TIME\n",
      "\n",
      "SELECT \n",
      "\tCAST(SYSUTCDATETIME() AS time) AS HighPrecision,\n",
      "\tCAST(GetUTCDate() AS time) AS LowPrecision;\n",
      "\n",
      "\n",
      "\n",
      "    > functions returning date and time parts\n",
      "\n",
      "Year(date)\n",
      "Month(date) ->integer\\\n",
      "Day(date)\n",
      "\n",
      "\n",
      "DateName(datepart,date)\n",
      "datepart: year, month, dayofyear,week,weekday\n",
      "\n",
      "select datename(year,getdate())\n",
      "select datename(month,getdate())\n",
      "select datename(dayofyear,getdate())\n",
      "select datename(week,getdate())\n",
      "select datename(weekday,getdate())\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "\n",
      "\n",
      "DatePart(datepart,date)\n",
      "datepart: year, month, dayofyear,week,weekday\n",
      "\n",
      "the values are all integer\n",
      "\n",
      "\n",
      "DateFromParts (year, month, day)\n",
      "\n",
      "builds a date\n",
      "\n",
      "    >\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\t-- Extract the year of the first vote\n",
      "\tDateName(year,first_vote_date)  AS first_vote_year,\n",
      "    -- Extract the month of the first vote\n",
      "\tDateName(month,first_vote_date) AS first_vote_month,\n",
      "    -- Extract the day of the first vote\n",
      "\tDateName(day,first_vote_date) AS first_vote_day\n",
      "FROM voters;\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "   \t-- Extract the year of the first vote\n",
      "\tYEAR(first_vote_date)  AS first_vote_year,\n",
      "    -- Extract the month of the first vote\n",
      "\tMONTH(first_vote_date) AS first_vote_month,\n",
      "    -- Extract the day of the first vote\n",
      "\tDAY(first_vote_date)   AS first_vote_day\n",
      "FROM voters\n",
      "-- The year of the first vote should be greater than 2015\n",
      "WHERE YEAR(first_vote_date) > 2015\n",
      "-- The day should not be the first day of the month\n",
      "  AND DAY(first_vote_date) <> 1;\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\tfirst_vote_date,\n",
      "    -- Select the name of the month of the first vote\n",
      "\tDateName(month, first_vote_date) AS first_vote_month\n",
      "FROM voters;\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\tfirst_vote_date,\n",
      "    -- Select the number of the day within the year\n",
      "\tDatePart(dayofyear,first_vote_date) AS first_vote_dayofyear\n",
      "FROM voters;\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\tfirst_vote_date,\n",
      "    -- Select day of the week from the first vote date\n",
      "\tDateName(weekday,first_vote_date) AS first_vote_dayofweek\n",
      "FROM voters;\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "   \t-- Extract the month number of the first vote\n",
      "\tDatePart(MONTH,first_vote_date) AS first_vote_month1,\n",
      "\t-- Extract the month name of the first vote\n",
      "    DatePart(MONTH,first_vote_date) AS first_vote_month2,\n",
      "\t-- Extract the weekday number of the first vote\n",
      "\tDatePart(WEEKDAY,first_vote_date) AS first_vote_weekday1,\n",
      "    -- Extract the weekday name of the first vote\n",
      "\tDatePart(WEEKDAY,first_vote_date) AS first_vote_weekday2\n",
      "FROM voters;\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "    -- Select the year of the first vote\n",
      "   \tYear(first_vote_date) AS first_vote_year, \n",
      "    -- Select the month of the first vote\n",
      "\tMonth(first_vote_date) AS first_vote_month,\n",
      "    -- Create a date as the start of the month of the first vote\n",
      "\tDateFromParts(Year(first_vote_date), Month(first_vote_date), 1) AS first_vote_starting_month\n",
      "FROM voters;\n",
      "\n",
      "\n",
      "    operations on dates\n",
      "\n",
      "DateAdd(datepart, number, date)\n",
      "1. year, month,day\n",
      "DateDiff(datepart,start_date1,end_date2)\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tfirst_vote_date,\n",
      "    -- Add 5 days to the first voting date\n",
      "\tDATEADD(DAY,5,first_vote_date) AS processing_vote_date\n",
      "  FROM voters;\n",
      "\n",
      "SELECT\n",
      "\t-- Subtract 476 days from the current date\n",
      "\tDATEADD(DAY,-476,GETDATE()) AS date_476days_ago;\n",
      "\n",
      "\n",
      "SELECT\n",
      "\tfirst_name,\n",
      "\tbirthdate,\n",
      "\tfirst_vote_date,\n",
      "    -- Select the diff between the 18th birthday and first vote\n",
      "\tDATEDIFF(Year, DATEADD(YEAR, 18, birthdate), first_vote_date) AS adult_years_until_vote\n",
      "FROM voters;\n",
      "\n",
      "SELECT \n",
      "\t-- Get the difference in weeks from 2019-01-01 until now\n",
      "\tDATEDIFF(WEEK, '2019-01-01', GETDATE()) AS weeks_passed;\n",
      "\n",
      "     validating if a expression is a date\n",
      "\n",
      "IsDate\n",
      "1. return type 1 is a date\n",
      "2. datetime2 ->0\n",
      "3. other type->0\n",
      "\n",
      "set dateformat\n",
      "1. sets the order of the date parts for intrepreting strings as dates\n",
      "\n",
      "set dateformat dmy\n",
      "\n",
      "declare @date1 nvarchar(20) ='12-30-2019'\n",
      "declare @date2 nvarchar(20) ='30-12-2019'\n",
      "\n",
      "select isdate(@date1) as invalid_dmy\n",
      "select isdate(@date2) as valid_dmy\n",
      "\n",
      "       >validate date\n",
      "set language\n",
      "1. English, Italian, Spanish\n",
      "\n",
      "set dateformat\n",
      "\n",
      "\n",
      "set language French;\n",
      "\n",
      "select\n",
      "isdate('12-30-2019') as mdy,\n",
      "isdate('30-12-2019') as dmy\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "DECLARE @date1 NVARCHAR(20) = '30.03.2019';\n",
      "\n",
      "-- Set the correct language\n",
      "SET LANGUAGE Dutch;\n",
      "SELECT\n",
      "\t@date1 AS initial_date,\n",
      "    -- Check that the date is valid\n",
      "\tISDATE(@date1) AS is_valid,\n",
      "    -- Select the name of the month\n",
      "\tDateName(Month,@date1) AS month_name;\n",
      "\n",
      "DECLARE @date1 NVARCHAR(20) = '32/12/13';\n",
      "\n",
      "-- Set the correct language\n",
      "SET LANGUAGE Croatian;\n",
      "SELECT\n",
      "\t@date1 AS initial_date,\n",
      "    -- Check that the date is valid\n",
      "\tISDATE(@date1) AS is_valid,\n",
      "    -- Select the name of the month\n",
      "\tDateName(month,@date1) AS month_name,\n",
      "\t-- Extract the year from the date\n",
      "\tYear(@date1) AS year_name;\n",
      "\n",
      "DECLARE @date1 NVARCHAR(20) = '12/18/55';\n",
      "\n",
      "-- Set the correct language\n",
      "SET LANGUAGE English;\n",
      "SELECT\n",
      "\t@date1 AS initial_date,\n",
      "    -- Check that the date is valid\n",
      "\tISDATE(@date1) AS is_valid,\n",
      "    -- Select the week day name\n",
      "\tDateName(week,@date1) AS week_day,\n",
      "\t-- Extract the year from the date\n",
      "\tYear(@date1) AS year_name;\n",
      "\n",
      "DECLARE @date1 NVARCHAR(20) = '12/18/55';\n",
      "\n",
      "-- Set the correct language\n",
      "SET LANGUAGE English;\n",
      "SELECT\n",
      "\t@date1 AS initial_date,\n",
      "    -- Check that the date is valid\n",
      "\tISDATE(@date1) AS is_valid,\n",
      "    -- Select the week day name\n",
      "\tDateName(week,@date1) AS week_day,\n",
      "\t-- Extract the year from the date\n",
      "\tYear(@date1) AS year_name;c\n",
      "\n",
      "SELECT\n",
      "\tfirst_name,\n",
      "    last_name,\n",
      "    birthdate,\n",
      "\tfirst_vote_date,\n",
      "\t-- Find out on which day of the week each participant voted \n",
      "\tDATENAME(weekday, first_vote_date) AS first_vote_weekday,\n",
      "\t-- Find out the year of the first vote\n",
      "\tYEAR(first_vote_date) AS first_vote_year,\n",
      "\t-- Find out the age of each participant when they joined the contest\n",
      "\tDateDiff(Year, birthdate, first_vote_date) AS age_at_first_vote\t\n",
      "FROM voters;\n",
      "\n",
      "SELECT\n",
      "\tfirst_name,\n",
      "    last_name,\n",
      "    birthdate,\n",
      "\tfirst_vote_date,\n",
      "\t-- Find out on which day of the week each participant voted \n",
      "\tDATENAME(weekday, first_vote_date) AS first_vote_weekday,\n",
      "\t-- Find out the year of the first vote\n",
      "\tYEAR(first_vote_date) AS first_vote_year,\n",
      "\t-- Discover the participants' age when they joined the contest\n",
      "\tDATEDIFF(YEAR, birthdate, first_vote_date) AS age_at_first_vote,\t\n",
      "\t-- Calculate the current age of each voter\n",
      "\tDATEDIFF(YEAR, birthdate, getdate()) AS current_age\n",
      "FROM voters;\n",
      "\n",
      "\n",
      "          functions for positions\n",
      "\n",
      "\n",
      "len()\n",
      "1.returns the number of characters in the string\n",
      "charindex()\n",
      "1. looks for a character expression in a given string\n",
      "2. returns its starting position\n",
      "3. charindex(expression_to_find, expression_to_search, [,start_location])\n",
      "\n",
      "patindex()\n",
      "1. returns the starting position of a pattern in an expression\n",
      "2. you can use wildcard characters\n",
      "%\n",
      "_\n",
      "[match any character in the brackets]\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "SELECT TOP 10 \n",
      "\tcompany, \n",
      "\tbroad_bean_origin,\n",
      "\t-- Calculate the length of the broad_bean_origin column\n",
      "\tlen(broad_bean_origin) AS length\n",
      "FROM ratings\n",
      "--Order the results based on the new column, descending\n",
      "ORDER BY len(broad_bean_origin);\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\temail \n",
      "FROM voters\n",
      "-- Look for the \"dan\" expression in the first_name\n",
      "WHERE CHARINDEX('dan',first_name) > 0;\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\temail \n",
      "FROM voters\n",
      "-- Look for the \"dan\" expression in the first_name\n",
      "WHERE CHARINDEX('dan', first_name) > 0 \n",
      "    -- Look for last_names that contain the letter \"z\"\n",
      "\tAND PATINDEX('%z',last_name) > 0;\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\temail \n",
      "FROM voters\n",
      "-- Look for the \"dan\" expression in the first_name\n",
      "WHERE CHARINDEX('dan', first_name) > 0 \n",
      "    -- Look for last_names that do not contain the letter \"z\"\n",
      "\tAND PATINDEX('%z',last_name) = 0;\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\temail \n",
      "FROM voters\n",
      "-- Look for first names that contain \"rr\" in the middle\n",
      "WHERE PATINDEX('%rr%',first_name)>0;\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\temail \n",
      "FROM voters\n",
      "-- Look for first names that contain \"rr\" in the middle\n",
      "WHERE PATINDEX('%rr%',first_name)>0;\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\temail \n",
      "FROM voters\n",
      "-- Look for first names that have an \"a\" followed by 0 or more letters and then have a \"w\"\n",
      "WHERE PATINDEX('%a%w%',first_name)>0;\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\temail \n",
      "FROM voters\n",
      "-- Look for first names that contain one of the letters: \"x\", \"w\", \"q\"\n",
      "WHERE PATINDEX('%[x,w,q]%',first_name)>0;\n",
      "\n",
      "\n",
      "    > functions for manipulating strings\n",
      "\n",
      "lower\n",
      "upper\n",
      "left \n",
      "right\n",
      "ltrim\n",
      "rtrim\n",
      "trim\n",
      "replace\n",
      "substring(expression, start, number of characters)\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tcompany,\n",
      "\tbean_type,\n",
      "\tbroad_bean_origin,\n",
      "    -- 'company' and 'broad_bean_origin' should be in uppercase\n",
      "\t'The company ' +  upper(company) + ' uses beans of type \"' + bean_type + '\", originating from ' + upper(broad_bean_origin) + '.'\n",
      "FROM ratings\n",
      "WHERE \n",
      "    -- The 'broad_bean_origin' should not be unknown\n",
      "\tLOWER(broad_bean_origin) NOT LIKE '%unknown%'\n",
      "     -- The 'bean_type' should not be unknown\n",
      "    AND LOWER(bean_type) NOT LIKE '%unknown%';\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\tcountry,\n",
      "    -- Select only the first 3 characters from the first name\n",
      "\tLEFT(first_name, 3) AS part1,\n",
      "    -- Select only the last 3 characters from the last name\n",
      "    RIGHT(last_name,3) AS part2\n",
      "FROM voters;\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\tcountry,\n",
      "    -- Select only the first 3 characters from the first name\n",
      "\tLEFT(first_name, 3) AS part1,\n",
      "    -- Select only the last 3 characters from the last name\n",
      "    RIGHT(last_name, 3) AS part2,\n",
      "    -- Select only the last 2 digits from the birth date\n",
      "    RIGHT(CAST(birthdate AS VARCHAR),2) AS part3\n",
      "FROM voters;\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\tcountry,\n",
      "    -- Select only the first 3 characters from the first name\n",
      "\tLEFT(first_name, 3) AS part1,\n",
      "    -- Select only the last 3 characters from the last name\n",
      "    RIGHT(last_name, 3) AS part2,\n",
      "    -- Select only the last 2 digits from the birth date\n",
      "    RIGHT(birthdate, 2) AS part3,\n",
      "    -- Create the alias for each voter\n",
      "    LEFT(first_name,3) + RIGHT(last_name,3) + '_' + RIGHT(birthdate, 2)\n",
      "FROM voters;\n",
      "\n",
      "SELECT \n",
      "\temail,\n",
      "    -- Extract 5 characters from email, starting at position 3\n",
      "\tSUBSTRING(email,3,5) AS some_letters\n",
      "FROM voters;\n",
      "DECLARE @sentence NVARCHAR(200) = 'Apples are neither oranges nor potatoes.'\n",
      "SELECT\n",
      "\t-- Extract the word \"Apples\" \n",
      "\tSUBSTRING(@sentence, 1, 6) AS fruit1,\n",
      "    -- Extract the word \"oranges\"\n",
      "\tSUBSTRING(@sentence, 20, 7) AS fruit2;\n",
      "\n",
      "SELECT \n",
      "\tfirst_name,\n",
      "\tlast_name,\n",
      "\temail,\n",
      "\t-- Replace \"yahoo.com\" with \"live.com\"\n",
      "\treplace(email,'yahoo.com','live.com') AS new_email\n",
      "FROM voters;\n",
      "\n",
      "SELECT \n",
      "\tcompany AS initial_name,\n",
      "    -- Replace '&' with 'and'\n",
      "\treplace(company,'&','and') AS new_name \n",
      "FROM ratings\n",
      "WHERE CHARINDEX('&', company) > 0;\n",
      "\n",
      "SELECT \n",
      "\tcompany AS old_company,\n",
      "    -- Remove the text '(Valrhona)' from the name\n",
      "\treplace(company,'(Valrhona)','') AS new_company,\n",
      "\tbean_type,\n",
      "\tbroad_bean_origin\n",
      "FROM ratings\n",
      "WHERE company = 'La Maison du Chocolat (Valrhona)';\n",
      "\n",
      "  > manipulating groups of strings\n",
      "\n",
      "concat\n",
      "\n",
      "concat_ws(separator, string1, string2,[stringN])\n",
      "\n",
      "string_agg(expression, separator)\n",
      "\n",
      "select string_agg(first_name, ', ') as list_of_names from voters\n",
      "\n",
      "select string_agg(concat(first_name, ' ', last_name, ' (', first_vote_date, ')'), char(13)) as list_of_names\n",
      "from voters\n",
      "\n",
      "select\n",
      "\tyear(first_vote_date) as voting_year,\n",
      "\tstring_agg(first_name, ', ') as voters\n",
      "from voters\n",
      "group by year(first_vote_date)\n",
      "\n",
      "\n",
      "select\n",
      "\tyear(first_vote_date) as voting_year,\n",
      "\tstring_agg(first_name, ', ') within group (order by first_name asc) as voters\n",
      "from voters\n",
      "group by Year(first_vote_date);\n",
      "\n",
      "\n",
      "select * from string_split('1,2,3,4',',')\n",
      "\n",
      "DECLARE @string1 NVARCHAR(100) = 'Chocolate with beans from';\n",
      "DECLARE @string2 NVARCHAR(100) = 'has a cocoa percentage of';\n",
      "\n",
      "SELECT \n",
      "\tbean_type,\n",
      "\tbean_origin,\n",
      "\tcocoa_percent,\n",
      "\t-- Create a message by concatenating values with \"+\"\n",
      "\t@string1 + ' ' + bean_origin + ' ' + @string2 + ' ' + CAST(cocoa_percent AS nvarchar) AS message1\n",
      "FROM ratings\n",
      "WHERE \n",
      "\tcompany = 'Ambrosia' \n",
      "\tAND bean_type <> 'Unknown';\n",
      "\n",
      "\n",
      "DECLARE @string1 NVARCHAR(100) = 'Chocolate with beans from';\n",
      "DECLARE @string2 NVARCHAR(100) = 'has a cocoa percentage of';\n",
      "\n",
      "SELECT \n",
      "\tbean_type,\n",
      "\tbean_origin,\n",
      "\tcocoa_percent,\n",
      "\t-- Create a message by concatenating values with \"+\"\n",
      "\t@string1 + ' ' + bean_origin + ' ' + @string2 + ' ' + CAST(cocoa_percent AS nvarchar) AS message1,\n",
      "\t-- Create a message by concatenating values with \"CONCAT()\"\n",
      "\tCONCAT(@string1, ' ', bean_origin, ' ', @string2, ' ', CAST(cocoa_percent AS nvarchar)) AS message2\n",
      "FROM ratings\n",
      "WHERE \n",
      "\tcompany = 'Ambrosia' \n",
      "\tAND bean_type <> 'Unknown';\n",
      "\n",
      "DECLARE @string1 NVARCHAR(100) = 'Chocolate with beans from';\n",
      "DECLARE @string2 NVARCHAR(100) = 'has a cocoa percentage of';\n",
      "\n",
      "SELECT \n",
      "\tbean_type,\n",
      "\tbean_origin,\n",
      "\tcocoa_percent,\n",
      "\t-- Create a message by concatenating values with \"+\"\n",
      "\t@string1 + ' ' + bean_origin + ' ' + @string2 + ' ' + CAST(cocoa_percent AS nvarchar) AS message1,\n",
      "\t-- Create a message by concatenating values with \"CONCAT()\"\n",
      "\tCONCAT(@string1, ' ', bean_origin, ' ', @string2, ' ', cocoa_percent) AS message2,\n",
      "\t-- Create a message by concatenating values with \"CONCAT_WS()\"\n",
      "\tCONCAT_WS(' ', @string1, bean_origin, @string2, cocoa_percent) AS message3\n",
      "FROM ratings\n",
      "WHERE \n",
      "\tcompany = 'Ambrosia' \n",
      "\tAND bean_type <> 'Unknown';\n",
      "\n",
      "\n",
      "SELECT\n",
      "\t-- Create a list with all bean origins, delimited by comma\n",
      "\tSTRING_AGG(bean_origin,', ') AS bean_origins\n",
      "FROM ratings\n",
      "WHERE company IN ('Bar Au Chocolat', 'Chocolate Con Amor', 'East Van Roasters');\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tcompany,\n",
      "    -- Create a list with all bean origins\n",
      "\tSTRING_AGG(bean_origin,',') AS bean_origins\n",
      "FROM ratings\n",
      "WHERE company IN ('Bar Au Chocolat', 'Chocolate Con Amor', 'East Van Roasters')\n",
      "-- Specify the columns used for grouping your data\n",
      "GROUP BY Company;\n",
      "\n",
      "SELECT \n",
      "\tcompany,\n",
      "    -- Create a list with all bean origins ordered alphabetically\n",
      "\tSTRING_AGG(bean_origin, ',') WITHIN GROUP (order by bean_origin) AS bean_origins\n",
      "FROM ratings\n",
      "WHERE company IN ('Bar Au Chocolat', 'Chocolate Con Amor', 'East Van Roasters')\n",
      "-- Specify the columns used for grouping your data\n",
      "GROUP BY company;\n",
      "\n",
      "\n",
      "DECLARE @phrase NVARCHAR(MAX) = 'In the morning I brush my teeth. In the afternoon I take a nap. In the evening I watch TV.'\n",
      "\n",
      "SELECT value\n",
      "FROM STRING_SPLIT(@phrase, '.');\n",
      "\n",
      "DECLARE @phrase NVARCHAR(MAX) = 'In the morning I brush my teeth. In the afternoon I take a nap. In the evening I watch TV.'\n",
      "\n",
      "SELECT value\n",
      "FROM STRING_SPLIT(@phrase,'.');\n",
      "\n",
      "SELECT\n",
      "\tfirst_name,\n",
      "    last_name,\n",
      "\tbirthdate,\n",
      "\temail,\n",
      "\tcountry\n",
      "FROM voters\n",
      "   -- Select only voters with a first name less than 5 characters\n",
      "WHERE LEN(first_name) < 5\n",
      "   -- Look for the desired pattern in the email address\n",
      "\tAND PATINDEX('j_a%@yahoo.com', email) > 0;     \n",
      "\n",
      "SELECT\n",
      "    -- Concatenate the first and last name\n",
      "\tconcat('***' , first_name, ' ', Upper(last_name), '***') AS name,\n",
      "    last_name,\n",
      "\tbirthdate,\n",
      "\temail,\n",
      "\tcountry\n",
      "FROM voters\n",
      "   -- Select only voters with a first name less than 5 characters\n",
      "WHERE LEN(first_name) < 5\n",
      "   -- Look for this pattern in the email address: \"j%[0-9]@yahoo.com\"\n",
      "\tAND PATINDEX('j_a%@yahoo.com', email) > 0;     \n",
      "\n",
      "\n",
      "SELECT\n",
      "    -- Concatenate the first and last name\n",
      "\tCONCAT('***' , first_name, ' ', UPPER(last_name), '***') AS name,\n",
      "    -- Mask the last two digits of the year\n",
      "    replace(birthdate, substring(CAST(birthdate AS varchar), 3, 2), 'XX') AS birthdate,\n",
      "\temail,\n",
      "\tcountry\n",
      "FROM voters\n",
      "   -- Select only voters with a first name less than 5 characters\n",
      "WHERE LEN(first_name) < 5\n",
      "   -- Look for this pattern in the email address: \"j%[0-9]@yahoo.com\"\n",
      "\tAND PATINDEX('j_a%@yahoo.com', email) > 0;   \n",
      "\n",
      "\n",
      "      >aggregate arithmetic functions\n",
      "\n",
      "count(all expression) -> number of all values from the expression\n",
      "count(distinct expression)\n",
      "count(*) -> all rows from a query\n",
      "1.returns the number of items in a group\n",
      "min(all expression)\n",
      "min(distinct expression)\n",
      " \n",
      "max(all expression)\n",
      "max(distinct expression)\n",
      "\n",
      "sum(all expression)\n",
      "sum(distinct expression)\n",
      "\n",
      "avg(all expression)\n",
      "avg(distinct expression)\n",
      "\n",
      "select company,\n",
      "\tavg(rating) as avg_rating\n",
      "from ratings\n",
      "group by company;\n",
      "\n",
      "\n",
      "    >\n",
      "\n",
      "SELECT \n",
      "\tgender, \n",
      "\t-- Count the number of voters for each group\n",
      "\tcount(*) AS voters,\n",
      "\t-- Calculate the total number of votes per group\n",
      "\tsum(total_votes) AS total_votes\n",
      "FROM voters\n",
      "GROUP BY gender;\n",
      "\n",
      "SELECT \n",
      "\tcompany,\n",
      "\t-- Calculate the average cocoa percent\n",
      "\tavg(cocoa_percent) AS avg_cocoa\n",
      "FROM ratings\n",
      "GROUP BY company;\n",
      "\n",
      "SELECT \n",
      "\tcompany,\n",
      "\t-- Calculate the average cocoa percent\n",
      "\tAVG(cocoa_percent) AS avg_cocoa,\n",
      "\t-- Calculate the minimum rating received by each company\n",
      "\tmin(rating) AS min_rating,\n",
      "\tMAX(rating) AS max_rating\t\n",
      "FROM ratings\n",
      "GROUP BY company;\n",
      "\n",
      "\n",
      "      analytic functions\n",
      "first_value(numeric_expression) over (partition by column order by column row_or_range frame)\n",
      "\n",
      "1. partition divide the results into partitions\n",
      "2. order by - order the result set\n",
      "3. row_or_range ->set the partition limits\n",
      "\n",
      "last_value(numeric_expression)\n",
      "1. returns the last value in an ordered set\n",
      "\n",
      "\n",
      "range between start_boundary and end_boundary\n",
      "rows between start_boundary and end_boundary\n",
      "\n",
      "UNBOUNDED PRECEDING -> first row in the partition\n",
      "UNBOUNDED FOLLOWING -> last row in the partition\n",
      "CURRENT ROW -> current row\n",
      "PRECEDING -> previous row\n",
      "FOLLOWING -> next row\n",
      "\n",
      "\n",
      "select\n",
      "\tfirst_name + ' ' + last_name as name,\n",
      "\tgender,\n",
      "\ttotal_votes as votes\n",
      "\tfirst_value(total_votes) over (partition by gender order by total_votes) as min_votes\n",
      "\tlast_value(total_votes) over (partition by gender order by total_votes) as max_votes\n",
      "from voters\n",
      "\n",
      "lag\n",
      "lead\n",
      "\n",
      "\n",
      "select broad_bean_origin as bean_origin,\n",
      "rating,\n",
      "cocoa_percent,\n",
      "LAG(cocoa_percent) over (order by rating) as percent_lower_rating,\n",
      "LEAD(cocoa_percent) over (order by rating) as percent_higher_rating\n",
      "from ratings\n",
      "where company='Felchlin'\n",
      "order by rating asc\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "SELECT \n",
      "\tbroad_bean_origin AS bean_origin,\n",
      "\trating,\n",
      "\tcocoa_percent,\n",
      "    -- Retrieve the cocoa % of the bar with the previous rating\n",
      "\tLAG(cocoa_percent) \n",
      "\t\tOVER(Partition by broad_bean_origin ORDER BY rating) AS percent_lower_rating\n",
      "FROM ratings\n",
      "WHERE company = 'Fruition'\n",
      "ORDER BY broad_bean_origin, rating ASC;\n",
      "\n",
      "SELECT \n",
      "\tbroad_bean_origin AS bean_origin,\n",
      "\trating,\n",
      "\tcocoa_percent,\n",
      "    -- Retrieve the cocoa % of the bar with the previous rating\n",
      "\tLAG(cocoa_percent) \n",
      "\t\tOVER(Partition by broad_bean_origin ORDER BY rating) AS percent_lower_rating\n",
      "FROM ratings\n",
      "WHERE company = 'Fruition'\n",
      "ORDER BY broad_bean_origin, rating ASC;\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tfirst_name + ' ' + last_name AS name,\n",
      "\tcountry,\n",
      "\tbirthdate,\n",
      "\t-- Retrieve the birthdate of the oldest voter per country\n",
      "\tFIRST_VALUE(birthdate) \n",
      "\tOVER (PARTITION BY country ORDER BY birthdate) AS oldest_voter,\n",
      "\t-- Retrieve the birthdate of the youngest voter per country\n",
      "\tLAST_VALUE(birthdate) \n",
      "\t\tOVER (PARTITION BY country ORDER BY birthdate ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
      "\t\t\t\t) AS youngest_voter\n",
      "FROM voters\n",
      "WHERE country IN ('Spain', 'USA');\n",
      "\n",
      "\n",
      "    >   math functions\n",
      "\n",
      "abs\n",
      "1. non negative values\n",
      "\n",
      "sign\n",
      "1. -1 if negative number\n",
      "2. 0 0\n",
      "3. 1 if positive\n",
      "\n",
      "ceiling(numeric_expression)\n",
      "1. returns the smallest integer greater than or equal to the expression\n",
      "\n",
      "floor (numeric_expression)\n",
      "1. returns the largest integer less than or equal to the expression\n",
      "\n",
      "round(numeric_expression, length)\n",
      "1. returns a numeric value, rounded to the specified length\n",
      "\n",
      "select ceiling (-50.49) as ceiling_neg,\n",
      "ceiling(73.71) as ceiling_pos,\n",
      "floor(73.71) as floor_pos,\n",
      "round(-50.493,1) as round_neg,\n",
      "round(73.715,2) as round_pos,\n",
      "-50, 74, 73,-50.5,73.72\n",
      "\n",
      "power(numeric_expression, power)\n",
      "1. returns the expression raised to the specified power\n",
      "\n",
      "square(numeric_expression)\n",
      "1. returns the square of the expression\n",
      "\n",
      "sqrt(numeric_expression)\n",
      "1. returns the square root of the expression\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "DECLARE @number1 DECIMAL(18,2) = -5.4;\n",
      "DECLARE @number2 DECIMAL(18,2) = 7.89;\n",
      "DECLARE @number3 DECIMAL(18,2) = 13.2;\n",
      "DECLARE @number4 DECIMAL(18,2) = 0.003;\n",
      "\n",
      "DECLARE @result DECIMAL(18,2) = @number1 * @number2 - @number3 - @number4;\n",
      "SELECT \n",
      "\t@result AS result,\n",
      "    -- Show the absolute value of the result\n",
      "\tABS(@result) AS abs_result,\n",
      "\tsign(@result) AS sign_result;\n",
      ";\n",
      "\n",
      "SELECT\n",
      "\trating,\n",
      "\t-- Round-up the rating to an integer value\n",
      "\tround(rating,0) AS round_up\n",
      "\t,ceiling(rating)\n",
      "FROM ratings;\n",
      "\n",
      "SELECT\n",
      "\trating,\n",
      "\t-- Round-up the rating to an integer value\n",
      "\tCEILING(rating) AS round_up,\n",
      "\t-- Round-down the rating to an integer value\n",
      "\tFLOOR(rating) AS round_down,\n",
      "\t-- Round the rating value to one decimal\n",
      "\tROUND(rating, 1) AS round_onedec,\n",
      "\tROUND(rating, 2) AS round_twodec\n",
      "FROM ratings;\n",
      "\n",
      "\n",
      "DECLARE @number DECIMAL(4, 2) = 4.5;\n",
      "DECLARE @power INT = 4;\n",
      "\n",
      "SELECT\n",
      "\t@number AS number,\n",
      "\t@power AS power,\n",
      "\t-- Raise the @number to the @power\n",
      "\tPOWER(@number,@power) AS number_to_power,\n",
      "\t-- Calculate the square of the @number\n",
      "\tSQUARE(@number) num_squared,\n",
      "\t-- Calculate the square root of the @number\n",
      "\tSQRT(@number) num_square_root;\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tcompany, \n",
      "    -- Select the number of cocoa flavors for each company\n",
      "\tCOUNT(*) AS flavors,\n",
      "    -- Select the minimum, maximum and average rating\n",
      "\tMIN(rating) AS lowest_score,\n",
      "\tMAX(rating) AS highest_score,\n",
      "\tAVG(rating) AS avg_score\t  \n",
      "FROM ratings\n",
      "GROUP BY company\n",
      "ORDER BY flavors DESC;\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tcompany, \n",
      "    -- Select the number of cocoa flavors for each company\n",
      "\tCOUNT(*) AS flavors,\n",
      "    -- Select the minimum, maximum and average rating\n",
      "\tMIN(rating) AS lowest_score,  \n",
      "\tMAX(rating) AS highest_score,   \n",
      "\tAVG(rating) AS avg_score,\n",
      "    -- Round the average rating to 1 decimal\n",
      "    ROUND(AVG(rating), 1) AS round_avg_score\t\n",
      "FROM ratings\n",
      "GROUP BY company\n",
      "ORDER BY flavors DESC;\n",
      "\n",
      "\n",
      "SELECT \n",
      "\tcompany, \n",
      "    -- Select the number of cocoa flavors for each company\n",
      "\tCOUNT(*) AS flavors,\n",
      "    -- Select the minimum, maximum and average rating\n",
      "\tMIN(rating) AS lowest_score,   \n",
      "\tMAX(rating) AS highest_score,   \n",
      "\tAVG(rating) AS avg_score,\n",
      "    -- Round the average rating to 1 decimal\n",
      "    ROUND(AVG(rating), 1) AS round_avg_score,\n",
      "    -- Round up and then down the aveg. rating to the next integer \n",
      "    CEILING(AVG(rating)) AS round_up_avg_score,   \n",
      "\tFLOOR(AVG(rating)) AS round_down_avg_score\n",
      "FROM ratings\n",
      "GROUP BY company\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\sql server stored procedures.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\sql server stored procedures.txt\n",
      "perform temporal exploratory data analysis (EDA) using sql data functions\n",
      "\n",
      "transactional data sets exist in all industries\n",
      "\n",
      "Exploratory data analysis process (iterative)\n",
      "ask questions -> apply methods -> interpret results\n",
      "\n",
      "get curious\n",
      "\n",
      "\n",
      "convert ( data_type [(length)], expression [,style])\n",
      "\n",
      "datepart (datepart, date)\n",
      "\n",
      "datename (datepart, date)\n",
      "\n",
      "datediff( datepart, startdate, enddate)\n",
      "\n",
      "datepart values= year, quarter, month, dayofyear, day, week, weekday, hour, minute, second, microsecond, nanosecond\n",
      "\n",
      "\n",
      "top 3 individuals with the most rides\n",
      "\n",
      "select\n",
      "top 3 count(ID) as NumberOfRides,\n",
      "DatePart(Hour, PickupDate) as Hour\n",
      "from YellowTripData\n",
      "Group by DatePart(Hour, PickupDate)\n",
      "order by count(ID) desc\n",
      "\n",
      "which day has the most revenue\n",
      "\n",
      "select\n",
      "top 3 round(sum(FareAmount),0) TotalFareAmount,\n",
      "DateName(weeday, PickupDate) as DayofWeek\n",
      "from YellowTripData\n",
      "Group by DateName(Weekday, PickupDate)\n",
      "Order by Sum(FareAmount) Desc\n",
      "\n",
      "\n",
      "what are the average ride times\n",
      "\n",
      "select\n",
      "avg(datediff(second, PickupDate, DropOffDate)/60\n",
      ") as AvgRideLengthInMin\n",
      "From YellowTripData\n",
      "Where DateName(Weekday, PickupDate)='Sunday'\n",
      "\n",
      "  \n",
      "\n",
      "SELECT\n",
      "  -- Select the date portion of StartDate\n",
      "  Convert(Date, StartDate) as StartDate,\n",
      "  -- Measure how many records exist for each StartDate\n",
      "  Count(*) as CountOfRows \n",
      "FROM CapitalBikeShare \n",
      "-- Group by the date portion of StartDate\n",
      "Group BY Convert(Date, StartDate)\n",
      "-- Sort the results by the date portion of StartDate\n",
      "Order BY Convert(Date, StartDate);\n",
      "\n",
      "\n",
      "SELECT\n",
      "\t-- Count the number of IDs\n",
      "\tCOUNT(ID) AS Count,\n",
      "    -- Use DATEPART() to evaluate the SECOND part of StartDate\n",
      "    \"StartDate\" = CASE WHEN DatePart(Second, StartDate) = 0 THEN 'SECONDS = 0'\n",
      "\t\t\t\t\t   WHEN DatePart(Second, StartDate) > 0 THEN 'SECONDS > 0' END\n",
      "FROM CapitalBikeShare\n",
      "GROUP BY\n",
      "    -- Use DATEPART() to Group By the CASE statement\n",
      "\tCASE WHEN DatePart(Second, StartDate) = 0 THEN 'SECONDS = 0'\n",
      "\t\t WHEN DatePart(Second, StartDate) > 0 THEN 'SECONDS > 0' END\n",
      "\n",
      "\n",
      "SELECT\n",
      "    -- Select the day of week value for StartDate\n",
      "\tDateName(Weekday, StartDate) as DayOfWeek,\n",
      "    -- Calculate TotalTripHours\n",
      "\tSum(DateDiff(Second, StartDate, EndDate))/ 3600 as TotalTripHours \n",
      "FROM CapitalBikeShare \n",
      "-- Group by the day of week\n",
      "GROUP BY DateName(Weekday, StartDate)\n",
      "-- Order TotalTripHours in descending order\n",
      "ORDER BY TotalTripHours DESC\n",
      "\n",
      "\n",
      "SELECT\n",
      "\t-- Calculate TotalRideHours using SUM() and DATEDIFF()\n",
      "  \tSum(DateDiff(Second, StartDate, EndDate))/ 3600 AS TotalRideHours,\n",
      "    -- Select the DATE portion of StartDate\n",
      "  \tConvert(Date, StartDate) AS DateOnly,\n",
      "    -- Select the WEEKDAY\n",
      "  \tDateName(Weekday, Convert(Date, StartDate)) AS DayOfWeek \n",
      "FROM CapitalBikeShare\n",
      "-- Only include Saturday\n",
      "WHERE DateName(Weekday, StartDate) = 'Saturday' \n",
      "GROUP BY CONVERT(DATE, StartDate);\n",
      "\n",
      "\n",
      "    > variables for datetime data\n",
      "\n",
      "declare @StartTime as time='08:00 am'\n",
      "\n",
      "declare @StartDateTime as datetime\n",
      "declare @BeginDate as date\n",
      "\n",
      "declare @TaxiRideDates Table( StartDate date, EndDate date)\n",
      "\n",
      "\n",
      "set @StartTime = '08:00 am'\n",
      "\n",
      "set @BeginDate=(\n",
      "select top 1 PickupDate\n",
      "From YellowTripData\n",
      "Order by PickupDate asc\n",
      ")\n",
      "\n",
      "\n",
      "    Cast\n",
      "\n",
      "declare @StartDateTime as datetime\n",
      "\n",
      "set @StartDateTime  = Cast(@BeginDate as datetime) + Cast(@StartTime as datetime)\n",
      "\n",
      "insert into @TaxiRideDates(StartDate,EndDate)\n",
      "select distinct\n",
      "cast(PickupDate as date),\n",
      "cast(DropOffDate as date)\n",
      "from YellowTripData\n",
      "\n",
      "\n",
      "-- Create @ShiftStartTime\n",
      "declare @ShiftStartTime AS time = '08:00 AM'\n",
      "\n",
      "-- Create @StartDate\n",
      "declare @StartDate AS date\n",
      "\n",
      "-- Set StartDate to the first StartDate from CapitalBikeShare\n",
      "set \n",
      "\t@StartDate = (\n",
      "    \tSELECT TOP 1 StartDate\n",
      "    \tFROM CapitalBikeShare \n",
      "    \tORDER BY StartDate ASC\n",
      "\t\t)\n",
      "\n",
      "-- Create ShiftStartDateTime\n",
      "declare @ShiftStartDateTime AS datetime\n",
      "\n",
      "-- Cast StartDate and ShiftStartTime to datetime data types\n",
      "set @ShiftStartDateTime = cast(@StartDate AS datetime) + cast(@ShiftStartTime AS datetime) \n",
      "\n",
      "SELECT @ShiftStartDateTime\n",
      "\n",
      "\n",
      "-- Declare @Shifts as a TABLE\n",
      "declare @Shifts as table(\n",
      "    -- Create StartDateTime column\n",
      "\tStartDateTime datetime,\n",
      "    -- Create EndDateTime column\n",
      "\tEndDateTime datetime)\n",
      "-- Populate @Shifts\n",
      "Insert into @Shifts (StartDateTime, EndDateTime)\n",
      "\tSELECT '3/1/2018 8:00 AM', '3/1/2018 4:00 PM'\n",
      "SELECT * \n",
      "FROM @Shifts\n",
      "\n",
      "\n",
      "-- Declare @RideDates\n",
      "declare @RideDates as table(\n",
      "    -- Define RideStart column\n",
      "\tRideStart Date, \n",
      "    -- Define RideEnd column\n",
      "    RideEnd Date)\n",
      "-- Populate @RideDates\n",
      "Insert into @RideDates(RideStart, RideEnd)\n",
      "-- Select the unique date values of StartDate and EndDate\n",
      "SELECT distinct\n",
      "    -- Cast StartDate as date\n",
      "\tcast(StartDate as date),\n",
      "    -- Cast EndDate as date\n",
      "\tcast(EndDate as date) \n",
      "FROM CapitalBikeShare \n",
      "SELECT * \n",
      "FROM @RideDates\n",
      "\n",
      "\n",
      "    >Data manipulation\n",
      "\n",
      "\n",
      "getDate\n",
      "1. get the current os local time\n",
      "\n",
      "\n",
      "declare @CurrentDateTime as datetime\n",
      "set @CurrentDateTime = GetDate()\n",
      "select @CurrentDateTime\n",
      "\n",
      "DateAdd(datepart,number,date)\n",
      "1. returns a date\n",
      "\n",
      "select DateDiff(day, date1,date2) from table1\n",
      "\n",
      "   how to get the beginning day of the week\n",
      "\n",
      "select dateadd( week, datediff(week, 0, getdate()),0)\n",
      "\n",
      "or 0 is 1/1/1900\n",
      "\n",
      "select DateDiff(month,'2/26/2018','3/3/2018')\n",
      "\n",
      "5 days 0 weeks 1 month\n",
      "\n",
      "\n",
      "get the first day of the month for the current date\n",
      "\n",
      "SELECT DateAdd(Month, DateDiff(Month, 0, GetDate()), 0)\n",
      "\n",
      "\n",
      "       user defined functions\n",
      "\n",
      "routine that can accept input parameters and perform action and return results\n",
      "\n",
      "\n",
      "allows for modular programming\n",
      "1. separate functionality of a program as separate modules\n",
      "\n",
      "\n",
      "create function GetTommorow()\n",
      "\treturns date as begin\n",
      "\t\treturn (select dateadd(day,1,GetDate()))\n",
      "end\n",
      "\n",
      "\n",
      "create function GetRideHrsOneDay(@DateParam date)\n",
      "\treturns numeric as begin\n",
      "\n",
      "\t\treturn (\n",
      "\n",
      "\t\tselect sum (datediff(second, PickupDate, DropoffDate))/3600\n",
      "\t\tfrom YellowTripData\n",
      "\t\twhere convert(date, PickupDate)=@DateParam\n",
      "\t\t)\n",
      "end;\n",
      "\n",
      "create function GetRideHrsDateRange(@StartDateParam date,@EndDateParam date)\n",
      "\treturns numeric as begin\n",
      "\n",
      "\t\treturn (\n",
      "\n",
      "\t\tselect sum (datediff(second, PickupDate, DropoffDate))/3600\n",
      "\t\tfrom YellowTripData\n",
      "\t\twhere PickupDate>@StartDateParam\n",
      "\t\tand DropoffDate < @EndDateParam\n",
      "\t\t)\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "-- Create GetYesterday()\n",
      "Create Function GetYesterday()\n",
      "-- Specify return data type\n",
      "returns date\n",
      "AS\n",
      "BEGIN\n",
      "-- Calculate yesterday's date value\n",
      "return (SELECT DateAdd(day, -1, GetDate()))\n",
      "END \n",
      "\n",
      "\n",
      "-- Create SumRideHrsSingleDay\n",
      "Create Function SumRideHrsSingleDay (@DateParam date)\n",
      "-- Specify return data type\n",
      "returns Numeric\n",
      "AS\n",
      "-- Begin\n",
      "Begin\n",
      "RETURN\n",
      "-- Add the difference between StartDate and EndDate\n",
      "(SELECT SUM(DATEDIFF(second, StartDate, EndDate))/3600\n",
      "FROM CapitalBikeShare\n",
      " -- Only include transactions where StartDate = @DateParm\n",
      "WHERE Cast(StartDate AS date) =@DateParam)\n",
      "-- End\n",
      "end;\n",
      "\n",
      "\n",
      "-- Create the function\n",
      "CREATE FUNCTION SumRideHrsDateRange (@StartDateParam datetime, @EndDateParam datetime)\n",
      "-- Specify return data type\n",
      "RETURNS numeric\n",
      "AS\n",
      "BEGIN\n",
      "RETURN\n",
      "-- Sum the difference between StartDate and EndDate\n",
      "(SELECT SUM(DATEDIFF(second, StartDate, EndDate))/3600\n",
      "FROM CapitalBikeShare\n",
      "-- Include only the relevant transactions\n",
      "WHERE StartDate > @StartDateParam and StartDate < @EndDateParam)\n",
      "\n",
      "\n",
      "\n",
      "      >Table valued UDFS\n",
      "\n",
      "\n",
      "  >inline table valued functions (ITVF) (preferred route)\n",
      "\n",
      "\n",
      "Create function SumLocationStats(\n",
      "\t@StartDate as datetime = '1/1/2017'\n",
      ") returns table as return\n",
      "\n",
      "select\n",
      "\tPULocationID as PickupLocation,\n",
      "\tCOUNT(ID) as RideCount,\n",
      "\tSUM(TripDistance) as TotalTripDistance\n",
      "from YellowTripData\n",
      "where Cast(PickupDate as Date) = @StartDate\n",
      "Group by PULocationID\n",
      "\n",
      "\n",
      "      table valued UDF\n",
      "\n",
      "create function CountTripAvgFareDay(\n",
      "@Month char(2),\n",
      "@Year char(4)\n",
      ") returns @TripCountAvgFare table(\n",
      "\tDropOffDate date,\n",
      "\tTripCount int,\n",
      "\tAvgFare numeric\n",
      ") as Begin \n",
      "\tinsert into @TripCountAvgFare\n",
      "select\n",
      "cast(DropOffDate as date),\n",
      "count(ID),\n",
      "Avg (FareAmount) as AvgFareAmt\n",
      "from YellowTripData\n",
      "where\n",
      "DatePart(month, DropOffDate)=@Month\n",
      "and DatePart(year, DropOffDate)=@Year\n",
      "Group by Cast(DropOffDate as date)\n",
      "return End;\n",
      "\n",
      "\n",
      "    >\n",
      "\n",
      "-- Create the function\n",
      "create function SumStationStats(@StartDate AS datetime)\n",
      "-- Specify return data type\n",
      "returns table\n",
      "AS\n",
      "RETURN\n",
      "SELECT\n",
      "\tStartStation,\n",
      "    -- Use COUNT() to select RideCount\n",
      "\tCount(ID) AS RideCount,\n",
      "    -- Use SUM() to calculate TotalDuration\n",
      "    Sum(Duration) AS TotalDuration\n",
      "FROM CapitalBikeShare\n",
      "WHERE CAST(StartDate as Date) = @StartDate\n",
      "-- Group by StartStation\n",
      "Group by StartStation;\n",
      "\n",
      "\n",
      "-- Create the function\n",
      "CREATE FUNCTION CountTripAvgDuration (@Month CHAR(2), @Year CHAR(4))\n",
      "-- Specify return variable\n",
      "RETURNS @DailyTripStats TABLE(\n",
      "\tTripDate\tdate,\n",
      "\tTripCount\tint,\n",
      "\tAvgDuration\tnumeric)\n",
      "AS\n",
      "BEGIN\n",
      "-- Insert query results into @DailyTripStats\n",
      "insert into @DailyTripStats\n",
      "SELECT\n",
      "    -- Cast StartDate as a date\n",
      "\tcast(StartDate AS date),\n",
      "    COUNT(ID),\n",
      "    AVG(Duration)\n",
      "FROM CapitalBikeShare\n",
      "WHERE\n",
      "\tDATEPART(month, StartDate) = @Month AND\n",
      "    DATEPART(year, StartDate) = @Year\n",
      "-- Group by StartDate as a date\n",
      "GROUP BY cast(StartDate AS date)\n",
      "-- Return\n",
      "return\n",
      "END\n",
      "\n",
      "\n",
      "   > UDFS in action\n",
      "\n",
      "select dbo.GetTommorow()\n",
      "\n",
      "declare @TotalRideHrs as numeric\n",
      "exec @TotalRideHrs = dbo.GetRideHrsOneDay @DateParm='1/15/2017'\n",
      "\n",
      "select 'Total Ride Hours for 1/15/2017',@TotalRideHrs\n",
      "\n",
      "\n",
      "declare @DateParm as date =\n",
      "\n",
      "--get the oldest date in YellowTripData\n",
      "\n",
      "(select top1 convert(date, PickupDate)\n",
      "\tfrom YellowTripData\n",
      "\torder by PickupDate desc)\n",
      "\n",
      "select @DateParam, dbo.GetRideHrsOneDay(@DateParam)\n",
      "\n",
      "\n",
      "select top 10 *\n",
      "from dbo.SumLocationStats('1/9/2017')\n",
      "order by RideCount desc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "declare @CountTripAvgFareDay table(\n",
      "DropOffDate date,\n",
      "TripCount int,\n",
      "AvgFare numeric)\n",
      "Insert into @CountTripAvgFareDay\n",
      "select top 10 * \n",
      "from dbo.CountTripAvgFareDay(01,2017)\n",
      "Order by DropOffDate ASC\n",
      "\n",
      "select * from @CountTripAvgFareDay\n",
      "\n",
      "\n",
      "   \n",
      "-- Create @BeginDate\n",
      "declare @BeginDate AS date = '3/1/2018'\n",
      "-- Create @EndDate\n",
      "declare @EndDate as  date = '3/10/2018' \n",
      "SELECT\n",
      "  -- Select @BeginDate\n",
      "  @BeginDate AS BeginDate,\n",
      "  -- Select @EndDate\n",
      "  @EndDate AS EndDate,\n",
      "  -- Execute SumRideHrsDateRange()\n",
      "  dbo.SumRideHrsDateRange(@BeginDate, @EndDate) AS TotalRideHrs\n",
      "\n",
      "\n",
      "-- Create @RideHrs\n",
      "declare @RideHrs AS numeric\n",
      "-- Execute SumRideHrsSingleDay function and store the result in @RideHrs\n",
      "exec @RideHrs = dbo.SumRideHrsSingleDay @DateParm = '3/5/2018' \n",
      "SELECT \n",
      "  'Total Ride Hours for 3/5/2018:', \n",
      "  @RideHrs\n",
      "\n",
      "-- Create @StationStats\n",
      "declare @StationStats as table(\n",
      "\tStartStation nvarchar(100), \n",
      "\tRideCount int, \n",
      "\tTotalDuration numeric)\n",
      "-- Populate @StationStats with the results of the function\n",
      "Insert into @StationStats\n",
      "SELECT TOP 10 *\n",
      "-- Execute SumStationStats with 3/15/2018\n",
      "FROM dbo.SumStationStats('3/15/2018') \n",
      "ORDER BY RideCount DESC\n",
      "-- Select all the records from @StationStats\n",
      "select * \n",
      "from @StationStats\n",
      "\n",
      "\n",
      "         Maintaining user defined functions\n",
      "\n",
      "alter function\n",
      "\n",
      "create or alter function SumLocationStats(@EndDate as datetime = '1/1/2017')\n",
      "returns tables as return\n",
      "select\n",
      "PULocationID as PickupLocation,\n",
      "Count(ID) as RideCount,\n",
      "Sum(TripDistance) as TotalTripDistance\n",
      "From YellowTripData\n",
      "where cast(DropOffData as Date) = @EndDate\n",
      "Group by PULocationID\n",
      "\n",
      "\n",
      "inline functions require that you drop them\n",
      "\n",
      "drop function dbo.CreateTripAvgFareDay\n",
      "\n",
      "then create the function.\n",
      "\n",
      "\n",
      "    Schemabinding\n",
      "1. specifies the schema is bound to the database objects that it references\n",
      "2. prevents changes to the schema if schema bound objects are referencing it\n",
      "\n",
      "\n",
      "create function GetRideHrsOneDay(@DateParam date)\n",
      "\treturns numeric \n",
      "\twith SCHEMABINDING\n",
      "as begin\n",
      "\n",
      "\t\treturn (\n",
      "\n",
      "\t\tselect sum (datediff(second, PickupDate, DropoffDate))/3600\n",
      "\t\tfrom YellowTripData\n",
      "\t\twhere convert(date, PickupDate)=@DateParam\n",
      "\t\t)\n",
      "end;\n",
      "\n",
      "   >\n",
      "\n",
      "-- Update SumStationStats\n",
      "Create or Alter FUNCTION dbo.SumStationStats(@EndDate AS Date)\n",
      "-- Enable SCHEMABINDING\n",
      "RETURNS TABLE with SCHEMABINDING\n",
      "AS\n",
      "RETURN\n",
      "SELECT\n",
      "\tStartStation,\n",
      "    COUNT(ID) AS RideCount,\n",
      "    SUM(DURATION) AS TotalDuration\n",
      "FROM dbo.CapitalBikeShare\n",
      "-- Cast EndDate as date and compare to @EndDate\n",
      "WHERE CAST(EndDate AS Date) = @EndDate\n",
      "GROUP BY StartStation;\n",
      "\n",
      "\n",
      "          >Stored Procedures\n",
      "\n",
      "a routine that can accept input parameters\n",
      "perform actions like execute, select, insert, update, delete\n",
      "return status (success or failure)\n",
      "return output parameters\n",
      "\n",
      "stored procedures can execute other stored procedures\n",
      "stored procedures can error handle with try catch\n",
      "stored procedures can select user defined functions\n",
      "\n",
      "\n",
      "create Procedure dbo.cuspGetRideHrsOneDay\n",
      "\t@DateParm date,\n",
      "\t@RideHrsOut numeric Output\n",
      "as\n",
      "set nocount on\n",
      "begin\n",
      "select\n",
      "@RideHrsOut=Sum(datediff(second,PickupDate,DropoffDate))/3600\n",
      "from YellowTripData\n",
      "where convert(date,PickupDate)=@DateParm\n",
      "return\n",
      "end;\n",
      "\n",
      "return -> is optional\n",
      "output -> it can be any datatype except table\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "-- Create the stored procedure\n",
      "create procedure dbo.cuspSumRideHrsSingleDay\n",
      "    -- Declare the input parameter\n",
      "\t@DateParm date,\n",
      "    -- Declare the output parameter\n",
      "\t@RideHrsOut numeric Output\n",
      "AS\n",
      "-- Don't send the row count \n",
      "set nocount on\n",
      "BEGIN\n",
      "-- Assign the query result to @RideHrsOut\n",
      "\tselect @RideHrsOut = SUM(DATEDIFF(second, StartDate, EndDate))/3600\n",
      "FROM CapitalBikeShare\n",
      "-- Cast StartDate as date and compare with @DateParm\n",
      "WHERE CAST(StartDate AS date) = @DateParm\n",
      "RETURN\n",
      "END\n",
      "\n",
      "        Create ,Read, Update, Delete (CRUD)\n",
      "\n",
      "Presentation Layer\n",
      "Business Logic Layer\n",
      "Data Access Layer\n",
      "Data Source  (stored procedures)\n",
      "\n",
      "\n",
      "create procedure dbo.cusp_TripSummaryCreate(\n",
      "@TripDate as date,\n",
      "@TripHours as numeric(18,0)\n",
      ")\n",
      "as begin\n",
      "insert into dbo.TripSummary(Date,TripHours)\n",
      "values\n",
      "(@TripDate,@TripHours)\n",
      "select Date, TripHours\n",
      "from dbo.TripSummary\n",
      "where Date=@TripDate \n",
      "end\n",
      "\n",
      "create procedure dbo.cusp_TripSummaryRead(\n",
      "@TripDate as date\n",
      ")\n",
      "as begin\n",
      "select Date, TripHours\n",
      "from dbo.TripSummary\n",
      "where Date=@TripDate \n",
      "end\n",
      "\n",
      "create procedure dbo.cusp_TripSummaryUpdate(\n",
      "@TripDate as date,\n",
      "@TripHours as numeric(18,0)\n",
      ")\n",
      "as begin\n",
      "update dbo.TripSummary\n",
      "set Date=@TripDate,\n",
      "TripHours=@TripHours\n",
      "where Date=@TripDate \n",
      "end\n",
      "\n",
      "\n",
      "create procedure dbo.cusp_TripSummaryDelete(\n",
      "@TripDate as date,\n",
      "@RowCountOut int OUTPUT\n",
      ")\n",
      "as begin\n",
      "delete from dbo.TripSummary\n",
      "where Date=@TripDate \n",
      "\n",
      "set @RowCountCount=@@ROWCOUNT\n",
      "end\n",
      "\n",
      "\n",
      "keeping stored procedures together by Table name convention\n",
      "\n",
      "1. cusp the table name and action\n",
      "\n",
      "\n",
      "stored procedure names must be unique across the schema\n",
      "\n",
      "set Nocount on -> prevents returning of the effected rows by the stored procedure\n",
      "\n",
      "  >\n",
      "\n",
      "-- Create the stored procedure\n",
      "CREATE PROCEDURE dbo.cusp_RideSummaryCreate \n",
      "    (@DateParm date, @RideHrsParm numeric)\n",
      "AS\n",
      "BEGIN\n",
      "SET NOCOUNT ON\n",
      "-- Insert into the Date and RideHours columns\n",
      "INSERT into dbo.RideSummary(Date,RideHours)\n",
      "-- Use values of @DateParm and @RideHrsParm\n",
      "Values(@DateParm, @RideHrsParm) \n",
      "\n",
      "-- Select the record that was just inserted\n",
      "select\n",
      "    -- Select Date column\n",
      "\tDate,\n",
      "    -- Select RideHours column\n",
      "    RideHours\n",
      "FROM dbo.RideSummary\n",
      "-- Check whether Date equals @DateParm\n",
      "WHERE Date = @DateParm\n",
      "END;\n",
      "\n",
      "\n",
      "-- Create the stored procedure\n",
      "create procedure dbo.cusp_RideSummaryUpdate\n",
      "\t-- Specify @Date input parameter\n",
      "\t(@Date date,\n",
      "     -- Specify @RideHrs input parameter\n",
      "     @RideHrs numeric(18,0))\n",
      "AS\n",
      "BEGIN\n",
      "SET NOCOUNT ON\n",
      "-- Update RideSummary\n",
      "Update RideSummary\n",
      "-- Set\n",
      "Set\n",
      "\tDate = @Date,\n",
      "    RideHours = @RideHrs\n",
      "-- Include records where Date equals @Date\n",
      "where Date=@Date\n",
      "END;\n",
      "\n",
      "-- Create the stored procedure\n",
      "CREATE PROCEDURE dbo.cuspRideSummaryDelete\n",
      "\t-- Specify @DateParm input parameter\n",
      "\t(@DateParm date,\n",
      "     -- Specify @RowCountOut output parameter\n",
      "     @RowCountOut int OUTPUT)\n",
      "AS\n",
      "BEGIN\n",
      "-- Delete record(s) where Date equals @DateParm\n",
      "delete FROM dbo.RideSummary\n",
      "where Date = @DateParm\n",
      "-- Set @RowCountOut to @@ROWCOUNT\n",
      "set @RowCountOut = @@ROWCOUNT\n",
      "END;\n",
      "\n",
      "    > exec\n",
      "\n",
      "1. no output parameter or return value\n",
      "2. store return value\n",
      "3. with output parameter\n",
      "\n",
      "exec dbo.cusp_TripSummaryUpdate\n",
      "\t@TripDate='1/5/2017'\n",
      "\t@TripHours='300'\n",
      "\n",
      "\n",
      "declare @RideHrs as numeric(18,0)\n",
      "\n",
      "exec dbo.cuspSumRideHrsOneDay\n",
      "\t@DateParm = '1/5/2017',\n",
      "\t@RideHrsOUt = @RideHrs OUTPUT\n",
      "\n",
      "\n",
      "declare @ReturnValue as int\n",
      "declare @RowCount as int\n",
      "\n",
      "exec @ReturnValue = \n",
      "\tdbo.cusp_TripSummaryUpdate\n",
      "\t@TripDate='1/5/2017'\n",
      "\t@TripHours =300,\n",
      "        @RowCountOut = @RowCount OUTPUT\n",
      "\n",
      "select @ReturnValue as ReturnValue, @RowCount\n",
      "\n",
      "a non zero value indicates an error has occurred\n",
      "\n",
      "\n",
      "declare @TripSummaryResultSet as table(\n",
      "\tTripData date,\n",
      "\tTripHours numeric (18,0))\n",
      "\n",
      "\n",
      "Insert into @TripSummaryResultSet\n",
      "Exec cusp_TripSummaryRead @TripDate='1/5/2017'\n",
      "\n",
      "\n",
      "    >\n",
      "\n",
      "\n",
      "-- Create @RideHrs\n",
      "declare @RideHrs AS numeric(18,0)\n",
      "-- Execute the stored procedure\n",
      "exec dbo.cuspSumRideHrsSingleDay\n",
      "    -- Pass the input parameter\n",
      "\t@DateParm = '3/1/2018',\n",
      "    -- Store the output in @RideHrs\n",
      "\t@RideHrsOut = @RideHrs OUTPUT\n",
      "-- Select @RideHrs\n",
      "select @RideHrs AS RideHours\n",
      "\n",
      "\n",
      "-- Create @ReturnStatus\n",
      "declare @ReturnStatus AS int\n",
      "-- Create @RowCount\n",
      "declare @RowCount AS int\n",
      "\n",
      "-- Execute the SP, storing the result in @ReturnStatus\n",
      "exec @ReturnStatus = dbo.cuspRideSummaryDelete\n",
      "    -- Specify @DateParm\n",
      "\t@DateParm = '3/1/2018',\n",
      "    -- Specify RowCountOut\n",
      "\t@RowCountOut = @RowCount OUTPUT\n",
      "\n",
      "-- Select the columns of interest\n",
      "select\n",
      "\t@ReturnStatus AS ReturnStatus,\n",
      "    @RowCount AS 'RowCount';\n",
      "\n",
      "\n",
      "        Try and catch\n",
      "\n",
      "\n",
      "how to handle errors\n",
      "anticipated, detection, and resolution of errors\n",
      "\n",
      "generic messages that are confusing to users\n",
      "\n",
      "\n",
      "alter procedure dbo.cusp_TripSummaryCreate\n",
      "\t@TripDate nvarchar(30),\n",
      "\t@RideHrs numeric,\n",
      "\t@ErrorMsg nvarchar(max) = null OUTPUT  --optional parameter by setting the value to null\n",
      "as\n",
      "\n",
      "\tbegin\n",
      "\t\tbegin try\n",
      "\t\t\tinsert into TripSummary(Date, TripHours)\n",
      "\t\t\tvalues(@TripDate, @RideHrs)\n",
      "\t\tend try\t\n",
      "\n",
      "\t\tbegin catch\n",
      "\t\t\tset @ErrorMsg = \"Error_Num: '+ Cast(ERROR_NUMBER() AS varchar) + ' Error_sev: '+ cast(ERROR_SEVERITY() AS varchar) + 'Error_Msg: '+ERROR_MESSGAE()\n",
      "\t\tend catch\n",
      "\tend\n",
      "\n",
      "\n",
      "\n",
      "declare @ErrorMsgOut as nvarchar(max)\n",
      "\n",
      "execute dbo.cusp_TripSummaryCreate\n",
      "\t@TripDate = '1/32/2018',\n",
      "\t@RideHrs=100,\n",
      "\t@ErrorMsg = @ErrorMsgOut OUTPUT\n",
      "\n",
      "select @ErrorMsgOut as ErrorMessage\n",
      "\n",
      "\n",
      "   > Throw\n",
      "\n",
      "1. initiates a error handling\n",
      "2. statements after throw will not be executed\n",
      "\n",
      "-- Alter the stored procedure\n",
      "CREATE OR ALTER PROCEDURE dbo.cuspRideSummaryDelete\n",
      "\t-- (Incorrectly) specify @DateParm\n",
      "\t@DateParm nvarchar(30),\n",
      "    -- Specify @Error\n",
      "\t@Error nvarchar(max) = NULL OUTPUT\n",
      "AS\n",
      "SET NOCOUNT ON\n",
      "BEGIN\n",
      "  -- Start of the TRY block\n",
      "  BEGIN TRY\n",
      "  \t  -- Delete\n",
      "      DELETE FROM RideSummary\n",
      "      WHERE Date = @DateParm\n",
      "  -- End of the TRY block\n",
      "  END TRY\n",
      "  -- Start of the CATCH block\n",
      "  BEGIN CATCH\n",
      "\t\tSET @Error = \n",
      "\t\t'Error_Number: '+ CAST(ERROR_NUMBER() AS VARCHAR) +\n",
      "\t\t'Error_Severity: '+ CAST(ERROR_SEVERITY() AS VARCHAR) +\n",
      "\t\t'Error_State: ' + CAST(ERROR_STATE() AS VARCHAR) + \n",
      "\t\t'Error_Message: ' + ERROR_MESSAGE() + \n",
      "\t\t'Error_Line: ' + CAST(ERROR_LINE() AS VARCHAR)\n",
      "  -- End of the CATCH block\n",
      "  END CATCH\n",
      "END;\n",
      "\n",
      "\n",
      "-- Create @ReturnCode\n",
      "DECLARE @ReturnCode int\n",
      "-- Create @ErrorOut\n",
      "declare @ErrorOut nvarchar(max)\n",
      "-- Execute the SP, storing the result in @ReturnCode\n",
      "exec @ReturnCode = dbo.cuspRideSummaryDelete\n",
      "    -- Specify @DateParm\n",
      "\t@DateParm = '1/32/2018',\n",
      "    -- Assign @ErrorOut to @Error\n",
      "\t@Error = @ErrorOut OUTPUT\n",
      "-- Select @ReturnCode and @ErrorOut\n",
      "select\n",
      "\t@ReturnCode AS ReturnCode,\n",
      "    @ErrorOut AS ErrorMessage;\n",
      "\n",
      "ReturnCode\tErrorMessage\n",
      "-6\tError_Number: 241 Error_Severity: 16 Error_State: 1 Error_Message: Conversion failed when converting date and/or time from character string. Error_Line: 9\n",
      "\n",
      "\n",
      "      >Business problem\n",
      "\n",
      "\n",
      "For each week day:\n",
      "1. average fare per distance\n",
      "2. ride count\n",
      "3. total ride time\n",
      "\n",
      "\n",
      "dates in the future\n",
      "\n",
      "select *\n",
      "from CapitalBikeShare\n",
      "where\n",
      "StartDate > GetDate()\n",
      "or EndDate > GetDate()\n",
      "or StartDate > EndDate\n",
      "\n",
      "look for divide by zero error when calculating Avg Fare/TripDistance\n",
      "\n",
      "data imputation methods to resolve\n",
      "1. mean\n",
      "2. hot deck\n",
      "3. omission\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "create procedure dbo.ImputeDurMean\n",
      "as\n",
      "begin\n",
      "declare @AvgTripDuration as float\n",
      "\n",
      "select @AvgTripDuration = Avg(Duration)\n",
      "from CapitalBikeShare\n",
      "where Duration > 0 \n",
      "\n",
      "Update CapitalBikeShare\n",
      "set Duration = @AvgTripDuration\n",
      "where Duration =0\n",
      "end;\n",
      "\n",
      "   > Hot Deck imputation \n",
      "1. missing values set to randomly selected value\n",
      "2. tablesample clause of from clause\n",
      "\n",
      "create function dbo.GetDurationHotDeck()\n",
      "returns decimal (18,4)\n",
      "as\n",
      "begin\n",
      "return (select top 1 Duration\n",
      "from CapitalBikeShare\n",
      "TABLESAMPLE (1000 rows)\n",
      "where duration>0)\n",
      "end\n",
      "\n",
      "the TABLESAMPLE randomly select rows\n",
      "\n",
      "\n",
      "SELECT\n",
      "StartDate,\n",
      "'TripDuration'=Case when Duration >0 then Duration\n",
      "else dbo.GetDurationHotDeck() end\n",
      "from CapitalBikeShare\n",
      "\n",
      "\n",
      "Select\n",
      "DateName(weekday, StartDate) as DayOfWeek,\n",
      "avg(Duration) as 'AvgDuration'\n",
      "from CapitalBikeShare\n",
      "where Duration > 0\n",
      "Group by DateName(weekday, StartDate)\n",
      "Order by Avg(Duration) desc\n",
      "\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "SELECT\n",
      "\t-- PickupDate is after today\n",
      "\tCOUNT (CASE WHEN PickupDate> GetDate() THEN 1 END) AS 'FuturePickup',\n",
      "    -- DropOffDate is after today\n",
      "\tCOUNT (CASE WHEN DropOffDate > GetDate() THEN 1 END) AS 'FutureDropOff',\n",
      "    -- PickupDate is after DropOffDate\n",
      "\tCOUNT (CASE WHEN PickupDate > GetDate() THEN 1 END) AS 'PickupBeforeDropoff',\n",
      "    -- TripDistance is 0\n",
      "\tCOUNT (CASE WHEN TripDistance = 0 THEN 1 END) AS 'ZeroTripDistance'  \n",
      "FROM YellowTripData;\n",
      "\n",
      "\n",
      "-- Create the stored procedure\n",
      "create procedure dbo.cuspImputeTripDistanceMean\n",
      "AS\n",
      "BEGIN\n",
      "-- Specify @AvgTripDistance variable\n",
      "declare @AvgTripDistance AS numeric (18,4)\n",
      "\n",
      "-- Calculate the average trip distance\n",
      "select @AvgTripDistance = Avg(TripDistance)\n",
      "FROM YellowTripData\n",
      "-- Only include trip distances greater than 0\n",
      "where TripDistance>0\n",
      "\n",
      "-- Update the records where trip distance is 0\n",
      "Update YellowTripData\n",
      "set TripDistance =  @AvgTripDistance\n",
      "WHERE TripDistance = 0\n",
      "END;\n",
      "\n",
      "\n",
      "-- Create the function\n",
      "Create function dbo.GetTripDistanceHotDeck()\n",
      "-- Specify return data type\n",
      "returns numeric(18,4)\n",
      "AS \n",
      "BEGIN\n",
      "RETURN\n",
      "\t-- Select the first TripDistance value\n",
      "\t(select TOP 1 TripDistance\n",
      "\tFROM YellowTripData\n",
      "    -- Sample 1000 records\n",
      "\tTABLESAMPLE(1000 rows)\n",
      "    -- Only include records where TripDistance is > 0\n",
      "\twhere TripDistance > 0)\n",
      "END;\n",
      "\n",
      "       >UDF\n",
      "\n",
      "1. average fare per distance\n",
      "2. ride count\n",
      "3. total ride time\n",
      "\n",
      "\n",
      "miles to kilometers\n",
      "currency to currency based on exchange rate\n",
      "\n",
      "\n",
      "Create Function dbo.ConvertMileToMeter(@miles numeric(18,2))\n",
      "returns numeric(18,2)\n",
      "as\n",
      "begin\n",
      "return (select @miles * 1609.34)\n",
      "end\n",
      "\n",
      "create function dbo.ConvertCurrency(@Currency numeric, @ExchangeRate numeric(18,2))\n",
      "returns numeric(18,2)\n",
      "as\n",
      "begin\n",
      "return (select @ExchangeRate * @Currency)\n",
      "end\n",
      "\n",
      "select TripDistance as 'MileDistance',\n",
      "dbo.ConvertMileToMeter(TripDistance) as 'MeterDistance',\n",
      "FareAmount as 'FareUSD',\n",
      "dbo.ConvertCurrency(FareAmount,'.78') as 'FareGBP\n",
      "from dbo.YellowTripData\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- Create @ReturnStatus\n",
      "declare @ReturnStatus AS int\n",
      "-- Execute the SP, storing the result in @ReturnStatus\n",
      "exec @ReturnStatus = dbo.cuspRideSummaryUpdate\n",
      "    -- Specify @DateParm\n",
      "\t@DateParm = '3/1/2018',\n",
      "    -- Specify @RideHrs\n",
      "\t@RideHrs = 300\n",
      "\n",
      "-- Select the columns of interest\n",
      "select\n",
      "\t@ReturnStatus AS ReturnStatus,\n",
      "    Date,\n",
      "    RideHours\n",
      "FROM dbo.RideSummary\n",
      "WHERE Date = '3/1/2018';\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "-- Create the function\n",
      "Create Function dbo.ConvertMileToKm (@miles numeric(18,2))\n",
      "-- Specify return data type\n",
      "returns numeric(18,2)\n",
      "AS\n",
      "BEGIN\n",
      "RETURN\n",
      "\t-- Convert Miles to Kilometers\n",
      "\t(select @miles * 1.609)\n",
      "END;\n",
      "\n",
      "-- Create the function\n",
      "CREATE FUNCTION dbo.GetShiftNumber (@Hour integer)\n",
      "-- Specify return data type\n",
      "RETURNS int\n",
      "AS\n",
      "BEGIN\n",
      "RETURN\n",
      "\t-- 12am (0) to 9am (9) shift\n",
      "\t(case when @Hour >= 0 and @Hour < 9 THEN 1\n",
      "     \t  -- 9am (9) to 5pm (17) shift\n",
      "\t\t when @Hour >= 9 and @Hour < 17 THEN 2\n",
      "          -- 5pm (17) to 12am (24) shift\n",
      "\t     when @Hour >= 17 and @Hour < 24 THEN 3 END)\n",
      "END;\n",
      "\n",
      "SELECT\n",
      "\t-- Select the first 100 records of PickupDate\n",
      "\tTOP 100 PickupDate,\n",
      "    -- Determine the shift value of PickupDate\n",
      "\tdbo.GetShiftNumber(DatePart(Hour, PickupDate)) AS 'Shift',\n",
      "    -- Select FareAmount\n",
      "\tFareAmount,\n",
      "    -- Convert FareAmount to Euro\n",
      "\tdbo.ConvertDollar(FareAmount, 0.87) AS 'FareinEuro',\n",
      "    -- Select TripDistance\n",
      "\tTripDistance,\n",
      "    -- Convert TripDistance to kilometers\n",
      "\tdbo.ConvertMileToKm(TripDistance) AS 'TripDistanceinKM'\n",
      "FROM YellowTripData\n",
      "-- Only include records for the 2nd shift\n",
      "WHERE dbo.GetShiftNumber(DatePart(Hour, PickupDate)) = 2;\n",
      "\n",
      "\n",
      "    formatting data\n",
      "\n",
      "select \n",
      "datename(weekday, StartDate) as 'DayOfWeek',\n",
      "sum(Duration) as TotalDuration\n",
      "from CapitalBikeShare\n",
      "Group by DateName(weekday, StartDate)\n",
      "order by DateName(weekday, StartDate)\n",
      "\n",
      "\n",
      "format(cast(StartDate as Date), 'd','de-de')\n",
      "\n",
      "en_us\n",
      "\n",
      "format(Sum(Duration),'#,0.00')\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "SELECT\n",
      "    -- Select the pickup day of week\n",
      "\tDATENAME(weekday, PickupDate) as DayofWeek,\n",
      "    -- Calculate TotalAmount per TripDistance\n",
      "\tCAST(AVG(TotalAmount/\n",
      "            -- Select TripDistance if it's more than 0\n",
      "\t\t\tCASE WHEN TripDistance > 0 THEN TripDistance\n",
      "                 -- Use GetTripDistanceHotDeck()\n",
      "     \t\t\t ELSE dbo.GetTripDistanceHotDeck() END) as decimal(10,2)) as 'AvgFare'\n",
      "FROM YellowTripData\n",
      "GROUP BY DATENAME(weekday, PickupDate)\n",
      "-- Order by the PickupDate day of week\n",
      "ORDER BY\n",
      "     case when DATENAME(weekday, PickupDate) = 'Monday' THEN 1\n",
      "         when DATENAME(weekday, PickupDate) = 'Tuesday' THEN 2\n",
      "         when DATENAME(weekday, PickupDate) = 'Wednesday' THEN 3\n",
      "         when DATENAME(weekday, PickupDate) = 'Thursday' THEN 4\n",
      "         when DATENAME(weekday, PickupDate) = 'Friday' THEN 5\n",
      "         when DATENAME(weekday, PickupDate) = 'Saturday' THEN 6\n",
      "         when DATENAME(weekday, PickupDate) = 'Sunday' THEN 7\n",
      "END ASC;\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "\n",
      "SELECT\n",
      "    -- Cast PickupDate as a date and display as a German date\n",
      "\tformat(cast(PickupDate AS date), 'd', 'de-de') AS 'PickupDate',\n",
      "\tZone.Borough,\n",
      "    -- Display TotalDistance in the German format\n",
      "\tformat(SUM(TripDistance), '#,0.00', 'de-de') AS 'TotalDistance',\n",
      "    -- Display TotalRideTime in the German format\n",
      "\tformat(SUM(DATEDIFF(minute, PickupDate, DropoffDate)), 'n', 'de-de') AS 'TotalRideTime',\n",
      "    -- Display TotalFare in German currency\n",
      "\tformat(SUM(TotalAmount), 'c', 'de-de') AS 'TotalFare'\n",
      "FROM YellowTripData\n",
      "INNER JOIN TaxiZoneLookup AS Zone \n",
      "ON PULocationID = Zone.LocationID \n",
      "GROUP BY\n",
      "\tCAST(PickupDate as date),\n",
      "    Zone.Borough \n",
      "ORDER BY\n",
      "\tCAST(PickupDate as date),\n",
      "    Zone.Borough;\n",
      "\n",
      "\n",
      "      putting it together\n",
      "\n",
      "select\n",
      "Datename(weekday, PickupDate) as 'Weekday',\n",
      "PickupDate,\n",
      "DropOffDate,\n",
      "TotalAmount,\n",
      "TripDistance,\n",
      "dbo.ConvertDollar(TotalAmount,.88)/dbo.ConvertMileToKm(TripDistance) as 'EuroFarePerKM',\n",
      "datadiff(second, PickupDate, DropOffDate)/60 as 'TotalRideMin'\n",
      "from YellowTripData\n",
      "where TripDistance >0\n",
      "GROUP BY DATENAME(weekday, PickupDate),Zone.Borough\n",
      "-- Order by the PickupDate day of week\n",
      "ORDER BY\n",
      "     case when DATENAME(weekday, PickupDate) = 'Monday' THEN 1\n",
      "         when DATENAME(weekday, PickupDate) = 'Tuesday' THEN 2\n",
      "         when DATENAME(weekday, PickupDate) = 'Wednesday' THEN 3\n",
      "         when DATENAME(weekday, PickupDate) = 'Thursday' THEN 4\n",
      "         when DATENAME(weekday, PickupDate) = 'Friday' THEN 5\n",
      "         when DATENAME(weekday, PickupDate) = 'Saturday' THEN 6\n",
      "         when DATENAME(weekday, PickupDate) = 'Sunday' THEN 7\n",
      "END ASC,\n",
      "Avg(dbo.ConvertDollar(TotalAmount,.77)/dbo.ConvertMileToKM(TripDistance)) desc;\n",
      "\n",
      "\n",
      "Create or alter procedure dbo.cuspPickupZoneShiftStats\n",
      "@Borough nvarchar(30)\n",
      "as\n",
      "begin\n",
      "\n",
      "end\n",
      "\n",
      "drop procedure if exists dbo.cuspPickupZoneShiftStats\n",
      "go\n",
      "create procedure dbo.cuspPickupZoneShiftStats\n",
      "@Borough nvarchar(30)\n",
      "as\n",
      "begin\n",
      "end\n",
      "\n",
      "\n",
      "  \n",
      "CREATE OR ALTER PROCEDURE dbo.cuspBoroughRideStats\n",
      "AS\n",
      "BEGIN\n",
      "SELECT\n",
      "    -- Calculate the pickup weekday\n",
      "\tDatename(weekday, PickupDate) AS 'Weekday',\n",
      "    -- Select the Borough\n",
      "\tZone.Borough AS 'PickupBorough',\n",
      "    -- Display AvgFarePerKM as German currency\n",
      "\tformat(AVG(dbo.ConvertDollar(TotalAmount, .88)/dbo.ConvertMiletoKM(TripDistance)), 'c', 'de-de') AS 'AvgFarePerKM',\n",
      "    -- Display RideCount in the German format\n",
      "\tformat(COUNT(ID), 'n', 'de-de') AS 'RideCount',\n",
      "    -- Display TotalRideMin in the German format\n",
      "\tformat(SUM(DATEDIFF(SECOND, PickupDate, DropOffDate))/60, 'n', 'de-de') AS 'TotalRideMin'\n",
      "FROM YellowTripData\n",
      "INNER JOIN TaxiZoneLookup AS Zone \n",
      "ON PULocationID = Zone.LocationID\n",
      "-- Only include records where TripDistance is greater than 0\n",
      "where TripDistance > 0\n",
      "-- Group by pickup weekday and Borough\n",
      "GROUP BY DATENAME(WEEKDAY, PickupDate), Zone.Borough\n",
      "ORDER BY CASE WHEN DATENAME(WEEKDAY, PickupDate) = 'Monday' THEN 1\n",
      "\t     \t  WHEN DATENAME(WEEKDAY, PickupDate) = 'Tuesday' THEN 2\n",
      "              WHEN DATENAME(WEEKDAY, PickupDate) = 'Wednesday' THEN 3\n",
      "              WHEN DATENAME(WEEKDAY, PickupDate) = 'Thursday' THEN 4\n",
      "              WHEN DATENAME(WEEKDAY, PickupDate) = 'Friday' THEN 5\n",
      "              WHEN DATENAME(WEEKDAY, PickupDate) = 'Saturday' THEN 6\n",
      "              WHEN DATENAME(WEEKDAY, PickupDate) = 'Sunday' THEN 7 END,  \n",
      "\t\t SUM(DATEDIFF(SECOND, PickupDate, DropOffDate))/60\n",
      "DESC\n",
      "END;\n",
      "\n",
      "\n",
      "-- Create SPResults\n",
      "Declare @SPResults as table(\n",
      "  \t-- Create Weekday\n",
      "\tWeekday nvarchar(30),\n",
      "    -- Create Borough\n",
      "\tBorough nvarchar(30),\n",
      "    -- Create AvgFarePerKM\n",
      "\tAvgFarePerKM nvarchar(30),\n",
      "    -- Create RideCount\n",
      "\tRideCount\tnvarchar(30),\n",
      "    -- Create TotalRideMin\n",
      "\tTotalRideMin\tnvarchar(30))\n",
      "\n",
      "-- Insert the results into @SPResults\n",
      "Insert into @SPResults\n",
      "-- Execute the SP\n",
      "exec dbo.cuspBoroughRideStats\n",
      "\n",
      "-- Select all the records from @SPresults \n",
      "select * from @SPresults;\n",
      "\n",
      "\n",
      "-- Create the stored procedure\n",
      "CREATE PROCEDURE dbo.cuspPickupZoneShiftStats\n",
      "\t-- Specify @Borough parameter\n",
      "\t@Borough nvarchar(30)\n",
      "AS\n",
      "BEGIN\n",
      "SELECT\n",
      "\tDATENAME(WEEKDAY, PickupDate) as 'Weekday',\n",
      "    -- Calculate the shift number\n",
      "\tdbo.getShiftNumber(DatePart(Hour, PickupDate)) as 'Shift',\n",
      "\tZone.Zone as 'Zone',\n",
      "\tFORMAT(AVG(dbo.ConvertDollar(TotalAmount, .77)/dbo.ConvertMiletoKM(TripDistance)), 'c', 'de-de') AS 'AvgFarePerKM',\n",
      "\tFORMAT(COUNT (ID),'n', 'de-de') as 'RideCount',\n",
      "\tFORMAT(SUM(DATEDIFF(SECOND, PickupDate, DropOffDate))/60, 'n', 'de-de') as 'TotalRideMin'\n",
      "FROM YellowTripData\n",
      "INNER JOIN TaxiZoneLookup as Zone on PULocationID = Zone.LocationID \n",
      "WHERE\n",
      "\tdbo.ConvertMiletoKM(TripDistance) > 0 AND\n",
      "\tZone.Borough = @Borough\n",
      "GROUP BY\n",
      "\tDATENAME(WEEKDAY, PickupDate),\n",
      "    -- Group by shift\n",
      "\tdbo.getShiftNumber(DatePart(Hour, PickupDate)),  \n",
      "\tZone.Zone\n",
      "ORDER BY CASE WHEN DATENAME(WEEKDAY, PickupDate) = 'Monday' THEN 1\n",
      "              WHEN DATENAME(WEEKDAY, PickupDate) = 'Tuesday' THEN 2\n",
      "              WHEN DATENAME(WEEKDAY, PickupDate) = 'Wednesday' THEN 3\n",
      "              WHEN DATENAME(WEEKDAY, PickupDate) = 'Thursday' THEN 4\n",
      "              WHEN DATENAME(WEEKDAY, PickupDate) = 'Friday' THEN 5\n",
      "              WHEN DATENAME(WEEKDAY, PickupDate) = 'Saturday' THEN 6\n",
      "              WHEN DATENAME(WEEKDAY, PickupDate) = 'Sunday' THEN 7 END,\n",
      "         -- Order by shift\n",
      "         dbo.getShiftNumber(DatePart(Hour, PickupDate)) ,\n",
      "         SUM(DATEDIFF(SECOND, PickupDate, DropOffDate))/60 DESC\n",
      "END;\n",
      "\n",
      "\n",
      "-- Create @Borough\n",
      "declare @Borough AS nvarchar(30) = 'Manhattan'\n",
      "-- Execute the SP\n",
      "exec dbo.cuspPickupZoneShiftStats\n",
      "    -- Pass @Borough\n",
      "\t@Borough = @Borough;\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\sql server time series.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\sql server time series.txt\n",
      "select getdate(), getutcdate() utc_time, sysdatetime() sys_datetime,sysutcdatetime() sys_utc_datetime\n",
      "\n",
      "datetime2:sysdatetime(), sysutcdatetime() \n",
      "\n",
      "parts:\n",
      "Year\n",
      "Month\n",
      "Day\n",
      "Day of Year\n",
      "Day of Week\n",
      "Week of year\n",
      "ISO week of year\n",
      "Minutes/seconds/millisecond/nanosecond\n",
      "\n",
      "select DatePart(year,@td) as TheYear\n",
      "select DateName(Month,@dt) as TheMonth\n",
      "select DateAdd(Day,1, @SomeTime) NextDay\n",
      "select DateAdd(Day,-1,@SomeTime) PriorDay\n",
      "\n",
      "\n",
      "declare @StartTime DateTime2(7)='2012-03-01 14:29:36'\n",
      "declare @EndTime DateTime2(7)='2012-03-01 18:00:00'\n",
      "\n",
      "Select DateDiff(Second, @StartTime, @EndTime) SecondsElapsed,\n",
      "DateDiff(Minute, @StartTime, @EndTime) MinutesElapsed,\n",
      "DateDiff(Hour, @StartTime, @EndTime) HoursElapsed\n",
      "\n",
      "  > sample\n",
      "\n",
      "DECLARE\n",
      "\t@BerlinWallFalls DATETIME2(7) = '1989-11-09 23:49:36.2294852';\n",
      "\n",
      "-- Fill in each date part\n",
      "SELECT\n",
      "\tDATEPART(Year, @BerlinWallFalls) AS TheYear,\n",
      "\tDATEPART(Month, @BerlinWallFalls) AS TheMonth,\n",
      "\tDATEPART(Day, @BerlinWallFalls) AS TheDay,\n",
      "\tDATEPART(DayOfYear, @BerlinWallFalls) AS TheDayOfYear,\n",
      "    -- Day of week is WEEKDAY\n",
      "\tDATEPART(WEEKDAY, @BerlinWallFalls) AS TheDayOfWeek,\n",
      "\tDATEPART(Week, @BerlinWallFalls) AS TheWeek,\n",
      "\tDATEPART(Second, @BerlinWallFalls) AS TheSecond,\n",
      "\tDATEPART(NanoSecond, @BerlinWallFalls) AS TheNanosecond;\n",
      "\n",
      "\n",
      "DECLARE\n",
      "\t@BerlinWallFalls DATETIME2(7) = '1989-11-09 23:49:36.2294852';\n",
      "\n",
      "-- Fill in the function to show the name of each date part\n",
      "SELECT\n",
      "\tDateName(YEAR, @BerlinWallFalls) AS TheYear,\n",
      "\tDateName(MONTH, @BerlinWallFalls) AS TheMonth,\n",
      "\tDateName(DAY, @BerlinWallFalls) AS TheDay,\n",
      "\tDateName(DAYOFYEAR, @BerlinWallFalls) AS TheDayOfYear,\n",
      "    -- Day of week is WEEKDAY\n",
      "\tDateName(WEEKDAY, @BerlinWallFalls) AS TheDayOfWeek,\n",
      "\tDateName(WEEK, @BerlinWallFalls) AS TheWeek,\n",
      "\tDateName(SECOND, @BerlinWallFalls) AS TheSecond,\n",
      "\tDateName(NANOSECOND, @BerlinWallFalls) AS TheNanosecond;\n",
      "\n",
      "\n",
      "DECLARE\n",
      "\t@LeapDay DATETIME2(7) = '2012-02-29 18:00:00';\n",
      "\n",
      "-- Fill in the date parts and intervals as needed\n",
      "SELECT\n",
      "\tDATEADD(Day, -1, @LeapDay) AS PriorDay,\n",
      "\tDATEADD(Day, 1, @LeapDay) AS NextDay,\n",
      "    -- For leap years, we need to move 4 years, not just 1\n",
      "\tDATEADD(YEAR, -4, @LeapDay) AS PriorLeapYear,\n",
      "\tDATEADD(YEAR, 4, @LeapDay) AS NextLeapYear,\n",
      "\tDATEADD(Year, -1, @LeapDay) AS PriorYear;\n",
      "\n",
      "\n",
      "DECLARE\n",
      "\t@PostLeapDay DATETIME2(7) = '2012-03-01 18:00:00',\n",
      "    @TwoDaysAgo DATETIME2(7);\n",
      "\n",
      "SELECT\n",
      "\t@TwoDaysAgo = DATEADD(DAY, -2, @PostLeapDay);\n",
      "\n",
      "SELECT\n",
      "\t@TwoDaysAgo AS TwoDaysAgo,\n",
      "\t@PostLeapDay AS SomeTime,\n",
      "    -- Fill in the appropriate function and date types\n",
      "\tDateDiff(Day, @TwoDaysAgo, @PostLeapDay) AS DaysDifference,\n",
      "\tDateDiff(Hour, @TwoDaysAgo, @PostLeapDay) AS HoursDifference,\n",
      "\tDateDiff(Minute, @TwoDaysAgo, @PostLeapDay) AS MinutesDifference;\n",
      "\n",
      "   Round to Day, Hour, and Minute\n",
      "\n",
      "SELECT\n",
      "\tDATEADD(DAY, DATEDIFF(DAY, 0, @SomeTime), 0) AS RoundedToDay,\n",
      "\tDATEADD(HOUR, DATEDIFF(HOUR, 0, @SomeTime), 0) AS RoundedToHour,\n",
      "\tDATEADD(MINUTE, DATEDIFF(MINUTE, 0, @SomeTime), 0) AS RoundedToMinute;\n",
      "\n",
      "       formatting functions\n",
      "\n",
      "Cast() : converting one data type to another data type\n",
      "Convert(): converts data types, control on the formatting\n",
      "Format(): more flexibilty\n",
      "\n",
      "\n",
      "declare\n",
      "   @SomeDate Datetime2(3) = '1991-06-04 08:00:09',\n",
      "   @SomeString NVarchar(30)='1991-06-04 08:00:09',\n",
      "   @OldDateTime Datetime = '1991-06-04 08:00:09'\n",
      "\n",
      "select Cast(@SomeDate as nvarchar(30)) DateToString,\n",
      "Cast(@SomeString as datetime2(3)) as StringToDate,\n",
      "Cast(@OldDateTime as nvarchar(30)) as OldDateToString;\n",
      "\n",
      "declare\n",
      "   @SomeDate Datetime2(3) = '1793-02-21 11:13:19.033'\n",
      "\n",
      "select Convert(nvarchar(30), @SomeDate,0) as DefaultForm,\n",
      "Convert(nvarchar(30), @SomeDate,1) as US_mdy,\n",
      "Convert(nvarchar(30), @SomeDate,101) as US_mmdyyyy,\n",
      "Convert(nvarchar(30), @SomeDate,120) as ODBC_sec\n",
      "\n",
      "126 - ISO8601 yyyy-mm-dd hh:mi:ss.mmm\n",
      "\n",
      "declare\n",
      "   @SomeDate Datetime2(3) = '1793-02-21 11:13:19.033'\n",
      "\n",
      "select format(@SomeDate,'d','en-US') as US_d,\n",
      "format(@SomeDate,'d','de-DE') as DE_d,\n",
      "format(@SomeDate,'D','de-DE') as DE_D,\n",
      "format(@SomeDate,'yyyy-MM-dd') as yMd\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "DECLARE\n",
      "\t@CubsWinWorldSeries DATETIME2(3) = '2016-11-03 00:30:29.245',\n",
      "\t@OlderDateType DATETIME = '2016-11-03 00:30:29.245';\n",
      "\n",
      "SELECT\n",
      "\t-- Fill in the missing function calls\n",
      "\tCast(@CubsWinWorldSeries AS DATE) AS CubsWinDateForm,\n",
      "\tCast(@CubsWinWorldSeries AS NVARCHAR(30)) AS CubsWinStringForm,\n",
      "\tCast(@CubsWinWorldSeries AS DATE) AS OlderDateForm,\n",
      "\tCast(@OlderDateType AS NVARCHAR(30)) AS OlderStringForm;\n",
      "\n",
      " > sample\n",
      "\n",
      "DECLARE\n",
      "\t@CubsWinWorldSeries DATETIME2(3) = '2016-11-03 00:30:29.245';\n",
      "\n",
      "SELECT\n",
      "\tcast(cast(@CubsWinWorldSeries AS date) AS NVARCHAR(30)) AS DateStringForm;\n",
      "\n",
      "\n",
      " > sample\n",
      "\n",
      "DECLARE\n",
      "\t@CubsWinWorldSeries DATETIME2(3) = '2016-11-03 00:30:29.245';\n",
      "\n",
      "SELECT\n",
      "\tconvert(Date,@CubsWinWorldSeries) AS CubsWinDateForm,\n",
      "\tconvert(NVARCHAR(30), @CubsWinWorldSeries,101) AS CubsWinStringForm;\n",
      "\n",
      "\n",
      "CubsWinDateForm\tCubsWinStringForm\n",
      "2016-11-03\t11/03/2016\n",
      "\n",
      "  >sample\n",
      "\n",
      "DECLARE\n",
      "\t@CubsWinWorldSeries DATETIME2(3) = '2016-11-03 00:30:29.245';\n",
      "\n",
      "SELECT\n",
      "\tconvert(NVARCHAR(30), @CubsWinWorldSeries, 0) AS DefaultForm,\n",
      "\tconvert(NVARCHAR(30), @CubsWinWorldSeries, 3) AS UK_dmy,\n",
      "\tconvert(NVARCHAR(30), @CubsWinWorldSeries, 1) AS US_mdy,\n",
      "\tconvert(NVARCHAR(30), @CubsWinWorldSeries, 103) AS UK_dmyyyy,\n",
      "\tconvert(NVARCHAR(30), @CubsWinWorldSeries, 101) AS US_mdyyyy;\n",
      "\n",
      "DefaultForm\tUK_dmy\tUS_mdy\tUK_dmyyyy\tUS_mdyyyy\n",
      "Nov  3 2016 12:30AM\t03/11/16\t11/03/16\t03/11/2016\t11/03/2016\n",
      "\n",
      "  > sample  > culture specific\n",
      "\n",
      "DECLARE\n",
      "\t@Python3ReleaseDate DATETIME2(3) = '2008-12-03 19:45:00.033';\n",
      "\n",
      "SELECT\n",
      "\t-- Fill in the function call and format parameter\n",
      "\tformat(@Python3ReleaseDate, 'd', 'en-US') AS US_d,\n",
      "\tformat(@Python3ReleaseDate, 'd', 'de-DE') AS DE_d,\n",
      "\t-- Fill in the locale for Japan\n",
      "\tformat(@Python3ReleaseDate, 'd', 'jp-JP') AS JP_d,\n",
      "\tformat(@Python3ReleaseDate, 'd', 'zh-cn') AS CN_d;\n",
      "\n",
      "US_d\tDE_d\tJP_d\tCN_d\n",
      "12/3/2008\t03.12.2008\t12/03/2008\t2008/12/3\n",
      "\n",
      "DECLARE\n",
      "\t@Python3ReleaseDate DATETIME2(3) = '2008-12-03 19:45:00.033';\n",
      "\n",
      "SELECT\n",
      "\t-- Fill in the format parameter\n",
      "\tFORMAT(@Python3ReleaseDate, 'D', 'en-US') AS US_D,\n",
      "\tFORMAT(@Python3ReleaseDate, 'D', 'de-DE') AS DE_D,\n",
      "\t-- Fill in the locale for Indonesia\n",
      "\tFORMAT(@Python3ReleaseDate, 'D', 'id-ID') AS ID_D,\n",
      "\tFORMAT(@Python3ReleaseDate, 'D', 'zh-cn') AS CN_D;\n",
      "\n",
      "US_D\tDE_D\tID_D\tCN_D\n",
      "Wednesday, December 3, 2008\tMittwoch, 3. Dezember 2008\tRabu, 03 Desember 2008\t2008?12?3?\n",
      "\n",
      "\n",
      "DECLARE\n",
      "\t@Python3ReleaseDate DATETIME2(3) = '2008-12-03 19:45:00.033';\n",
      "    \n",
      "SELECT\n",
      "\t-- 20081203\n",
      "\tFORMAT(@Python3ReleaseDate, 'yyyyMMdd') AS F1,\n",
      "\t-- 2008-12-03\n",
      "\tFORMAT(@Python3ReleaseDate, 'yyyy-MM-dd') AS F2,\n",
      "\t-- Dec 03+2008 (the + is just a \"+\" character)\n",
      "\tFORMAT(@Python3ReleaseDate, 'MMM dd+yyyy') AS F3,\n",
      "\t-- 12 08 03 (month, two-digit year, day)\n",
      "\tFORMAT(@Python3ReleaseDate, 'MM yy dd') AS F4,\n",
      "\t-- 03 07:45 2008.00\n",
      "    -- (day hour:minute year.second)\n",
      "\tFORMAT(@Python3ReleaseDate, 'dd hh:mm yyyy.ss') AS F5;\n",
      "\n",
      "F1\tF2\tF3\tF4\tF5\n",
      "20081203\t2008-12-03\tDec 03+2008\t12 00 08\t03 07:45 2008.00\n",
      "\n",
      "\n",
      "     > calendar tables\n",
      "\n",
      "DateKey\n",
      "Date\n",
      "Day\n",
      "DayOfWeek\n",
      "DayName\n",
      "IsAWeekEnd\n",
      "FiscalYear\n",
      "CalendarYear\n",
      "CalendarMonth\n",
      "CalendarQuarter\n",
      "\n",
      "Holiday Name\n",
      "Lunar details\n",
      "ISO week of year\n",
      "\n",
      "calendar tables simplify queries\n",
      "\n",
      "    apply()\n",
      "\n",
      "executes on function on each row in the resultset\n",
      "\n",
      "  > sample\n",
      "\n",
      "SELECT\n",
      "\tc.Date\n",
      "FROM dbo.Calendar c\n",
      "WHERE\n",
      "\tc.MonthName = 'December'\n",
      "\tAND c.DayName = 'Tuesday'\n",
      "\tAND c.CalendarYear BETWEEN 2008 AND 2010\n",
      "ORDER BY\n",
      "\tc.Date;\n",
      "\n",
      " > sample\n",
      "\n",
      "SELECT\n",
      "\tc.Date\n",
      "FROM dbo.Calendar c\n",
      "WHERE\n",
      "    -- Instead of month, use the fiscal week\n",
      "\tc.FiscalWeekOfYear = 29\n",
      "    -- Instead of calendar year, use fiscal year\n",
      "\tAND c.FiscalYear = 2019\n",
      "ORDER BY\n",
      "\tc.Date ASC;\n",
      "\n",
      "   sample\n",
      "\n",
      "SELECT\n",
      "\tir.IncidentDate,\n",
      "\tc.FiscalDayOfYear,\n",
      "\tc.FiscalWeekOfYear\n",
      "FROM dbo.IncidentRollup ir\n",
      "\tINNER JOIN dbo.Calendar c\n",
      "\t\tON ir.IncidentDate = c.Date\n",
      "WHERE\n",
      "    -- Incident type 3\n",
      "\tir.IncidentTypeID = 3\n",
      "    -- Fiscal year 2019\n",
      "\tAND c.FiscalYear = 2019\n",
      "    -- Fiscal quarter 3\n",
      "\tAND c.FiscalQuarter = 3;\n",
      "\n",
      "   sample\n",
      "\n",
      "SELECT\n",
      "\tir.IncidentDate,\n",
      "\tc.FiscalDayOfYear,\n",
      "\tc.FiscalWeekOfYear\n",
      "FROM dbo.IncidentRollup ir\n",
      "\tINNER JOIN dbo.Calendar c\n",
      "\t\tON ir.IncidentDate = c.Date\n",
      "WHERE\n",
      "    -- Incident type 4\n",
      "\tir.IncidentTypeID = 4\n",
      "    -- Fiscal year 2019\n",
      "\tAND c.FiscalYear = 2019\n",
      "    -- Beyond fiscal week of year 30\n",
      "\tAND c.FiscalWeekOfYear >30\n",
      "    -- Only return weekends\n",
      "\tAND c.IsWeekEnd = 1;\n",
      "\n",
      "\n",
      "     >Dates from parts\n",
      "\n",
      "DateFromParts(year,month,day)\n",
      "TimeFromParts(hours,minute,second, fraction, precision)\n",
      "DateTimeFromParts(Year,month,day, hour, minute, second, ms)\n",
      "DateTime2FromParts(Year,month,day,hour,minute,second,fraction,precision)\n",
      "SmallDateTimeFromParts(year,month,day,hour,minute)\n",
      "DateTimeOffsetFromParts(year,month,day,hour,minute, second, fraction, hour_offset, minute_offset, precision)\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "select \n",
      "DateTimeFromParts(1918,11,11,05,45,17,995) as DT,\n",
      "DateTime2FromParts(1918,11,11,05,45,17,0,0) as DT20,\n",
      "DateTime2FromParts(1918,11,11,05,45,17,995,3) as DT23,\n",
      "DateTime2FromParts(1918,11,11,05,45,17,9951234,7) as DT27v\n",
      "\n",
      "\n",
      "india 5 hours and 30 minutes ahead\n",
      "\n",
      "\n",
      "select \n",
      "DateTimeOffsetFromParts(2009,08,14,21,00,00,0,5,30,0) IndiaTimezone,\n",
      "DateTimeOffsetFromParts(2009,08,14,21,00,00,0,5,30,0) at time zone 'utc' as utc\n",
      "\n",
      "IndiaTimezone\tutc\n",
      "2009-08-14 21:00:00 +05:30\t2009-08-14 15:30:00 +00:00\n",
      "\n",
      "   sample\n",
      "\n",
      "-- Create dates from component parts on the calendar table\n",
      "SELECT TOP(10)\n",
      "\tDATEFROMPARTS(c.CalendarYear, c.CalendarMonth, c.Day) AS CalendarDate\n",
      "FROM dbo.Calendar c\n",
      "WHERE\n",
      "\tc.CalendarYear = 2017\n",
      "ORDER BY\n",
      "\tc.FiscalDayOfYear ASC;\n",
      "\n",
      "It is important to note that using DATEFROMPARTS() in the WHERE clause like this can lead to performance problems with large data sets because we will need to check every row to determine if the output of DATEFROMPARTS() matches our search argument.\n",
      "\n",
      "SELECT\n",
      "\t-- Mark the date and time the lunar module touched down\n",
      "    -- Use 24-hour notation for hours, so e.g., 9 PM is 21\n",
      "\tDATETIME2FROMPARTS(1969, 7, 20, 20, 17, 00, 000, 0) AS TheEagleHasLanded,\n",
      "\t-- Mark the date and time the lunar module took back off\n",
      "    -- Use 24-hour notation for hours, so e.g., 9 PM is 21\n",
      "\tDATETIMEFROMPARTS(1969, 7, 21, 18, 54, 00, 000) AS MoonDeparture;\n",
      "\n",
      "TheEagleHasLanded\tMoonDeparture\n",
      "1969-07-20 20:17:00\t1969-07-21 18:54:00\n",
      "\n",
      "\n",
      "SELECT\n",
      "\t-- Fill in the millisecond PRIOR TO chaos\n",
      "\tDATETIMEOFFSETFROMPARTS(2038, 01, 19, 03, 14, 07, 999, 0, 0, 3) AS LastMoment,\n",
      "    -- Fill in the date and time when we will experience the Y2.038K problem\n",
      "    -- Then convert to the Eastern Standard Time time zone\n",
      "\tDATETIMEOFFSETFROMPARTS(2038, 01, 19, 03, 14, 08, 0, 0, 0, 3) AT TIME ZONE 'Eastern Standard Time' AS TimeForChaos;\n",
      "\n",
      "\n",
      "LastMoment\tTimeForChaos\n",
      "2038-01-19 03:14:07.9990000 +00:00\t2038-01-18 22:14:08.0000000 -05:00\n",
      "\n",
      "\n",
      "    >translating date string\n",
      "\n",
      "Cast string to datetimes\n",
      "\n",
      "select cast('09/14/99' as date) as usdate,\n",
      "convert(datetime2(3), 'April 4, 2019 11:52:29.998 PM') as April4\n",
      "\n",
      "select parse('25 Dezember 2014' as date using 'de-de') as Weihnachten\n",
      "\n",
      "\n",
      "     Setting language\n",
      "\n",
      "set language='FRENCH'\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "SELECT\n",
      "\td.DateText AS String,\n",
      "\t-- Cast as DATE\n",
      "\tcast(d.DateText AS Date) AS StringAsDate,\n",
      "\t-- Cast as DATETIME2(7)\n",
      "\tcast(d.DateText AS DateTime2(7)) AS StringAsDateTime2\n",
      "FROM dbo.Dates d;\n",
      "\n",
      "String\tStringAsDate\tStringAsDateTime2\n",
      "2019-04-01 18:08:19.290\t2019-04-01\t2019-04-01 18:08:19.290000\n",
      "2019-04-07 06:14:30\t2019-04-07\t2019-04-07 06:14:3\n",
      "\n",
      "\n",
      "SET LANGUAGE 'GERMAN'\n",
      "\n",
      "SELECT\n",
      "\td.DateText AS String,\n",
      "\t-- Convert to DATE\n",
      "\tConvert(Date, d.DateText) AS StringAsDate,\n",
      "\t-- Convert to DATETIME2(7)\n",
      "\tConvert(DateTime2(7), d.DateText) AS StringAsDateTime2\n",
      "FROM dbo.Dates d;\n",
      "\n",
      "String\tStringAsDate\tStringAsDateTime2\n",
      "2019-04-01 18:08:19.290\t2019-04-01\t2019-04-01 18:08:19.290000\n",
      "2019-04-07 06:14:30\t2019-04-07\t2019-04-07 06:14:30\n",
      "\n",
      "\n",
      "SELECT\n",
      "\td.DateText AS String,\n",
      "\t-- Parse as DATE using German\n",
      "\tParse(d.DateText AS DATE USING 'de-de') AS StringAsDate,\n",
      "\t-- Parse as DATETIME2(7) using German\n",
      "\tParse(d.DateText AS DATETIME2(7) USING 'de-de') AS StringAsDateTime2\n",
      "FROM dbo.Dates d;\n",
      "\n",
      "String\tStringAsDate\tStringAsDateTime2\n",
      "2019-04-01 18:08:19.290\t2019-04-01\t2019-04-01 18:08:19.290000\n",
      "2019-04-07 06:14:30\t2019-04-07\t2019-04-07 06:14:30\n",
      "\n",
      "\n",
      "       >Working with Offsets\n",
      "\n",
      "\n",
      "datetimeoffset\n",
      "(date, time, utc offset)\n",
      "\n",
      "declare @SomeDate DateTimeOffset='2019-04-10 12:59:02.3908505 -04:00'\n",
      "\n",
      "select \n",
      "@SomeDate DT4, \n",
      "SwitchOffset(@SomeDate,'-07:00') as LATime\n",
      "\n",
      "\n",
      "declare @SomeDate datetime2(3)='2019-04-10 12:59:02.390'\n",
      "\n",
      "select ToDateTimeOffset(@SomeDate,'-04:00') as EDT\n",
      "\n",
      "EDT\n",
      "2019-04-10 12:59:02.390 -04:00\n",
      "\n",
      "\n",
      "move from an offset of -5 to an offset of 7\n",
      "\n",
      "\n",
      "declare @SomeDate datetime2(3)='2016-09-04 02:28:29.681'\n",
      "select\n",
      "ToDateTimeOffset(DateAdd(Hour,7,@SomeDate),'+02:00') as bonntime\n",
      "\n",
      "bonntime\n",
      "2016-09-04 09:28:29.681 +02:00\n",
      "\n",
      "  > sample\n",
      "\n",
      "DECLARE\n",
      "\t@OlympicsUTC NVARCHAR(50) = N'2016-08-08 23:00:00';\n",
      "\n",
      "SELECT\n",
      "\t-- Fill in the time zone for Brasilia, Brazil\n",
      "\tToDateTimeOffset(@OlympicsUTC, '-03:00') AS BrasiliaTime,\n",
      "\t-- Fill in the time zone for Chicago, Illinois\n",
      "\tToDateTimeOffset(@OlympicsUTC, '-05:00') AS ChicagoTime,\n",
      "\t-- Fill in the time zone for New Delhi, India\n",
      "\tToDateTimeOffset(@OlympicsUTC, '+05:30') AS NewDelhiTime;\n",
      "\n",
      "\n",
      "select tzi.name,\n",
      "\ttzi.current_utc_offset,\n",
      "\ttzi.is_currently_dst\n",
      "\tfrom sys.time_zone_info tzi\n",
      "\twhere tzi.name like '%Time Zone%'\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "DECLARE\n",
      "\t@OlympicsUTC NVARCHAR(50) = N'2016-08-08 23:00:00';\n",
      "\n",
      "SELECT\n",
      "\t-- Fill in the time zone for Brasilia, Brazil\n",
      "\tswitchoffset(@OlympicsUTC, '-03:00') AS BrasiliaTime,\n",
      "\t-- Fill in the time zone for Chicago, Illinois\n",
      "\tswitchoffset(@OlympicsUTC, '-05:00') AS ChicagoTime,\n",
      "\t-- Fill in the time zone for New Delhi, India\n",
      "\tswitchoffset(@OlympicsUTC, '+05:30') AS NewDelhiTime;\n",
      "\n",
      "\n",
      "BrasiliaTime\tChicagoTime\tNewDelhiTime\n",
      "2016-08-08 20:00:00.0000000 -03:00\t2016-08-08 18:00:00.0000000 -05:00\t2016-08-09 04:30:00.0000000 +05:30\n",
      "\n",
      "\n",
      "DECLARE\n",
      "\t@OlympicsClosingUTC DATETIME2(0) = '2016-08-21 23:00:00';\n",
      "\n",
      "SELECT\n",
      "\t-- Fill in 7 hours back and a '-07:00' offset\n",
      "\tToDateTimeOffset(DATEADD(HOUR, -7, @OlympicsClosingUTC), '-07:00') AS PhoenixTime,\n",
      "\t-- Fill in 12 hours forward and a '+12:00' offset.\n",
      "\tToDateTimeOffset(DATEADD(HOUR, 12, @OlympicsClosingUTC), '+12:00') AS TuvaluTime;\n",
      "\n",
      "PhoenixTime\tTuvaluTime\n",
      "2016-08-21 16:00:00.0000000 -07:00\t2016-08-22 11:00:00.0000000 +12:00\n",
      "\n",
      "       handling invalid dates\n",
      "\n",
      "safe functions\n",
      "Try_Cast\n",
      "Try_Convert\n",
      "Try_Parse\n",
      "\n",
      "invalid dates are converted to null\n",
      "\n",
      " > sample\n",
      "\n",
      "DECLARE\n",
      "\t@GoodDateINTL NVARCHAR(30) = '2019-03-01 18:23:27.920',\n",
      "\t@GoodDateDE NVARCHAR(30) = '13.4.2019',\n",
      "\t@GoodDateUS NVARCHAR(30) = '4/13/2019',\n",
      "\t@BadDate NVARCHAR(30) = N'SOME BAD DATE';\n",
      "\n",
      "-- The prior solution using TRY_CONVERT\n",
      "SELECT\n",
      "\tTRY_CONVERT(DATETIME2(3), @GoodDateINTL) AS GoodDateINTL,\n",
      "\tTRY_CONVERT(DATE, @GoodDateDE) AS GoodDateDE,\n",
      "\tTRY_CONVERT(DATE, @GoodDateUS) AS GoodDateUS,\n",
      "\tTRY_CONVERT(DATETIME2(3), @BadDate) AS BadDate;\n",
      "\n",
      "SELECT\n",
      "\t-- Fill in the correct data type based on our input\n",
      "\tTry_cast(@GoodDateINTL AS DATETIME2(3)) AS GoodDateINTL,\n",
      "    -- Be sure to match these data types with the\n",
      "    -- TRY_CONVERT() examples above!\n",
      "\ttry_cast(@GoodDateDE AS DATE) AS GoodDateDE,\n",
      "\ttry_cast(@GoodDateUS AS date) AS GoodDateUS,\n",
      "\ttry_cast(@BadDate AS Date) AS BadDate;\n",
      "\n",
      "SELECT\n",
      "\ttry_parse(@GoodDateINTL AS DATETIME2(3)) AS GoodDateINTL,\n",
      "    -- Fill in the correct region based on our input\n",
      "    -- Be sure to match these data types with the\n",
      "    -- TRY_CAST() examples above!\n",
      "\ttry_parse(@GoodDateDE AS date USING 'de-de') AS GoodDateDE,\n",
      "\ttry_parse(@GoodDateUS AS date USING 'en-us') AS GoodDateUS,\n",
      "    -- TRY_PARSE can't fix completely invalid dates\n",
      "\ttry_parse(@BadDate AS DATETIME2(3) USING 'sk-sk') AS BadDate;\n",
      "\n",
      "GoodDateINTL\tGoodDateDE\tGoodDateUS\tBadDate\n",
      "2019-03-01 18:23:27.920000\t2019-04-13\t2019-04-13\tnull\n",
      "\n",
      "\n",
      "   TIME ZONE\n",
      "\n",
      "WITH EventDates AS\n",
      "(\n",
      "    SELECT\n",
      "        -- Fill in the missing try-conversion function\n",
      "        TRY_CONVERT(DATETIME2(3), it.EventDate) AT TIME ZONE it.TimeZone AS EventDateOffset,\n",
      "        it.TimeZone\n",
      "    FROM dbo.ImportedTime it\n",
      "        INNER JOIN sys.time_zone_info tzi\n",
      "\t\t\tON it.TimeZone = tzi.name\n",
      ")\n",
      "SELECT\n",
      "    -- Fill in the approppriate event date to convert\n",
      "\tCONVERT(NVARCHAR(50), ed.EventDateOffset) AS EventDateOffsetString,\n",
      "\tCONVERT(DATETIME2(0), ed.EventDateOffset) AS EventDateLocal,\n",
      "\ted.TimeZone,\n",
      "    -- Convert from a DATETIMEOFFSET to DATETIME at UTC\n",
      "\tCAST(ed.EventDateOffset AT TIME ZONE 'UTC' AS DATETIME2(0)) AS EventDateUTC,\n",
      "    -- Convert from a DATETIMEOFFSET to DATETIME with time zone\n",
      "\tCAST(ed.EventDateOffset AT TIME ZONE 'US Eastern Standard Time'  AS DATETIME2(0)) AS EventDateUSEast\n",
      "FROM EventDates ed;\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "-- Try out how fast the TRY_CONVERT() function is\n",
      "-- by try-converting each DateText value to DATE\n",
      "DECLARE @StartTimeConvert DATETIME2(7) = SYSUTCDATETIME();\n",
      "SELECT try_convert(date,DateText) AS TestDate FROM #DateText;\n",
      "DECLARE @EndTimeConvert DATETIME2(7) = SYSUTCDATETIME();\n",
      "\n",
      "-- Determine how much time the conversion took by\n",
      "-- calculating the difference from start time to end time\n",
      "SELECT\n",
      "    DATEDIFF(MILLISECOND, @StartTimeConvert, @EndTimeConvert) AS ExecutionTimeConvert;\n",
      "\n",
      "\n",
      "-- Try out how fast the TRY_PARSE() function is\n",
      "-- by try-parsing each DateText value to DATE\n",
      "DECLARE @StartTimeParse DATETIME2(7) = SYSUTCDATETIME();\n",
      "SELECT try_parse(DateText as datetime2(3) using 'en-us') AS TestDate FROM #DateText;\n",
      "DECLARE @EndTimeParse DATETIME2(7) = SYSUTCDATETIME();\n",
      "\n",
      "-- Determine how much time the conversion took by\n",
      "-- calculating the difference from start time to end time\n",
      "SELECT\n",
      "    DATEDIFF(MILLISECOND, @StartTimeParse, @EndTimeParse) AS ExecutionTimeParse;\n",
      "\n",
      "\n",
      "    >Aggregation    \n",
      "\n",
      "Count \n",
      "Count_Big()\n",
      "Count(Distinct)\n",
      "\n",
      "\n",
      "Sum()\n",
      "Min()\n",
      "Max()\n",
      "\n",
      "\n",
      "Count(d.YR)\n",
      "#a count of rows where the value if not null\n",
      "\n",
      "count(nullif(d.YR,1990))\n",
      "#if the year is 1990 then null then count the non null rows\n",
      "where yr is not null\n",
      "\n",
      "select count(distinct c.CalendarYear) as Years,\n",
      "Count(Distinct nullif(c.CalendarYear,2010)) as Y2\n",
      "from dbo.Calendar c\n",
      "\n",
      "  > pivoting using max and case\n",
      "\n",
      "select\n",
      "\tmax(case when ir.IncidentTypeID=1\n",
      "\tthen ir.IncidentDate else null) as I1,\n",
      "\tmax(case when ir.IncidentTypeID=2\n",
      "\tthen ir.IncidentDate else null) as I2\n",
      "from dbo.IncidentRollup ir\n",
      "\n",
      "\n",
      "   sample \n",
      "\n",
      "-- Fill in the appropriate aggregate functions\n",
      "SELECT\n",
      "\tit.IncidentType,\n",
      "\tcount(1) AS NumberOfRows,\n",
      "\tsum(ir.NumberOfIncidents) AS TotalNumberOfIncidents,\n",
      "\tmin(ir.NumberOfIncidents) AS MinNumberOfIncidents,\n",
      "\tmax(ir.NumberOfIncidents) AS MaxNumberOfIncidents,\n",
      "\tmin(ir.IncidentDate) As MinIncidentDate,\n",
      "\tmax(ir.IncidentDate) AS MaxIncidentDate\n",
      "FROM dbo.IncidentRollup ir\n",
      "\tINNER JOIN dbo.IncidentType it\n",
      "\t\tON ir.IncidentTypeID = it.IncidentTypeID\n",
      "WHERE\n",
      "\tir.IncidentDate BETWEEN '2019-08-01' AND '2019-10-31'\n",
      "GROUP BY\n",
      "\tit.IncidentType;\n",
      "\n",
      "-- Fill in the functions and columns\n",
      "SELECT\n",
      "\tcount(distinct ir.IncidentTypeID) AS NumberOfIncidentTypes,\n",
      "\tcount(distinct ir.IncidentDate) AS NumberOfDaysWithIncidents\n",
      "FROM dbo.IncidentRollup ir\n",
      "WHERE\n",
      "ir.IncidentDate BETWEEN '2019-08-01' AND '2019-10-31';\n",
      "\n",
      "\n",
      "SELECT\n",
      "\tit.IncidentType,\n",
      "    -- Fill in the appropriate expression\n",
      "\tSUM(case WHEN ir.NumberOfIncidents > 5 THEN 1 ELSE 0 end) AS NumberOfBigIncidentDays,\n",
      "    -- Number of incidents will always be at least 1, so\n",
      "    -- no need to check the minimum value, just that it's\n",
      "    -- less than or equal to 5\n",
      "    SUM(case WHEN ir.NumberOfIncidents <= 5 THEN 1 ELSE 0 end) AS NumberOfSmallIncidentDays\n",
      "FROM dbo.IncidentRollup ir\n",
      "\tINNER JOIN dbo.IncidentType it\n",
      "\t\tON ir.IncidentTypeID = it.IncidentTypeID\n",
      "WHERE\n",
      "\tir.IncidentDate BETWEEN '2019-08-01' AND '2019-10-31'\n",
      "GROUP BY\n",
      "it.IncidentType;\n",
      "\n",
      "  > statistical aggregate functions\n",
      "\n",
      "Avg() Mean\n",
      "Stdev() Standard Deviation\n",
      "stdevp() Population Standard Deviation\n",
      "\n",
      "var() variance\n",
      "varp() population variance\n",
      "\n",
      "\n",
      " > median\n",
      "select top(1)\n",
      "\tpercentile_cont(0.5)\n",
      "\t\twithin group (order by l.someval desc)\n",
      "\tover() as medianIncidents\n",
      "from dbo.LargeTable l\n",
      "\n",
      "\n",
      "SELECT\n",
      "CustomerName,\n",
      "percentile_cont(0.5) \n",
      "within group(order by Amount_Paid)\n",
      "over(partition by CustomerName ) as medianPaid\n",
      "  FROM [ReportData].[dbo].[fARCashReceipts]\n",
      "\n",
      "over allows use to partition our data and get a window\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "-- Fill in the missing function names\n",
      "SELECT\n",
      "\tit.IncidentType,\n",
      "\tavg(ir.NumberOfIncidents) AS MeanNumberOfIncidents,\n",
      "\tavg(CAST(ir.NumberOfIncidents AS DECIMAL(4,2))) AS MeanNumberOfIncidents,\n",
      "\tstdev(ir.NumberOfIncidents) AS NumberOfIncidentsStandardDeviation,\n",
      "\tvar(ir.NumberOfIncidents) AS NumberOfIncidentsVariance,\n",
      "\tcount(1) AS NumberOfRows\n",
      "FROM dbo.IncidentRollup ir\n",
      "\tINNER JOIN dbo.IncidentType it\n",
      "\t\tON ir.IncidentTypeID = it.IncidentTypeID\n",
      "\tINNER JOIN dbo.Calendar c\n",
      "\t\tON ir.IncidentDate = c.Date\n",
      "WHERE\n",
      "\tc.CalendarQuarter = 2\n",
      "\tAND c.CalendarYear = 2020\n",
      "GROUP BY\n",
      "it.IncidentType;\n",
      "\n",
      "SELECT DISTINCT\n",
      "\tit.IncidentType,\n",
      "\tAVG(CAST(ir.NumberOfIncidents AS DECIMAL(4,2)))\n",
      "\t    OVER(PARTITION BY it.IncidentType) AS MeanNumberOfIncidents,\n",
      "    --- Fill in the missing value\n",
      "\tPERCENTILE_CONT(0.5)\n",
      "    \t-- Inside our group, order by number of incidents DESC\n",
      "    \tWITHIN GROUP (ORDER BY ir.NumberOfIncidents DESC)\n",
      "        -- Do this for each IncidentType value\n",
      "        OVER (PARTITION BY it.IncidentType) AS MedianNumberOfIncidents,\n",
      "\tCOUNT(1) OVER (PARTITION BY it.IncidentType) AS NumberOfRows\n",
      "FROM dbo.IncidentRollup ir\n",
      "\tINNER JOIN dbo.IncidentType it\n",
      "\t\tON ir.IncidentTypeID = it.IncidentTypeID\n",
      "\tINNER JOIN dbo.Calendar c\n",
      "\t\tON ir.IncidentDate = c.Date\n",
      "WHERE\n",
      "\tc.CalendarQuarter = 2\n",
      "\tAND c.CalendarYear = 2020;\n",
      "\n",
      "\n",
      "      >downsampling and upsampling data\n",
      "\n",
      "downsampling is changing to a courser type\n",
      "\n",
      "cast(mydate as date) mydate\n",
      "\n",
      "removing time\n",
      "\n",
      "\n",
      "select\n",
      " dateadd(hour,datediff(hour,0,SomeDate),0) as SomeDate\n",
      "from dbo.SomeTable\n",
      "\n",
      "datediff(hour, 0, '2019-08-11 06:21:16') = 1048470\n",
      "\n",
      "dateadd(hour,1048470,0) = '2019-08-11 06:00:00'\n",
      "\n",
      "\n",
      "      >rollup, cube, group settings\n",
      "\n",
      "rollup works best with hierarchial data\n",
      "\n",
      "\n",
      "\n",
      "select\n",
      "\tt.Month,\n",
      "\tt.Day\n",
      "\tSum(t.Events) as Events\n",
      "from Table\n",
      "Group by\n",
      "\tt.Month,\n",
      "\tt.Day\n",
      "with Rollup\n",
      "order by\n",
      "\tt.Month,\n",
      "\tt.Day\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "select\n",
      "\tt.Month,\n",
      "\tt.Day\n",
      "\tSum(t.Events) as Events\n",
      "from Table\n",
      "Group by\n",
      "\tt.Month,\n",
      "\tt.Day\n",
      "with Cube\n",
      "order by\n",
      "\tt.Month,\n",
      "\tt.Day\n",
      "\n",
      "With Grouping sets we control the level of aggregation\n",
      "\n",
      "select\n",
      "\tt.IncidentType,\n",
      "\tt.Office,\n",
      "\tSum(t.Events) as Events\n",
      "From Table\n",
      "Group by Grouping sets\n",
      "(\n",
      "\t(t.IncidentType, t.Office),\n",
      "\t()  -- grand total\n",
      ")\n",
      "order by \n",
      "\tt.IncidentType,\n",
      "\tt.Office\n",
      "\n",
      "\n",
      "  > sample rollup\n",
      "\n",
      "SELECT\n",
      "\tc.CalendarYear,\n",
      "\tc.CalendarQuarterName,\n",
      "\tc.CalendarMonth,\n",
      "    -- Include the sum of incidents by day over each range\n",
      "\tSum(ir.NumberOfIncidents) AS NumberOfIncidents\n",
      "FROM dbo.IncidentRollup ir\n",
      "\tINNER JOIN dbo.Calendar c\n",
      "\t\tON ir.IncidentDate = c.Date\n",
      "WHERE\n",
      "\tir.IncidentTypeID = 2\n",
      "GROUP BY\n",
      "\t-- GROUP BY needs to include all non-aggregated columns\n",
      "\tc.CalendarYear,\n",
      "\tc.CalendarQuarterName,\n",
      "\tc.CalendarMonth\n",
      "-- Fill in your grouping operator\n",
      "WITH ROLLUP\n",
      "ORDER BY\n",
      "\tc.CalendarYear,\n",
      "\tc.CalendarQuarterName,\n",
      "\tc.CalendarMonth;\n",
      "\n",
      "  >sample cube\n",
      "\n",
      "select\n",
      "DatePart(Quarter,IncidentDate) Quarter,\n",
      "DatePart(Year,IncidentDate) Year,\n",
      "Sum(NumberOfIncidents)\n",
      "from dbo.IncidentRollup\n",
      "group by\n",
      "DatePart(Quarter,IncidentDate) ,\n",
      "DatePart(Year,IncidentDate)\n",
      "with Cube\n",
      "\n",
      "    sample Group sets\n",
      "\n",
      "SELECT\n",
      "\tc.CalendarYear,\n",
      "\tc.CalendarQuarterName,\n",
      "\tc.CalendarMonth,\n",
      "\tSUM(ir.NumberOfIncidents) AS NumberOfIncidents\n",
      "FROM dbo.IncidentRollup ir\n",
      "\tINNER JOIN dbo.Calendar c\n",
      "\t\tON ir.IncidentDate = c.Date\n",
      "WHERE\n",
      "\tir.IncidentTypeID = 2\n",
      "-- Fill in your grouping operator here\n",
      "GROUP BY Grouping sets\n",
      "(\n",
      "  \t-- Group in hierarchical order:  calendar year,\n",
      "    -- calendar quarter name, calendar month\n",
      "\t(c.CalendarYear, c.CalendarQuarterName, c.CalendarMonth),\n",
      "  \t-- Group by calendar year\n",
      "\t(c.CalendarYear),\n",
      "    -- This remains blank; it gives us the grand total\n",
      "\t()\n",
      ")\n",
      "ORDER BY\n",
      "\tc.CalendarYear,\n",
      "\tc.CalendarQuarterName,\n",
      "\tc.CalendarMonth;\n",
      "\n",
      "\n",
      "  >using aggregation over windows\n",
      "\n",
      "\n",
      "Row_Number(): unique ascending value starting from 1\n",
      "Rank(): Ascending integer value starting with 1 but can have ties and can skip numbers\n",
      "Dense_Rank(): ascending value starting from 1 but does not skip numbers even with ties.\n",
      "\n",
      "RunsScore=8,7,7,6,6,3\n",
      "\n",
      "\n",
      "declare @scores as table\n",
      "(\n",
      "RunsScored int\n",
      ")\n",
      "insert into @scores(RunsScored) \n",
      "values(8),(7),(7),(6),(6),(3)\n",
      "\n",
      "select s.RunsScored, \n",
      "Row_Number() over( order by s.RunsScored desc) as rn\n",
      "from @scores s\n",
      "order by s.RunsScored desc\n",
      "\n",
      "\n",
      "select s.RunsScored, \n",
      "Rank() over( order by s.RunsScored desc) as rk,\n",
      "Dense_Rank() over( order by s.RunsScored desc) as drk\n",
      "from @scores s\n",
      "order by s.RunsScored desc\n",
      "\n",
      "RunsScored\trk\tdrk\n",
      "8\t1\t1\n",
      "7\t2\t2\n",
      "7\t2\t2\n",
      "6\t4\t3\n",
      "6\t4\t3\n",
      "3\t6\t4\n",
      "\n",
      "declare @scores as table\n",
      "(\n",
      "Team varchar(3),\n",
      "RunsScored int\n",
      ")\n",
      "insert into @scores(Team,RunsScored) \n",
      "values('AZ',8),('FLA',7),('FLA',7),('AZ',6),('FLA',  6),('AZ',3)\n",
      "\n",
      "select s.RunsScored, \n",
      "Rank() over( order by s.RunsScored desc) as rk,\n",
      "Dense_Rank() over( order by s.RunsScored desc) as drk\n",
      "from @scores s\n",
      "order by s.RunsScored desc\n",
      "\n",
      "select s.Team,\n",
      "s.RunsScored,\n",
      "Row_Number() over(partition by s.Team order by s.RunsScored desc) as rn\n",
      "from @scores s\n",
      "order by s.RunsScored desc\n",
      "\n",
      "Team\tRunsScored\trn\n",
      "AZ\t8\t1\n",
      "FLA\t7\t1\n",
      "FLA\t7\t2\n",
      "FLA\t6\t3\n",
      "AZ\t6\t2\n",
      "AZ\t3\t3\n",
      "\n",
      "   max , min, avg, sum, count\n",
      "\n",
      "select s.Team,\n",
      "s.RunsScored,\n",
      "Max(s.RunsScored) over(partition by s.Team order by s.RunsScored desc) as MaxRuns\n",
      "from @scores s\n",
      "order by s.RunsScored desc\n",
      "\n",
      "Team\tRunsScored\tMaxRuns\n",
      "AZ\t8\t8\n",
      "FLA\t7\t7\n",
      "FLA\t7\t7\n",
      "FLA\t6\t7\n",
      "AZ\t6\t8\n",
      "AZ\t3\t8\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "SELECT\n",
      "\tir.IncidentDate,\n",
      "\tir.NumberOfIncidents,\n",
      "    -- Fill in each window function and ordering\n",
      "    -- Note that all of these are in descending order!\n",
      "\tRow_Number() OVER (Order by ir.NumberOfIncidents desc) AS rownum,\n",
      "\tRank() OVER (order by ir.NumberOfIncidents desc) AS rk,\n",
      "\tDense_Rank() OVER (order by ir.NumberOfIncidents desc) AS dr\n",
      "FROM dbo.IncidentRollup ir\n",
      "WHERE\n",
      "\tir.IncidentTypeID = 3\n",
      "\tAND ir.NumberOfIncidents >= 8\n",
      "ORDER BY\n",
      "\tir.NumberOfIncidents DESC;\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "SELECT\n",
      "\tir.IncidentDate,\n",
      "\tir.NumberOfIncidents,\n",
      "    -- Fill in the correct aggregate functions\n",
      "    -- You do not need to fill in the OVER clause\n",
      "\tSum(ir.NumberOfIncidents) OVER () AS SumOfIncidents,\n",
      "\tMin(ir.NumberOfIncidents) OVER () AS LowestNumberOfIncidents,\n",
      "\tMax(ir.NumberOfIncidents) OVER () AS HighestNumberOfIncidents,\n",
      "\tCount(ir.NumberOfIncidents) OVER () AS CountOfIncidents\n",
      "FROM dbo.IncidentRollup ir\n",
      "WHERE\n",
      "\tir.IncidentDate BETWEEN '2019-07-01' AND '2019-07-31'\n",
      "AND ir.IncidentTypeID = 3;\n",
      "\n",
      "\n",
      "   > calculating running totals and moving averages\n",
      "\n",
      "\n",
      "select\n",
      "\ts.Team\n",
      "\t,s.Game\n",
      "\t,s.RunsScored\n",
      "\t,sum(s.RunsScored) Over( partition by s.Team order by s.Game asc\n",
      "\t\trange between unbounded preceding and current row) as TotalRuns\n",
      "\tfrom #scores s\n",
      "drop table #scores\n",
      "\n",
      "\n",
      "range: specify a range of results and duplicates processed all at once (Unbounded and current row)\n",
      "\n",
      "rows: specify number of rows to include and duplicates processed a row at a time (unbounded, current row, and number of rows)\n",
      "\n",
      "\n",
      "select\n",
      "\ts.Team,\n",
      "\ts.Game,\n",
      "\ts.RunsScored,\n",
      "\tAvg(s.RunsScored) over(\n",
      "\tpartition by s.Team\n",
      "\torder by s.Game asc\n",
      "\trows between 1 preceding\n",
      "\tand current row) as AvgRuns\n",
      "\tfrom #scores\n",
      "\n",
      "\n",
      "Team\tGame\tRunsScored\tAvgRuns\n",
      "AZ\t1\t8\t8\n",
      "AZ\t2\t6\t7\n",
      "AZ\t3\t3\t4\n",
      "FLA\t1\t7\t7\n",
      "FLA\t2\t7\t7\n",
      "FLA\t3\t6\t6\n",
      "\n",
      "\n",
      "   sample    running total\n",
      "\n",
      "SELECT\n",
      "\tir.IncidentDate,\n",
      "\tir.IncidentTypeID,\n",
      "\tir.NumberOfIncidents,\n",
      "    -- Get the total number of incidents\n",
      "\tSum(ir.NumberOfIncidents) OVER (\n",
      "      \t-- Do this for each incident type ID\n",
      "\t\tPARTITION BY ir.IncidentTypeID\n",
      "      \t-- Sort by the incident date\n",
      "\t\tORDER BY ir.IncidentDate\n",
      "\t) AS NumberOfIncidents\n",
      "FROM dbo.IncidentRollup ir\n",
      "\tINNER JOIN dbo.Calendar c\n",
      "\t\tON ir.IncidentDate = c.Date\n",
      "WHERE\n",
      "\tc.CalendarYear = 2019\n",
      "\tAND c.CalendarMonth = 7\n",
      "\tAND ir.IncidentTypeID IN (1, 2)\n",
      "ORDER BY\n",
      "\tir.IncidentTypeID,\n",
      "\tir.IncidentDate;\n",
      "\n",
      "\n",
      "  > sample  > moving average 6 days\n",
      "\n",
      "SELECT\n",
      "\tir.IncidentDate,\n",
      "\tir.IncidentTypeID,\n",
      "\tir.NumberOfIncidents,\n",
      "    -- Fill in the correct window function\n",
      "\tAvg(ir.NumberOfIncidents) OVER (\n",
      "\t\tPARTITION BY ir.IncidentTypeID\n",
      "\t\tORDER BY ir.IncidentDate\n",
      "      \t-- Fill in the three parts of the window frame\n",
      "\t\trows BETWEEN 6 preceding AND current row\n",
      "\t) AS MeanNumberOfIncidents\n",
      "FROM dbo.IncidentRollup ir\n",
      "\tINNER JOIN dbo.Calendar c\n",
      "\t\tON ir.IncidentDate = c.Date\n",
      "WHERE\n",
      "\tc.CalendarYear = 2019\n",
      "\tAND c.CalendarMonth IN (7, 8)\n",
      "\tAND ir.IncidentTypeID = 1\n",
      "ORDER BY\n",
      "\tir.IncidentTypeID,\n",
      "\tir.IncidentDate;\n",
      "\n",
      "\n",
      "   lead and lag\n",
      "\n",
      "past, present and future\n",
      "\n",
      "lag() prior window\n",
      "\n",
      "\n",
      "select \n",
      "Team,\n",
      "Game,\n",
      "RunsScored,\n",
      "Lag(RunsScored) over(partition by s.Team order by Game,RunsScored) PriorRun\n",
      "from #scores s\n",
      "order by Team,Game,RunsScored\n",
      "\n",
      "lead() next windwo\n",
      "\n",
      "select \n",
      "Team,\n",
      "Game,\n",
      "RunsScored,\n",
      "Lead(RunsScored) over(partition by s.Team order by Game,RunsScored) PriorRun\n",
      "from #scores s\n",
      "order by Team,Game,RunsScored\n",
      "\n",
      "\n",
      "select \n",
      "Team,\n",
      "Game,\n",
      "RunsScored,\n",
      "Lag(RunsScored,2) over(partition by s.Team order by Game,RunsScored) PriorRun\n",
      "from #scores s\n",
      "order by Team,Game,RunsScored\n",
      "\n",
      "look two rows back in the partition window\n",
      "\n",
      "\n",
      "lag and lead execute after the where clause\n",
      "\n",
      "common table expression\n",
      "\n",
      "with records as(\n",
      "select Date, Lag(Val,1) Over(Order by Date) as PriorValue,\n",
      "Val\n",
      "from t\n",
      ")\n",
      "select r.Date, r.PriorVal, r.Val from records r\n",
      "where\n",
      "r.Date>'2019-01-02'\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "\n",
      "SELECT\n",
      "\tir.IncidentDate,\n",
      "\tir.IncidentTypeID,\n",
      "    -- Get the prior day's number of incidents\n",
      "\tLag(ir.NumberOfIncidents,1) OVER (\n",
      "      \t-- Partition by incident type ID\n",
      "\t\tPARTITION BY ir.IncidentTypeID\n",
      "      \t-- Order by incident date\n",
      "\t\tORDER BY ir.IncidentDate\n",
      "\t) AS PriorDayIncidents,\n",
      "\tir.NumberOfIncidents AS CurrentDayIncidents,\n",
      "    -- Get the next day's number of incidents\n",
      "\tLead(ir.NumberOfIncidents, 1) OVER (\n",
      "      \t-- Partition by incident type ID\n",
      "\t\tPARTITION BY ir.IncidentTypeID\n",
      "      \t-- Order by incident date\n",
      "\t\tORDER BY ir.IncidentDate\n",
      "\t) AS NextDayIncidents\n",
      "FROM dbo.IncidentRollup ir\n",
      "WHERE\n",
      "\tir.IncidentDate >= '2019-07-02'\n",
      "\tAND ir.IncidentDate <= '2019-07-31'\n",
      "\tAND ir.IncidentTypeID IN (1, 2)\n",
      "ORDER BY\n",
      "\tir.IncidentTypeID,\n",
      "\tir.IncidentDate;\n",
      "\n",
      "\n",
      "\n",
      "SELECT\n",
      "\tir.IncidentDate,\n",
      "\tir.IncidentTypeID,\n",
      "    -- Fill in two periods ago\n",
      "\tLag(ir.IncidentDate, 2) OVER (\n",
      "\t\tPARTITION BY ir.IncidentTypeID\n",
      "\t\tORDER BY ir.IncidentDate\n",
      "\t) AS Trailing2Day,\n",
      "    -- Fill in one period ago\n",
      "\tLag(ir.IncidentDate, 1) OVER (\n",
      "\t\tPARTITION BY ir.IncidentTypeID\n",
      "\t\tORDER BY ir.IncidentDate\n",
      "\t) AS Trailing1Day,\n",
      "\tir.NumberOfIncidents AS CurrentDayIncidents,\n",
      "    -- Fill in next period\n",
      "\tlead(ir.IncidentDate, 1) OVER (\n",
      "\t\tPARTITION BY ir.IncidentTypeID\n",
      "\t\tORDER BY ir.IncidentDate\n",
      "\t) AS NextDay\n",
      "FROM dbo.IncidentRollup ir\n",
      "WHERE\n",
      "\tir.IncidentDate >= '2019-07-01'\n",
      "\tAND ir.IncidentDate <= '2019-07-31'\n",
      "\tAND ir.IncidentTypeID IN (1, 2)\n",
      "ORDER BY\n",
      "\tir.IncidentTypeID,\n",
      "\tir.IncidentDate;\n",
      "\n",
      "   > days since the last incident and days until the next incident\n",
      "\n",
      "\n",
      "SELECT\n",
      "\tir.IncidentDate,\n",
      "\tir.IncidentTypeID,\n",
      "    -- Fill in the days since last incident\n",
      "\tDateDiff(DAY, Lag(ir.IncidentDate, 1) OVER (\n",
      "\t\tPARTITION BY ir.IncidentTypeID\n",
      "\t\tORDER BY ir.IncidentDate\n",
      "\t), ir.IncidentDate) AS DaysSinceLastIncident,\n",
      "    -- Fill in the days until next incident\n",
      "\tDateDiff(DAY, ir.IncidentDate, Lead(ir.IncidentDate, 1) OVER (\n",
      "\t\tPARTITION BY ir.IncidentTypeID\n",
      "\t\tORDER BY ir.IncidentDate\n",
      "\t)) AS DaysUntilNextIncident\n",
      "FROM dbo.IncidentRollup ir\n",
      "WHERE\n",
      "\tir.IncidentDate >= '2019-07-02'\n",
      "\tAND ir.IncidentDate <= '2019-07-31'\n",
      "\tAND ir.IncidentTypeID IN (1, 2)\n",
      "ORDER BY\n",
      "\tir.IncidentTypeID,\n",
      "\tir.IncidentDate;\n",
      "\n",
      "    maximum\n",
      "\n",
      "start time\n",
      "end time\n",
      "products ordered\n",
      "\n",
      "\n",
      "-- This section focuses on entrances:  CustomerVisitStart\n",
      "SELECT\n",
      "\tdsv.CustomerID,\n",
      "\tdsv.CustomerVisitStart AS TimeUTC,\n",
      "\t1 AS EntryCount,\n",
      "    -- We want to know each customer's entrance stream\n",
      "    -- Get a unique, ascending row number\n",
      "\tRow_Number() OVER (\n",
      "      -- Break this out by customer ID\n",
      "      PARTITION BY dsv.customerID\n",
      "      -- Ordered by the customer visit start date\n",
      "      ORDER BY dsv.CustomerVisitStart\n",
      "    ) AS StartOrdinal\n",
      "FROM dbo.DaySpaVisit dsv\n",
      "UNION ALL\n",
      "-- This section focuses on departures:  CustomerVisitEnd\n",
      "SELECT\n",
      "\tdsv.CustomerID,\n",
      "\tdsv.CustomerVisitEnd AS TimeUTC,\n",
      "\t-1 AS EntryCount,\n",
      "\tNULL AS StartOrdinal\n",
      "FROM dbo.DaySpaVisit dsv\n",
      "\n",
      "\n",
      "SELECT s.*,\n",
      "    -- Build a stream of all check-in and check-out events\n",
      "\tRow_Number() OVER (\n",
      "      -- Break this out by customer ID\n",
      "      PARTITION BY s.CustomerID\n",
      "      -- Order by event time and then the start ordinal\n",
      "      -- value (in case of exact time matches)\n",
      "      ORDER BY s.TimeUTC, s.StartOrdinal\n",
      "    ) AS StartOrEndOrdinal\n",
      "FROM #StartStopPoints s;\n",
      "\n",
      "\n",
      "SELECT\n",
      "\ts.CustomerID,\n",
      "\tMAX(2 * s.StartOrdinal - s.StartOrEndOrdinal) AS MaxConcurrentCustomerVisits\n",
      "FROM #StartStopOrder s\n",
      "WHERE s.EntryCount = 1\n",
      "GROUP BY s.CustomerID\n",
      "-- The difference between 2 * start ordinal and the start/end\n",
      "-- ordinal represents the number of concurrent visits\n",
      "HAVING MAX(2 * s.StartOrdinal - s.StartOrEndOrdinal) > 2\n",
      "-- Sort by the largest number of max concurrent customer visits\n",
      "ORDER BY MaxConcurrentCustomerVisits desc;\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\time series visualization.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\time series visualization.txt\n",
      "time series are a fundamental way to store and analyze many types of data\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Read in the file content in a DataFrame called discoveries\n",
      "discoveries = pd.read_csv(url_discoveries)\n",
      "\n",
      "# Display the first five lines of the DataFrame\n",
      "print(discoveries.head())\n",
      "\n",
      "# Print the data type of each column in discoveries\n",
      "print(discoveries.dtypes)\n",
      "\n",
      "df=pd.read_csv('co2-concentration.csv',parse_dates=['Date'],index_col='Date')\n",
      "print(df.columns)\n",
      "plt.clf()\n",
      "fig,ax = plt.subplots(figsize=(12,4))\n",
      "df['CO2'].plot(ax=ax, color='blue',linewidth=3, fontsize=12)\n",
      "plt.style.use('fivethirtyeight')\n",
      "plt.ylabel('CO2',fontsize=16)\n",
      "plt.title('CO2 Levels over time')\n",
      "#plt.style.use('ggplot')\n",
      "plt.show()\n",
      "\n",
      "print(plt.style.available)\n",
      "\n",
      "'Solarize_Light2', '_classic_test_patch', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10'\n",
      "\n",
      "\n",
      "# Import the matplotlib.pyplot sub-module\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Use the ggplot style\n",
      "plt.style.use('ggplot')\n",
      "ax2 = discoveries.plot()\n",
      "\n",
      "# Set the title\n",
      "ax2.set_title('ggplot Style')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Plot a line chart of the discoveries DataFrame using the specified arguments\n",
      "ax = discoveries.plot(color='blue', figsize=(8, 3), linewidth=2, fontsize=6)\n",
      "\n",
      "# Specify the title in your plot\n",
      "ax.set_title('Number of great inventions and scientific discoveries from 1860 to 1959', fontsize=8)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "         stackoverflow\n",
      "\n",
      "\n",
      "\n",
      "using barh to plot different times by start and stop ranges\n",
      "https://stackoverflow.com/questions/50883054/how-to-create-a-historical-timeline-with-python/66739012#66739012\n",
      "\n",
      "event = data_set_adj['EnglishName']\n",
      "begin = data_set_adj['Start']\n",
      "end = data_set_adj['Finish']\n",
      "length =  data_set_adj['Length']\n",
      "dynasty = data_set_adj['Dynasty']\n",
      "dynasty_col = data_set_adj['Dynasty_col']\n",
      "\n",
      "dict_dynasty = dict(zip(dynasty.unique(), range(0,4*len(dynasty.unique()),4)))\n",
      "\n",
      "levels = np.tile([-1.2,1.2, -0.8, 0.8, -0.4, 0.4],\n",
      "                 int(np.ceil(len(begin)/6)))[:len(begin)]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use('ggplot')\n",
      "plt.figure(figsize=(20,10))\n",
      "\n",
      "for x in range(len(dynasty)):   \n",
      "    plt.vlines(begin.iloc[x]+length.iloc[x]/2, dict_dynasty[dynasty.iloc[x]], dict_dynasty[dynasty.iloc[x]]+levels[x], color=\"tab:red\")\n",
      "    plt.barh(dict_dynasty[dynasty.iloc[x]], (end.iloc[x]-begin.iloc[x]), color=dynasty_col.iloc[x], height =0.3 ,left=begin.iloc[x], edgecolor = \"black\", alpha = 0.5)\n",
      "    if x%2==0:\n",
      "        plt.text(begin.iloc[x] + length.iloc[x]/2, \n",
      "                 dict_dynasty[dynasty.iloc[x]]+1.6*levels[x], event.iloc[x], \n",
      "                 ha='center', fontsize = '8')\n",
      "    else:\n",
      "        plt.text(begin.iloc[x] + length.iloc[x]/2, \n",
      "                 dict_dynasty[dynasty.iloc[x]]+1.25*levels[x], event.iloc[x], \n",
      "                 ha='center', fontsize = '8')\n",
      "plt.tick_params(axis='both', which='major', labelsize=15)\n",
      "plt.tick_params(axis='both', which='minor', labelsize=20)\n",
      "plt.title('Chinese Dynasties', fontsize = '25')\n",
      "plt.xlabel('Year', fontsize = '20')\n",
      "ax = plt.gca()\n",
      "ax.axes.yaxis.set_visible(False)\n",
      "plt.xlim(900, 1915)\n",
      "plt.ylim(-4,28)\n",
      "\n",
      "\n",
      "  > pair plots\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_theme('notebook', style='dark')\n",
      "plt.style.use(\"dark_background\")\n",
      "df = sns.load_dataset('iris')\n",
      "g = sns.PairGrid(df)\n",
      "g.map_upper(sns.scatterplot, color='crimson')\n",
      "g.map_lower(sns.scatterplot, color='limegreen')\n",
      "g.map_diag(plt.hist, color='skyblue')\n",
      "plt.show()\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "            customizing\n",
      "\n",
      "slicing time series data\n",
      "\n",
      "discoveries['1960':'1970']\n",
      "discoveries['1950-01':'1950-12']\n",
      "discoveries['1950-01-01:'1950-12-31']\n",
      "\n",
      "df_subset=discoveries['1960':'1970']\n",
      "ax=df_subset.plot(color='blue', fontsize=14)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "ax.axvline(x='1969-01-01',color='red',linestyle='--')\n",
      "ax.axhline(y=100,color='green',linestyle='--')\n",
      "ax.discoveries.plot(color='blue')\n",
      "ax.set_xlabel('Date')\n",
      "ax.set_ylabel('Number of great discoveries')\n",
      "ax.axvline('1969-01-01', color='red', linestyle='--')\n",
      "ax.axhline(4,color='green', linestyle='--')\n",
      "\n",
      "ax.axvspan('1964-01-01','1968-01-01', color='red', alpha=0.5)\n",
      "\n",
      "ax.axhspan(8,6, color='green', alpha=0.2)\n",
      "c\n",
      "\n",
      " > sample\n",
      "\n",
      "# Select the subset of data between 1945 and 1950\n",
      "discoveries_subset_1 = discoveries['1945':'1950']\n",
      "\n",
      "# Plot the time series in your DataFrame as a blue area chart\n",
      "ax = discoveries_subset_1.plot(color='blue', fontsize=15)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "# Select the subset of data between 1939 and 1958\n",
      "discoveries_subset_2 =discoveries['1939':'1958']\n",
      "\n",
      "# Plot the time series in your DataFrame as a blue area chart\n",
      "ax = discoveries_subset_2.plot(color='blue', fontsize=15)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "# Select the subset of data between 1939 and 1958\n",
      "discoveries_subset_2 =discoveries['1939':'1958']\n",
      "\n",
      "# Plot the time series in your DataFrame as a blue area chart\n",
      "ax = discoveries_subset_2.plot(color='blue', fontsize=15)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Plot your the discoveries time series\n",
      "ax = discoveries.plot(color='blue', fontsize=6)\n",
      "\n",
      "# Add a vertical red shaded region\n",
      "ax.axvspan('1900-01-01', '1915-01-01', color='red', alpha=0.3)\n",
      "\n",
      "# Add a horizontal green shaded region\n",
      "ax.axhspan(6, 8, color='green', alpha=0.3)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "      > cleaning your data\n",
      "\n",
      "finding missing values in a dataframe\n",
      "print(df.isnull)\n",
      "print(df.isnull().sum())\n",
      "\n",
      "shows as nan in the graph\n",
      "\n",
      "df-df.fillna(method='bfill') #back fill\n",
      "\n",
      "ffill #forward fill\n",
      "\n",
      "   sample\n",
      "\n",
      "# Display first seven rows of co2_levels\n",
      "print(co2_levels.head(7))\n",
      "\n",
      "datestamp    co2\n",
      "0  1958-03-29  316.1\n",
      "1  1958-04-05  317.3\n",
      "2  1958-04-12  317.6\n",
      "3  1958-04-19  317.5\n",
      "4  1958-04-26  316.4\n",
      "5  1958-05-03  316.9\n",
      "6  1958-05-10    NaN\n",
      "\n",
      "# Set datestamp column as index\n",
      "co2_levels = co2_levels.set_index('datestamp')\n",
      "\n",
      "# Print out the number of missing values\n",
      "print(co2_levels.isnull().sum())\n",
      "\n",
      "co2    59\n",
      "\n",
      "# Impute missing values with the next valid observation\n",
      "co2_levels = co2_levels.fillna(method='bfill')\n",
      "\n",
      "# Print out the number of missing values\n",
      "print(co2_levels.isnull().sum())\n",
      "\n",
      "datestamp    0\n",
      "co2          0\n",
      "\n",
      "\n",
      "        plot aggregates of your data\n",
      "\n",
      "moving average\n",
      "\n",
      "in the field of time series analysis, a moving average can be used fore many purposes: smoothing out short-term fluctuations, removing outliers, highlighting long-term trends or cycles\n",
      "\n",
      "co2_levels_mean = co2_levels.rolling(window=52).mean()\n",
      "\n",
      "ax= co2_levels_mean.plot()\n",
      "ax.set_xlabel('Date')\n",
      "ax.set_ylabel('The values of my Y axis')\n",
      "ax.set_title('52 weeks rolling mean of my time series.)\n",
      "\n",
      "\n",
      "co2_levels_mean = df.rolling(window=52)['CO2'].mean()\n",
      "co2_levels_mean.plot(ax=ax, color='red')\n",
      "\n",
      "\n",
      "co2_levels.index.year\n",
      "co2_levels.index.month\n",
      "\n",
      "index_month = co2_levels.index.month\n",
      "\n",
      "co2_levels_by_month = co2_levels.groupby(index_month).mean()\n",
      "\n",
      "co2_levels_by_month.plot()\n",
      "\n",
      "\n",
      "\n",
      "  > sample   > show 2 std deviations above and below the co2 data\n",
      "\n",
      "# Compute the 52 weeks rolling mean of the co2_levels DataFrame\n",
      "ma = co2_levels.rolling(window=52).mean()\n",
      "\n",
      "# Compute the 52 weeks rolling standard deviation of the co2_levels DataFrame\n",
      "mstd = co2_levels.rolling(window=52).std()\n",
      "\n",
      "# Add the upper bound column to the ma DataFrame\n",
      "ma['upper'] = ma['co2'] + (2 * mstd['co2'])\n",
      "\n",
      "# Add the lower bound column to the ma DataFrame\n",
      "ma['lower'] = ma['co2'] - (2 * mstd['co2'])\n",
      "\n",
      "# Plot the content of the ma DataFrame\n",
      "ax = ma.plot(linewidth=0.8, fontsize=6)\n",
      "\n",
      "# Specify labels, legend, and show the plot\n",
      "ax.set_xlabel('Date', fontsize=10)\n",
      "ax.set_ylabel('CO2 levels in Mauai Hawaii', fontsize=10)\n",
      "ax.set_title('Rolling mean and variance of CO2 levels\\nin Mauai Hawaii from 1958 to 2001', fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Get month for each dates in the index of co2_levels\n",
      "index_month = co2_levels.index.month\n",
      "\n",
      "# Compute the mean CO2 levels for each month of the year\n",
      "mean_co2_levels_by_month = co2_levels.groupby(index_month).mean()\n",
      "\n",
      "# Plot the mean CO2 levels for each month of the year\n",
      "mean_co2_levels_by_month.plot(fontsize=6)\n",
      "\n",
      "# Specify the fontsize on the legend\n",
      "plt.legend(fontsize=10)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     > summarizing the values in your time series data\n",
      "\n",
      "1. what is the average value of this data\n",
      "2. what is the maximum value observed in this time series\n",
      "\n",
      ".describe()\n",
      "\n",
      "df[['CO2','adjusted CO2']].boxplot()\n",
      "\n",
      "helps you visualize the distribution of your data\n",
      "\n",
      "summarizing your data with histograms\n",
      "\n",
      "fig,ax = plt.subplots(figsize=(12,4))\n",
      "df[['CO2','adjusted CO2']].hist(ax=ax)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   density plots\n",
      "\n",
      "df.plot(kind='density')\n",
      "\n",
      "    sample\n",
      "\n",
      "# Print out summary statistics of the co2_levels DataFrame\n",
      "print(co2_levels.describe())\n",
      "\n",
      "# Print out the minima of the co2 column in the co2_levels DataFrame\n",
      "print(co2_levels['co2'].min())\n",
      "\n",
      "# Print out the maxima of the co2 column in the co2_levels DataFrame\n",
      "print(co2_levels['co2'].max())\n",
      "\n",
      "co2\n",
      "count  2284.000000\n",
      "mean    339.657750\n",
      "std      17.100899\n",
      "min     313.000000\n",
      "25%     323.975000\n",
      "50%     337.700000\n",
      "75%     354.500000\n",
      "max     373.900000\n",
      "313.0\n",
      "373.9\n",
      "\n",
      "\n",
      "# Generate a boxplot\n",
      "ax = co2_levels.boxplot()\n",
      "\n",
      "# Set the labels and display the plot\n",
      "ax.set_xlabel('CO2', fontsize=10)\n",
      "ax.set_ylabel('Boxplot CO2 levels in Maui Hawaii', fontsize=10)\n",
      "plt.legend(fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Generate a histogram\n",
      "ax = co2_levels.plot(kind='hist',bins=50, fontsize=6)\n",
      "\n",
      "# Set the labels and display the plot\n",
      "ax.set_xlabel('CO2', fontsize=10)\n",
      "ax.set_ylabel('Histogram of CO2 levels in Maui Hawaii', fontsize=10)\n",
      "plt.legend(fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "# Display density plot of CO2 levels values\n",
      "ax = co2_levels.plot(kind='density', linewidth=4, fontsize=6)\n",
      "\n",
      "# Annotate x-axis labels\n",
      "ax.set_xlabel('CO2', fontsize=10)\n",
      "\n",
      "# Annotate y-axis labels\n",
      "ax.set_ylabel('Density plot of CO2 levels in Maui Hawaii', fontsize=10)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "          autocorrelation and partial autocorrelation\n",
      "\n",
      "autocorrelation is measured as a correlation between a time series and a delayed copy of itself\n",
      "\n",
      "values are lagged by 3 time points\n",
      "\n",
      "it is used to find repetitive patterns or periodic signal in time series\n",
      "\n",
      "autocorrelation can be applied to any signal, not just time series.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics import tsaplots\n",
      "\n",
      "fig = tsaplots.plot_acf(co2_levels['co2'], lags=40)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   > partial autocorrelation in a time series data\n",
      "1. contrary to autocorrelation, partial correlation removes the effect of previous time points.\n",
      "\n",
      "a partial autocorrelation function of order 3 returns the correlation between our time series and the lagged values of itself by 3 time points after removing all effects attributable to lags 1 and 2\n",
      "\n",
      "\n",
      "\n",
      "fig = tsaplots.plot_pacf(co2_levels['co2'], lags=40)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "print('correlations close to 1 or -1 indicate there is strong correlation between the lag time series')\n",
      "\n",
      "beyond the blue areas then the correlations are statistically significant\n",
      "\n",
      "   sample\n",
      "\n",
      "# Import required libraries\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use('fivethirtyeight')\n",
      "from statsmodels.graphics import tsaplots\n",
      "\n",
      "# Display the autocorrelation plot of your time series\n",
      "fig = tsaplots.plot_acf(co2_levels['co2'], lags=40)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "If autocorrelation values are close to 0, then values between consecutive observations are not correlated with one another. Inversely, autocorrelations values close to 1 or -1 indicate that there exists strong positive or negative correlations between consecutive observations, respectively.\n",
      "\n",
      "\n",
      "# Import required libraries\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use('fivethirtyeight')\n",
      "from statsmodels.graphics import tsaplots\n",
      "\n",
      "# Display the partial autocorrelation plot of your time series\n",
      "fig = tsaplots.plot_pacf(co2_levels['co2'], lags=40)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "If partial autocorrelation values are close to 0, then values between observations and lagged observations are not correlated with one another. Inversely, partial autocorrelations with values close to 1 or -1 indicate that there exists strong positive or negative correlations between the lagged observations of the time series.\n",
      "\n",
      "\n",
      "    > seasonality trend and noise\n",
      "\n",
      "1. seasonality: does the data display a clear periodic pattern\n",
      "\n",
      "2. trend: does the data follow a consistent upward or downward slope\n",
      "\n",
      "3. noise: are their outlier points or missing values that are not consistent with the rest of the data.\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from pylab import rcParams\n",
      "\n",
      "rcParams['figure.figsize']=11,9\n",
      "decomposition=sm.tsa.seasonal_decompose(\n",
      "\tco2_levels['co2'])\n",
      "\n",
      "fig=decomposition.plot()\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "returns: residual, seasonal, trend, observed\n",
      "\n",
      "print(dir(decomposition))\n",
      "\n",
      "\n",
      "print(type(df.index))\n",
      "rcParams['figure.figsize']=11,9\n",
      "decomposition=sm.tsa.seasonal_decompose(x=df['CO2'],model='additive', extrapolate_trend='freq', period=1)\n",
      "decomposition.plot()\n",
      "plt.show()\n",
      "\n",
      "decomposition_seasonal=decomposition.seasonal\n",
      "ax= decomposition_seasonal.plot(figsize=(14,2))\n",
      "ax.set_xlabel('Date')\n",
      "ax.set_ylabel('Seasonality of time series')\n",
      "ax.set_title('Seasonal values of the time series')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "# Import statsmodels.api as sm\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Perform time series decompositon\n",
      "decomposition = sm.tsa.seasonal_decompose(co2_levels['co2'])\n",
      "\n",
      "# Print the seasonality component\n",
      "print(decomposition.seasonal)\n",
      "\n",
      "datestamp\n",
      "1958-03-29    1.028042\n",
      "1958-04-05    1.235242\n",
      "1958-04-12    1.412344\n",
      "\n",
      "2001-10-13   -2.351296\n",
      "2001-10-20   -2.072159\n",
      "2001-10-27   -1.802325\n",
      "2001-11-03   -1.509391\n",
      "2001-11-10   -1.284167\n",
      "2001-11-17   -1.024060\n",
      "2001-11-24   -0.791949\n",
      "2001-12-01   -0.525044\n",
      "2001-12-08   -0.392799\n",
      "2001-12-15   -0.134838\n",
      "2001-12-22    0.116056\n",
      "2001-12-29    0.285354\n",
      "\n",
      "\n",
      "# Extract the trend component\n",
      "trend = decomposition.trend\n",
      "\n",
      "# Plot the values of the trend\n",
      "ax = trend.plot(figsize=(12, 6), fontsize=6)\n",
      "\n",
      "# Specify axis labels\n",
      "ax.set_xlabel('Date', fontsize=10)\n",
      "ax.set_title('Seasonal component the CO2 time-series', fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "    > case project - air passengers\n",
      "\n",
      "# Plot the time series in your dataframe\n",
      "ax = airline.plot(color='blue', fontsize=12)\n",
      "\n",
      "# Add a red vertical line at the date 1955-12-01\n",
      "ax.axvline('1955-12-01', color='red', linestyle='--')\n",
      "\n",
      "# Specify the labels in your plot\n",
      "ax.set_xlabel('Date', fontsize=12)\n",
      "ax.set_ylabel('Number of Monthly Airline Passengers', fontsize=12)\n",
      "plt.show()\n",
      "\n",
      "https://www.youtube.com/watch?v=uxxkG4uKThY\n",
      "\n",
      "# Print out the number of missing values\n",
      "print(airline.isnull().sum())\n",
      "\n",
      "# Print out summary statistics of the airline DataFrame\n",
      "print(airline.describe())\n",
      "\n",
      "# Display boxplot of airline values\n",
      "ax = airline.boxplot()\n",
      "\n",
      "# Specify the title of your plot\n",
      "ax.set_title('Boxplot of Monthly Airline\\nPassengers Count', fontsize=20)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Get month for each dates from the index of airline\n",
      "index_month = airline.index.month\n",
      "\n",
      "# Compute the mean number of passengers for each month of the year\n",
      "mean_airline_by_month = airline.groupby(index_month).mean()\n",
      "\n",
      "# Plot the mean number of passengers for each month of the year\n",
      "mean_airline_by_month.plot()\n",
      "plt.legend(fontsize=20)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Import statsmodels.api as sm\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Perform time series decompositon\n",
      "decomposition = sm.tsa.seasonal_decompose(airline)\n",
      "\n",
      "# Extract the trend and seasonal components\n",
      "trend = decomposition.trend\n",
      "seasonal = decomposition.seasonal\n",
      "\n",
      "\n",
      "# Print the first 5 rows of airline_decomposed\n",
      "print(airline_decomposed.head(5))\n",
      "\n",
      "# Plot the values of the airline_decomposed DataFrame\n",
      "ax = airline_decomposed.plot(figsize=(12, 6), fontsize=15)\n",
      "\n",
      "# Specify axis labels\n",
      "ax.set_xlabel('Date', fontsize=15)\n",
      "plt.legend(fontsize=15)\n",
      "plt.show()\n",
      "\n",
      "     working with more than one time series\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use('fivethirtyeight')\n",
      "ax=df.plot.area(figsize=(12,4), fontsize=14)\n",
      "plt.show()\n",
      "\n",
      "    sample\n",
      "\n",
      "# Read in meat DataFrame\n",
      "print(url_meat)\n",
      "meat = pd.read_csv(url_meat)\n",
      "\n",
      "# Review the first five lines of the meat DataFrame\n",
      "print(meat.head(5))\n",
      "\n",
      "# Convert the date column to a datestamp type\n",
      "meat['date'] = pd.to_datetime(meat['date'])\n",
      "\n",
      "# Set the date column as the index of your DataFrame meat\n",
      "meat = meat.set_index('date')\n",
      "\n",
      "# Print the summary statistics of the DataFrame\n",
      "print(meat.describe())\n",
      "\n",
      "# Plot time series dataset\n",
      "ax = meat.plot(linewidth=2,fontsize=12)\n",
      "\n",
      "# Additional customizations\n",
      "ax.set_xlabel('Date')\n",
      "ax.legend(fontsize=15)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "# Plot an area chart\n",
      "ax = meat.plot.area(fontsize=12)\n",
      "\n",
      "# Additional customizations\n",
      "ax.set_xlabel('Date')\n",
      "ax.legend(fontsize=15)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "       Plot multiple time series\n",
      "\n",
      "COLUMNS=['beef', 'veal', 'pork', 'lamb_and_mutton', 'broilers','other_chicken', 'turkey']\n",
      "df_summary=df[COLUMNS].agg(['mean','sum'])\n",
      "\n",
      "plt.clf()\n",
      "ax=df[COLUMNS].plot(colormap='Dark2',figsize=(10,7))\n",
      "\n",
      "ax.table(cellText=df_summary[COLUMNS].values,colWidths=[0.3]*len(df_summary[COLUMNS].columns),\\\n",
      "    rowLabels=df_summary.index,\n",
      "    colLabels=df_summary[COLUMNS].columns,\n",
      "    loc='top')\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "df.plot(subplots=True,\n",
      "\tlinewidth=0.5,\n",
      "\tlayout(2,4),\n",
      "\tfigsize=(16,10),\n",
      "\tsharex=False,\n",
      "\tsharey=False)\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Plot time series dataset using the cubehelix color palette\n",
      "ax = meat.plot(colormap='cubehelix', fontsize=15)\n",
      "\n",
      "# Additional customizations\n",
      "ax.set_xlabel('Date')\n",
      "ax.legend(fontsize=18)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "# Plot time series dataset using the cubehelix color palette\n",
      "ax = meat.plot(colormap='PuOr', fontsize=15)\n",
      "\n",
      "# Additional customizations\n",
      "ax.set_xlabel('Date')\n",
      "ax.legend(fontsize=18)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "   > with cell data\n",
      "\n",
      "# Plot the meat data\n",
      "ax = meat.plot(fontsize=6, linewidth=1)\n",
      "\n",
      "# Add x-axis labels\n",
      "ax.set_xlabel('Date', fontsize=6)\n",
      "\n",
      "# Add summary table information to the plot\n",
      "ax.table(cellText=meat_mean.values,\n",
      "         colWidths = [0.15]*len(meat_mean.columns),\n",
      "         rowLabels=meat_mean.index,\n",
      "         colLabels=meat_mean.columns,\n",
      "         loc='top')\n",
      "\n",
      "# Specify the fontsize and location of your legend\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=3, fontsize=6)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "# Create a facetted graph with 2 rows and 4 columns\n",
      "meat.plot(subplots=True, \n",
      "          layout=(2,4), \n",
      "          sharex=False, \n",
      "          sharey=False, \n",
      "          colormap='viridis', \n",
      "          fontsize=2, \n",
      "          legend=False, \n",
      "          linewidth=0.2)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "       Find relationship between multiple timeseries\n",
      "\n",
      "The correlation coefficient is a measure used to determine the strength or lack of relationship between two variables\n",
      "\n",
      "pearson coefficient can be used to compute the correlation coefficient between variables for which the relationship is thought to be linear\n",
      "\n",
      "kendall tau or spearman rank can be used to compute the correlation coefficient between variables for which the relationship is thought to be non-linear\n",
      "\n",
      "from scipy.stats.stats import pearsonr\n",
      "from scipy.stats.stats import spearmanr\n",
      "from scipy.stats.stats import kendalltau\n",
      "\n",
      "pearsonr(x,y)\n",
      "spearmanr(x,y)\n",
      "kendalltau(x,y)\n",
      "\n",
      "   > correlation matrix\n",
      "\n",
      "when computing the correlation coefficient between more than two variables, you obtain a correlation matrix\n",
      "\n",
      "range:[-1,1] (negative and positive correlation)\n",
      "\n",
      "0: no relationship\n",
      "\n",
      "corr_p = meat[['beef','veal','turkey']].corr(method='pearson')\n",
      "\n",
      "(method='spearman')\n",
      "\n",
      "\n",
      "corr_p = df[COLUMNS].corr(method='pearson')\n",
      "#print(corr_p)\n",
      "cmap=sns.diverging_palette(h_neg=10, h_pos=240, as_cmap=True)\n",
      "sns.heatmap(corr_p, center=0, cmap=cmap, linewidths=1,\n",
      "annot=True, fmt=\".2f\")\n",
      "\n",
      "sns.clustermap(corr_mat)\n",
      "\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Print the correlation matrix between the beef and pork columns using the spearman method\n",
      "print(meat[['beef', 'pork']].corr(method='spearman'))\n",
      "\n",
      "# Print the correlation between beef and pork columns\n",
      "print(0.827587)\n",
      "\n",
      "# Compute the correlation between the pork, veal and turkey columns using the pearson method\n",
      "print(meat[['pork', 'veal', 'turkey']].corr(method='pearson'))\n",
      "\n",
      "# Print the correlation between veal and pork columns\n",
      "print(-0.827587)\n",
      "\n",
      "# Print the correlation between veal and turkey columns\n",
      "print(-0.768366)\n",
      "\n",
      "# Print the correlation between pork and turkey columns\n",
      "print(0.835215)\n",
      "\n",
      "\n",
      "# Import seaborn library\n",
      "import seaborn as sns\n",
      "\n",
      "# Get correlation matrix of the meat DataFrame: corr_meat\n",
      "corr_meat = meat.corr(method='spearman')\n",
      "\n",
      "\n",
      "# Customize the heatmap of the corr_meat correlation matrix\n",
      "sns.heatmap(corr_meat,\n",
      "            annot=True,\n",
      "            linewidths=0.4,\n",
      "            annot_kws={\"size\": 10})\n",
      "\n",
      "plt.xticks(rotation=90)\n",
      "plt.yticks(rotation=0) \n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Import seaborn library\n",
      "import seaborn as sns\n",
      "\n",
      "# Get correlation matrix of the meat DataFrame\n",
      "corr_meat = corr_meat = meat.corr(method='spearman')\n",
      "\n",
      "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
      "fig = sns.clustermap(corr_meat,\n",
      "                     row_cluster=True,\n",
      "                     col_cluster=True,\n",
      "                     figsize=(10, 10))\n",
      "\n",
      "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
      "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "      >Case analysis\n",
      "\n",
      "jobs dataset\n",
      "\n",
      "# Read in jobs file\n",
      "jobs = pd.read_csv(url_jobs)\n",
      "\n",
      "# Print first five lines of your DataFrame\n",
      "print(jobs.head(5))\n",
      "\n",
      "# Check the type of each column in your DataFrame\n",
      "print(jobs.dtypes)\n",
      "\n",
      "# Convert datestamp column to a datetime object\n",
      "jobs['datetime'] = pd.to_datetime(jobs['datestamp'])\n",
      "\n",
      "# Set the datestamp columns as the index of your DataFrame\n",
      "jobs = jobs.set_index('datestamp')\n",
      "\n",
      "# Check the number of missing values in each column\n",
      "print(jobs.isnull().sum())\n",
      "\n",
      "\n",
      "# Generate a boxplot\n",
      "jobs.boxplot(fontsize=6, vert=False)\n",
      "plt.show()\n",
      "\n",
      "# Generate numerical summaries\n",
      "print(jobs.describe())\n",
      "\n",
      "# Print the name of the time series with the highest mean\n",
      "print('Agriculture')\n",
      "\n",
      "# Print the name of the time series with the highest variability\n",
      "print('Construction')\n",
      "\n",
      "\n",
      "     beyond summary statistics\n",
      "\n",
      "jobs.plot(subplots=True,layout=(4,4), figsize=(20,16),sharex=True,sharey=False)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "ax=jobs.plot(figsize=(20,14), colormap='Dark2')\n",
      "\n",
      "ax.axvline('2008-01-01', color='black',linestyle='--')\n",
      "\n",
      "ax.axvline('2009-01-01', color='black', linestyle='--')\n",
      "\n",
      "index_month=jobs.index.month\n",
      "jobs_by_month=jobs.groupby(index_month).mean()\n",
      "print(jobs_by_month)\n",
      "\n",
      "ax=jobs_by_month.plot(figsize=(12,5), colormap='Dark2')\n",
      "ax.legend(bbox_to_anchor=(1.0,0.5),loc='center left')\n",
      "\n",
      "\n",
      "   > sample\n",
      "\n",
      "# A subset of the jobs DataFrame\n",
      "jobs_subset = jobs[['Finance', 'Information', 'Manufacturing', 'Construction']]\n",
      "\n",
      "# Print the first 5 rows of jobs_subset\n",
      "print(jobs_subset.head(5))\n",
      "\n",
      "# Create a facetted graph with 2 rows and 2 columns\n",
      "ax = jobs_subset.plot(subplots=True,\n",
      "                      layout=(2,2),\n",
      "                      sharex=False,\n",
      "                      sharey=False,\n",
      "                      linewidth=0.7,\n",
      "                      fontsize=3,\n",
      "                      legend=False)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "# Plot all time series in the jobs DataFrame\n",
      "ax = jobs.plot(colormap='Spectral', fontsize=6, linewidth=0.8)\n",
      "\n",
      "# Set labels and legend\n",
      "ax.set_xlabel('Date', fontsize=10)\n",
      "ax.set_ylabel('Unemployment Rate', fontsize=10)\n",
      "ax.set_title('Unemployment rate of U.S. workers by industry', fontsize=10)\n",
      "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
      "\n",
      "# Annotate your plots with vertical lines\n",
      "ax.axvline('2001-07-01', color='blue', linestyle='--', linewidth=0.8)\n",
      "ax.axvline('2008-09-01', color='blue', linestyle='--', linewidth=0.8)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "      effect seasonally on jobs\n",
      "\n",
      "# Extract the month from the index of jobs\n",
      "index_month = jobs.index.month\n",
      "\n",
      "# Compute the mean unemployment rate for each month\n",
      "jobs_by_month = jobs.groupby(index_month).mean()\n",
      "\n",
      "# Plot the mean unemployment rate for each month\n",
      "ax = jobs_by_month.plot(fontsize=6, linewidth=1)\n",
      "\n",
      "# Set axis labels and legend\n",
      "ax.set_xlabel('Month', fontsize=10)\n",
      "ax.set_ylabel('Mean unemployment rate', fontsize=10)\n",
      "ax.legend(bbox_to_anchor=(0.8, 0.6), fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    2008 melt down\n",
      "\n",
      "# Extract of the year in each date indices of the jobs DataFrame\n",
      "index_year = jobs.index.year\n",
      "\n",
      "# Compute the mean unemployment rate for each year\n",
      "jobs_by_year = jobs.groupby(index_year).mean()\n",
      "\n",
      "# Plot the mean unemployment rate for each year\n",
      "ax = jobs_by_year.plot(fontsize=6, linewidth=1)\n",
      "\n",
      "# Set axis labels and legend\n",
      "ax.set_xlabel('Year', fontsize=10)\n",
      "ax.set_ylabel('Mean unemployment rate', fontsize=10)\n",
      "ax.legend(bbox_to_anchor=(0.1, 0.5), fontsize=10)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     > decomposing time series data\n",
      "\n",
      "\n",
      "my_dict={}\n",
      "\n",
      "import statsmodel.api as sm\n",
      "\n",
      "ts_names=df.columns\n",
      "\n",
      "for ts in ts_names:\n",
      "\tts_decomposition = sm.tsa.seasonal_decompose(jobs[ts])\n",
      "\tmy_dict[ts]=ts_decomposition\n",
      "\n",
      "\n",
      "my_dict.trend={}\n",
      "\n",
      "for ts in ts_names:\n",
      "\tmy_dict_trend[ts]=my_dict[ts].trend\n",
      "\n",
      "trend_df=pd.DataFrame.from_dict(my_dict_trend)\n",
      "\n",
      "print(trend_df)\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "\n",
      "# Initialize dictionary\n",
      "jobs_decomp={}\n",
      "\n",
      "# Get the names of each time series in the DataFrame\n",
      "ts_names=jobs.columns\n",
      "\n",
      "\n",
      "# Run time series decomposition on each time series of the DataFrame\n",
      "for ts in ts_names:\n",
      "    ts_decomposition = sm.tsa.seasonal_decompose(jobs[ts])\n",
      "    jobs_decomp[ts] =ts_decomposition\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Extract the seasonal values for the decomposition of each time series\n",
      "for ts in jobs_names:\n",
      "    jobs_seasonal[ts] = jobs_decomp[ts].seasonal\n",
      "    \n",
      "# Create a DataFrame from the jobs_seasonal dictionary\n",
      "seasonality_df = pd.DataFrame.from_dict(jobs_seasonal)\n",
      "\n",
      "# Remove the label for the index\n",
      "seasonality_df.index.name = None\n",
      "\n",
      "# Create a faceted plot of the seasonality_df DataFrame\n",
      "jobs_seasonal.plot(subplots=True,\n",
      "                   layout=(4,4),\n",
      "                   sharey=False,\n",
      "                   fontsize=2,\n",
      "                   linewidth=0.3,\n",
      "                   legend=False)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "      >compute correlations between time series\n",
      "\n",
      "\n",
      "trend_corr = trend_df.corr(method='spearman')\n",
      "\n",
      "fig=sns.clustermap(trend_corr, annot=True, linewidth=0.4)\n",
      "\n",
      "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(),\n",
      "rotation=0)\n",
      "\n",
      "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(),rotation=90)\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Get correlation matrix of the seasonality_df DataFrame\n",
      "seasonality_corr = seasonality_df.corr(method='spearman')\n",
      "\n",
      "# Customize the clustermap of the seasonality_corr correlation matrix\n",
      "fig = sns.clustermap(seasonality_corr, annot=True, annot_kws={\"size\": 4}, linewidths=.4, figsize=(15, 10))\n",
      "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
      "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
      "plt.show()\n",
      "\n",
      "# Print the correlation between the seasonalities of the Government and Education & Health industries\n",
      "print(seasonality_corr)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['datetime']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a search engine.txt\n",
      "\t12      Fearing the actions of a god-like Super Hero l...\n",
      "\n",
      "data engineering.txt\n",
      "\t5. actions .count() or .first()\n",
      "\n",
      "data science for business.txt\n",
      "\t1. actions\n",
      "\n",
      "kpi.txt\n",
      "\t#customer actions\n",
      "\n",
      "kpi.txt\n",
      "\t2. not required to track users actions across time\n",
      "\n",
      "kpi.txt\n",
      "\t3. can treat simpler actions as responses to the test\n",
      "\n",
      "machine learning for business.txt\n",
      "\tidentify causal relationship of how much certain actions affect an outcome of interest\n",
      "\n",
      "machine learning for business.txt\n",
      "\tc. what are the business actions to take. Prioritize and invest more in markets with higher predicted demand\n",
      "\n",
      "machine learning for business.txt\n",
      "\tc. What business actions will we take (what are the targets)\n",
      "\n",
      "machine learning for business.txt\n",
      "\tRun certain actions eg marketing campaign, for group A and no actions for group B\n",
      "\n",
      "pyspark and big data.txt\n",
      "\tspark operations = transformations + actions\n",
      "\n",
      "pyspark and big data.txt\n",
      "\t2. actions perform computation on the rdds\n",
      "\n",
      "pyspark and big data.txt\n",
      "\tRDD actions\n",
      "\n",
      "pyspark and big data.txt\n",
      "\t>>>>> Advance RDD actions\n",
      "\n",
      "pyspark and big data.txt\n",
      "\tPair RDD actions leverage the key-value data\n",
      "\n",
      "pyspark and big data.txt\n",
      "\tdataframes support transformations and actions\n",
      "\n",
      "pyspark sql.txt\n",
      "\t# Run actions on both dataframes\n",
      "\n",
      "pyspark sql.txt\n",
      "\t# Run actions both dataframes\n",
      "\n",
      "sql server stored procedures.txt\n",
      "\tperform actions like execute, select, insert, update, delete\n",
      "\n",
      "sql triggers.txt\n",
      "\t1. to initiate actions when manipulating data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import codecs\n",
    "import json\n",
    "import re\n",
    "\n",
    "pattern=\"(\\s{1}actions\\s{1})\"\n",
    "path= 'C:\\\\Users\\\\dnishimoto.BOISE\\\\python'  \n",
    "for filename in [item for item in os.listdir(path) if item.endswith(\".txt\")  ]:\n",
    "    if os.access(path + \"\\\\\" + filename, os.R_OK):\n",
    "        with open(path + \"\\\\\" + filename,\"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if re.search(pattern,line):\n",
    "                    print(filename)\n",
    "                    print(\"\\t{}\".format(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
