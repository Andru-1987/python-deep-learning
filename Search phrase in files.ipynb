{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\creating a search engine.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\creating a search engine.txt\n",
      "print(df1.columns)\n",
      "\n",
      "df1 = pd.get_dummies(df1, columns=['feature 5'])\n",
      "\n",
      "\n",
      " >Basic feature extraction\n",
      "\n",
      "1. Number of characters\n",
      "\n",
      "\n",
      "text=\"I don't know\"\n",
      "num_char=len(text)\n",
      "print(num_char)\n",
      "\n",
      "\n",
      "df['num_chars'] = df['review'].apply(len)\n",
      "\n",
      " >Computing the number of words\n",
      "\n",
      "text=\"Mary had a little lamb\"\n",
      "\n",
      "words= text.split()\n",
      "\n",
      "print(words)\n",
      "\n",
      "print(len(words))\n",
      "\n",
      "\n",
      "def word_count(string) :\n",
      "\twords= string.split()\n",
      "\treturn len(words)\n",
      "\n",
      "\n",
      "df['num_words']=df['review'].apply(word_count)\n",
      "\n",
      "def avg_word_length(x):\n",
      "\twords=x.split()\n",
      "\tword_lengths= [len(word) for word in words]\n",
      "\t\n",
      "\tavg_word_length= sum(word_lengths)/len(words)\n",
      "\treturn(avg_word_length)\n",
      "\n",
      "df['avg_word_length']=df['review'].apply(avg_word_length)\n",
      "\n",
      "\n",
      "define hashtag_count(string):\n",
      "\twords= string.split()\n",
      "\thashtags=[word for words if word.startswith('#')]\n",
      "\treturn len(hashtags)\n",
      "\n",
      "\n",
      "other features:\n",
      "1. number of sentences\n",
      "2. number of paragraphs\n",
      "3. words starting with an uppercase\n",
      "4. All capital words\n",
      "5. numeric quantities\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Create a feature char_count\n",
      "tweets['char_count'] = tweets['content'].apply(len)\n",
      "\n",
      "# Print the average character count\n",
      "print(tweets['char_count'].mean())\n",
      "\n",
      " >Sample\n",
      "\n",
      "def count_words(string):\n",
      "\t# Split the string into words\n",
      "    words = string.split()\n",
      "    \n",
      "    # Return the number of words\n",
      "    return len(words)\n",
      "\n",
      "# Create a new feature word_count\n",
      "ted['word_count'] = ted['transcript'].apply(count_words)\n",
      "\n",
      "# Print the average word count of the talks\n",
      "print(ted['word_count'].mean())\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "\n",
      "# Function that returns numner of hashtags in a string\n",
      "def count_hashtags(string):\n",
      "\t# Split the string into words\n",
      "    words = string.split()\n",
      "    \n",
      "    # Create a list of words that are hashtags\n",
      "    hashtags = [word for word in words if word.startswith('@')]\n",
      "    \n",
      "    \n",
      "    \n",
      "    # Return number of hashtags\n",
      "    return(len(hashtags))\n",
      "\n",
      "# Create a feature hashtag_count and display distribution\n",
      "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
      "tweets['hashtag_count'].hist()\n",
      "plt.title('Hashtag count distribution')\n",
      "plt.show()\n",
      "\n",
      " >Overview of readability test\n",
      "\n",
      "1. determine readability of an english passage\n",
      "2. scale ranging from primary school up to college graduate level\n",
      "3. a mathematical formula utilizing word, syllable and sentence count\n",
      "\n",
      "flesch\n",
      "gunning\n",
      "smog\n",
      "dale-chall score\n",
      "\n",
      "\n",
      "flesch\n",
      "1. one of the oldest and most widely used tests\n",
      "2. dependant on two factors\n",
      "a. The greater the average sentence length, the harder the text is to read\n",
      "b. The greater the average number of syllables in a word, the harder the text is to read\n",
      "\n",
      "90-100 grade 5\n",
      "50-60 grade 10-12\n",
      "30-50 grade college\n",
      "0-30 grade college graduate\n",
      "\n",
      "fog\n",
      "1. dependent on average sentence length\n",
      "2. greater the percentage of complex words, the harder the text is to read (3 or more syllables)\n",
      "\n",
      "fog index \n",
      "17 college graduate\n",
      "16 college senior\n",
      "15 college junior\n",
      "14 college sophmore\n",
      "13 college freshman\n",
      "12 high school senior\n",
      "11 high school junior\n",
      "10 high school sophmore\n",
      "\n",
      "6 sixth grade\n",
      "\n",
      "from textatistic import Textatistic\n",
      "\n",
      "readability_scores = Textatistic(text).scores\n",
      "\n",
      "print(readability_scores['flesch_score'])\n",
      "print(readability_scores['gunningfog_score'])\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "# Import Textatistic\n",
      "from textatistic import Textatistic\n",
      "\n",
      "# Compute the readability scores \n",
      "readability_scores = Textatistic(sisyphus_essay).scores\n",
      "\n",
      "# Print the flesch reading ease score\n",
      "flesch = readability_scores['flesch_score']\n",
      "print(\"The Flesch Reading Ease is %.2f\" % (flesch))\n",
      "\n",
      " Sample college level\n",
      "\n",
      "# Import Textatistic\n",
      "from textatistic import Textatistic\n",
      "\n",
      "# List of excerpts\n",
      "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
      "\n",
      "# Loop through excerpts and compute gunning fog index\n",
      "gunning_fog_scores = []\n",
      "for excerpt in excerpts:\n",
      "  readability_scores = Textatistic(excerpt).scores\n",
      "  gunning_fog = readability_scores['gunningfog_score']\n",
      "  gunning_fog_scores.append(gunning_fog)\n",
      "\n",
      "# Print the gunning fog indices\n",
      "print(gunning_fog_scores)\n",
      "\n",
      "\n",
      " >Tokenization and Lemmatization\n",
      "1. converting words into lowercase\n",
      "2. removing leading and trailing whitespace\n",
      "3. removing punctuation\n",
      "4. removing stopwords\n",
      "5. expanding contractions\n",
      "6. removing special characters\n",
      "\n",
      "\n",
      "corpus = nltk.sent_tokenize(paragraph)    \n",
      "\n",
      "\n",
      "for i in range(len(corpus )):\n",
      "    corpus [i] = corpus [i].lower()\n",
      "    corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
      "    corpus [i] = re.sub(r'\\s+',' ',corpus [i])\n",
      "\n",
      "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
      "\n",
      "#to handle the error to create the symlinks\n",
      "\n",
      "python -m spacy link en_core_web_sm en_core_web_sm\n",
      "\n",
      "import spacy\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "doc=nlp(paragraph)\n",
      "tokens=[token.text for token in doc]\n",
      "print(tokens)\n",
      "\n",
      "lemmatization is converting a word into its base form\n",
      "\n",
      "lemmas =[token.lemma_ for token in doc]\n",
      "print(lemmas)\n",
      "\n",
      "reducing or reduces or reduced or reduction -> reduce\n",
      "am or are or is -> be\n",
      "\n",
      "n't -> not\n",
      "'ve -> have\n",
      "\n",
      "every pronoun is converted into -PRON-\n",
      "\n",
      "  Sample\n",
      "\n",
      "import spacy\n",
      "\n",
      "# Load the en_core_web_sm model\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "# Create a Doc object\n",
      "doc=nlp(gettysburg)\n",
      "\n",
      "# Generate the tokens\n",
      "tokens=[token.text for token in doc]\n",
      "print(tokens)\n",
      "\n",
      " >Sample\n",
      "\n",
      "import spacy\n",
      "\n",
      "# Load the en_core_web_sm model\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "# Create a Doc object\n",
      "doc = nlp(gettysburg)\n",
      "\n",
      "# Generate lemmas\n",
      "lemmas =[token.lemma_ for token in doc]\n",
      "\n",
      "print(lemmas)\n",
      "\n",
      " Text Cleaning\n",
      "1. remove extra whitespace and escape sequences\n",
      "2. punctuations\n",
      "3. special characters\n",
      "4. stopwords\n",
      "\n",
      "isalpha\n",
      "use regex\n",
      "\n",
      "lemmas =[token.lemma_ for token in doc]\n",
      "lemmas =[lemma for lemma in lemmas\n",
      "        if lemma.isalpha() or lemma == '-PRON-'\n",
      "        ]\n",
      "\n",
      "stopwords\n",
      "1. words that occur extremely commonly\n",
      "2. articles, be verbs, pronouns\n",
      "\n",
      "stopwords=spacy.lang.en.stop_words.STOP_WORDS\n",
      "\n",
      "other text preprocessing techniques\n",
      "1. removing html/xml tags\n",
      "2. replacing accented characters\n",
      "3. correcting spelling errors\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "# Load model and create Doc object\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "doc = nlp(blog)\n",
      "\n",
      "# Generate lemmatized tokens\n",
      "lemmas = [token.lemma_ for token in doc]\n",
      "\n",
      "# Remove stopwords and non-alphabetic tokens\n",
      "a_lemmas = [lemma for lemma in lemmas \n",
      "            if lemma.isalpha and lemma not in stopwords]\n",
      "\n",
      "# Print string after text cleaning\n",
      "print(' '.join(a_lemmas))\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Function to preprocess text\n",
      "def preprocess(text):\n",
      "  \t# Create Doc object\n",
      "    doc = nlp(text, disable=['ner', 'parser'])\n",
      "    # Generate lemmas\n",
      "    lemmas = [token.lemma_ for token in doc]\n",
      "    # Remove stopwords and non-alphabetic characters\n",
      "    a_lemmas = [lemma for lemma in lemmas \n",
      "            if lemma.isalpha() and lemma not in stopwords]\n",
      "    \n",
      "    return ' '.join(a_lemmas)\n",
      "  \n",
      "# Apply preprocess to ted['transcript']\n",
      "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
      "print(ted['transcript'])\n",
      "\n",
      "\n",
      "  Parts of speech\n",
      "1. word-sense diambiguation\n",
      "\n",
      "the bear is a majestic animal  (noun)\n",
      "please bear with me (verb)\n",
      "\n",
      "POS tagging\n",
      "1. assigning every word, to its corresponding part of speech\n",
      "\n",
      "\"Jane is an amazing guiatarist\"\n",
      "\n",
      "Jane -> noun\n",
      "Is ->verb\n",
      "an ->determinate\n",
      "amazing ->adjective\n",
      "guitarist ->noun\n",
      "\n",
      "20 parts of speech\n",
      "\n",
      " Sample\n",
      "\n",
      "# Load the en_core_web_sm model\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "# Create a Doc object\n",
      "doc = nlp(lotf)\n",
      "\n",
      "# Generate tokens and pos tags\n",
      "pos = [(token.text, token.pos_) for token in doc]\n",
      "print(pos)\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "# Returns number of proper nouns\n",
      "def proper_nouns(text, model=nlp):\n",
      "  \t# Create doc object\n",
      "    doc = model(text)\n",
      "    # Generate list of POS tags\n",
      "    pos = [token.pos_ for token in doc]\n",
      "    \n",
      "    # Return number of proper nouns\n",
      "    return pos.count('PROPN')\n",
      "\n",
      "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n",
      "\n",
      " >sample\n",
      "\n",
      "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
      "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
      "\n",
      "# Compute mean of proper nouns\n",
      "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
      "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
      "\n",
      "# Compute mean of other nouns\n",
      "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
      "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
      "\n",
      "# Print results\n",
      "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
      "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))\n",
      "\n",
      " Named entity recognition\n",
      "1. efficient search algorithm\n",
      "2. question answering\n",
      "3. news article classification\n",
      "4. customer service\n",
      "\n",
      "named entity recognition is anything with a proper noun or name or noun\n",
      "\n",
      "organized into person, organization, or country\n",
      "\n",
      "\"john doe is a software engineer working at google. he lives in france.\"\n",
      "m\n",
      "joe doe-> person\n",
      "google->organization\n",
      "france -> country\n",
      "\n",
      "named_entity=[(entity.text,entity.label_) for entity in doc.ents]\n",
      "print(named_entity)\n",
      "\n",
      "space can identify more than 15 categories of named entities\n",
      "\n",
      "person : people, including fictional\n",
      "norp: nationalities or religious or political groups\n",
      "fac : buildings, airports, highways, bridgees\n",
      "org: companies, agencies, institutions\n",
      "gpe: countries, cities, states\n",
      "\n",
      "\n",
      "  sample\n",
      "\n",
      "# Load the required model\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "# Create a Doc instance \n",
      "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
      "doc = nlp(text)\n",
      "\n",
      "# Print all named entities and their labels\n",
      "for ent in doc.ents:\n",
      "    print(ent.text, ent.label_)\n",
      "\n",
      "\n",
      "def find_persons(text):\n",
      "  # Create Doc object\n",
      "  doc = nlp(text)\n",
      "  \n",
      "  # Identify the persons\n",
      "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
      "  \n",
      "  # Return persons\n",
      "  return persons\n",
      "\n",
      " >Vectorization and building a bag of words model\n",
      "\n",
      "1. Converting text into vectors\n",
      "2. bag of words model is extracting word tokens\n",
      "a. computing the frequency of word tokens\n",
      "b. computing a word vector out of these frequencies and volculabory of corpus\n",
      "\n",
      "corpus\n",
      "\"the lion is the king of the jungle\"\n",
      "\n",
      "\"Lions have a lifespan of a decade\"\n",
      "\n",
      "\"The Lion is an endangered species\"\n",
      "\n",
      "a bag of words builts a vocabulary\n",
      "\n",
      "the second dimension is the frequency the word occurs\n",
      "\n",
      "No punctuations\n",
      "no stopwords\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer= CountVectorizer()\n",
      "\n",
      "bow_matrix= vectorizer.fit_transform(corpus)\n",
      "\n",
      "#sparse array\n",
      "print(bow_matrix.toarray())\n",
      "\n",
      "print(bow_lem_matrix.shape)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Convert bow_matrix into a DataFrame\n",
      "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
      "\n",
      "# Map the column names to vocabulary \n",
      "bow_df.columns = vectorizer.get_feature_names()\n",
      "\n",
      "# Print bow_df\n",
      "print(bow_df)\n",
      "\n",
      " >Using Count Vectorizer\n",
      "\n",
      "lowercase True\n",
      "strip_accents\n",
      "stop_words: english, list, none\n",
      "token_pattern:regex\n",
      "tokenizer: function\n",
      "\n",
      "\n",
      "countVector converts a corpus into a matrix of numeric vectors\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer=CountVectorizer(strip_accents='ascii', stop_words='english, lowercase=False)\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test= train_test_split(df['message'],df['label'],test_size=0.25)\n",
      "\n",
      "X_train_bow=vectorizer.fit_transform(X_train)\n",
      "\n",
      "#Generate test Bow vectors\n",
      "\n",
      "X_test_box= vectorizer.transform(X_test)\n",
      "\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "clf=MultinomialNB()\n",
      "\n",
      "clf.fit(X_train_bow, y_train)\n",
      "\n",
      "accuracy = clf.score(X_test_bow,y_test)\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Import CountVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Create a CountVectorizer object\n",
      "vectorizer=CountVectorizer(strip_accents='ascii', stop_words='english', lowercase=False)\n",
      "\n",
      "# Fit and transform X_train\n",
      "X_train_bow = vectorizer.fit_transform(X_train)\n",
      "\n",
      "# Transform X_test\n",
      "X_test_bow = vectorizer.transform(X_test)\n",
      "\n",
      "# Print shape of X_train_bow and X_test_bow\n",
      "print(X_train_bow.shape)\n",
      "print(X_test_bow.shape)\n",
      "\n",
      "# Create a MultinomialNB object\n",
      "clf=MultinomialNB()\n",
      "\n",
      "clf.fit(X_train_bow, y_train)\n",
      "\n",
      "# Measure the accuracy\n",
      "accuracy = clf.score(X_test_bow,y_test)\n",
      "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
      "\n",
      "# Predict the sentiment of a negative review\n",
      "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
      "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
      "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
      "\n",
      " >Building n-gram models\n",
      "\n",
      "in the bag of words the context of the words is lost\n",
      "\n",
      "n-grams is a contigous sequence of n elements(or words) in a given document\n",
      "\n",
      "the ngrams can be used to account for context\n",
      "\n",
      "n-grams are used for\n",
      "1. sentence completion\n",
      "2. spelling correction\n",
      "3. machine translation correction\n",
      "\n",
      "ngrams= CountVectorizer(ngram=range(1,3))\n",
      "\n",
      "shortcomings\n",
      "1. curse of dimensionality\n",
      "2. keep n-grams small\n",
      "\n",
      " Sample\n",
      "\n",
      "# Generate n-grams upto n=1\n",
      "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
      "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
      "\n",
      "# Generate n-grams upto n=2\n",
      "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
      "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
      "\n",
      "# Generate n-grams upto n=3\n",
      "vectorizer_ng3 = CountVectorizer(ngram_range=(1,3))\n",
      "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
      "\n",
      "# Print the number of features for each model\n",
      "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))\n",
      "\n",
      "# Define an instance of MultinomialNB \n",
      "clf_ng = MultinomialNB()\n",
      "\n",
      " Sample 2\n",
      "\n",
      "start_time = time.time()\n",
      "# Splitting the data into training and test sets\n",
      "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
      "\n",
      "# Generating ngrams\n",
      "vectorizer = CountVectorizer()\n",
      "train_X = vectorizer.fit_transform(train_X)\n",
      "test_X = vectorizer.transform(test_X)\n",
      "\n",
      "# Fit classifier\n",
      "clf = MultinomialNB()\n",
      "clf.fit(train_X, train_y)\n",
      "\n",
      "# Print accuracy, time and number of dimensions\n",
      "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))\n",
      "\n",
      "# Fit the classifier\n",
      "clf_ng.fit(X_train_ng, y_train)\n",
      "\n",
      "# Measure the accuracy\n",
      "accuracy = clf_ng.score(X_test_ng, y_test)\n",
      "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
      "\n",
      "# Predict the sentiment of a negative review\n",
      "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
      "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
      "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
      "\n",
      " >Sample 3\n",
      "\n",
      "start_time = time.time()\n",
      "# Splitting the data into training and test sets\n",
      "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
      "\n",
      "# Generating ngrams\n",
      "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
      "train_X = vectorizer.fit_transform(train_X)\n",
      "test_X = vectorizer.transform(test_X)\n",
      "\n",
      "# Fit classifier\n",
      "clf = MultinomialNB()\n",
      "clf.fit(train_X, train_y)\n",
      "\n",
      "# Print accuracy, time and number of dimensions\n",
      "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))\n",
      "\n",
      "\n",
      "  > building tf-idf document vectors\n",
      "\n",
      "1. a document containing the word human in five places has a dimension of 5\n",
      "2. some words occur commonly across all documents in the corpus\n",
      "a. suppose one document has jupiter and universe occurring 20 times each where jupiter rarely occurs in the other documents and universe is common.\n",
      "b. Jupiter should be given a larger weight on account of its exclusivity.\n",
      "\n",
      "\n",
      "automatically detect stopwords instead of depending on a generated list.\n",
      "\n",
      "search based on a ranking of pages\n",
      "\n",
      "recommender system\n",
      "\n",
      "better performance during predictive predicting\n",
      "\n",
      " term frequency inverse document frequency\n",
      "\n",
      "1. proportional to term frequency\n",
      "2. inverse function of the number of documents in which it occurs\n",
      "\n",
      "5* log(20/8) = 2   where the word library occurs 5 times in 20 documents in corpus and library occurs in 8 of them.\n",
      "\n",
      "A high tf-idf weight the more important the word is in classifying the document. the word is exclusive to that document.\n",
      "\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "vectorizer = TfidfVectorizer() \n",
      "\n",
      "tfidf_matrix=vectorizer.fit_transform(corpus)\n",
      "print(tfidf_matrix.toarray())\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Create TfidfVectorizer object\n",
      "vectorizer = TfidfVectorizer() \n",
      "\n",
      "# Generate matrix of word vectors\n",
      "tfidf_matrix = vectorizer.fit_transform(ted)\n",
      "\n",
      "# Print the shape of tfidf_matrix\n",
      "print(tfidf_matrix.shape)\n",
      "\n",
      "\n",
      " how similarity two documents are\n",
      "\n",
      "sim(A,B) = cos(theta) = a.b / ||A|| * ||B|| (magnitude)\n",
      "\n",
      "dot product\n",
      "v . W = v1*w1+v2*w2+v3*w3\n",
      "\n",
      "A=(4,7,1) B=(5,2,3)\n",
      "A.B = (4x5)+(7*2)+(1*3) =37\n",
      "\n",
      "Magnitude of a vector\n",
      "1 length of the vector\n",
      "2. d= sqrt( (4-5)**2 + (7-2)**2+ (1-3)**2)\n",
      "\n",
      "||A||= sqrt(4**2+7**2+1**2)\n",
      "||B|| = sqrt(5**2+2**2+3**2)\n",
      "\n",
      "cos=37/sqrt(66) * sqrt(38)\n",
      ".738\n",
      "\n",
      "bound between -1 and 1\n",
      "\n",
      "NLP\n",
      "0 means no similarity\n",
      "1 means the documents are identical\n",
      "\n",
      "\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "\n",
      "#takes in 2d arrays as arguments\n",
      "\n",
      "A=(4,7,1)\n",
      "B=(5,2,3)\n",
      "score cosine_similarity([A],[B])\n",
      "\n",
      "print(score)\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Initialize numpy vectors\n",
      "A = np.array([1,3])\n",
      "B = np.array([-2,2])\n",
      "\n",
      "# Compute dot product\n",
      "dot_prod = np.dot(A, B)\n",
      "\n",
      "# Print dot product\n",
      "print(dot_prod)\n",
      "\n",
      "\n",
      " Sample\n",
      "# Initialize an instance of tf-idf Vectorizer\n",
      "tfidf_vectorizer = TfidfVectorizer() \n",
      "\n",
      "# Generate the tf-idf vectors for the corpus\n",
      "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
      "\n",
      "# Compute and print the cosine similarity matrix\n",
      "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
      "print(cosine_sim)\n",
      "\n",
      " Movie recommender\n",
      "\n",
      "title, overview\n",
      "\n",
      "fnc= take in a title and output movies with similar overviews\n",
      "\n",
      "1. Text preprocessing\n",
      "2. generate tf-idf vectors\n",
      "3. generate cosine similarity matrix\n",
      "\n",
      "\n",
      "The recommender function\n",
      "1. takes a movie title, cosine similarity matrix, and indices series as arguments\n",
      "\n",
      "2. Extract pairwise cosine similarity scors for the movie\n",
      "\n",
      "3. Sorts the scores in descending order\n",
      "\n",
      "4. Output titles corresponding to the highest scores\n",
      "\n",
      "5. Ignore the highest similarity score of 1\n",
      "\n",
      "\n",
      "tfidf_vectorizer = TfidfVectorizer() \n",
      "\n",
      "# Generate the tf-idf vectors for the corpus\n",
      "tfidf_matrix = tfidf_vectorizer.fit_transform(movie_plots)\n",
      "\n",
      "cosine_sim=cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
      "\n",
      " >Linear kernel\n",
      "\n",
      "from sklearn.metrics.pairwise import linear_kernel\n",
      "\n",
      "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "# Record start time\n",
      "start = time.time()\n",
      "\n",
      "# Compute cosine similarity matrix\n",
      "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
      "\n",
      "# Print cosine similarity matrix\n",
      "print(cosine_sim)\n",
      "\n",
      "# Print time taken\n",
      "print(\"Time taken: %s seconds\" %(time.time() - start))\n",
      "\n",
      " movie plots\n",
      "\n",
      "0       Following the death of District Attorney Harve...\n",
      "1       The Dark Knight of Gotham City confronts a das...\n",
      "2       The Dark Knight of Gotham City begins his war ...\n",
      "3       Having defeated the Joker, Batman now faces th...\n",
      "4       Along with crime-fighting partner Robin and ne...\n",
      "5       An old flame of Bruce Wayne's strolls into tow...\n",
      "6       The Dynamic Duo faces four super-villains who ...\n",
      "7       Driven by tragedy, billionaire Bruce Wayne ded...\n",
      "8       Batman faces his ultimate challenge as the mys...\n",
      "9       Two men come to Gotham City: Bruce Wayne after...\n",
      "10      Batman has not been seen for ten years. A new ...\n",
      "11      Batman has stopped the reign of terror that Th...\n",
      "12      Fearing the actions of a god-like Super Hero l...\n",
      "13      Led by Woody, Andy's toys live happily in his ...\n",
      "14      When siblings Judy and Peter discover an encha...\n",
      "15      A family wedding reignites the ancient feud be...\n",
      "16      Cheated on, mistreated and stepped on, the wom...\n",
      "17      Just when George Banks has recovered from his ...\n",
      "18      Obsessive master thief, Neil McCauley leads a ...\n",
      "19      An ugly duckling having undergone a remarkable...\n",
      "20      A mischievous young boy, Tom Sawyer, witnesses...\n",
      "21      International action superstar Jean Claude Van...\n",
      "22      James Bond must unmask the mysterious head of ...\n",
      "23      Widowed U.S. president Andrew Shepherd, one of...\n",
      "24      When a lawyer shows up at the vampire's doorst...\n",
      "25      An outcast half-wolf risks his life to prevent...\n",
      "26      An all-star cast powers this epic look at Amer...\n",
      "27      Morgan Adams and her slave, William Shaw, are ...\n",
      "28      The life of the gambling paradise â€“ Las Vegas ...\n",
      "29      Rich Mr. Dashwood dies, leaving his second wif...\n",
      "\n",
      "indices\n",
      "\n",
      "The Dark Knight Rises                         0\n",
      "Batman Forever                                1\n",
      "Batman                                        2\n",
      "Batman Returns                                3\n",
      "Batman & Robin                                4\n",
      "Batman: Mask of the Phantasm                  5\n",
      "Batman                                        6\n",
      "Batman Begins                                 7\n",
      "Batman: Under the Red Hood                    8\n",
      "Batman: Year One                              9\n",
      "Batman: The Dark Knight Returns, Part 1      10\n",
      "Batman: The Dark Knight Returns, Part 2      11\n",
      "Batman v Superman: Dawn of Justice           12\n",
      "Toy Story                                    13\n",
      "Jumanji                                      14\n",
      "Grumpier Old Men                             15\n",
      "Waiting to Exhale                            16\n",
      "Father of the Bride Part II                  17\n",
      "Heat                                         18\n",
      "Sabrina                                      19\n",
      "Tom and Huck                                 20\n",
      "Sudden Death                                 21\n",
      "GoldenEye                                    22\n",
      "The American President                       23\n",
      "Dracula: Dead and Loving It                  24\n",
      "Balto                                        25\n",
      "Nixon                                        26\n",
      "Cutthroat Island                             27\n",
      "Casino                                       28\n",
      "Sense and Sensibility                        29\n",
      "                                           ... \n",
      "Army of Darkness                            978\n",
      "The Big Blue                                979\n",
      "Ran                                         980\n",
      "The Killer                                  981\n",
      "Psycho                                      982\n",
      "The Blues Brothers                          983\n",
      "The Godfather: Part II                      984\n",
      "Full Metal Jacket                           985\n",
      "A Grand Day Out                             986\n",
      "Henry V                                     987\n",
      "Amadeus                                     988\n",
      "The Quiet Man                               989\n",
      "Once Upon a Time in America                 990\n",
      "Raging Bull                                 991\n",
      "Annie Hall                                  992\n",
      "The Right Stuff                             993\n",
      "Stalker                                     994\n",
      "Das Boot                                    995\n",
      "The Sting                                   996\n",
      "Harold and Maude                            997\n",
      "Trust                                       998\n",
      "The Seventh Seal                            999\n",
      "Local Hero                                 1000\n",
      "The Terminator                             1001\n",
      "Braindead                                  1002\n",
      "Glory                                      1003\n",
      "Rosencrantz & Guildenstern Are Dead        1004\n",
      "Manhattan                                  1005\n",
      "Miller's Crossing                          1006\n",
      "Dead Poets Society                         1007\n",
      "Length: 1008, dtype: int64>\n",
      "1                              Batman Forever\n",
      "2                                      Batman\n",
      "3                              Batman Returns\n",
      "8                  Batman: Under the Red Hood\n",
      "9                            Batman: Year One\n",
      "10    Batman: The Dark Knight Returns, Part 1\n",
      "11    Batman: The Dark Knight Returns, Part 2\n",
      "5                Batman: Mask of the Phantasm\n",
      "7                               Batman Begins\n",
      "4                              Batman & Robin\n",
      "Name: title, dtype: object\n",
      "\n",
      "\n",
      "tfidf = TfidfVectorizer(stop_words='english')\n",
      "\n",
      "# Construct the TF-IDF matrix\n",
      "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
      "\n",
      "# Generate the cosine similarity matrix\n",
      "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
      " \n",
      " Generate recommendations \n",
      "#print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))\n",
      "\n",
      "\n",
      "# Generate mapping between titles and index\n",
      "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
      "\n",
      "def get_recommendations(title, cosine_sim, indices):\n",
      "    # Get index of movie that matches title\n",
      "    idx = indices[title]\n",
      "    # Sort the movies based on the similarity scores\n",
      "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
      "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
      "    # Get the scores for 10 most similar movies\n",
      "    sim_scores = sim_scores[1:11]\n",
      "    # Get the movie indices\n",
      "    movie_indices = [i[0] for i in sim_scores]\n",
      "    # Return the top 10 most similar movies\n",
      "    return metadata['title'].iloc[movie_indices]\n",
      "\n",
      "\n",
      " TED similar articles\n",
      "\n",
      "# Initialize the TfidfVectorizer \n",
      "tfidf = TfidfVectorizer(stop_words='english')\n",
      "\n",
      "\n",
      "# Construct the TF-IDF matrix\n",
      "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
      "\n",
      "# Generate the cosine similarity matrix\n",
      "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
      " \n",
      "# Generate recommendations \n",
      "print(get_recommendations( '5 ways to kill your dreams', cosine_sim, indices))\n",
      "\n",
      " Beyond n-gram word\n",
      "\n",
      "1. Word embeddings is mapping words into an n-dimensional vector space\n",
      "2. produced using deep learning and huge amounts of data\n",
      "3. the vectors can be used to discern how similar two words are to each other.\n",
      "4. Used to detect synonyms and antonyms\n",
      "5. Captures complex relationships\n",
      "\n",
      "King->Queen, Man->Woman\n",
      "France->Paris and Russia->Moscow\n",
      "\n",
      "dependent on spacy model; independent of dataset you use\n",
      "\n",
      "\n",
      "nlp=spacy.load('en_core_web_lg')\n",
      "doc=nlp('I am happy')\n",
      "\n",
      "python -m spacy download en_core_web_lg\n",
      "python -m spacy link en_core_web_lg en_core_web_lg\n",
      "\n",
      "for token in doc:\n",
      "\tprint(token.vector)\n",
      "\n",
      "\n",
      "doc=nlp('happy joyous sad\")\n",
      "\n",
      "for token1 in doc:\n",
      "\tfor token2 in doc:\n",
      "\tprint(token1.text,token2.text,token1.similarity(token2))\n",
      "\n",
      "\n",
      "t1=nlp(\"I am happy\")\n",
      "t2=nlp(\"I am sad\")\n",
      "t3=nlp(\"I am joyous\")\n",
      "\n",
      "t1.similarity(t2)\n",
      "\n",
      " Sample\n",
      "\n",
      "# Create the doc object\n",
      "print(sent)\n",
      "doc = nlp(sent)\n",
      "\n",
      "# Compute pairwise similarity scores\n",
      "for token1 in doc:\n",
      "  for token2 in doc:\n",
      "    print(token1.text, token2.text, token1.similarity(token2))\n",
      "\n",
      " Sample\n",
      "\n",
      "# Create Doc objects\n",
      "mother_doc = nlp(mother)\n",
      "hopes_doc = nlp(hopes)\n",
      "hey_doc = nlp(hey)\n",
      "\n",
      "# Print similarity between mother and hopes\n",
      "print(mother_doc.similarity(hopes_doc))\n",
      "\n",
      "# Print similarity between mother and hey\n",
      "print(mother_doc.similarity(hey_doc))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\customer churn.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\customer churn.txt\n",
      "churn is when a customer ends a relationship with the company\n",
      "\n",
      "non-contractual churn (consumer loyalty)\n",
      "involuntary churn (expiration or non payment)\n",
      "\n",
      "Customer\n",
      "1. Lack of usage\n",
      "2. Poor service\n",
      "3. Better price\n",
      "\n",
      "Domain/industry knowledge\n",
      "\n",
      "Telco churn dataset.\n",
      "\n",
      "telecom features\n",
      "1. voice mail\n",
      "2. international calling\n",
      "3. cost for the service\n",
      "4. customer usage\n",
      "5. customer churn indicator\n",
      "\n",
      "churn is defined as the customer cancelling their cellular plan at a given point in time.\n",
      "\n",
      "print(telco['Churn'].value_counts())\n",
      "\n",
      " > Sample using groupby\n",
      "print(telco.groupby(['Churn']).count())\n",
      "print(telco.groupby(['Churn']).std())\n",
      "print(telco.groupby('State')['Churn'].value_counts())\n",
      "\n",
      " >seaborn\n",
      "understand how your variables are distributed\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "sns.distplot(telco['Account_Length')\n",
      "\n",
      "sns.boxplot(x='Churn', y='Account_Length', data=telco,sym=\"\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#The bell curve means that the data is normally distributed. This means that the data\n",
      "can be simulated by random sampling to increase the accurracy of the prediction\n",
      "\n",
      "sns.boxplot(x=\"Churn\", y='Account_length',data=telco)\n",
      "plt.show()\n",
      "\n",
      "#The line in the middle represents the median\n",
      "#The colored boxes represent the middle 50% of each group\n",
      "\n",
      "#The floating points represent outliers\n",
      "sym=\"\" removes the outliers\n",
      "\n",
      "\n",
      "sns.boxplot(x='Churn', y='Account_Length', data=telco,sym=\"\", hue='StreamingMovies')\n",
      "plt.show()\n",
      "\n",
      " >Sample\n",
      "Day_Mins\n",
      "Eve_Mins\n",
      "Night_Mins\n",
      "Intl_Mins\n",
      "\n",
      "# Import matplotlib and seaborn\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Visualize the distribution of 'Day_Mins'\n",
      "\n",
      "sns.distplot(telco['Day_Mins'])\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#If the data was not normal distributed, you would apply a feature transformation\n",
      "\n",
      "#In such cases, the extreme values could be identified and removed in order to make the distribution more Gaussian. These extreme values are often called outliers\n",
      "\n",
      "#Taking the square root and the logarithm of the observation in order to make the distribution normal belongs to a class of transforms called power transforms.\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Import matplotlib and seaborn\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Create the box plot\n",
      "sns.boxplot(x = 'Churn',\n",
      "          y = 'CustServ_Calls',\n",
      "          data = telco)\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "        >Churn Prediction Fundamentals\n",
      "\n",
      "test decision trees and logistic regression models (compare the models)\n",
      "\n",
      "churn definition depends on company\n",
      "1. churn happens when a customer stops buying or engaging with the company\n",
      "2. The business context could be contractual or non-contractual\n",
      "3. Failing to update subscription can cause involuntary churn\n",
      "4. Contractual churn happens explicitly when customers decide to terminate the relationship\n",
      "5. Non contractual churn happens on online shopping or when the customer stops shopping\n",
      "\n",
      "\n",
      "Encoding churn\n",
      "1=Churn\n",
      "0=No churn\n",
      "Or it could be a string churn and no churn\n",
      "\n",
      "Increase accuracy with under sampling or over sampling techniques\n",
      "\n",
      "train,test = train_test_split(telcom, test_size=.25)\n",
      "\n",
      "separate the independant features and the target variable\n",
      "\n",
      "target==['Churn']\n",
      "custid=['CustomerId']\n",
      "\n",
      "cols=[col for col in telcom.columns if col not in custid+target]\n",
      "\n",
      "train_X = train[cols]\n",
      "train_Y = train[target]\n",
      "test_X = test[cols]\n",
      "test_Y = test[target]\n",
      "\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Print the unique Churn values\n",
      "print(set(telcom['Churn']))\n",
      "\n",
      "# Calculate the ratio size of each churn group\n",
      "telcom.groupby(['Churn']).size() / telcom.shape[0] * 100\n",
      "\n",
      "# Import the function for splitting data to train and test\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Split the data into train and test\n",
      "train, test = train_test_split(telcom, test_size = .25)\n",
      "\n",
      "\n",
      "# Store column names from `telcom` excluding target variable and customer ID\n",
      "cols = [col for col in telcom.columns if col not in custid + target]\n",
      "\n",
      "# Extract training features\n",
      "train_X = train[cols]\n",
      "\n",
      "# Extract training target\n",
      "train_Y = train[target]\n",
      "\n",
      "# Extract testing features\n",
      "test_X = test[cols]\n",
      "\n",
      "# Extract testing target\n",
      "test_Y = test[target]\n",
      "\n",
      "\n",
      "      Predicting with Logistic Regression\n",
      "\n",
      "1. Statistical classification model for binary responses\n",
      "2. Models log-odds of the probabilty of the target\n",
      "\n",
      "odds= is the probability of the odd occurring divided by the probabiity of the event not occurring\n",
      "\n",
      "p/1-p\n",
      "\n",
      "helps to find the decision boundary between the two coeffiencts but keeping the variables linearly relatived.\n",
      "\n",
      "Accuracy - the % of correctly predicted labels (both churn and non-churn)\n",
      "Precision - the % of total models positive class predictions (here - predicted as Churn) that wee correctly classified\n",
      "Recall - The % of total positive class samples (all churned customers) that were correctly classified\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy\n",
      "\n",
      "pred_train_Y=logreg.predict(train_X)\n",
      "pred_test_Y= logreg.predict(test_X)\n",
      "\n",
      "train_accuracy = accuracy_score(train_Y, pred_train_Y)\n",
      "test_accuracy=accuracy_score(test_Y, pred_test_Y)\n",
      "\n",
      "from sklearn.metrics import precision_score, recall_score\n",
      "\n",
      "train_precision = round(precision_score(train_Y. pred_train_Y,4)\n",
      "test_precision=round(precision_score(test_Y,pred_test_Y),4)\n",
      "\n",
      "\n",
      "  Regularization\n",
      "\n",
      "* Introduces penalty coefficient in the model building phase\n",
      "* Addresses over-fitting (when patterns are memorized by the model)\n",
      "\n",
      "-- the classifier does well at recalling the predictions on the training data but does not do well on the testing data\n",
      "\n",
      "L1 Regularization and feature selection\n",
      "-- reduces the number of features and makes the model more predictable\n",
      "\n",
      "L1 regularization called LASSO can be called explicitly, and this approach performs\n",
      "feature selection by shrinking some of the model coefficients to zero\n",
      "\n",
      "logreg = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
      "logreg.fit(train_X,train_Y)\n",
      "\n",
      "C=0 to 1\n",
      "\n",
      "C=[1,.5,.25,.1,.05,.25,.01,.005,.0025]\n",
      "\n",
      "l1_metrics=np.zeros(len(C),5))\n",
      "l1_metrics[:,0]=C\n",
      "\n",
      "for index in range(0, len(C)):\n",
      "\tlogreg = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
      "\tlogreg.fit(train_X,train_Y)\n",
      "\tpred_test_Y= logreg.predict(test_X)\n",
      "\n",
      "\tl1_metrics[index,1]=np.count_nonzero(logreg.coef_)\n",
      "\tl1_metrics[index,1]=accuracy_score(test_Y, pred_test_Y)\n",
      "\tl1_metrics[index,1]=precision_score(test_Y,pred_test_Y)\n",
      "\tl1_metrics[index,1]=recall_score(test_Y,pred_test_Y)\n",
      "col_names=['C','non-zero coeffs','accuracy','precision','recall']\n",
      "print(pd.DataFrame(l1_metrics, columns=col_names)\n",
      "\n",
      "we want a model that has reduced complexity but similar performance metrics\n",
      "\n",
      "Non-Zero coeffs are feature count\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Fit logistic regression on training data\n",
      "logreg.fit(train_X, train_Y)\n",
      "\n",
      "# Predict churn labels on testing data\n",
      "pred_test_Y = logreg.predict(test_X)\n",
      "\n",
      "# Calculate accuracy score on testing data\n",
      "test_accuracy = accuracy_score(test_Y, pred_test_Y)\n",
      "\n",
      "# Print test accuracy score rounded to 4 decimals\n",
      "print('Test accuracy:', round(test_accuracy, 4))\n",
      "\n",
      "  >Sample\n",
      "\n",
      "# Initialize logistic regression instance \n",
      "logreg = LogisticRegression(penalty='l1', C=0.025, solver='liblinear')\n",
      "\n",
      "# Fit the model on training data\n",
      "logreg.fit(train_X, train_Y)\n",
      "\n",
      "# Predict churn values on test data\n",
      "pred_test_Y = logreg.predict(test_X)\n",
      "\n",
      "# Print the accuracy score on test data\n",
      "print('Test accuracy:', round(accuracy_score(test_Y, pred_test_Y), 4))\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Run a for loop over the range of C list length\n",
      "for index in range(0, len(C)):\n",
      "  # Initialize and fit Logistic Regression with the C candidate\n",
      "  logreg = LogisticRegression(penalty='l1', C=C[index], solver='liblinear')\n",
      "  logreg.fit(train_X, train_Y)\n",
      "  # Predict churn on the testing data\n",
      "  pred_test_Y = logreg.predict(test_X)\n",
      "  # Create non-zero count and recall score columns\n",
      "  l1_metrics[index,1] = np.count_nonzero(logreg.coef_)\n",
      "  l1_metrics[index,2] = recall_score(test_Y, pred_test_Y)\n",
      "\n",
      "\n",
      "         >Decision Tree\n",
      "\n",
      "if else rules\n",
      "\n",
      "dt= DecisionTreeClassifier(max_depth=2, random_state=1)\n",
      "\n",
      "dt.fit(X_train, y_train)\n",
      "\n",
      "pred_test= dt.predict(X_test)\n",
      "pred_train= dt.predict(X_train)\n",
      "\n",
      "\n",
      "buffer=pd.Series(pred_test)\n",
      "buffer.value_counts().plot(kind='pie')\n",
      "plt.show()\n",
      "\n",
      "print(\"0 none churn 1 churn\")\n",
      "\n",
      "print(\"Training accuracy:\",round(accuracy_score(y_train,pred_train),4))\n",
      "print(\"Testing accuracy:\", round(accuracy_score(y_test, pred_test),4))\n",
      "\n",
      "\n",
      "depth_list=list(range(2,15))\n",
      "depth_tuning = np.zeros((len(depth_list),4))\n",
      "depth_tuning[:,0]=depth_list\n",
      "\n",
      "for index in range(len(depth_list)):\n",
      "    mytree=DecisionTreeClassifier(max_depth=depth_list[index])\n",
      "    mytree.fit(X_train,y_train)\n",
      "    pred_test_Y= mytree.predict(X_test)\n",
      "\n",
      "    depth_tuning[index,1]=accuracy_score(y_test,pred_test_Y)\n",
      "    depth_tuning[index,2]=precision_score(y_test,pred_test_Y)\n",
      "    depth_tuning[index,3]=recall_score(y_test,pred_test_Y)\n",
      "    \n",
      "col_names=['Max_Depth','Accuracy','Precision','Recall']\n",
      "print(pd.DataFrame(depth_tuning, columns=col_names))\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Initialize decision tree classifier\n",
      "mytree = tree.DecisionTreeClassifier()\n",
      "\n",
      "# Fit the decision tree on training data\n",
      "mytree.fit(train_X, train_Y)\n",
      "\n",
      "# Predict churn labels on testing data\n",
      "pred_test_Y = mytree.predict(test_X)\n",
      "\n",
      "# Calculate accuracy score on testing data\n",
      "test_accuracy = accuracy_score(test_Y, pred_test_Y)\n",
      "\n",
      "# Print test accuracy\n",
      "print('Test accuracy:', round(test_accuracy, 4))\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Run a for loop over the range of depth list length\n",
      "for index in range(0, len(depth_list)):\n",
      "  # Initialize and fit decision tree with the `max_depth` candidate\n",
      "  mytree = DecisionTreeClassifier(max_depth=depth_list[index])\n",
      "  mytree.fit(train_X, train_Y)\n",
      "  # Predict churn on the testing data\n",
      "  pred_test_Y = mytree.predict(test_X)\n",
      "  # Calculate the recall score \n",
      "  depth_tuning[index,1] = recall_score(test_Y, pred_test_Y)\n",
      "\n",
      "\n",
      " >Identifying insights into churn\n",
      "\n",
      "from sklearn import tree\n",
      "import graphviz\n",
      "\n",
      "\n",
      "exported=tree.export_graphviz(\n",
      "\tdecision_tree=mytree,\n",
      "\tout_file=None,\n",
      "\tfeature_names=cols,\n",
      "\tprecision=1,\n",
      "\tclass_names=['Not churn','Churn'],\n",
      "\tfilled=True)\n",
      "\n",
      "graph=graphviz.Source(exported)\n",
      "display(graph)\n",
      "\n",
      "\n",
      "<<<<<Logistic regression coefficients\n",
      "\n",
      "1. Logistic regression returns beta coefficients\n",
      "2. The coeffients can to be intrepretated as the log-odds of churn associated with 1 unit increase in the feature\n",
      "\n",
      "logb p/(1-p)\n",
      "\n",
      "log of odds is hard to intrepret\n",
      "\n",
      "logreg.coef_\n",
      "\n",
      "* calculate the exponent of the coefficients\n",
      "* This gives us the change in odds associated with 1 unit increase in the feature\n",
      "\n",
      "\n",
      "coefficients = pd.concat([pd.DataFrame(train_X.columns),\n",
      "pd.DataFrame(np.transpose(logit.coef_))],\n",
      "axis=1)\n",
      "\n",
      "coefficients.columns=['Feature','Coefficient']\n",
      "\n",
      "coefficients['Exp_Coefficients']=np.exp(coefficients['Coefficient'])\n",
      "coefficients=cefficients[coefficients['Coefficients]!=0]\n",
      "print(coefficients.sort_value(by=['Coefficient']))\n",
      "\n",
      "\n",
      "*values less than 1 decrease the odds\n",
      "*values greater than 1 increase the odds\n",
      "\n",
      "One additional year of tenure decrease churn odds by 60%\n",
      "\n",
      "\n",
      "   >Customer Lifetime Value basics (CLV)\n",
      "\n",
      "*CLV is the amount of money a company expect to earn in a lifetime\n",
      "\n",
      "Historical CLV = (revenues)*Profit Margin\n",
      "\n",
      "* Does not account for tenure, retention and churn rates\n",
      "\n",
      "* Does not account for new customers and their future revenue\n",
      "\n",
      "\n",
      "CLV = Average Revenue (for a certain period of time) * Profit Margin * Average Lifespan\n",
      "\n",
      "* lifespan is knowledge about its customers or the average lifespan of the customer churn.\n",
      "\n",
      "CLV (avg.revenue per purchase * avg.frequency* profit margin) * average lifespan\n",
      "\n",
      "* does not account for customer retention rates\n",
      "\n",
      "CLV = (Average Revenue * Profit Margin) * Retention Rate/Churn Rate\n",
      "\n",
      "churn= 1- retention\n",
      "\n",
      "cohort_sizes=cohort_counts.iloc[:,0]\n",
      "retention=cohorts_counts.divide(cohort_sizes,axis=0)\n",
      "churn=1-retention\n",
      "\n",
      "sns.heatmap(retention, annot=True, vmin=0, vmax=0.5, map=\"Y1Gn\")\n",
      "\n",
      "\n",
      "  Sample (calculate retention and churn)\n",
      "\n",
      "# Extract cohort sizes from the first column of cohort_counts\n",
      "cohort_sizes = cohort_counts.iloc[:,0]\n",
      "\n",
      "# Calculate retention by dividing the counts with the cohort sizes\n",
      "retention = cohort_counts.divide(cohort_sizes, axis=0)\n",
      "\n",
      "# Calculate churn\n",
      "churn = 1 - retention\n",
      "\n",
      "# Print the retention table\n",
      "print(churn)\n",
      "print(cohort_counts.shape)\n",
      "\n",
      "\n",
      "  Sample (calculate retention rate and churn rate)\n",
      "\n",
      "Now that you have calculated the monthly retention and churn metrics for monthly customer cohorts, you can calculate the overall mean retention and churn rates. You will use the .mean() method twice in a row (this is called \"chaining\") to calculate the overall mean\n",
      "\n",
      "# Calculate the mean retention rate\n",
      "retention_rate = retention.iloc[:,1:].mean().mean()\n",
      "\n",
      "# Calculate the mean churn rate\n",
      "churn_rate = churn.iloc[:,1:].mean().mean()\n",
      "\n",
      "# Print rounded retention and churn rates\n",
      "print('Retention rate: {:.2f}; Churn rate: {:.2f}'.format(retention_rate, churn_rate))\n",
      "\n",
      "    CLV\n",
      "1. goal clv measure customers in terms of revenue or profit\n",
      "2. benchmark customers\n",
      "3. identify maximum investment to gain customer acquistion\n",
      "\n",
      "CLV = Average Revenue * Retention Rate/churn rate\n",
      "\n",
      "\n",
      "      >Basic CLV\n",
      "\n",
      "1, Calculate monthly spent by the customer\n",
      "\n",
      "monthly_revenue = online.groupby(['CustomerID','InvoiceMonth'])\n",
      "['TotalSum'].sum().mean()\n",
      "\n",
      "monthly_revenue=np.mean(month_revenue)\n",
      "\n",
      "lifespan_months=36\n",
      "\n",
      "clv_basic=monthly_revenue * lifespn_months\n",
      "\n",
      "print('Average basic CLV is (:1f) USD'.format(clv_basic))\n",
      "\n",
      "       Granular CLV calculation\n",
      "\n",
      "revenue_per_purchase= online.groupby(['InvoiceNo']).['TotalSum'].mean().mean()\n",
      "\n",
      "##overall revenue for a purchase\n",
      "\n",
      "freq=online.groupby(['CustomerId','InvoicedMonth'])['InvoiceMonth'].nunique().mean()\n",
      "\n",
      "##calculate the average number of unique invoices per customer per month\n",
      "\n",
      "lifespan_months=36\n",
      "\n",
      "clv_granular= revenue_per_purchase * freq * lifespan_months\n",
      "\n",
      "print('Average granular CLV is (:,1f) USD'.format(clv_granular))\n",
      "\n",
      "print('Revenue per purchase is (:,1f) USD'.format(revenue_per_purchase)\n",
      "\n",
      "print('Frequency per month is (:,1f) USD'.format(freq)\n",
      "\n",
      "\n",
      "       Traditional CLV calculation\n",
      "\n",
      "monthly_revenue= online.groupby(['CustomerID','InvoiceMonth'])['TotalSum'].sum().mean()\n",
      "\n",
      "retention_rate=retention.iloc[:,1].mean().mean()\n",
      "\n",
      "churn_rate=1-retention_rate\n",
      "\n",
      "clv_traditional=month_revenue * (retention_rate/churn_rate)\n",
      "\n",
      "print('Average traditional clv is (:.1f) % retention_rate'.format(clv_traditional, retention_rate*100))\n",
      "\n",
      "   Which method to use\n",
      "\n",
      "1. depends on business model\n",
      "2.traditional clv model - assumes churn is definitive - customer dies.  The customer is assumed to not come back if they have churned once.\n",
      "3. traditional model is not robust at low retention values\n",
      "4. hardest thing to predict - frequency in the future\n",
      "\n",
      "\n",
      "  Sample (basic clv of 36 months)\n",
      "\n",
      "\n",
      "# Calculate monthly spend per customer\n",
      "monthly_revenue = online.groupby(['CustomerID','InvoiceMonth'])['TotalSum'].sum().mean()\n",
      "\n",
      "# Calculate average monthly spend\n",
      "monthly_revenue = np.mean(monthly_revenue)\n",
      "\n",
      "# Define lifespan to 36 months\n",
      "lifespan_months = 36\n",
      "\n",
      "# Calculate basic CLV\n",
      "clv_basic = monthly_revenue * lifespan_months\n",
      "\n",
      "# Print the basic CLV value\n",
      "print('Average basic CLV is {:.1f} USD'.format(clv_basic))\n",
      "\n",
      "\n",
      " >Sample (granular)\n",
      "\n",
      "# Calculate average revenue per invoice\n",
      "revenue_per_purchase = online.groupby(['InvoiceNo'])['TotalSum'].mean().mean()\n",
      "\n",
      "# Calculate average number of unique invoices per customer per month\n",
      "frequency_per_month = online.groupby(['CustomerID','InvoiceMonth'])['InvoiceNo'].nunique().mean()\n",
      "\n",
      "# Define lifespan to 36 months\n",
      "lifespan_months = 36\n",
      "\n",
      "# Calculate granular CLV\n",
      "clv_granular = revenue_per_purchase * frequency_per_month * lifespan_months\n",
      "\n",
      "# Print granular CLV value\n",
      "print('Average granular CLV is {:.1f} USD'.format(clv_granular))\n",
      "\n",
      " >Sample (traditional)\n",
      "\n",
      "#Now you will calculate one of the most popular descriptive CLV models that accounts for the retention and churn rates. This gives a more robust estimate, but comes with certain assumptions that have to be validated\n",
      "\n",
      "# Calculate monthly spend per customer\n",
      "monthly_revenue = online.groupby(['CustomerID','InvoiceMonth'])['TotalSum'].sum().mean()\n",
      "\n",
      "# Calculate average monthly retention rate\n",
      "retention_rate = retention.iloc[:,1:].mean().mean()\n",
      "\n",
      "# Calculate average monthly churn rate\n",
      "churn_rate = 1 - retention_rate\n",
      "\n",
      "# Calculate traditional CLV \n",
      "clv_traditional = monthly_revenue * (retention_rate / churn_rate)\n",
      "\n",
      "# Print traditional CLV and the retention rate values\n",
      "print('Average traditional CLV is {:.1f} USD at {:.1f} % retention_rate'.format(clv_traditional, retention_rate*100))\n",
      "\n",
      "#As you can see, the traditional CLV formula yields a much lower estimate as it accounts for monthly retention which is quite low for this company.\n",
      "\n",
      "\n",
      "         Data preparation for purchase prediction\n",
      "\n",
      "* regression to predict purchasing\n",
      "* simplest model is linear regression\n",
      "* target variable is either continous or count\n",
      "\n",
      "* count data (number of active days) work better with poisson or negative binomal regression\n",
      "\n",
      "RFM - recency, frequency, or monetary features\n",
      "\n",
      "explore the sales distribution by month\n",
      "\n",
      "online.groupby(['InvoiceMonth']).size()\n",
      "\n",
      "#prints out the number of observations per month\n",
      "\n",
      "online_X=online[online['InvoiceMonth']='2011-11']\n",
      "\n",
      "#calculate the recency\n",
      "\n",
      "NOW= dt.datetime(2011,11,1)\n",
      "\n",
      "features = online_X.groupby('CustomerID').agg({\n",
      "\t'InvoiceDate': lambda x(NOW-x.max())days,\n",
      "\t'InvoiceMo': pd.Series.nunique,\n",
      "\t'TotalSum': np.sum,\n",
      "\t'Quantity': ['mean','sum']\n",
      "\n",
      "}).reset_index()\n",
      "\n",
      "features.columns=['CustomerID','recency','frequency','monetary','quantity_avg','quantity_total']\n",
      "\n",
      "#recency is the now date - the lastest invoice date\n",
      "#frequency by counting the unique number of invoice\n",
      "#sum the revenue for that customer\n",
      "#calculate the quantity and sum of the quantities\n",
      "#reindex makes sure the columns are not stored as an index for use later\n",
      "\n",
      "  Calculate the target variable\n",
      "\n",
      "#build a pivot table\n",
      "\n",
      "cust_month_tx= pd.pivot_table(data=online, index=['CustomerID'],\n",
      "\tvalues='InvoiceNo',\n",
      "\tcolumns=['InvoiceMonth'],\n",
      "\taggfunc=pd.Series.nunique, fill_value=0)\n",
      "\n",
      "print(cust_month_tx.head())\n",
      "\n",
      "#the result is a matrix of unique invoices per month by customer ID\n",
      "\n",
      "#use the last month of data\n",
      "\n",
      "#store the identifier and the target variable as separate list\n",
      "\n",
      "custid=['CustomerID']\n",
      "target=['2011-11']\n",
      "\n",
      "Y=cust_month_tx[target]\n",
      "\n",
      "cols=[col for col in features.columns if col not in custid]\n",
      "\n",
      "X=featurs(cols)\n",
      "\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "train_X,test_X,train_Y,test_Y= train_test_split(X,Y,\n",
      "\ttest_size=0.25, random_state=99)\n",
      "\n",
      "\n",
      "print(train_X.shape, train_Y.shape, test_\n",
      "X.shape, test_Y.shape)\n",
      "\n",
      "  >Sample (building features of Recency, Frequency, and Monetary)\n",
      "\n",
      "\n",
      "# Define the snapshot date\n",
      "NOW = dt.datetime(2011,11,1)\n",
      "\n",
      "# Calculate recency by subtracting current date from the latest InvoiceDate\n",
      "features = online_X.groupby('CustomerID').agg({\n",
      "  'InvoiceDate': lambda x: (NOW - x.max()).days,\n",
      "  # Calculate frequency by counting unique number of invoices\n",
      "  'InvoiceNo': pd.Series.nunique,\n",
      "  # Calculate monetary value by summing all spend values\n",
      "  'TotalSum': np.sum,\n",
      "  # Calculate average and total quantity\n",
      "  'Quantity': ['mean', 'sum']}).reset_index()\n",
      "\n",
      "# Rename the columns\n",
      "features.columns = ['CustomerID', 'recency', 'frequency', 'monetary', 'quantity_avg', 'quantity_total']\n",
      "\n",
      "\n",
      "# Build a pivot table counting invoices for each customer monthly\n",
      "cust_month_tx = pd.pivot_table(data=online, values='InvoiceNo',\n",
      "                               index=['CustomerID'], columns=['InvoiceMonth'],\n",
      "                               aggfunc=pd.Series.nunique, fill_value=0)\n",
      "\n",
      "# Store November 2011 data column name as a list\n",
      "target = ['2011-11']\n",
      "\n",
      "# Store target value as `Y`\n",
      "Y = cust_month_tx[target]\n",
      "\n",
      "# Store customer identifier column name as a list\n",
      "custid = ['CustomerID']\n",
      "\n",
      "# Select feature column names excluding customer identifier\n",
      "cols = [col for col in features.columns if col not in custid]\n",
      "\n",
      "# Extract the features as `X`\n",
      "X = features[cols]\n",
      "\n",
      "# Split data to training and testing\n",
      "Train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.25, random_state=99)\n",
      "\n",
      "\n",
      "   Predicting next months transactions\n",
      "\n",
      "Use linear regression to predict next months transactions\n",
      "initializing the model\n",
      "fit and predict\n",
      "measure\n",
      "\n",
      "\n",
      "root mean squared error (RMSE) - Square root of the average squared differences between prediction and actuals\n",
      "a. subtract the predicted and actuals\n",
      "b. square the results\n",
      "c. calculate the average\n",
      "d. take the square root to get a normalized measurement\n",
      "\n",
      "\n",
      "    Mean absolute error (MAE)\n",
      "mean absolute error - Average absolute difference between the predicted and actuals\n",
      "\n",
      "    Mean absolute percentage error (MAPE)\n",
      "average percentage difference between prediction and actuals\n",
      "normalized between 0 and 100 percent (actuals can't be zero)\n",
      "\n",
      "R-squared: statistical measure that represents the percentage proportion of variance that is explained by the model.  \n",
      "applies only to regression\n",
      "(Higer is better)\n",
      "\n",
      "coefficient p-values - probability that the regression coefficient is observed due to chance.  (lower is better)\n",
      "threshholds are 5% to 10%  (measures the significance of the null hypothesis)\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "linreg=LinearRegression()\n",
      "\n",
      "linreq.fit(train_X,train_Y)\n",
      "\n",
      "train_pred_Y= linreq.predict(train_X)\n",
      "test_pred_Y = linreq.predict(test_X)\n",
      "\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "rmse_train=np.sqrt(mean_squared_error(train_Y,train_pred_Y))\n",
      "mae_train=mean_absolute_error(train_Y,train_pred_Y)\n",
      "\n",
      "rmse_test=np.sqrt(mean_squared_error(test_Y,test_pred_Y))\n",
      "mae_test=mean_absolute_error(test_Y,test_pred_Y)\n",
      "\n",
      "print('RMSE train: (:3f): RMSE test: (:3f)\\nMAE train :{:3f}, MAE test: {:3f}'.format(rmse_train,rmse_test, mae_train, mae_test))\n",
      "\n",
      "\n",
      " >Interpreting the coefficients\n",
      "\n",
      "1. statistical significance - standard statistical significant is 95%\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "train_Y=np.array(train_Y)\n",
      "\n",
      "#Ordinary Least Square Model (curve fitting algorithm)\n",
      "\n",
      "olsreg = sm.OLS(train_Y, train_X)\n",
      "olsreg=olsreg.fit()\n",
      "\n",
      "print(olsreg.summary())\n",
      "\n",
      "#R-squared is the percentage of explained variance.  What percentage does the model explain of the variation? (higher is better)\n",
      "\n",
      "check the P-value coefficients\n",
      "(change in the output variable if one unit changed in the feature).  Some of the coeffiencts are not statistically significant. 1-significance = 100-95% or 5%  (look features for p values less than 5%)\n",
      "\n",
      "\n",
      "Sample \n",
      "\n",
      "# Initialize linear regression instance\n",
      "linreg = LinearRegression()\n",
      "\n",
      "# Fit the model to training dataset\n",
      "linreg.fit(train_X, train_Y)\n",
      "\n",
      "# Predict the target variable for training data\n",
      "train_pred_Y = linreg.predict(train_X)\n",
      "\n",
      "# Predict the target variable for testing data\n",
      "test_pred_Y = linreg.predict(test_X)\n",
      "\n",
      "\n",
      "#This is a critical step where you are measuring how \"close\" are the model predictions compared to actual values.\n",
      "\n",
      "# Calculate root mean squared error on training data\n",
      "rmse_train = np.sqrt(mean_squared_error(train_Y, train_pred_Y))\n",
      "\n",
      "# Calculate mean absolute error on training data\n",
      "mae_train = mean_absolute_error(train_Y, train_pred_Y)\n",
      "\n",
      "# Calculate root mean squared error on testing data\n",
      "rmse_test = np.sqrt(mean_squared_error(test_Y, test_pred_Y))\n",
      "\n",
      "# Calculate mean absolute error on testing data\n",
      "mae_test = mean_absolute_error(test_Y, test_pred_Y)\n",
      "\n",
      "# Print the performance metrics\n",
      "print('RMSE train: {}; RMSE test: {}\\nMAE train: {}, MAE test: {}'.format(rmse_train, rmse_test, mae_train, mae_test))\n",
      "\n",
      "\n",
      " >Sample OLS \n",
      "\n",
      "OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.488\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.487\n",
      "Method:                 Least Squares   F-statistic:                              480.3\n",
      "Date:                Mon, 07 Sep 2020   Prob (F-statistic):                        0.00\n",
      "Time:                        22:03:37   Log-Likelihood:                         -2769.8\n",
      "No. Observations:                2529   AIC:                                      5550.\n",
      "Df Residuals:                    2524   BIC:                                      5579.\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "recency            0.0002      0.000      1.701      0.089   -2.92e-05       0.000\n",
      "frequency          0.1316      0.003     38.000      0.000       0.125       0.138\n",
      "monetary        1.001e-06   3.59e-05      0.028      0.978   -6.95e-05    7.15e-05\n",
      "quantity_avg       0.0001      0.000      0.803      0.422      -0.000       0.000\n",
      "quantity_total    -0.0001   5.74e-05     -2.562      0.010      -0.000   -3.45e-05\n",
      "==============================================================================\n",
      "Omnibus:                      987.494   Durbin-Watson:                   1.978\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5536.657\n",
      "Skew:                           1.762   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.334   Cond. No.                         249.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "In [2]: \n",
      "\n",
      "\n",
      "  Customer and product segmentation on basics\n",
      "\n",
      "wholesale.head()\n",
      "\n",
      "1. Fresh\n",
      "2. Milk\n",
      "3. Grocery\n",
      "4. Frozen\n",
      "5. Detergents_Paper\n",
      "6. Delicassens\n",
      "\n",
      "Unsupervised learning models\n",
      "\n",
      "* k-means\n",
      "* non-negative matrix factorization nmf\n",
      "\n",
      "1. initialize the model\n",
      "2. fit the model\n",
      "3. assign cluster values\n",
      "\n",
      "wholesale.agg(['mean','std']).round(0)\n",
      "\n",
      "averages= wholesale.mean()\n",
      "st_dev = wholesale.std()\n",
      "x_names=wholesale.columns\n",
      "x_ix= np.arange(wholesale.shape[1])\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.bar(x_ix-0.2, averages, color='grey', label='Average', width=0.4)\n",
      "plt.bar(x_ix+0.2, std_dev, color='orange',' label='Standard Deviation', width=0.4)\n",
      "plt.xticks(x_ix, x_names, rotation=90)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "sns.pairplot(wholesale,diag_kind='kde')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      " >Sample (pairplot)\n",
      "\n",
      "# Print the header of the `wholesale` dataset\n",
      "print(wholesale.head())\n",
      "\n",
      "# Plot the pairwise relationships between the variables\n",
      "sns.pairplot(wholesale, diag_kind='kde')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "  Sample (bar plot average and standard deviation)\n",
      "\n",
      "# Create column names list and same length integer list\n",
      "x_names = wholesale.columns\n",
      "x_ix = np.arange(wholesale.shape[1])\n",
      "\n",
      "# Plot the averages data in gray and standard deviations in orange \n",
      "plt.bar(x=x_ix-0.2, height=averages, color='grey', label='Average', width=0.4)\n",
      "plt.bar(x=x_ix+0.2, height=std_devs, color='orange', label='Standard Deviation', width=0.4)\n",
      "\n",
      "# Add x-axis labels and rotate\n",
      "plt.xticks(ticks=x_ix, labels=x_names, rotation=90)\n",
      "\n",
      "# Add the legend and display the chart\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  >Data preparation for segmentation\n",
      "\n",
      "1. start with k-means\n",
      "2. k-means works well when the data is normally distributed\n",
      "a. mean=0\n",
      "b. standard deviation=1\n",
      "\n",
      "Non-negative matrix factorization works well with on draw sparse matrices\n",
      "\n",
      "wholesale_log = np.log(wholesale)\n",
      "\n",
      "sns.pairplot(wholesale_log, diag_kind='kde')\n",
      "plt.show()\n",
      "\n",
      "#result in less skewed data\n",
      "\n",
      " >Box-cox transformation\n",
      "\n",
      "\n",
      "from scipy import stats\n",
      "\n",
      "def boxcox_df(x):\n",
      "\tx_boxcox, _ = stats.boxcox(x)\n",
      "\treturn x_boxcox\n",
      "\n",
      "wholesale_boxcox = wholesale.apply(boxcox_df,axis, 0)\n",
      "\n",
      "sns.pairplot(wholesale_boxcox, diag_kind='kde')\n",
      "plt.show()\n",
      "\n",
      "   Scale the data\n",
      "1. Subtract column average from each column value\n",
      "2. Divide each column value by column standard deviation\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler=StandardScaler()\n",
      "\n",
      "scaler.fit(wholesale_boxcox)\n",
      "\n",
      "#numpy array\n",
      "wholesale_scaled= scaler.transform(wholesale_box)\n",
      "\n",
      "wholesale_scaled_df=pd.DataFrame(data=whosale_scaled,\n",
      "\tindex=wholesale_boxcox.index,\n",
      "\tcolumns=wholesale_boxcox.columns)\n",
      "\n",
      "wholesale_scaled_df.agg(['mean','[std']).round()\n",
      "\n",
      " >Sample sns pairplot\n",
      "\n",
      "# Define custom Box Cox transformation function\n",
      "def boxcox_df(x):\n",
      "    x_boxcox, _ = stats.boxcox(x)\n",
      "    return x_boxcox\n",
      "\n",
      "# Apply the function to the `wholesale` dataset\n",
      "wholesale_boxcox = wholesale.apply(boxcox_df, axis=0)\n",
      "\n",
      "# Plot the pairwise relationships between the transformed variables \n",
      "sns.pairplot(wholesale_boxcox, diag_kind='kde')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample (Scaling)\n",
      "\n",
      "# Fit the initialized `scaler` instance on the Box-Cox transformed dataset\n",
      "scaler.fit(wholesale_boxcox)\n",
      "\n",
      "# Transform and store the scaled dataset as `wholesale_scaled`\n",
      "wholesale_scaled = scaler.transform(wholesale_boxcox)\n",
      "\n",
      "# Create a `pandas` DataFrame from the scaled dataset\n",
      "wholesale_scaled_df = pd.DataFrame(data=wholesale_scaled,\n",
      "                                       index=wholesale_boxcox.index,\n",
      "                                       columns=wholesale_boxcox.columns)\n",
      "\n",
      "# Print the mean and standard deviation for all columns\n",
      "print(wholesale_scaled_df.agg(['mean','std']).round())\n",
      "\n",
      "  >Kmeans\n",
      "\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans= KMeans(n_cluster=k)\n",
      "\n",
      "kmeans.fit(wholesale_scaled_df)\n",
      "\n",
      "\n",
      "#Use the original df not the scaled one\n",
      "wholesale_kmeans4 = wholesale.assign(segment=kmeans.labels_)\n",
      "\n",
      "\n",
      " >NMF\n",
      "\n",
      "from sklearn.decomposition import NMF\n",
      "\n",
      "nmf=NMF(k)\n",
      "nmf.fit(wholesale)\n",
      "\n",
      "components=pd.DataFrame(nmf.components_, columns=wholesale.columns)\n",
      "\n",
      "segment_weights= pd.DataFrame(nmf.transform(wholesale, columns=component.index)\n",
      "\n",
      "segment_weights.index=wholesale.index\n",
      "\n",
      "wholesale_nmf= wholesale.assign(segment=segment_weights.idxmax(axis=1))\n",
      "\n",
      "#new column - which cluster weight is largest for each customer\n",
      "\n",
      "  Defining k\n",
      "elbow criterion method to get the optimal number of k clusters\n",
      "a. iterate through a number of k values\n",
      "b. running cluster for each on the same data\n",
      "c. calculate sum of squared errors (se) for each\n",
      "d. plot the sse against k and identify the elbow of diminishing incremental improvements\n",
      "\n",
      "  Samples (NMF heatmap)\n",
      "\n",
      "# Create the W matrix\n",
      "W = pd.DataFrame(data=nmf.transform(wholesale), columns=components.index)\n",
      "W.index = wholesale.index\n",
      "\n",
      "# Assign the column name where the corresponding value is the largest\n",
      "wholesale_nmf3 = wholesale.assign(segment = W.idxmax(axis=1))\n",
      "\n",
      "# Calculate the average column values per each segment\n",
      "nmf3_averages = wholesale_nmf3.groupby('segment').mean().round(0)\n",
      "\n",
      "# Plot the average values as heatmap\n",
      "sns.heatmap(nmf3_averages.T, cmap='YlGnBu')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "sse={}\n",
      "\n",
      "for k in range(1,11):\n",
      "\tkmeans=KMeans(n_clusters=k, random_state=333)\n",
      "\tkmeans.fit(wholesale_scaled_df)\n",
      "\tsse(k)=kmeans.inertia_\n",
      "\n",
      "\n",
      "plt.title('Elbow criterion method chart')\n",
      "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
      "plt.show()\n",
      "\n",
      "build meanful segmentation\n",
      "can you give the segmentation a name given the clustering.\n",
      "\n",
      " >Sample (elbow)\n",
      "\n",
      "# Create empty sse dictionary\n",
      "sse = {}\n",
      "\n",
      "# Fit KMeans algorithm on k values between 1 and 11\n",
      "for k in range(1, 11):\n",
      "    kmeans = KMeans(n_clusters=k, random_state=333)\n",
      "    kmeans.fit(wholesale_scaled_df)\n",
      "    sse[k] = kmeans.inertia_\n",
      "\n",
      "# Add the title to the plot\n",
      "plt.title('Elbow criterion method chart')\n",
      "\n",
      "# Create and display a scatter plot\n",
      "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample (KMeans)\n",
      "\n",
      "# Import `KMeans` module\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Initialize `KMeans` with 4 clusters\n",
      "kmeans=KMeans(n_clusters=4, random_state=123)\n",
      "\n",
      "# Fit the model on the pre-processed dataset\n",
      "kmeans.fit(wholesale_scaled_df)\n",
      "\n",
      "# Assign the generated labels to a new column\n",
      "wholesale_kmeans4 = wholesale.assign(segment = kmeans.labels_)\n",
      "\n",
      "  Sample (NMF)\n",
      "\n",
      "# Import the non-negative matrix factorization module\n",
      "from sklearn.decomposition import NMF\n",
      "\n",
      "# Initialize NMF instance with 4 components\n",
      "nmf = NMF(4)\n",
      "\n",
      "# Fit the model on the wholesale sales data\n",
      "nmf.fit(wholesale)\n",
      "\n",
      "# Extract the components \n",
      "components = pd.DataFrame(data=nmf.components_, columns=wholesale.columns)\n",
      "\n",
      "       >Visualize and interpret segmentation solutions\n",
      "1. Calculate average/median/other percentile values for each variable by segment\n",
      "2. Calculate relative importance for each variable by segment\n",
      "3. Visualize using a heatmap\n",
      "\n",
      "\n",
      "\n",
      "kmeans4_averages= wholesale_kmeans4.groupby(['segment']).mean().round(0)\n",
      "\n",
      "print(kmeans4_averages)\n",
      "\n",
      "The four segments have different average values for fresh, milk, grocery, frozen, detergents_paper, delicassen\n",
      "\n",
      "sns.heatmap(kmeans4_averages.T, cmap='Y1GnBu')\n",
      "plt.show()\n",
      "\n",
      " >Plot average NMF segmentation attributes\n",
      "\n",
      "nmf4_averages=wholesale_nmf4.groupby('segment').mean().round(0)\n",
      "sns.heatmap(nmf4_averages.T, cmap='Y1GnBu')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample (heatmap kmeans clusters)\n",
      "# Group by the segment label and calculate average column values\n",
      "kmeans3_averages= wholesale_kmeans3.groupby(['segment']).mean().round(0)\n",
      "\n",
      "# Print the average column values per each segment\n",
      "print(kmeans3_averages)\n",
      "\n",
      "# Create a heatmap on the average column values per each segment\n",
      "sns.heatmap(kmeans3_averages.T, cmap='YlGnBu')\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<<<<<<<<  Predicting Customer Churn\n",
      "    One hot encoding\n",
      "\n",
      "1. numeric one for a category in a column\n",
      "\n",
      "print(telco.dtypes) to find the objects to encode\n",
      "\n",
      "    Standardization\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "df=StandardScaler().fit_transform(df)\n",
      "\n",
      "  Sample (encoding)\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Vmail_Plan'\n",
      "telco['Vmail_Plan'] = telco['Vmail_Plan'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Churn'\n",
      "telco['Churn'] = telco['Churn'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Print the results to verify\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Perform one hot encoding on 'State'\n",
      "telco_state = pd.get_dummies(telco['State'])\n",
      "\n",
      "print(telco_state)\n",
      "\n",
      "\n",
      " >Sample (Scaler)\n",
      "\n",
      "# Import StandardScaler\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Scale telco using StandardScaler\n",
      "telco_scaled = StandardScaler().fit_transform(telco)\n",
      "\n",
      "# Add column names back for readability\n",
      "telco_scaled_df = pd.DataFrame(telco_scaled, columns=[\"Intl_Calls\", \"Night_Mins\"])\n",
      "\n",
      "# Print summary statistics\n",
      "print(telco_scaled_df.describe())\n",
      "\n",
      "\n",
      "       Feature selection\n",
      "\n",
      "Unique identifiers that need to be dropped\n",
      "1. phone numbers\n",
      "2. customerid\n",
      "3. account numbers\n",
      "\n",
      "telco.drop(['Soc_Sec'], axis=1)\n",
      "\n",
      "Features that are highly correlated to features can be dropped because they offer no additional information to the model.\n",
      "\n",
      "telco.corr()\n",
      "\n",
      "remove features that are highly correlated\n",
      "\n",
      "should consult with business and subject matter experts\n",
      "\n",
      "A new feature could be\n",
      "\n",
      "Total_Minutes = Day_Mins+Eve_Mins+Night_Mins+Intl_Mins\n",
      "\n",
      "understanding the ratio of minutes and charge\n",
      "\n",
      "telco['Day_Cost']=telco['Day_Mins']/telco['Day_Charge']\n",
      "\n",
      "\n",
      " >Sample dropping columns\n",
      "\n",
      "# Drop the unnecessary features\n",
      "telco = telco.drop(['Area_Code','Phone'],axis=1)\n",
      "\n",
      "  Sample (new features)\n",
      "\n",
      "# Create the new feature\n",
      "telco['Avg_Night_Calls'] = telco['Night_Mins']/telco['Night_Calls']\n",
      "\n",
      "# Print the first five rows of 'Avg_Night_Calls'\n",
      "print(telco['Avg_Night_Calls'])\n",
      "\n",
      "      >Making predictions\n",
      "\n",
      "Logistic Regression: Good baseline offering simplicity and interpretability.\n",
      "\n",
      "Can not capture more complex relationships in the dataset\n",
      "\n",
      "Random forest\n",
      "\n",
      "Support vector machines\n",
      "\n",
      "  >Suport Vector Machine\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "\n",
      "svc.fit(telco[features], telco['target'])\n",
      "\n",
      "1. features must be contineous values\n",
      "2. dataframes or numpy arrays\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "decision_function_shape='ovr', degree=3, gamma='auto',\n",
      "kernel='rbf', max_iter=-1, probability=False,\n",
      "random_state=None, shrinking=True, tol=0.001,\n",
      "verbose=False)\n",
      "\n",
      "prediction = svc.predict(new_customer)\n",
      "print(prediction)\n",
      "\n",
      "\n",
      " >Sample (Logistic Regression)\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "clf = LogisticRegression()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "New customer\n",
      "\n",
      "Account_Length  Vmail_Message  Day_Mins  Eve_Mins  Night_Mins  ...  Eve_Charge  Night_Calls  Night_Charge  Intl_Calls  Intl_Charge\n",
      "0              91             23     232.4     186.0       190.5  ...       15.81          128          8.57           3         3.32\n",
      "\n",
      "\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "  >Sample (Decision Tree Classifier)\n",
      "\n",
      "# Import DecisionTreeClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "\n",
      "clf=DecisionTreeClassifier()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "# Predict the label of new_customer\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "       evaluate model performance\n",
      "1. Compute its accuracy\n",
      "2. Accuracy = Correct Predictions/total number of data points\n",
      "training data may not represent actual data\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(telco['data'],\n",
      "\ttelco['target'],\n",
      "\ttest_size=0.2,\n",
      "\trandom_state=42)\n",
      "\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "svc.fit(X_train, y_train)\n",
      "svc.predict(X_test)\n",
      "\n",
      "svc.score(X_test,y_test)\n",
      "\n",
      "#overfitting means the model has become to sensitive to noise in the training data\n",
      "\n",
      "#underfitting is means not capturing trends in the training data\n",
      "\n",
      "  Sample - Train Test Split\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(X,\n",
      "\ty,\n",
      "\ttest_size=0.3,\n",
      "\trandom_state=42)\n",
      "\n",
      "print(X_train.shape, X_test.shape)\n",
      "\n",
      " >Sample - (Random Forest Classifier)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "# Compute accuracy\n",
      "print(clf.score(X_test, y_test))\n",
      "\n",
      "        >Model Metrics\n",
      "\n",
      "\n",
      "<<<<<<<<  Predicting Customer Churn\n",
      "    One hot encoding\n",
      "\n",
      "1. numeric one for a category in a column\n",
      "\n",
      "print(telco.dtypes) to find the objects to encode\n",
      "\n",
      "    Standardization\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "df=StandardScaler().fit_transform(df)\n",
      "\n",
      "  Sample (encoding)\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Vmail_Plan'\n",
      "telco['Vmail_Plan'] = telco['Vmail_Plan'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Replace 'no' with 0 and 'yes' with 1 in 'Churn'\n",
      "telco['Churn'] = telco['Churn'].replace({'no': 0 , 'yes': 1})\n",
      "\n",
      "# Print the results to verify\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Perform one hot encoding on 'State'\n",
      "telco_state = pd.get_dummies(telco['State'])\n",
      "\n",
      "print(telco_state)\n",
      "\n",
      "\n",
      " >Sample (Scaler)\n",
      "\n",
      "# Import StandardScaler\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Scale telco using StandardScaler\n",
      "telco_scaled = StandardScaler().fit_transform(telco)\n",
      "\n",
      "# Add column names back for readability\n",
      "telco_scaled_df = pd.DataFrame(telco_scaled, columns=[\"Intl_Calls\", \"Night_Mins\"])\n",
      "\n",
      "# Print summary statistics\n",
      "print(telco_scaled_df.describe())\n",
      "\n",
      "\n",
      "       Feature selection\n",
      "\n",
      "Unique identifiers that need to be dropped\n",
      "1. phone numbers\n",
      "2. customerid\n",
      "3. account numbers\n",
      "\n",
      "telco.drop(['Soc_Sec'], axis=1)\n",
      "\n",
      "Features that are highly correlated to features can be dropped because they offer no additional information to the model.\n",
      "\n",
      "telco.corr()\n",
      "\n",
      "remove features that are highly correlated\n",
      "\n",
      "should consult with business and subject matter experts\n",
      "\n",
      "A new feature could be\n",
      "\n",
      "Total_Minutes = Day_Mins+Eve_Mins+Night_Mins+Intl_Mins\n",
      "\n",
      "understanding the ratio of minutes and charge\n",
      "\n",
      "telco['Day_Cost']=telco['Day_Mins']/telco['Day_Charge']\n",
      "\n",
      "\n",
      " >Sample dropping columns\n",
      "\n",
      "# Drop the unnecessary features\n",
      "telco = telco.drop(['Area_Code','Phone'],axis=1)\n",
      "\n",
      "  Sample (new features)\n",
      "\n",
      "# Create the new feature\n",
      "telco['Avg_Night_Calls'] = telco['Night_Mins']/telco['Night_Calls']\n",
      "\n",
      "# Print the first five rows of 'Avg_Night_Calls'\n",
      "print(telco['Avg_Night_Calls'])\n",
      "\n",
      "      >Making predictions\n",
      "\n",
      "Logistic Regression: Good baseline offering simplicity and interpretability.\n",
      "\n",
      "Can not capture more complex relationships in the dataset\n",
      "\n",
      "Random forest\n",
      "\n",
      "Support vector machines\n",
      "\n",
      "  >Suport Vector Machine\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "\n",
      "svc.fit(telco[features], telco['target'])\n",
      "\n",
      "1. features must be contineous values\n",
      "2. dataframes or numpy arrays\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "decision_function_shape='ovr', degree=3, gamma='auto',\n",
      "kernel='rbf', max_iter=-1, probability=False,\n",
      "random_state=None, shrinking=True, tol=0.001,\n",
      "verbose=False)\n",
      "\n",
      "prediction = svc.predict(new_customer)\n",
      "print(prediction)\n",
      "\n",
      "\n",
      " >Sample (Logistic Regression)\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "clf = LogisticRegression()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "New customer\n",
      "\n",
      "Account_Length  Vmail_Message  Day_Mins  Eve_Mins  Night_Mins  ...  Eve_Charge  Night_Calls  Night_Charge  Intl_Calls  Intl_Charge\n",
      "0              91             23     232.4     186.0       190.5  ...       15.81          128          8.57           3         3.32\n",
      "\n",
      "\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "  >Sample (Decision Tree Classifier)\n",
      "\n",
      "# Import DecisionTreeClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "\n",
      "clf=DecisionTreeClassifier()\n",
      "\n",
      "# Fit the classifier\n",
      "clf.fit(telco[features], telco['Churn'])\n",
      "\n",
      "# Predict the label of new_customer\n",
      "print(clf.predict(new_customer))\n",
      "\n",
      "       evaluate model performance\n",
      "1. Compute its accuracy\n",
      "2. Accuracy = Correct Predictions/total number of data points\n",
      "training data may not represent actual data\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(telco['data'],\n",
      "\ttelco['target'],\n",
      "\ttest_size=0.2,\n",
      "\trandom_state=42)\n",
      "\n",
      "\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc=SVC()\n",
      "svc.fit(X_train, y_train)\n",
      "svc.predict(X_test)\n",
      "\n",
      "svc.score(X_test,y_test)\n",
      "\n",
      "#overfitting means the model has become to sensitive to noise in the training data\n",
      "\n",
      "#underfitting is means not capturing trends in the training data\n",
      "\n",
      "  Sample - Train Test Split\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "X_train, X_test, y_train, y_test=train_test_split(X,\n",
      "\ty,\n",
      "\ttest_size=0.3,\n",
      "\trandom_state=42)\n",
      "\n",
      "print(X_train.shape, X_test.shape)\n",
      "\n",
      " >Sample - (Random Forest Classifier)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "# Compute accuracy\n",
      "print(clf.score(X_test, y_test))\n",
      "\n",
      "        >Model Metrics\n",
      "\n",
      "imbalanced classes\n",
      "\n",
      "telco['Churn'].value_counts()\n",
      "\n",
      "up balancing and down balancing\n",
      "\n",
      "confusion matrix\n",
      "\n",
      "\n",
      "          Churn   \t\tNo Churn\n",
      "\n",
      "Churn\tTrue positive\t\tFalse positive\n",
      "\n",
      "No Churn false Negatives\tTrue Negatives\n",
      "\n",
      "\n",
      "Precision = true positives/(true positives+false positives)\n",
      "\n",
      "* high precision means there are not many false positives\n",
      "\n",
      "Recall sensitivity = true positives/(true positives+false negatives)\n",
      "* a high recall means it correctly recalled most churners\n",
      "* minimizes false negatives\n",
      "\n",
      "high precision when the offer is expensive\n",
      "\n",
      "high recall if losing customers is expensive\n",
      "\n",
      "\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "cm=confusion_matrix(y_test,y_pred)\n",
      "\n",
      " >Sample (confusion Matrix)\n",
      "\n",
      "# Import confusion_matrix\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "# Print the confusion matrix\n",
      "print(confusion_matrix(y_test,y_pred)\n",
      "\n",
      ")\n",
      "\n",
      "\n",
      "  Sample (confusion Matrix)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2))\n",
      "\n",
      " >Sample (confusion matrix - RandomForestClassifier)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import confusion_matrix\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "# Print confusion matrix\n",
      "print(confusion_matrix(y_test,y_pred))\n",
      "\n",
      " >Sample (score_precision)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import precision_score\n",
      "from sklearn.metrics import precision_score\n",
      "\n",
      "print(precision_score(y_test,y_pred))\n",
      "\n",
      "  >Sample (Precision and recall)\n",
      "\n",
      "# Import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature variable\n",
      "X = telco.drop('Churn', axis=1)\n",
      "\n",
      "# Create target variable\n",
      "y = telco['Churn']\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
      "\n",
      "# Import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import precision_score\n",
      "from sklearn.metrics import precision_score, recall_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "# Print the precision\n",
      "print(precision_score(y_test,y_pred))\n",
      "\n",
      "print(recall_score(y_test,y_pred))\n",
      "\n",
      "print(confusion_matrix(y_test,y_pred))\n",
      "\n",
      "<<<<<<Other model metrics\n",
      "\n",
      "receiving operating curve ROC \n",
      "\n",
      "every prediction your classifier makes has an associated probability.\n",
      "\n",
      "> 50% belongs to the positive class\n",
      "\n",
      "* default probability threshold in scikit-learn is 50%\n",
      "\n",
      "measuring the true positive rate against the false positive rate we get the roc curve\n",
      "\n",
      "Area under the curve \n",
      "* a large area would have a well performing model\n",
      "\n",
      "* AUC allows you to compare the performance of different classifiers\n",
      "\n",
      "y_pred_prob= logreg.predict_proba(X_test)[:,1]\n",
      "\n",
      "*0 column- the probablity the first column will be 0\n",
      "*1 column= the probability the second column will be 1\n",
      "\n",
      "from sklearn.metrics import roc_curve\n",
      "\n",
      "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
      "\n",
      "fpr=false positive rate\n",
      "tpr=true positive rate\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.plot(fpr,tpr)\n",
      "plt.xlabel(\"False Positive Rate\")\n",
      "plt.ylabel(\"True Positive Rate\")\n",
      "plt.plot([0,1],[0,1],\"k--\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "auc= roc_auc_score(y_test, y_pred)\n",
      "\n",
      "  Sample (Print probabilities - ROC curve)\n",
      "\n",
      "# Generate the probabilities\n",
      "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
      "print(y_pred_prob)\n",
      "\n",
      "from sklearn.metrics import roc_curve\n",
      "\n",
      "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
      "\n",
      "# Plot the ROC curve\n",
      "plt.plot(fpr,tpr)\n",
      "\n",
      "# Add labels and diagonal line\n",
      "plt.xlabel(\"False Positive Rate\")\n",
      "plt.ylabel(\"True Positive Rate\")\n",
      "plt.plot([0, 1], [0, 1], \"k--\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  Sample ( auc)\n",
      "\n",
      "# Import roc_auc_score\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "# Print the AUC\n",
      "print(roc_auc_score(y_test, y_pred_prob)\n",
      "\n",
      " >Precision - recall curve\n",
      "\n",
      "Another way to evaluate model performance is using a precision-recall curve, which shows the tradeoff between precision and recall for different thresholds.\n",
      "\n",
      "  Sample F1 score\n",
      "\n",
      "\n",
      "f1=2 * (precision * recall) / (precision + recall)\n",
      "\n",
      "F1 score is it incorporates both precision and recall into a single metric\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Import f1_score\n",
      "from sklearn.metrics import f1_score\n",
      "\n",
      "# Print the F1 score\n",
      "print(f1_score(y_test, y_pred))\n",
      "\n",
      "\n",
      "#a high F1 score is a sign of a well-performing model\n",
      "\n",
      "    >Tuning your model          \n",
      "\n",
      "hyper parameters of the random forest model\n",
      "1. n_estimators = number of trees\n",
      "2. criterion = quality of split\n",
      "3. max_features= number of features for best split\n",
      "4. max_depth= max depth of tree\n",
      "5. min_sample_splits=minimum samples to spit node\n",
      "6. bootstrap = whether bootstrap samples are used\n",
      "\n",
      "grid search is brute force search\n",
      "a. returns the best model fit\n",
      "\n",
      "cross validation\n",
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "param_grid={'n_estimators':np.arange(10,51)}\n",
      "\n",
      "clf_cv= GridSearchCV(RandomForestClassifier(), param_grid)\n",
      "\n",
      "clf_cv.fit(X,y)\n",
      "\n",
      "print(clf_cv.best_params_)\n",
      "\n",
      "print(clf_cv.best_score_)\n",
      "\n",
      "   >Sample GridSearchCV\n",
      "\n",
      "# Import GridSearchCV\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Create the hyperparameter grid\n",
      "param_grid = {'max_features': ['auto', 'sqrt', 'log2']}\n",
      "\n",
      "# Call GridSearchCV\n",
      "grid_search = GridSearchCV(RandomForestClassifier(), param_grid)\n",
      "\n",
      "# Fit the model\n",
      "grid_search.fit(X, y)\n",
      "\n",
      "print(grid_search.best_params_)\n",
      "print(grid_search.best_score_)\n",
      "\n",
      "  Sample (complex hyper parameter)\n",
      "\n",
      "\n",
      "# Import GridSearchCV\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Create the hyperparameter grid\n",
      "param_grid = {\"max_depth\": [3, None],\n",
      "              \"max_features\": [1, 3, 10],\n",
      "              \"bootstrap\": [True, False],\n",
      "              \"criterion\": [\"gini\", \"entropy\"]}\n",
      "\n",
      "# Call GridSearchCV\n",
      "grid_search = GridSearchCV(RandomForestClassifier(),param_grid)\n",
      "# Fit the model\n",
      "grid_search.fit(X, y)\n",
      "\n",
      "print(grid_search.best_params_)\n",
      "print(grid_search.best_score_)\n",
      "\n",
      "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': None, 'max_features': 10}\n",
      "0.9534953495349535\n",
      "\n",
      "   Sample (Random Grid Search)\n",
      "\n",
      "we could randomly jump around the grid and try different combinations.\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "# Create the hyperparameter grid\n",
      "param_dist = {\"max_depth\": [3, None],\n",
      "              \"max_features\": randint(1, 11),\n",
      "              \"bootstrap\": [True, False],\n",
      "              \"criterion\": [\"gini\", \"entropy\"]}\n",
      "\n",
      "# Call RandomizedSearchCV\n",
      "random_search = RandomizedSearchCV(RandomForestClassifier(),param_dist)\n",
      "# Fit the model)\n",
      "random_search.fit(X, y)\n",
      "\n",
      "print(random_search.best_params_)\n",
      "print(random_search.best_score_)\n",
      "\n",
      "\n",
      "         >Feature importances\n",
      "\n",
      "* scoring represents how much each feature contributes to a prediction.\n",
      "* visualization is an effective way to communicate results to stakeholders\n",
      "1. which features are important drivers of churn\n",
      "2. which features can be removed from the model\n",
      "\n",
      "interpretability might be reasons you get buyin from stakeholders\n",
      "\n",
      "\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "clf.feature_importances_\n",
      "\n",
      "  Sample (calculating feature importances)\n",
      "\n",
      "# Calculate feature importances\n",
      "importances = clf.feature_importances_\n",
      "\n",
      "# Create plot\n",
      "plt.barh(range(X.shape[1]), importances)\n",
      "plt.show()\n",
      "\n",
      "In order to make the plot more readable, we need to do achieve two goals:\n",
      "\n",
      "Re-order the bars in ascending order.\n",
      "Add labels to the plot that correspond to the feature names.\n",
      "\n",
      " >Sort features by importance\n",
      "\n",
      "# Sort importances\n",
      "sorted_index = np.argsort(importances)\n",
      "\n",
      "# Create labels\n",
      "labels = X.columns[sorted_index]\n",
      "\n",
      "# Clear current plot\n",
      "plt.clf()\n",
      "\n",
      "# Create plot\n",
      "plt.barh(range(X.shape[1]), importances[sorted_index], tick_label=labels)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    Adding New Features\n",
      "\n",
      "additional data sources: customer service, web logs, email campaigns, network, transactions, and signal strength\n",
      "\n",
      "1. can improve model performances\n",
      "2. avoid underfitting\n",
      "\n",
      "churn features:\n",
      "Region Code,\n",
      "Total Charges,\n",
      "Total Minutes,\n",
      "Minutes per Call\n",
      "Cost per Call\n",
      "Total Calls\n",
      "\n",
      "\n",
      "Compare both ROC curves\n",
      "\n",
      "Discuss with business the benefits and costs of incorporating the additional features\n",
      "1. improved return on investment\n",
      "2. decreased cost\n",
      "3. increased performance\n",
      "\n",
      "Benefits must exceed costs\n",
      "\n",
      "\n",
      "  Sample Adding additional features\n",
      "\n",
      "# Import necessary modules\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Create training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
      "\n",
      "# Instantiate the classifier\n",
      "clf = RandomForestClassifier()\n",
      "\n",
      "# Fit to the data\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "# Print the accuracy\n",
      "print(clf.score(X_test, y_test))\n",
      "\n",
      "# Predict the labels of the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Print the F1 score\n",
      "print(f1_score(y_test,y_pred))\n",
      "\n",
      "\n",
      "\n",
      " >Exploratory Data Analysis in python\n",
      " >Designing Machine Learning Workflows in python\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\deep learning with keras - binary classification.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\deep learning with keras - binary classification.txt\n",
      "x and y coordinates and labels 0 or 1 representing the colors\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "sns.pairplot(circles, hue=\"target\")\n",
      "\n",
      "topology \n",
      "\n",
      "1. two input layer one for x and y\n",
      "2. four hidden layer\n",
      "3. one output layer\n",
      "\n",
      "sigmoid function = 1/(1+e**-Z)\n",
      " >\n",
      "\n",
      "model= Sequential()\n",
      "\n",
      "model.add(Dense(4, input_shape=(2,),\n",
      "activation='sigmoid'\n",
      "))\n",
      "\n",
      "#model.add(Dense(50, activation='relu'))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(optimizer=Adam(0.01),loss='binary_crossentropy')\n",
      "#model.compile(optimizer=Adam(0.01),loss='mae')\n",
      "\n",
      "model.summary()\n",
      "\n",
      "plot_model(model, to_file='model.png')\n",
      "img=plt.imread('model.png')\n",
      "plt.imshow(img)\n",
      "plt.show()\n",
      "\n",
      "model.compile(optimizer='sgd',loss='binary_crossentropy')\n",
      "\n",
      "model.train(coordinates, labels, epochs=20)\n",
      "\n",
      "preds= model.predict(coordinates)\n",
      "\n",
      "\n",
      " >\n",
      "\n",
      "variance, skewness, kurtosis, entropy, class\n",
      "\n",
      "# Import seaborn\n",
      "import seaborn as sns\n",
      "\n",
      "print(banknotes.keys)\n",
      "\n",
      "# Use pairplot and set the hue to be our class\n",
      "sns.pairplot(banknotes, hue='class') \n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "# Describe the data\n",
      "print('Dataset stats: \\n', banknotes.describe)\n",
      "\n",
      "# Count the number of observations of each class\n",
      "print('Observations per class: \\n', banknotes['class'].value_counts)\n",
      "\n",
      "  multi class classification\n",
      "\n",
      "xCoord, yCoord competitor\n",
      "\n",
      "1. 2 input, 128 dense, 64 dense, 32 dense, 4 outputs\n",
      "\n",
      "softmax\n",
      ".6 Michael\n",
      ".1 Susan\n",
      ".2 Kate\n",
      ".1 Steve\n",
      "\n",
      "\n",
      "model.add(Dense(4, activation='softmax'))\n",
      "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
      "\n",
      "The log loss decreases as the model becomes more accurate in predicting.\n",
      "\n",
      "\n",
      "  >To Categorical\n",
      " 2 input, [128,64,32] hidden layer, 4 output\n",
      "\n",
      "import pandas as pd\n",
      "from keras.utils import to_categorical\n",
      "\n",
      "df=pd.read_csv('data.csv')\n",
      "\n",
      "df.response=pd.Categorical(df.response)\n",
      "df.response=df.response.cat.codes\n",
      "\n",
      "#turn response variable into one-hot encode response vector\n",
      "y=to_categorical(df.response)\n",
      "\n",
      " >\n",
      "# Import to_categorical from keras utils module\n",
      "from keras.utils import to_categorical\n",
      "\n",
      "# Instantiate a sequential model\n",
      "model = Sequential()\n",
      "  \n",
      "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
      "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
      "model.add(Dense(64, activation='relu'))\n",
      "model.add(Dense(32, activation='relu'))\n",
      "  \n",
      "# Add a dense layer with as many neurons as competitors\n",
      "model.add(Dense(4, activation=\"softmax\"))\n",
      "  \n",
      "# Compile your model using categorical_crossentropy loss\n",
      "model.compile(loss=\"categorical_crossentropy\",\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "              \n",
      "model.summary()\n",
      "\n",
      "\n",
      "\n",
      "# Transform into a categorical variable\n",
      "darts.competitor = pd.Categorical(darts.competitor)\n",
      "\n",
      "# Assign a number to each category (label encoding)\n",
      "darts.competitor = darts.competitor.cat.codes \n",
      "\n",
      "# Print the label encoded competitors\n",
      "print('Label encoded competitors: \\n',darts.competitor.head())\n",
      "\n",
      "# Transform into a categorical variable\n",
      "darts.competitor = pd.Categorical(darts.competitor)\n",
      "\n",
      "\n",
      "# Use to_categorical on your labels\n",
      "coordinates = darts.drop(['competitor'], axis=1)\n",
      "competitors = to_categorical(darts.competitor)\n",
      "\n",
      "# Now print the to_categorical() result\n",
      "print('One-hot encoded competitors: \\n',competitors)\n",
      "\n",
      "# Train your model on the training data for 200 epochs\n",
      "model.fit(coord_train,competitors_train,epochs=200)\n",
      "\n",
      "# , your model accuracy on the test data\n",
      "accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
      "\n",
      "# Print accuracy\n",
      "print('Accuracy:', accuracy)\n",
      "\n",
      "# Predict on coords_small_test\n",
      "preds = model.predict(coords_small_test)\n",
      "\n",
      "# Print preds vs true values\n",
      "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
      "for i,pred in enumerate(preds):\n",
      "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
      "\n",
      "# Predict on coords_small_test\n",
      "preds = model.predict(coords_small_test)\n",
      "\n",
      "# Print preds vs true values\n",
      "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
      "for i,pred in enumerate(preds):\n",
      "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
      "\n",
      "# Extract the indexes of the highest probable predictions\n",
      "preds = [np.argmax(pred) for pred in preds]\n",
      "\n",
      "# Print preds vs true values\n",
      "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
      "for i,pred in enumerate(preds):\n",
      "  print(\"{:25} | {}\".format(pred,competitors_small_test[i]))\n",
      "\n",
      "\n",
      " > multi-label\n",
      "\n",
      "model=Sequential()\n",
      "\n",
      "model.add(Dense(2,input_shape=(1,)))\n",
      "\n",
      "model.add(Dense(3,activation='sigmoid'))\n",
      "\n",
      "#each output will be between 0 and 1\n",
      "\n",
      "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
      "model.fit(X_train, y_train, epochs=100, validation_split=0.2)\n",
      "\n",
      "#one versus rest classification\n",
      "\n",
      "#sensor measurements result in parcels to water\n",
      "\n",
      "Multi-label classification problems differ from multi-class problems in that each observation can be labeled with zero or more classes. So classes are not mutually exclusive. \n",
      "\n",
      "To account for this behavior what we do is have an output layer with as many neurons as classes but this time, unlike in multi-class problems, each output neuron has a sigmoid activation function. This makes the output layer able to output a number between 0 and 1 in any of its neurons.\n",
      "\n",
      " \n",
      "\n",
      "# Instantiate a Sequential model\n",
      "\n",
      "model=Sequential()\n",
      "\n",
      "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
      "\n",
      "model.add(Dense(64,input_shape=(20,),activiation='relu'))\n",
      "\n",
      "# Add an output layer of 3 neurons with sigmoid activation\n",
      "model.add(Dense(3,activation='sigmoid'))\n",
      "\n",
      "# Compile your model with adam and binary crossentropy loss\n",
      "model.compile(optimizer=\"adam\",\n",
      "           loss='binary_crossentropy',\n",
      "           metrics=['accuracy'])\n",
      "\n",
      "model.summary()\n",
      "\n",
      "# Train for 100 epochs using a validation split of 0.2\n",
      "model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2)\n",
      "\n",
      "# Predict on sensors_test and round up the predictions\n",
      "preds = model.predict(sensors_test)\n",
      "preds_rounded = np.round(preds)\n",
      "\n",
      "# Print rounded preds\n",
      "print('Rounded Predictions: \\n', preds_rounded)\n",
      "\n",
      "# Evaluate your model's accuracy on the test data\n",
      "accuracy = model.evaluate(sensors_test, parcels_test)[1]\n",
      "\n",
      "# Print accuracy\n",
      "print('Accuracy:', accuracy)\n",
      "\n",
      "\n",
      " callbacks\n",
      "1) EarlyStopping\n",
      "2) ModelCheckpoint\n",
      "3) History\n",
      "\n",
      "print(history.history['loss'])\n",
      "print(history.history['acc'])\n",
      "print(history.history['val_loss'])\n",
      "print(history.history['val_acc'])\n",
      "\n",
      "history, modelcheckpoint, earlystopping\n",
      "\n",
      "fit does callbacks\n",
      "\n",
      "#print(history.history['loss'])\n",
      "plt.figure()\n",
      "plt.plot(history.history['loss'])\n",
      "plt.xlabel('loss')\n",
      "plt.show()\n",
      "\n",
      "  >Early stopping\n",
      "#useful because we don't know how many epochs will be required to complete training\n",
      "\n",
      "from keras.callbacks import EarlyStopping\n",
      "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
      "\n",
      "#patience is the number of epochs to improve before the model is stopped\n",
      "\n",
      "\n",
      "model.fit(X_train, y_train, epochs=100,\n",
      "validation_data=(X_test,y_test),\n",
      "callbacks=[early_stopping])\n",
      "\n",
      "\n",
      " >model checkpoint\n",
      "\n",
      "from keras.callbacks import ModelCheckpoint\n",
      "\n",
      "#allows up to save the model as strings\n",
      "\n",
      "model_save=ModelCheckpoint('best_model.hdf5', save_best_only=True)\n",
      "\n",
      "model.fit(X_train, y_train, epochs=100,\n",
      "validation_data=(X_test,y_test),\n",
      "callbacks=[model_save])\n",
      "\n",
      " >\n",
      "\n",
      "# Train your model and save its history\n",
      "history = model.fit(X_train, y_train, epochs = 50,\n",
      "               validation_data=(X_test, y_test))\n",
      "\n",
      "# Plot train vs test loss during training\n",
      "plot_loss(history.history['loss'], history.history['val_loss'])\n",
      "\n",
      "# Plot train vs test accuracy during training\n",
      "plot_accuracy(history.history['acc'], history.history['val_acc'])\n",
      "\n",
      "# Import the early stopping callback\n",
      "from keras.callbacks import EarlyStopping\n",
      "\n",
      "# Define a callback to monitor val_acc\n",
      "monitor_val_acc = EarlyStopping(monitor='val_acc', \n",
      "                       patience=5)\n",
      "\n",
      "# Train your model using the early stopping callback\n",
      "model.fit(X_train, y_train, \n",
      "           epochs=1000, validation_data=(X_test,y_test),\n",
      "           callbacks=[monitor_val_acc])\n",
      "\n",
      "\n",
      " >\n",
      "\n",
      "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
      "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
      "\n",
      "# Early stop on validation accuracy\n",
      "monitor_val_acc = EarlyStopping(monitor ='val_acc', patience=3)\n",
      "\n",
      "# Save the best model as best_banknote_model.hdf5\n",
      "modelCheckpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only = True)\n",
      "\n",
      "# Fit your model for a stupid amount of epochs\n",
      "history = model.fit(X_train, y_train,\n",
      "                    epochs = 10000000,\n",
      "                    callbacks = [monitor_val_acc, modelCheckpoint],\n",
      "                    validation_data = (X_test, y_test))\n",
      "\n",
      " >Learning Curves\n",
      "\n",
      "1) loss learning curves decrease as epochs go by\n",
      "\n",
      "accuracy learning curves\n",
      "1) increase as epochs go by\n",
      "\n",
      "****model overfitting can be identified if the training curves and the validation curves diverge\n",
      "\n",
      "\n",
      "init_weights=model.get_weights()\n",
      "\n",
      "train_accs[]\n",
      "tests_accs[]\n",
      "\n",
      "for train_size in train_sizes:\n",
      "\tX_train_frac, -, y_train_frac, = train_test_split(X_train,y_train,train_size=train_size)\n",
      "\tmodel.set_weights(initial_weights)\n",
      "\tmodel.fit(X_train_frac, y_train_frac, epochs=100, verbose=0,\n",
      "\tcallbacks[EarlyStopping(monitor='loss', patience=1)]\n",
      "\t\n",
      "\ttrain_acc=model.evalute(X_train_frac, y_train_frac, verbose=0)[1]\n",
      "\ttrain_accs.append(train_acc)\n",
      "\n",
      "\n",
      "\ttest_acc=model.evalute(X_test_frac, y_test_frac, verbose=0)[1]\n",
      "\ttest_accs.append(train_acc)\n",
      "\n",
      "\n",
      " >\n",
      "\n",
      "# Instantiate a Sequential model\n",
      "model = Sequential()\n",
      "\n",
      "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
      "model.add(Dense(16, input_shape = (64,), activation = 'relu'))\n",
      "\n",
      "# Output layer with 10 neurons (one per digit) and softmax\n",
      "model.add(Dense(10, activation='softmax'))\n",
      "\n",
      "# Compile your model\n",
      "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
      "\n",
      "# Test if your model works and can process input data\n",
      "print(model.predict(X_train))\n",
      "\n",
      "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
      "history = model.fit(X_train, y_train, epochs=60, validation_data=(X_test, y_test), verbose=0)\n",
      "\n",
      "# Extract from the history object loss and val_loss to plot the learning curve\n",
      "plot_loss(history.history['loss'],history.history['val_loss'])\n",
      "\n",
      "for size in training_sizes:\n",
      "  \t# Get a fraction of training data (we only care about the training data)\n",
      "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
      "\n",
      "    # Reset the model to the initial weights and train it on the new data fraction\n",
      "    model.set_weights(initial_weights)\n",
      "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
      "\n",
      "    # Evaluate and store the train fraction and the complete test set results\n",
      "    train_accs.append(model.evaluate(X_train_frac, y_train_frac)[1])\n",
      "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
      "    \n",
      "# Plot train vs test accuracies\n",
      "plot_results(train_accs, test_accs)\n",
      "\n",
      "  Activation Functions\n",
      "\n",
      "a=sum of inputs * weights + bias\n",
      "\n",
      "a is passed into an activation function producing y\n",
      "\n",
      "1. sigmoid varies between 0 and 1\n",
      "2. tanh varies between -1 and 1\n",
      "3. relu varies between 0 and infinity\n",
      "4. Leaky relu between negative value and infinity\n",
      "\n",
      "np.random.seed(1)\n",
      "\n",
      "def get_model(act_function):\n",
      "\tmodel=Sequential()\n",
      "\tmodel.add(Dense(4, input_shape=(2,), activation=act_function))\n",
      "\tmodel.add(Dense(1, activation='sigmoid'))\n",
      "\treturn model\n",
      "\n",
      "\n",
      "activations=['relu','sigmoid','tanh','leaky_relu']\n",
      "\n",
      "for funct in activations:\n",
      "\tmodel= get_model(act_function=funct)\n",
      "\thistory=model.fit(X_train, y_train,\n",
      "\t\tvalidation_data=(X_test,y_test),\n",
      "\t\tepochs=100,verbose=False)\n",
      "\tactivation_results[funct]=history\n",
      "\n",
      "val_loss_per_funct = {k:v.history['val_loss] for k,v in activation_results.items()}\n",
      "\n",
      "val_loss_curves.pd.DataFrame(val_loss_per_funct)\n",
      "\n",
      "val_loss_curves.plot(title='Loss per Activation function')\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "# Activation functions to try\n",
      "activations = ['relu','leaky_relu','sigmoid','tanh']\n",
      "\n",
      "# Loop over the activation functions\n",
      "activation_results = {}\n",
      "\n",
      "for act in activations:\n",
      "  # Get a new model with the current activation\n",
      "  model = get_model(act)\n",
      "  # Fit the model\n",
      "  history = model.fit(X_train, y_train,\n",
      "\t\tvalidation_data=(X_test,y_test),\n",
      "\t\tepochs=20,verbose=0)\n",
      "  activation_results[act] = history\n",
      "\n",
      "# Create a dataframe from val_loss_per_function\n",
      "val_loss=  {k:v.history['val_loss'] for k,v in activation_results.items()}\n",
      "\n",
      "# Call plot on the dataframe\n",
      "val_loss_per_function=pd.DataFrame(val_loss)\n",
      "val_loss_per_function.plot()\n",
      "plt.show()\n",
      "\n",
      "# Create a dataframe from val_acc_per_function\n",
      "val_acc =  {k:v.history['val_acc'] for k,v in activation_results.items()}\n",
      "\n",
      "# Call plot on the dataframe\n",
      "val_acc_per_function=pd.DataFrame(val_acc)\n",
      "val_acc_per_function.plot()\n",
      "plt.show()\n",
      "\n",
      "  batch size and batch normalization\n",
      "\n",
      "1. mini-batches advantages\n",
      "a. networks train faster (more weight updates in same amount of time)\n",
      "b. less RAM memory required, can train on huge datasets\n",
      "c. noise can help networks reach a lower error, escaping local minima\n",
      "\n",
      "2. mini-batches disadvantages\n",
      "a. more iterations need to be run\n",
      "b. need to be adjusted, we need to find a good batch size\n",
      "\n",
      "keras uses a batch size of 32\n",
      "\n",
      "The smaller a batch size, the more weight updates per epoch, but at a cost of a more unstable gradient descent. Specially if the batch size is too small and it's not \n",
      "\n",
      "standardization \n",
      "data-mean/standard deviation\n",
      "\n",
      "batch normalization makes sure that independently of the changes, the inputs to the next layer are normalized\n",
      "\n",
      "batch normalization advantages\n",
      "1. improves gradient flow\n",
      "2. allows higher learning rates\n",
      "3. reduces dependence on weight initializations\n",
      "4. acts as an unintended form of regularization\n",
      "5. limits internal covariate shift\n",
      "\n",
      "from keras.layers import BatchNormalization\n",
      "\n",
      "model=Sequential()\n",
      "model.add(Dense(3, input_shape=(2,), activation='relu'))\n",
      "model.add(BatchNormalization())\n",
      "model.add(Dense(1,activation='sigmoid')\n",
      "\n",
      " \n",
      "\n",
      "model = get_model()\n",
      "\n",
      "# Fit your model for 5 epochs with a batch of size the training set\n",
      "model.fit(X_train, y_train, epochs=5, batch_size=len(X_train))\n",
      "print(\"\\n The accuracy when using the whole training set as a batch was: \",\n",
      "      model.evaluate(X_test, y_test)[1])\n",
      "\n",
      " >\n",
      "\n",
      "# Import batch normalization from keras layers\n",
      "from keras.layers import BatchNormalization\n",
      "\n",
      "# Build your deep network\n",
      "batchnorm_model = Sequential()\n",
      "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
      "batchnorm_model.add(BatchNormalization())\n",
      "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
      "batchnorm_model.add(BatchNormalization())\n",
      "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
      "batchnorm_model.add(BatchNormalization())\n",
      "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
      "\n",
      "# Compile your model with sgd\n",
      "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "\n",
      "# Train your standard model, storing its history\n",
      "history1 = standard_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
      "\n",
      "# Train the batch normalized model you recently built, store its history\n",
      "history2 = batchnorm_model.fit(X_test, y_test, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
      "\n",
      "# Call compare_acc_histories passing in both model histories\n",
      "compare_histories_acc(history1, history2)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\Dimensions reduction PCA.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\Dimensions reduction PCA.txt\n",
      " >Dimensions reduction PCA\n",
      "\n",
      "1. express data in a compress formed\n",
      "2. a big deal in big data sets\n",
      "3. remove less-informative \"noise\" features which cause problems for prediction tasks e.g. classification regression.\n",
      "\n",
      "\n",
      " >Principal component Analysis (PCA)\n",
      "\n",
      "1. A fundamental dimension reduction technique\n",
      "2. Rotates data samples to be aligned with axes\n",
      "3. shifts data so they have mean 0\n",
      "\n",
      "pca is sckit-learn component like kmeans or standardscaler\n",
      "\n",
      "fit learns the transformation from given data\n",
      "transform applies the learned transformation\n",
      "\n",
      "PCA features are not correlated\n",
      "1. features of dataset are often correlated like total_phenols\n",
      "and od280\n",
      "\n",
      "2. PCA aligns the data with axes\n",
      "3. resulting PCA features are not linearly correlated\n",
      "4. Pearson correlation is used to determine linear correlation\n",
      "\n",
      " > Covariance\n",
      "\n",
      "1) A measure of how two quantities vary together\n",
      "2) if the distance from the x mean and the distance from the y mean are positive than the point is positively correlated\n",
      "\n",
      "1/m (x-x_mean)*(y-y_mean)  if positive then positively correlated\n",
      "\n",
      "if y is below the mean then the variable is negatively correlated\n",
      "\n",
      "pearson correlation = covariance/(std of x)*(std of y)\n",
      "\n",
      "pearson is a value between -1 and 1\n",
      "\n",
      "value of 0 means no linear correlation\n",
      "\n",
      "Principal components\n",
      "1. Principal components = directions of variance\n",
      "2. PCA aligns principal components with the axes\n",
      "\n",
      "print(model.components_)\n",
      "\n",
      " >Sample pearson correlation of grains width and length\n",
      "\n",
      "# Perform the necessary imports\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.stats import pearsonr\n",
      "\n",
      "\n",
      "# Assign the 0th column of grains: width\n",
      "width = grains[:,0]\n",
      "\n",
      "# Assign the 1st column of grains: length\n",
      "length = grains[:,1]\n",
      "\n",
      "# Scatter plot width vs length\n",
      "plt.scatter(width,length)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "# Calculate the Pearson correlation\n",
      "correlation, pvalue = pearsonr(width,length)\n",
      "\n",
      "# Display the correlation\n",
      "print(correlation)\n",
      "\n",
      "  Sample\n",
      "\n",
      "from sklearn.datasets import load_wine\n",
      "from sklearn.decomposition import PCA\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.stats import pearsonr\n",
      "\n",
      "data = load_wine()\n",
      "X=data.data\n",
      "y=data.target\n",
      "print(data.feature_names)\n",
      "\n",
      "model=PCA()\n",
      "model.fit(X)\n",
      "pca_features=model.transform(X)\n",
      "#print(pca_features)\n",
      "#transformed returns an array with the same number of rows and columns as the original sample\n",
      "\n",
      "xs = pca_features[:,0]\n",
      "\n",
      "# Assign 1st column of pca_features: ys\n",
      "ys = pca_features[:,1]\n",
      "\n",
      "# Scatter plot xs vs ys\n",
      "plt.scatter(xs, ys)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "correlation, pvalue = pearsonr(xs,ys)\n",
      "\n",
      "print(\"the pca correlation \",correlation)\n",
      "\n",
      "  Intrinsic dimension\n",
      "\n",
      "1. Intrinsic dimension is the number of features needed to approximate the dataset\n",
      "2. Essential idea behind dimension reduction\n",
      "3. What is the most compact representation of the samples\n",
      "\n",
      "\n",
      "iris data\n",
      "take three measurements: sepal length, sepal width, petal width\n",
      "\n",
      "when viewed in 3 dimensional space the features mostly lye on a single plane.  Therefore, the data can be represented using two features without losing to much data.\n",
      "\n",
      "1. Intrinsic dimensions equal the number of pca features with significant variance.\n",
      "\n",
      "2. PCA features are ordered by variance descending\n",
      "\n",
      "3. Intrinsic dimension is the number of PCA features with significant variance.\n",
      "\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca=PCA()\n",
      "pca.fit(samples)\n",
      "\n",
      "features = range(pca.n_components_)\n",
      "\n",
      "plt.bar(features, pca.explained_variance_)\n",
      "plt.xticks(features)\n",
      "plt.ylabel('variance')\n",
      "plt.xlabel('pca features')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  >Sample\n",
      "\n",
      "# Make a scatter plot of the untransformed points\n",
      "plt.scatter(grains[:,0], grains[:,1])\n",
      "\n",
      "# Create a PCA instance: model\n",
      "model = PCA()\n",
      "\n",
      "# Fit model to points\n",
      "model.fit(grains)\n",
      "\n",
      "# Get the mean of the grain samples: mean\n",
      "mean = model.mean_\n",
      "\n",
      "# Get the first principal component: first_pc\n",
      "first_pc = model.components_[0,:]\n",
      "\n",
      "# Plot first_pc as an arrow, starting at mean\n",
      "plt.arrow(mean[0],mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
      "\n",
      "# Keep axes on same scale\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      " >Standard Scalar\n",
      "\n",
      "\n",
      "# Perform the necessary imports\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import make_pipeline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Create scaler: scaler\n",
      "scaler = StandardScaler()\n",
      "\n",
      "# Create a PCA instance: pca\n",
      "pca = PCA()\n",
      "\n",
      "# Create pipeline: pipeline\n",
      "pipeline = make_pipeline(scaler, pca)\n",
      "\n",
      "# Fit the pipeline to 'samples'\n",
      "pipeline.fit(samples)\n",
      "\n",
      "# Plot the explained variances\n",
      "features = range(pca.n_components_)\n",
      "plt.bar(features, pca.explained_variance_)\n",
      "plt.xlabel('PCA feature')\n",
      "plt.ylabel('variance')\n",
      "plt.xticks(features)\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Dimension reduction with PCA\n",
      "\n",
      "1. same data with less features\n",
      "2. assumes low variance are noise\n",
      "3. specify how many features to keep\n",
      "\n",
      "PCA(n_components=2)\n",
      "\n",
      "Intrinsic dimension is a good choice\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca=PCA(n_components=2)\n",
      "pca.fit(samples)\n",
      "transformed=pca.transform(samples)\n",
      "print(transformed.shape)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "xs=transformed[:,0]\n",
      "ys=transformed[:,1]\n",
      "plt.scatter(xs,ys,c=species)\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Word frequency arrays\n",
      "1. rows represent documents, columns represent words\n",
      "2. entries measure presence of each word in each document\n",
      "3. most entries have a measure of 0\n",
      "\n",
      "sparse arrays and csr_matrix\n",
      "\n",
      "scipy.sparse.csr_matrix\n",
      "\n",
      "csr_matrix remembers only the non-zero entries (saves space)\n",
      "\n",
      "use scikit-learn TruncatedSVD because pca does not support csr_matrix\n",
      "\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "\n",
      "model=TruncatedSVD(n_components=3)\n",
      "model.fit(documents)\n",
      "TruncatedSVD(algorithm='randomize',...)\n",
      "transformed=model.transform(documents)\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Import PCA\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# Create a PCA model with 2 components: pca\n",
      "pca = PCA(n_components=2)\n",
      "\n",
      "# Fit the PCA instance to the scaled samples\n",
      "pca.fit(scaled_samples)\n",
      "\n",
      "# Transform the scaled samples: pca_features\n",
      "pca_features = pca.transform(scaled_samples)\n",
      "\n",
      "# Print the shape of pca_features\n",
      "print(pca_features.shape)\n",
      "\n",
      " Sample\n",
      "\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Create a TfidfVectorizer: tfidf\n",
      "tfidf = TfidfVectorizer() \n",
      "\n",
      "# Apply fit_transform to document: csr_mat\n",
      "csr_mat = tfidf.fit_transform(documents)\n",
      "\n",
      "# Print result of toarray() method\n",
      "print(csr_mat.toarray())\n",
      "\n",
      "# Get the words: words\n",
      "words = tfidf.get_feature_names()\n",
      "\n",
      "# Print words\n",
      "print(words)\n",
      "\n",
      "  Sample Kmeans pipeline\n",
      "\n",
      "# Perform the necessary imports\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "\n",
      "# Create a TruncatedSVD instance: svd\n",
      "model=TruncatedSVD(n_components=50)\n",
      "\n",
      "# Create a KMeans instance: kmeans\n",
      "kmeans = KMeans(n_clusters=6)\n",
      "\n",
      "# Create a pipeline: pipeline\n",
      "pipeline = make_pipeline(model,kmeans)\n",
      "\n",
      " Sample \n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Fit the pipeline to articles\n",
      "pipeline.fit(articles)\n",
      "\n",
      "# Calculate the cluster labels: labels\n",
      "labels = pipeline.predict(articles)\n",
      "\n",
      "# Create a DataFrame aligning labels and titles: df\n",
      "df = pd.DataFrame({'label': labels, 'article': titles})\n",
      "\n",
      "# Display df sorted by cluster label\n",
      "print(df.sort_values('label'))\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\dimensions reduction techniques.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\dimensions reduction techniques.txt\n",
      "df.shape\n",
      "each row should be an observation\n",
      "\n",
      "remove columns with vary little variance\n",
      "user pd.describe()\n",
      "\n",
      "the column generation had a std of 0 and min and max values that were the same.  you can drop the generation column.\n",
      "\n",
      "pd.describe(exclude='number')\n",
      "\n",
      "describes only non numeric columns\n",
      "\n",
      "  sample    combine list of column names\n",
      "\n",
      "# Remove the feature without variance from this list\n",
      "number_cols = ['HP', 'Attack', 'Defense']\n",
      "\n",
      "# Leave this list as is for now\n",
      "non_number_cols = ['Name', 'Type', 'Legendary']\n",
      "\n",
      "print(pokemon_df.columns)\n",
      "# Sub-select by combining the lists with chosen features\n",
      "df_selected = pokemon_df[number_cols + non_number_cols]\n",
      "\n",
      "# Prints the first 5 lines of the new dataframe\n",
      "print(df_selected.head())\n",
      "\n",
      "print(df_selected.describe(exclude='number'))\n",
      "\n",
      "#remove the columns with almost all similarities\n",
      "\n",
      "\n",
      "       >Reducing dimensionality\n",
      "\n",
      "your dataset will be less complex\n",
      "your dataset will require less disk space\n",
      "\n",
      "training will require less computation time\n",
      "you will have less of chance of overfitting.\n",
      "\n",
      "decide on which features are important\n",
      "\n",
      "dropping a column\n",
      "insurance_df.drop('favorite color', axis=1)\n",
      "\n",
      "       Exploring the dataset\n",
      "\n",
      "sns.pairplot(ansur_df, hue='gender', diag_kind='hist')\n",
      "\n",
      "it provides an one by one comparison of all numeric columns in the dataframe as a scatter plot\n",
      "\n",
      "removing features with very little information prevents information loss.\n",
      "\n",
      "Extract new features from the existing features\n",
      "\n",
      "pca\n",
      "\n",
      "\n",
      "   >sample using pairplot\n",
      "\n",
      "# Create a pairplot and color the points using the 'Gender' feature\n",
      "sns.pairplot(ansur_df_1,hue='Gender', kind='reg', diag_kind='hist')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   sample  > remove stature_m\n",
      "#US Army ANSUR body measurement dataset\n",
      "\n",
      "print(ansur_df_1.columns)\n",
      "# Remove one of the redundant features\n",
      "reduced_df = ansur_df_1.drop('stature_m', axis=1)\n",
      "\n",
      "# Create a pairplot and color the points using the 'Gender' feature\n",
      "sns.pairplot(reduced_df, hue='Gender')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "Index(['Gender', 'footlength', 'headlength', 'n_legs'], dtype='object')\n",
      "\n",
      "  sample remove n_legs which has low variance\n",
      "\n",
      "# Remove the redundant feature\n",
      "reduced_df = ansur_df_2.drop('n_legs', axis=1)\n",
      "\n",
      "# Create a pairplot and color the points using the 'Gender' feature\n",
      "sns.pairplot(reduced_df, hue='Gender', diag_kind='hist')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "         >t-SNE visualization\n",
      "\n",
      "\n",
      "t-SNE is a way to visual high dimensional data using feature extraction\n",
      "\n",
      "t-SNE maximize distance in 2 dimensional space that are different in high dimensional space\n",
      "\n",
      "items that are close to each other may cluster\n",
      "\n",
      "non_numericnon_numeric=['BMI_class','Height_class','Gender','Component','Branch']\n",
      "\n",
      "df_numeric=df.drop(non_numeric,axis=1)\n",
      "\n",
      "df_numeric.shape\n",
      "\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "\n",
      "m=TSNE(learning_rate=50)\n",
      "\n",
      "learning rates 10 to 1000 range\n",
      "\n",
      "tnse_features = m.fit_transform(df_numeric)\n",
      "tsne_features[1:4,:]\n",
      "\n",
      "sns.scatterplot(x='x',y='y', hue='BMI_class', data=df)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "bmi_class: Overweight, normal, underweight\n",
      "\n",
      "\n",
      "Over weight 25 to 29.9\n",
      "Normal weight 18.5 to 24.9\n",
      "Under weight 18.5 or less\n",
      "\n",
      "weight in lbs * 703/ heightin**2\n",
      "\n",
      "\n",
      "Tall >5'9\n",
      "Normal >5'4 to <5'9\n",
      "short <5'4\n",
      "\n",
      "  > Sample tsne  higher dimensional view of the data\n",
      "\n",
      "# Non-numerical columns in the dataset\n",
      "non_numeric = ['Branch', 'Gender', 'Component']\n",
      "\n",
      "# Drop the non-numerical columns from df\n",
      "df_numeric = df.drop(non_numeric, axis=1)\n",
      "\n",
      "# Create a t-SNE model with learning rate 50\n",
      "m = TSNE(learning_rate=50)\n",
      "\n",
      "# Fit and transform the t-SNE model on the numeric dataset\n",
      "tsne_features = m.fit_transform(df_numeric)\n",
      "print(tsne_features.shape)\n",
      "\n",
      "# Color the points according to Army Component\n",
      "sns.scatterplot(x=\"x\", y=\"y\", hue=\"Component\", data=df)\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "    The curse of dimensionality\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc= SVC()\n",
      "\n",
      "svc.fit(X_train, y_train)\n",
      "print(accuracy_score(y_test,svc.predict(X_test))\n",
      "\n",
      "print(accuracy_score(y_train, svc.predict(X_train))\n",
      "\n",
      "\n",
      "features: city, price, n_floors, n_bathrooms, surface_m2\n",
      "\n",
      "increase the number of observations to ensure generalization.  otherwise the model memorize the smaller training set overfitting and it does not generalize well.\n",
      "\n",
      "observations should increase exponentially with the number of features\n",
      "\n",
      "this is called the curse of dimensionality\n",
      "\n",
      "\n",
      "  Sample load and split train and test\n",
      "\n",
      "# Import train_test_split()\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Select the Gender column as the feature to be predicted (y)\n",
      "y = ansur_df['Gender']\n",
      "\n",
      "# Remove the Gender column to create the training data\n",
      "X = ansur_df.drop('Gender', axis=1)c\n",
      "\n",
      "# Perform a 70% train and 30% test data split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
      "\n",
      "print(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))\n",
      "\n",
      "\n",
      "   sample  > fit and predict using svc\n",
      "\n",
      "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Create an instance of the Support Vector Classification class\n",
      "svc = SVC()\n",
      "\n",
      "# Fit the model to the training data\n",
      "svc.fit(X_train, y_train)\n",
      "\n",
      "# Calculate accuracy scores on both train and test data\n",
      "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
      "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
      "\n",
      "print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
      "\n",
      "output: 49.7% accuracy\n",
      "\n",
      "\n",
      "        >features with missing values or little variance\n",
      "\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "\n",
      "sel = VarianceThreshold(threshold=1)\n",
      "sel.fit(ansur_df)\n",
      "\n",
      "mask=sel.get_support()\n",
      "print(mask)\n",
      "\n",
      "reduced_df=ansur_df.loc[:,mask]\n",
      "\n",
      "print(reduced_df.shape)\n",
      "\n",
      "\n",
      "   >normalize the variance\n",
      "\n",
      "sel=VarianceThreshold(threshold=0.005)\n",
      "set.fit(ansur_df / ansur_df.mean())\n",
      "\n",
      "\n",
      "   missing values     >.repairing\n",
      "\n",
      "df.isna().sum()\n",
      "\n",
      "df.isna().sum()/len(df)\n",
      "\n",
      "mask=df.isna().sum()/len(df)<0.3\n",
      "\n",
      "reduced_df=df.loc[:,mask]\n",
      "\n",
      "reduced_df.head()\n",
      "\n",
      "\n",
      "  >sample   > create boxplot\n",
      "\n",
      "# Create the boxplot\n",
      "head_df.boxplot()\n",
      "\n",
      "\n",
      "   sample  > boxplot   normalize   print the variance\n",
      "\n",
      "# Normalize the data\n",
      "normalized_df = head_df / head_df.mean()\n",
      "\n",
      "# Print the variances of the normalized data\n",
      "print(normalized_df.var())\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  >sample  > remove columns with low variance\n",
      "\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "\n",
      "# Create a VarianceThreshold feature selector\n",
      "sel = VarianceThreshold(threshold=0.001)\n",
      "\n",
      "# Fit the selector to normalized head_df\n",
      "sel.fit(head_df / head_df.mean())\n",
      "\n",
      "# Create a boolean mask\n",
      "mask = sel.get_support()\n",
      "\n",
      "# Apply the mask to create a reduced dataframe\n",
      "reduced_df = head_df.loc[:, mask]\n",
      "\n",
      "print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))\n",
      "\n",
      "  sample remove the missing values using a mask\n",
      "\n",
      "# Create a boolean mask on whether each feature less than 50% missing values.\n",
      "mask = school_df.isna().sum() / len(school_df) < 0.5\n",
      "\n",
      "# Create a reduced dataset by applying the mask\n",
      "reduced_df = school_df.loc[:,mask]\n",
      "\n",
      "print(school_df.shape)\n",
      "print(reduced_df.shape)\n",
      "\n",
      "\n",
      "            >Pairwise correlation\n",
      "\n",
      "sns.pairplot(ansur, hue=gender)\n",
      "\n",
      "strength of correlation coefficient\n",
      "\n",
      "r=-1 and r=0  and r=1\n",
      "\n",
      "\n",
      "-1 is perfectly negative correlation\n",
      "1 is perfectly postive correlation\n",
      "0 is no correlation\n",
      "\n",
      "weights_df_corr()\n",
      "\n",
      "the dialog tells us that each feature is perfectly correlated to itself\n",
      "\n",
      "visual the correlation using the seaborn heatmap\n",
      "\n",
      "cmap=sns.diverging_palette(h_neg=10, h_pos=240, as_cmap=True)\n",
      "\n",
      "sns.heatmap(weights_df.corr(), center=0, cmap=cmap, linewidths=1,\n",
      "annot=True, fmt=\".2f\")\n",
      "\n",
      " > remove the diagonal feature referencing itself\n",
      "\n",
      "corr= weights_df.corr()\n",
      "\n",
      "mask=np.triu(np.ones_like(corr,dtype=bool))\n",
      "\n",
      "remove\n",
      "SubjectNumericRace\n",
      "DODRace\n",
      "\n",
      "\n",
      "  sample  > create a heatmap of the correlation\n",
      "\n",
      "# Create the correlation matrix\n",
      "corr = ansur_df.corr()\n",
      "\n",
      "# Draw the heatmap\n",
      "sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  sample  > add a mask\n",
      "\n",
      "# Create the correlation matrix\n",
      "corr = ansur_df.corr()\n",
      "\n",
      "# Generate a mask for the upper triangle\n",
      "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
      "\n",
      "\n",
      "sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\",mask=mask)\n",
      "plt.show()\n",
      "\n",
      "  > removing highly correlated features\n",
      "\n",
      "\n",
      "-1 and 1 and 0\n",
      "\n",
      "drop features that are close to 1 or -1\n",
      "\n",
      "cervical height and suprastermale height\n",
      "chest height and suprastermale height\n",
      "chest height and cericale height\n",
      "\n",
      "\n",
      "corr_df=chest_df.corr().abs()\n",
      "mask=np.triu(np.ones_like(corr_df,dtype=bool))\n",
      "\n",
      "\n",
      "tri_df=corr_matrix.mask(mask)\n",
      "\n",
      "to_drop=[c for c in tri_df.columns if any(tri_df[c]>0.95)]\n",
      "\n",
      "print(to_drop)\n",
      "\n",
      "reduced_df=chest_df.drop(to_drop,axis=1)\n",
      "\n",
      "\n",
      "  > sample  > dropping highly correlated features from the dataframe\n",
      "\n",
      "# Calculate the correlation matrix and take the absolute value\n",
      "corr_matrix = ansur_df.corr().abs()\n",
      "\n",
      "# Create a True/False mask and apply it\n",
      "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
      "tri_df = corr_matrix.mask(mask)\n",
      "\n",
      "# List column names of highly correlated features (r > 0.95)\n",
      "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
      "\n",
      "# Drop the features in the to_drop list\n",
      "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
      "\n",
      "print(\"The reduced dataframe has {} columns.\".format(reduced_df.shape[1]))\n",
      "\n",
      "\n",
      "  >predict gender\n",
      "\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler = StandardScaler()\n",
      "\n",
      "X_train_std= scaler.fit_transform(X_train)\n",
      "\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "lr=LogisticRegression()\n",
      "lr.fit(X_train_std, y_train)\n",
      "\n",
      "X_test_std= scaler.transform(X_test)\n",
      "\n",
      "y_pred=lr.predict(X_test_std)\n",
      "print(accurancy_score(y_test, y_pred))\n",
      "\n",
      "print(lr.coef_)\n",
      "\n",
      "output: array[[-3, 0.14, 7.46, 1.22, 0.87]])\n",
      "\n",
      "coefficients close to zero will contribute little to the end result\n",
      "\n",
      "print(dict(zip(X.column, abs(lr.coef_[0]))))\n",
      "\n",
      "{'chestdepth': 3.0,\n",
      "'handlength':0.14,\n",
      "'neckcircumference':7.46,\n",
      "'shoulderlength':1.22,\n",
      "'earlength':0.87\n",
      "}\n",
      "\n",
      "remove handlength\n",
      "\n",
      "         >Recursive Feature Elimination\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "\n",
      "rfe=RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n",
      "\n",
      "scaler = StandardScaler()\n",
      "X_train_std= scaler.fit_transform(X_train)\n",
      "\n",
      "rfe.fit(X_train_std, y_train)\n",
      "\n",
      "X.columns[rfe.support_]\n",
      "\n",
      "print(dict(zip(X.columns,rfe.ranking_)))\n",
      "\n",
      "high values mean the feature was dropped early on\n",
      "\n",
      "\n",
      "  > Sample  > test features contribution using logistic regression\n",
      "#Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression\n",
      "\n",
      "# Fit the scaler on the training features and transform these in one go\n",
      "X_train_std = scaler.fit_transform(X_train)\n",
      "\n",
      "# Fit the logistic regression model on the scaled training data\n",
      "lr=LogisticRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "\n",
      "# Scale the test features\n",
      "X_test_std = scaler.transform(X_test)\n",
      "\n",
      "# Predict diabetes presence on the scaled test set\n",
      "y_pred = lr.predict(X_test_std)\n",
      "\n",
      "# Prints accuracy metrics and feature coefficients\n",
      "print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \n",
      "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
      "\n",
      "79.6% accuracy on test set.\n",
      "{'pregnant': 0.04, 'glucose': 1.23, 'diastolic': 0.03, 'triceps': 0.24, 'insulin': 0.19, 'bmi': 0.38, 'family': 0.34, 'age': 0.34}\n",
      "\n",
      "\n",
      " >sample  > remove diastolic\n",
      "\n",
      "# Remove the feature with the lowest model coefficient\n",
      "X = diabetes_df[['pregnant', 'glucose',  'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
      "\n",
      "# Performs a 25-75% train test split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
      "\n",
      "# Scales features and fits the logistic regression model\n",
      "lr.fit(scaler.fit_transform(X_train), y_train)\n",
      "\n",
      "# Calculates the accuracy on the test set and prints coefficients\n",
      "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
      "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
      "\n",
      "\n",
      "\n",
      "  Sample  > RFE   > dropping feature columns\n",
      "\n",
      "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
      "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n",
      "\n",
      "# Fits the eliminator to the data\n",
      "rfe.fit(X_train, y_train)\n",
      "\n",
      "# Print the features and their ranking (high = dropped early on)\n",
      "print(dict(zip(X.columns, rfe.ranking_)))\n",
      "\n",
      "# Print the features that are not eliminated\n",
      "print(X.columns[rfe.support_])\n",
      "\n",
      "# Calculates the test set accuracy\n",
      "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
      "\n",
      "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
      "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n",
      "\n",
      "# Fits the eliminator to the data\n",
      "rfe.fit(X_train, y_train)\n",
      "\n",
      "# Print the features and their ranking (high = dropped early on)\n",
      "print(dict(zip(X.columns, rfe.ranking_)))\n",
      "\n",
      "# Print the features that are not eliminated\n",
      "print(X.columns[rfe.support_])\n",
      "\n",
      "# Calculates the test set accuracy\n",
      "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
      " \n",
      "\n",
      "{'pregnant': 5, 'glucose': 1, 'diastolic': 6, 'triceps': 3, 'insulin': 4, 'bmi': 1, 'family': 2, 'age': 1}\n",
      "Index(['glucose', 'bmi', 'age'], dtype='object')\n",
      "80.6% accuracy on test set.\n",
      "\n",
      "\n",
      "diastolic and pregnant dropped early\n",
      "\n",
      "tricept and bmi\n",
      "insulin and glucose\n",
      "\n",
      "        >Random forest classifer\n",
      "\n",
      "ensemble of multiple decision trees \n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "rf=RandomForestClassifier()\n",
      "rf.fit(X_train, y_train)\n",
      "\n",
      "print(rf.feature_importances_)\n",
      "print(sum(rf.feature_importances_))\n",
      "\n",
      "#always sum to 1\n",
      "\n",
      "\n",
      "mask=rf.feature_importances_ > 0.1\n",
      "\n",
      "print(mask)\n",
      "\n",
      "X_reduced=X.loc[:,mask]\n",
      "print(X_reduced.columns)\n",
      "\n",
      "  >drop the least 10 important features at a cycle\n",
      "\n",
      "rfe=RFE(esimator=RandomForestClassifier(),\n",
      "n_features_to_select=6, step=10, verbose=1)\n",
      "\n",
      "#drop the least 10 important features at a cycle\n",
      "\n",
      "print(X.columns[rfe.support_])\n",
      "\n",
      "#contains the remaining features in the model\n",
      "\n",
      "  >sample  > use a randomforestclassifier to determine feature importance\n",
      "\n",
      "# Perform a 75% training and 25% test data split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
      "\n",
      "# Fit the random forest model to the training data\n",
      "rf = RandomForestClassifier(random_state=0)\n",
      "rf.fit(X_train, y_train)\n",
      "\n",
      "# Calculate the accuracy\n",
      "acc = accuracy_score(y_test, rf.predict(X_test))\n",
      "\n",
      "# Print the importances per feature\n",
      "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
      "\n",
      "# Print accuracy\n",
      "print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
      "\n",
      "\n",
      "{'pregnant': 0.09, 'glucose': 0.21, 'diastolic': 0.08, 'triceps': 0.11, 'insulin': 0.13, 'bmi': 0.09, 'family': 0.12, 'age': 0.16}\n",
      "77.6% accuracy on test set.\n",
      "\n",
      "\n",
      "   >sample   > measure feature importances\n",
      "\n",
      "# Create a mask for features importances above the threshold\n",
      "mask = rf.feature_importances_>0.15\n",
      "\n",
      "# Prints out the mask\n",
      "print(mask)\n",
      "\n",
      "mask = rf.feature_importances_ > 0.15\n",
      "\n",
      "# Apply the mask to the feature dataset X\n",
      "reduced_X = X.loc[:,mask]\n",
      "\n",
      "# prints out the selected column names\n",
      "print(reduced_X.columns)\n",
      "\n",
      "output:  'glucose', 'age'\n",
      "\n",
      "   >sample  > RFE\n",
      "\n",
      "# Wrap the feature eliminator around the random forest model\n",
      "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
      "\n",
      "# Fit the model to the training data\n",
      "rfe.fit(X_train, y_train)\n",
      "\n",
      "# Create a mask using an attribute of rfe\n",
      "mask = rfe.support_\n",
      "\n",
      "# Apply the mask to the feature dataset X and print the result\n",
      "reduced_X = X.loc[:, mask]\n",
      "print(reduced_X.columns)\n",
      "\n",
      "output: Index(['glucose', 'insulin'], dtype='object')\n",
      "\n",
      "\n",
      "   Linear regressor\n",
      "linear moe\n",
      "x1,x2,x3 target y \n",
      "where y is a contineous value\n",
      "\n",
      "normal distribution\n",
      "\n",
      "the coefficients determine the affect the feature has on the target\n",
      "\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lr=LinearRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "\n",
      "print(lr.coef_)\n",
      "\n",
      "print(lr.intercept_)\n",
      "\n",
      "r2 tells us the variance of prediction and whether the data is linear or non linear\n",
      "\n",
      "the model tries to fit through the data by minimizing the loss function  (MSE)\n",
      "\n",
      "mse or mean square error creates the linear line through your data.  r2 tells you if the linear regressor is linear or non linear.  regularization helps reduce overfit of the data by smoothing your distribution to look more guassian.\n",
      "\n",
      "regularization will try to keep the model simple by keeping the coefficients low\n",
      "\n",
      "if the model is too low it might overfit, if the model is too high it might become inaccurate\n",
      "\n",
      "\n",
      "la = Lasso()\n",
      "la.fit(X_train, y_train)\n",
      "\n",
      "print(la.coef_)\n",
      "\n",
      "change the alpha\n",
      "la=Lasso(alpha=0.05)\n",
      "\n",
      "output: [4.91 1.76 0]\n",
      "\n",
      "\n",
      "  > sample  > regularize and lasso\n",
      "\n",
      "\n",
      "# Set the test size to 30% to get a 70-30% train test split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=0)\n",
      "\n",
      "# Fit the scaler on the training features and transform these in one go\n",
      "X_train_std = scaler.fit_transform(X_train,y_train)\n",
      "\n",
      "# Create the Lasso model\n",
      "la = Lasso()\n",
      "\n",
      "# Fit it to the standardized training data\n",
      "la.fit(X_train_std,y_train)\n",
      "\n",
      "\n",
      "print(la.coef_)\n",
      "\n",
      " > sample  > using R2 to determine the number of ignored features\n",
      "\n",
      "# Transform the test set with the pre-fitted scaler\n",
      "X_test_std = scaler.transform(X_test)\n",
      "\n",
      "# Calculate the coefficient of determination (R squared) on X_test_std\n",
      "r_squared = la.score(X_test_std, y_test)\n",
      "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
      "\n",
      "# Create a list that has True values when coefficients equal 0\n",
      "zero_coef = la.coef_ == 0\n",
      "\n",
      "# Calculate how many features have a zero coefficient\n",
      "n_ignored = sum(zero_coef)\n",
      "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))\n",
      "\n",
      "    combining features\n",
      "\n",
      "\n",
      "from sklearn.linear_model import Lasso\n",
      "\n",
      "la=Lasso(alpha=0.05)\n",
      "la.fit(X_train, y_train)\n",
      "\n",
      "print(la.coef_)\n",
      "\n",
      "print(la.score(X_test,y_test))\n",
      "\n",
      "  >lassoCV\n",
      "\n",
      "from sklearn.linear_model import LassoCV\n",
      "\n",
      "lcv=LassoCV()\n",
      "\n",
      "lcv.fit(X_train, y_train)\n",
      "print(lcv.alpha_)\n",
      "\n",
      "mask= lcv.coef_ !=0\n",
      "print(mask)\n",
      "\n",
      "reduced_X=X.loc[:,mask]\n",
      "\n",
      "\n",
      "  Combining feature selectors\n",
      "\n",
      "Random forest is a combination of decision trees\n",
      "It is based on the idea that a combination of models can combine to form a strong one\n",
      "\n",
      "\n",
      "from sklearn.linear_model import LassoCV\n",
      "\n",
      "lcv=LassoCV()\n",
      "\n",
      "lcv.fit(X_train, y_train)\n",
      "lcv.score(X_test, y_test)\n",
      "\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "rfe_rf= RFE(estimator=RandomForestRegressor(),\n",
      "\tn_features_to_select =66, step =5, verbose=1)\n",
      "\n",
      "rfe_rf.fit(X_train, y_train)\n",
      "\n",
      "rf_mask=rfe_rf.support_\n",
      "\n",
      "\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "rfe_gb= RFE(estimator=GradientBoostingRegressor(),\n",
      "\tn_features_to_select =66, step =5, verbose=1)\n",
      "\n",
      "rfe_gb.fit(X_train, y_train)\n",
      "\n",
      "gb_mask=rfe_rg.support_\n",
      "\n",
      "votes=np.sum([lcv_mask, rf_mask, gb_mask],axis=0)\n",
      "print(votes)\n",
      "\n",
      "mask=votes>=2\n",
      "\n",
      "reduced_X = X.loc[:,mask]\n",
      "\n",
      "\n",
      "  >Sample    lassoCV   >\n",
      "\n",
      "from sklearn.linear_model import LassoCV\n",
      "\n",
      "# Create and fit the LassoCV model on the training set\n",
      "lcv = LassoCV()\n",
      "lcv.fit(X_train,y_train)\n",
      "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
      "\n",
      "# Calculate R squared on the test set\n",
      "r_squared = lcv.score(X_test,y_test)\n",
      "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
      "\n",
      "# Create a mask for coefficients not equal to zero\n",
      "lcv_mask = lcv.coef_!=0\n",
      "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))\n",
      "\n",
      "X.loc[:,lcv_mask].columns\n",
      "\n",
      "\n",
      "Output: Optimal alpha = 0.089\n",
      "The model explains 88.2% of the test set variance\n",
      "26 features out of 32 selected\n",
      "\n",
      "['acromialheight', 'bideltoidbreadth', 'buttockcircumference', 'buttockpopliteallength', 'chestcircumference', 'chestheight', 'earprotrusion', 'footbreadthhorizontal',\n",
      "       'forearmcircumferenceflexed', 'handlength', 'headbreadth', 'heelbreadth', 'hipbreadth', 'interscyeii', 'lateralfemoralepicondyleheight', 'lateralmalleolusheight', 'radialestylionlength',\n",
      "       'shouldercircumference', 'shoulderelbowlength', 'thighcircumference', 'thighclearance', 'verticaltrunkcircumferenceusa', 'waistcircumference', 'waistdepth', 'wristheight', 'BMI'],\n",
      "      dtype='object')\n",
      "\n",
      "  >sample    RFE   > GradientBoostRegressor\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
      "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
      "             n_features_to_select=10, step=3, verbose=1)\n",
      "rfe_gb.fit(X_train, y_train)\n",
      "\n",
      "# Calculate the R squared on the test set\n",
      "r_squared = rfe_gb.score(X_test,y_test)\n",
      "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
      "\n",
      "gb_mask = rfe_gb.support_!=0\n",
      "print(X.loc[:,gb_mask].columns) \n",
      "\n",
      "\n",
      "Index(['bideltoidbreadth', 'buttockcircumference', 'chestcircumference', 'forearmcircumferenceflexed', 'hipbreadth', 'lateralmalleolusheight', 'shouldercircumference', 'thighcircumference',\n",
      "       'waistcircumference', 'BMI'],\n",
      "      dtype='object')\n",
      "\n",
      "   sample    rfe with RandomForestRegressor\n",
      "\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
      "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
      "             n_features_to_select=10, step=3, verbose=1)\n",
      "rfe_rf.fit(X_train, y_train)\n",
      "\n",
      "# Calculate the R squared on the test set\n",
      "r_squared = rfe_rf.score(X_test, y_test)\n",
      "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
      "\n",
      "# Assign the support array to gb_mask\n",
      "rf_mask = rfe_rf.support_\n",
      "\n",
      "  sample sum the masks\n",
      "\n",
      "# Sum the votes of the three models\n",
      "votes = np.sum([lcv_mask,rf_mask,gb_mask],axis=0)\n",
      "print(votes)\n",
      "\n",
      "meta_mask = votes>=2\n",
      "print(meta_mask)\n",
      "\n",
      "X_reduced = X.loc[:,meta_mask]\n",
      "print(X_reduced.columns)\n",
      "\n",
      "# Plug the reduced dataset into a linear regression pipeline\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
      "lm.fit(scaler.fit_transform(X_train), y_train)\n",
      "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
      "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))\n",
      "\n",
      "\n",
      "Index(['chestcircumference', 'forearmcircumferenceflexed', 'hipbreadth', 'thighcircumference', 'waistcircumference', 'wristheight', 'BMI'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "In [1]:\n",
      "\n",
      "\n",
      "    Feature Extraction\n",
      "\n",
      "feature extraction are new features resulting from the combinations of existing features.\n",
      "\n",
      "df_body['BMI']=df['Weight kg']/df_body['Height m']**2\n",
      "\n",
      "weight and height are obsolete\n",
      "\n",
      "leg_df['leg mm']=leg_df[['right leg mm','left leg mm']].mean(axis=1)\n",
      "\n",
      "    pca\n",
      "\n",
      "scaler = StandardScaler()\n",
      "\n",
      "df_std=pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
      "\n",
      "footlength and handlength\n",
      "\n",
      "people with big feet tend to have big hands\n",
      "\n",
      "principal components\n",
      "\n",
      "   sample  > combine quantity and revenue into price and drop the columns\n",
      "\n",
      "# Calculate the price from the quantity sold and revenue\n",
      "sales_df['price'] = sales_df['revenue']/sales_df['quantity']\n",
      "\n",
      "# Drop the quantity and revenue features\n",
      "reduced_df = sales_df.drop(['quantity','revenue'], axis=1)\n",
      "\n",
      "print(reduced_df.head())\n",
      "\n",
      "   sample  > add three columns into a new column and drop them\n",
      "\n",
      "# Calculate the mean height\n",
      "height_df['height'] = height_df[['height_1','height_2','height_3']].mean(axis=1)\n",
      "\n",
      "print(height_df.columns)\n",
      "# Drop the 3 original height features\n",
      "reduced_df = height_df.drop(['height_1','height_2','height_3'], axis=1)\n",
      "\n",
      "print(reduced_df.head())\n",
      "\n",
      "\n",
      "      >Principal component analysis\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler = StandardScaler()\n",
      "std_df = scaler.fit_transform(df)\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca=PCA()\n",
      "print(pca.fit_transform(std_df))\n",
      "\n",
      "pca.fit(std_df)\n",
      "\n",
      "print(pca.explained_variance_ratio_)\n",
      "\n",
      "\n",
      "\n",
      "   sample   > standard scaler\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Create the scaler and standardize the data\n",
      "scaler = StandardScaler()\n",
      "ansur_std = scaler.fit_transform(ansur_df)\n",
      "\n",
      "\n",
      "   sample  > pca fit transform\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# Create the scaler and standardize the data\n",
      "scaler = StandardScaler()\n",
      "ansur_std = scaler.fit_transform(ansur_df)\n",
      "\n",
      "# Create the PCA instance and fit and transform the data with pca\n",
      "pca = PCA()\n",
      "pc = pca.fit_transform(ansur_std)\n",
      "\n",
      "# This changes the numpy array output back to a dataframe\n",
      "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
      "\n",
      "\n",
      "sns.pairplot(data=pc_df)\n",
      "plt.show()\n",
      "\n",
      "  >sample  > pca component\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# Scale the data\n",
      "scaler = StandardScaler()\n",
      "ansur_std = scaler.fit_transform(ansur_df)\n",
      "\n",
      "# Apply PCA\n",
      "pca = PCA()\n",
      "pca.fit(ansur_std)\n",
      "\n",
      "# Inspect the explained variance ratio per component\n",
      "print(pca.explained_variance_ratio_)\n",
      "\n",
      "print(pca.explained_variance_ratio_.cumsum())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0.61449404 0.19893965 0.06803095 0.03770499 0.03031502 0.0171759\n",
      " 0.01072762 0.00656681 0.00634743 0.00436015 0.0026586  0.00202617\n",
      " 0.00065268]\n",
      "\n",
      "\n",
      "        PCA applications\n",
      "\n",
      "one downside to pca is the remaining components can be hard to intrept.\n",
      "\n",
      "print(pca.components_)\n",
      "\n",
      "this tells to what extent the component is affected by a feature\n",
      "\n",
      "PC 1 = 0.71x hand length + 0.71 foot length\n",
      "PC 2 = -071 x hand length + 0.71 x foot length\n",
      "\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "pipe=Pipeline([\n",
      "\t('scaler',StandardScaler()),\n",
      "\t('reducer',PCA())])\n",
      "\n",
      "pc=pipe.fit_transform(ansur_df)\n",
      "print(pc[:,2])\n",
      "\n",
      "ansur_categories['PC 1'] = pc[:,0]\n",
      "ansur_categories['PC 2'] = pc[:,1]\n",
      "\n",
      "\n",
      "sns.scatterplot(data=ansur_categories,\n",
      "x='PC 1', y='PC 2', hue='Height_class', alpha=0.4)\n",
      "\n",
      "\n",
      "   Add a classifier to the pipeline\n",
      "pipe=Pipeline([\n",
      "\t('scaler',StandardScaler()),\n",
      "\t('reducer',PCA(n_components=3)),\n",
      "\t('classifier', RandomForestClassifier())\n",
      "])\n",
      "\n",
      "\n",
      "pipe.fit(X_train,y_train)\n",
      "print(pipe.steps[1])\n",
      "\n",
      "print(pipe.steps[1][1].explained_variance_ratio_.cumsum())\n",
      "\n",
      "\n",
      "print(pipe.score(X_test,y_test))\n",
      "\n",
      "\n",
      "   sample  > build the pca pipeline\n",
      "\n",
      "# Build the pipeline\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "        \t\t ('reducer', PCA(n_components=2))])\n",
      "\n",
      "# Fit it to the dataset and extract the component vectors\n",
      "pipe.fit(poke_df)\n",
      "vectors = pipe.steps[1][1].components_.round(2)\n",
      "\n",
      "# Print feature effects\n",
      "print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
      "print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))\n",
      "\n",
      "  sample pca pipeline\n",
      "\n",
      "# Build the pipeline\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "                 ('reducer', PCA(n_components=2))])\n",
      "\n",
      "# Fit the pipeline to poke_df and transform the data\n",
      "pc = pipe.fit_transform(poke_df)\n",
      "\n",
      "print(pc)\n",
      "\n",
      "[[-1.5563747  -0.02148212]\n",
      " [-0.36286656 -0.05026854]\n",
      " [ 1.28015158 -0.06272022]\n",
      " ...\n",
      " [ 2.45821626 -0.51588158]\n",
      " [ 3.5303971  -0.95106516]\n",
      " [ 2.23378629  0.53762985]]\n",
      "\n",
      "Index(['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'], dtype='object')\n",
      "\n",
      "poke_cat_df['PC 1'] = pc[:, 0]\n",
      "poke_cat_df['PC 2'] = pc[:, 1]\n",
      "\n",
      "print(poke_cat_df.head())\n",
      "\n",
      "# Use the Type feature to color the PC 1 vs PC 2 scatterplot\n",
      "sns.scatterplot(data=poke_cat_df, \n",
      "                x='PC 1', y='PC 2', hue='Type')\n",
      "plt.show()\n",
      "\n",
      "  sample  > pipeline with pca and randomforest classifier\n",
      "\n",
      "# Build the pipeline\n",
      "pipe = Pipeline([\n",
      "        ('scaler', StandardScaler()),\n",
      "        ('reducer', PCA(n_components=2)),\n",
      "        ('classifier',  RandomForestClassifier(random_state=0))])\n",
      "\n",
      "\n",
      "# Fit the pipeline to the training data\n",
      "pipe.fit(X_train,y_train)\n",
      "\n",
      "# Prints the explained variance ratio\n",
      "print(pipe.steps[1][1].explained_variance_ratio_)\n",
      "\n",
      "# Score the accuracy on the test set\n",
      "accuracy = pipe.score(X_test,y_test)\n",
      "\n",
      "# Prints the model accuracy\n",
      "print('{0:.1%} test set accuracy'.format(accuracy))\n",
      "\n",
      "\n",
      "[0.45624044 0.17767414 0.12858833]\n",
      "95.0% test set accuracy\n",
      "\n",
      "     >Principal component selection\n",
      "\n",
      "pipe= Pipeline([\n",
      "('scaler', StandardScaler()),\n",
      "('reducer',PCA(n_components=0.9))])\n",
      "\n",
      "#explains 90% of the variance\n",
      "\n",
      "pipe.fit(poke_df)\n",
      "\n",
      "print(len(pipe.steps[1][1].components_))\n",
      "\n",
      "There is no right answer to the number of components i should keep. It depends on how much information you are willing to lose to reduce complexity\n",
      "\n",
      "var=pipe.steps[1][1].explain_variance_ratio_\n",
      "\n",
      "plt.plot(var)\n",
      "\n",
      "plt.xlabel('Principal component index')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.show()\n",
      "\n",
      "X=pca.inverse_transform(pc)\n",
      "\n",
      "moves from principal component space back to feature space.\n",
      "\n",
      "2914 grayscale values\n",
      "62x47 pixels=2914 grayscale values\n",
      "\n",
      "test\n",
      "(15,2914)\n",
      "15 pictures\n",
      "training\n",
      "(1333,2914)\n",
      "1333 images\n",
      "\n",
      "pipe= Pipeline([\n",
      "('scaler', StandardScaler()),\n",
      "('reducer',PCA(n_components=290))])\n",
      "\n",
      "pipe.fit(X_train)\n",
      "\n",
      "pc=pipe.fit_transform(X_test)\n",
      "\n",
      "print(pc.shape)\n",
      "15,290\n",
      "\n",
      "10 fold number reduction in features\n",
      "\n",
      "X_rebuilt=pipe.inverse_transform(pc)\n",
      "print(X_rebuilt.shape)\n",
      "\n",
      "img_plotter(X_rebuilt)\n",
      "\n",
      "\n",
      "   sample  > pipeline\n",
      "\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "        \t\t ('reducer', PCA(n_components=0.8))])\n",
      "\n",
      "# Fit the pipe to the data\n",
      "pipe.fit(ansur_df)\n",
      "\n",
      "print('{} components selected'.format(len(pipe.steps[1][1].components_)))\n",
      "\n",
      "11 components selected\n",
      "\n",
      ".9 n_components requires 23 components selected\n",
      "\n",
      " > sample pipeline     variance elbow\n",
      "\n",
      "# Pipeline a scaler and pca selecting 10 components\n",
      "pipe = Pipeline([('scaler', StandardScaler()),\n",
      "        \t\t ('reducer', PCA(n_components=10))])\n",
      "\n",
      "# Fit the pipe to the data\n",
      "pipe.fit(ansur_df)\n",
      "\n",
      "\n",
      "# Plot the explained variance ratio\n",
      "plt.plot(pipe.steps[1][1].explained_variance_ratio_)\n",
      "\n",
      "plt.xlabel('Principal component index')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.show()\n",
      "\n",
      "   sample  > hand written numbers\n",
      "\n",
      "plot_digits(X_test)\n",
      "\n",
      "print(X_test.shape)\n",
      "(16,784)\n",
      "\n",
      "\n",
      "    sample  > pc transform\n",
      "\n",
      "# Transform the input data to principal components\n",
      "pc = pipe.transform(X_test)\n",
      "\n",
      "\n",
      "# Prints the number of features per dataset\n",
      "print(\"X_test has {} features\".format(X_test.shape[1]))\n",
      "print(\"pc has {} features\".format(pc.shape[1]))\n",
      "\n",
      "X_test has 784 features\n",
      "pc has 78 features\n",
      "\n",
      "# Inverse transform the components to original feature space\n",
      "X_rebuilt = pipe.inverse_transform(pc)\n",
      "\n",
      "# Prints the number of features\n",
      "print(\"X_rebuilt has {} features\".format(X_rebuilt.shape[1]))\n",
      "\n",
      "X_rebuilt has 784 features\n",
      "\n",
      "# Plot the reconstructed data\n",
      "plot_digits(X_rebuilt)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\generating features.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\generating features.txt\n",
      "taking raw data and generating features\n",
      "\n",
      "different types of data\n",
      "1. converting to vectors\n",
      "2. distributing normally\n",
      "\n",
      "contineous (integer and floats)\n",
      "categorical (limited set of values)\n",
      "ordinal (ranked values)\n",
      "boolean (true/false)\n",
      "datetime\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "df= pd.read_csv(path_to_csv_file)\n",
      "\n",
      "print(df.head())\n",
      "\n",
      "print(df.columns)\n",
      "\n",
      "stack overflow dataset\n",
      "\n",
      "SurveyData\n",
      "FormalEducation\n",
      "ConvertedSalary\n",
      "Hobby\n",
      "Country\n",
      "StackOverflowJobsRecommend\n",
      "VersionControl\n",
      "Age\n",
      "Years Experience\n",
      "Gender\n",
      "RawSalary\n",
      "\n",
      "print(df.dtypes)\n",
      "\n",
      "only_ints = df.select_dtypes(include=['int'])\n",
      "print(only_ints.columns)\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Import so_survey_csv into so_survey_df\n",
      "so_survey_df = pd.read_csv(so_survey_csv)\n",
      "\n",
      "print(so_survey_df.head())\n",
      "\n",
      "print(so_survey_df.dtypes)\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Create subset of only the numeric columns\n",
      "so_numeric_df = so_survey_df.select_dtypes(include=['int','float'])\n",
      "\n",
      "# Print the column names contained in so_survey_df_num\n",
      "print(so_numeric_df.columns)\n",
      "\n",
      "\n",
      "  Dealing with Categorical Variables\n",
      "\n",
      "1 - India\n",
      "2 - Usa\n",
      "\n",
      "encoding categorical features\n",
      "1) One-hot encoding\n",
      "2) dummy encoding\n",
      "\n",
      "    One-hot encoding\n",
      "categories are converted to columns with possible values of 0 or 1\n",
      "\n",
      "pd.get_dummies(df, columns=['Country'], prefix='C')\n",
      "\n",
      "#prefix improves readibility\n",
      "\n",
      "pd.get_dummies(df, columns=['Country'], prefix='C', drop_first=True)\n",
      "\n",
      "#drops the first category because it is inferred\n",
      "\n",
      "  Limiting your columns\n",
      "\n",
      "counts=df['Country'].value_counts()\n",
      "\n",
      "mask= df['Country'].isin(counts[counts<5].index)\n",
      "\n",
      "#a mask is a list of booleans\n",
      "\n",
      "df['Country'][mask]='Other'\n",
      "counts=df['Country'].value_counts()\n",
      "\n",
      " >Sample\n",
      "\n",
      "one_hot_encoded = pd.get_dummies(so_survey_df, columns=['Country'], prefix='OH')\n",
      "\n",
      "# Print the columns names\n",
      "print(one_hot_encoded.columns)\n",
      "\n",
      " >Sample drop first column\n",
      "\n",
      "# Create dummy variables for the Country column\n",
      "dummy = pd.get_dummies(so_survey_df, columns=['Country'], drop_first=True, prefix='DM')\n",
      "\n",
      "# Print the columns names\n",
      "print(dummy.columns)\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Create a series out of the Country column\n",
      "countries = so_survey_df['Country']\n",
      "\n",
      "# Get the counts of each category\n",
      "country_counts = countries.value_counts()\n",
      "\n",
      "# Print the count values for each category\n",
      "print(country_counts)\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Create a series out of the Country column\n",
      "countries = so_survey_df['Country']\n",
      "\n",
      "# Get the counts of each category\n",
      "country_counts = countries.value_counts()\n",
      "\n",
      "# Create a mask for only categories that occur less than 10 times\n",
      "mask = countries.isin(country_counts[country_counts<10].index)\n",
      "\n",
      "# Print the top 5 rows in the mask series\n",
      "print(mask.head())\n",
      "print(countries[mask])\n",
      "\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Create a series out of the Country column\n",
      "countries = so_survey_df['Country']\n",
      "\n",
      "# Get the counts of each category\n",
      "country_counts = countries.value_counts()\n",
      "\n",
      "# Create a mask for only categories that occur less than 10 times\n",
      "mask = countries.isin(country_counts[country_counts < 10].index)\n",
      "\n",
      "# Label all other categories as Other\n",
      "countries[mask] = 'Other'\n",
      "\n",
      "# Print the updated category counts\n",
      "print(pd.value_counts(countries))\n",
      "\n",
      " >Types of numeric features\n",
      "1. is the magnitude the most important feature.  Decide if a threshhold is more important than a reoccurring pattern\n",
      "\n",
      "df['Binary_Violation']=0\n",
      "df.loc[df['Number_Of_Violations']>0,'Binary_Violation']=1\n",
      "\n",
      "\n",
      "\n",
      "df['Binned_Group]=pd.cut(\n",
      "df['Number_Of_Violations'],\n",
      "bins=[-np.inf,0,2, np.inf]\n",
      "labels=[1,2,3])\n",
      "\n",
      "\n",
      "  >Sample\n",
      "\n",
      "# Create the Paid_Job column filled with zeros\n",
      "so_survey_df['Paid_Job'] = 0\n",
      "\n",
      "# Replace all the Paid_Job values where ConvertedSalary is > 0\n",
      "so_survey_df.loc[so_survey_df['ConvertedSalary']>0, 'Paid_Job'] = 1\n",
      "\n",
      "# Print the first five rows of the columns\n",
      "print(so_survey_df[['Paid_Job', 'ConvertedSalary']].head())\n",
      "\n",
      "\n",
      " Sample\n",
      "# Bin the continuous variable ConvertedSalary into 5 bins\n",
      "so_survey_df['equal_binned'] = pd.cut(so_survey_df['ConvertedSalary'], 5\n",
      ")\n",
      "\n",
      "# Print the first 5 rows of the equal_binned column\n",
      "print(so_survey_df[['equal_binned', 'ConvertedSalary']].head())\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Import numpy\n",
      "import numpy as np\n",
      "\n",
      "# Specify the boundaries of the bins\n",
      "bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\n",
      "\n",
      "# Bin labels\n",
      "labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
      "\n",
      "# Bin the continuous variable ConvertedSalary using these boundaries\n",
      "so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedSalary'], \n",
      "                                         bins=bins, labels=labels)\n",
      "\n",
      "# Print the first 5 rows of the boundary_binned column\n",
      "print(so_survey_df[['boundary_binned', 'ConvertedSalary']].head())\n",
      "\n",
      " >Messy and missing values\n",
      "1. Data not being collected properly\n",
      "2. Collection and management errors\n",
      "3. Data intentionally being omitted\n",
      "4. Could be created due to transformation of the data\n",
      "\n",
      "print(df.info()) #to look at how complete the data is\n",
      "\n",
      "print(df.isnull()) # will show where the rows are null\n",
      "print (df.isnull().sum())\n",
      "\n",
      "print(df.notnull())\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Subset the DataFrame\n",
      "sub_df = so_survey_df[['Age','Gender']]\n",
      "\n",
      "# Print the number of non-missing values\n",
      "print(sub_df.info())\n",
      "\n",
      " >Sample (missing values)\n",
      "# Print the locations of the missing values\n",
      "print(sub_df.head(10).isnull())\n",
      "\n",
      "  Sample (not missing values)\n",
      "\n",
      "# Print the locations of the non-missing values\n",
      "print(sub_df.head(10).notnull())\n",
      "\n",
      "\n",
      " >Dealing with missing values\n",
      "\n",
      "1. If you are certain that data is being randomly omitted then apply complete case analysis\n",
      "a. The record is excluded from the model if any of its records are missing\n",
      "\n",
      "df.dropna(how='any')\n",
      "#drops rows with missing values \n",
      "\n",
      "df.dropna(subset=['VersionControl'])\n",
      "\n",
      " >Issues with deletion\n",
      "* it deletes valid data points\n",
      "* relies on randomness\n",
      "* reduces information and degrees of freedom\n",
      "\n",
      "df[VersionControl'].fillna(value='None given', inplace=True)\n",
      "\n",
      " Recording missing values\n",
      "\n",
      "df['SalaryGiven']= df['ConvertedSalary'].notnull()\n",
      "\n",
      "df.drop(columns=['ConvertedSalary']) #drops a specific column\n",
      "\n",
      "\n",
      " >Sample\n",
      "print(so_survey_df.info)\n",
      "\n",
      "print(so_survey_df.shape)\n",
      "\n",
      "\n",
      " >Sample (drop incomplete rows)\n",
      "# Create a new DataFrame dropping all incomplete rows\n",
      "no_missing_values_rows = so_survey_df.dropna(how='any')\n",
      "\n",
      "# Print the shape of the new DataFrame\n",
      "print(no_missing_values_rows.shape)\n",
      "\n",
      " >Sample (drop incomplete columns)\n",
      "# Create a new DataFrame dropping all columns with incomplete rows\n",
      "no_missing_values_cols = so_survey_df.dropna(how='any', axis=1)\n",
      "\n",
      "# Print the shape of the new DataFrame\n",
      "print(no_missing_values_cols.shape)\n",
      "\n",
      "  Sample (drop rows where a column has incomplete data)\n",
      "\n",
      "# Drop all rows where Gender is missing\n",
      "no_gender = so_survey_df.dropna(subset=['Gender'])\n",
      "\n",
      "# Print the shape of the new DataFrame\n",
      "print(no_gender.shape)\n",
      "\n",
      " >Sample print occurrences\n",
      "# Print the count of occurrences\n",
      "print(so_survey_df['Gender'].value_counts())\n",
      "\n",
      " >Sample replace missing values\n",
      "\n",
      "# Replace missing values\n",
      "so_survey_df['Gender'].fillna(value='Not given', inplace=True)\n",
      "\n",
      "# Print the count of each value\n",
      "print(so_survey_df['Gender'].value_counts())\n",
      "\n",
      "\n",
      "    Filling contineous missing values\n",
      "\n",
      "* if you building a predictive model, you may not have the option of deleting the rows\n",
      "\n",
      "\n",
      "strategies for replacing values for\n",
      "1. categorical columns: replace missing values with the [most common occurring value] or with a string that flags missing values as ['None']\n",
      "2. numerical columns: replace missing values with a suitable value\n",
      "a. mean\n",
      "b. median\n",
      "\n",
      "*It can lead to bias of the variant or covariances of the model\n",
      "\n",
      "print(df['ConvertedSalary'].mean())\n",
      "print(df['ConvertedSalary'].median())\n",
      "\n",
      "df['ConvertedSalary']=df['ConvertedSalary'].fillna(df['ConvertedSalary'].mean())\n",
      "\n",
      "df['ConvertedSalary']=df['ConvertedSalary']\\.astype('int64')  #reduces the precision to an int\n",
      "\n",
      "or\n",
      "\n",
      "df['ConvertedSalary']=df['ConvertedSalary'].fillna(round(df['ConvertedSalary'].mean()))\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Print the first five rows of StackOverflowJobsRecommend column\n",
      "print(so_survey_df['StackOverflowJobsRecommend'].head(5))\n",
      "\n",
      " >Sample filling the missing data with the mean\n",
      "\n",
      "# Fill missing values with the mean\n",
      "so_survey_df['StackOverflowJobsRecommend'].fillna(so_survey_df['StackOverflowJobsRecommend'].mean(), inplace=True)\n",
      "\n",
      "# Print the first five rows of StackOverflowJobsRecommend column\n",
      "print(so_survey_df['StackOverflowJobsRecommend'].head())\n",
      "\n",
      "  Sample Round the mean\n",
      "\n",
      "# Fill missing values with the mean\n",
      "so_survey_df['StackOverflowJobsRecommend'].fillna(so_survey_df['StackOverflowJobsRecommend'].mean(), inplace=True)\n",
      "\n",
      "# Round the StackOverflowJobsRecommend values\n",
      "so_survey_df['StackOverflowJobsRecommend'] = round(so_survey_df['StackOverflowJobsRecommend'])\n",
      "\n",
      "# Print the top 5 rows\n",
      "print(so_survey_df['StackOverflowJobsRecommend'].head())\n",
      "\n",
      "\n",
      "    >Dealing with other data issues\n",
      "\n",
      "\n",
      "1. data imported from other source might prevent it from being read as a number\n",
      "\n",
      "print(df['RawSalary'].dtype)\n",
      "output: type object\n",
      "\n",
      "df['RawSalary'] = df['RawSalary'].str.replace(',','')\n",
      "\n",
      "better\n",
      "\n",
      "df['RawSalary']=df['RawSalary'].astype('float')\n",
      "\n",
      "df['RawSalary']=to_numeric(df['RawSalary'],type='coerce')\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Remove the commas in the column\n",
      "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace(',', '')\n",
      "\n",
      "\n",
      "# Remove the dollar signs in the column\n",
      "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('$','')\n",
      "\n",
      " >Sample (show the rows with missing values)\n",
      "\n",
      "# Attempt to convert the column to numeric values\n",
      "numeric_vals = pd.to_numeric(so_survey_df['RawSalary'], errors='coerce')\n",
      "\n",
      "# Find the indexes of missing values\n",
      "idx = numeric_vals.isnull()\n",
      "\n",
      "# Print the relevant rows\n",
      "print(so_survey_df['RawSalary'][idx])\n",
      "\n",
      "  Sample convert to float\n",
      "\n",
      "# Replace the offending characters\n",
      "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('Â£','')\n",
      "\n",
      "# Convert the column to float\n",
      "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].astype('float')\n",
      "\n",
      "# Print the column\n",
      "print(so_survey_df['RawSalary'])\n",
      "\n",
      "  Sample (chaining methods)\n",
      "# Use method chaining\n",
      "so_survey_df['RawSalary'] = so_survey_df['RawSalary']\\\n",
      "                              .str.replace(',','')\\\n",
      "                              .str.replace('$','')\\\n",
      "                              .str.replace('Â£','')\\\n",
      "                              .astype('float')\n",
      " \n",
      "# Print the RawSalary column\n",
      "print(so_survey_df['RawSalary'])\n",
      "\n",
      "\n",
      "            Data distributions *************\n",
      "\n",
      "Understand how your data is distributed\n",
      "\n",
      "1. Most models require your data to be on the same scale\n",
      "\n",
      "2. Normal distributions follow a bell shape\n",
      "68% of the data lies within 1 standard deviations from the mean\n",
      "95% of the data lies within 2 standard deviations from the mean\n",
      "99% of the data lies within 3 standard deviations from the mean\n",
      "\n",
      "df.hist()\n",
      "plt.show()\n",
      "\n",
      "skewed data to the left is referred to as having a long right tail\n",
      "\n",
      "box plot\n",
      "interquartile range (IQR)\n",
      "mean and Q1=25 percentile and Q3=75 percentile\n",
      "\n",
      "Maximum=Q3 + 1.5 IQR\n",
      "Minimum=Q1 - 1.5 IQR\n",
      "\n",
      "\n",
      "df[['Age','YearsCode']].boxplot()\n",
      "plt.show()\n",
      "\n",
      "  Paring distributions\n",
      "\n",
      "import seaborn as sns\n",
      "sns.pairplot(df)\n",
      "\n",
      " >Details about the data distribution\n",
      "\n",
      "df.describe()\n",
      "\n",
      "1. remove outliers\n",
      "2. ensure comparisons\n",
      "\n",
      "\n",
      " >Samples\n",
      "\n",
      "# Create a histogram\n",
      "so_numeric_df.hist()\n",
      "plt.show()\n",
      "\n",
      "Age, YearsExperience,ConvertedSalary\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Create a boxplot of two columns\n",
      "so_numeric_df[['Age', 'Years Experience']].boxplot()\n",
      "plt.show()\n",
      "\n",
      " Sample\n",
      "# Create a boxplot of ConvertedSalary\n",
      "\n",
      "df[['ConvertedSalary']].boxplot()\n",
      "plt.show()\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Import packages\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Plot pairwise relationships\n",
      "sns.pairplot(so_numeric_df)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "          >Scaling and Transformations\n",
      "\n",
      "normalization involves (min and max scaling)\n",
      "\n",
      "min and max scaling are between 0 and 1\n",
      "\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "scaler.fit(df[['Age']])\n",
      "\n",
      "df['normalized_age']=scaler.transform(df[['Age']])\n",
      "\n",
      "\n",
      "The other scaler is called Standardization\n",
      "1. Standardization finds the mean of the data and centerizes the data around the mean\n",
      "2. The number of standard deviations is used as the new values\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler=StandardScaler()\n",
      "\n",
      "scaler.fit(df[['Age]])\n",
      "df['standardized_col]= scaler.transform(df[['Age']])\n",
      "\n",
      "\n",
      "\n",
      " >Log Transformation\n",
      "\n",
      "from sklearn.preprocessing import PowerTransformer\n",
      "\n",
      "log = PowerTransformer()\n",
      "\n",
      "log.fit(df[['ConvertedSalary']])\n",
      "\n",
      "df['log_ConvertedSalary']=log.transform(df[['ConvertedSalary']])\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "# Import MinMaxScaler\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "# Instantiate MinMaxScaler\n",
      "MM_scaler = MinMaxScaler()\n",
      "\n",
      "# Fit MM_scaler to the data\n",
      "MM_scaler.fit(so_numeric_df[['Age']])\n",
      "\n",
      "# Transform the data using the fitted scaler\n",
      "so_numeric_df['Age_MM'] = MM_scaler.transform(so_numeric_df[['Age']])\n",
      "\n",
      "# Compare the origional and transformed column\n",
      "print(so_numeric_df[['Age_MM', 'Age']].head())\n",
      "\n",
      " >Sample (standardScaler)\n",
      "# Import StandardScaler\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "\n",
      "# Instantiate StandardScaler\n",
      "SS_scaler = StandardScaler()\n",
      "\n",
      "# Fit SS_scaler to the data\n",
      "SS_scaler.fit(so_numeric_df[['Age']])\n",
      "\n",
      "# Transform the data using the fitted scaler\n",
      "so_numeric_df['Age_SS'] = SS_scaler.transform(so_numeric_df[['Age']])\n",
      "\n",
      "# Compare the origional and transformed column\n",
      "print(so_numeric_df[['Age_SS', 'Age']].head())\n",
      "\n",
      "\n",
      " >Power Transform (moving from one coordinate system to another - to achieve normal distribution)\n",
      "\n",
      "# Import PowerTransformer\n",
      "from sklearn.preprocessing import PowerTransformer\n",
      "\n",
      "# Instantiate PowerTransformer\n",
      "pow_trans = PowerTransformer()\n",
      "\n",
      "\n",
      "# Train the transform on the data\n",
      "pow_trans.fit(so_numeric_df[['ConvertedSalary']])\n",
      "\n",
      "# Apply the power transform to the data\n",
      "so_numeric_df['ConvertedSalary_LG'] = pow_trans.transform(so_numeric_df[['ConvertedSalary']])\n",
      "\n",
      "# Plot the data before and after the transformation\n",
      "so_numeric_df[['ConvertedSalary', 'ConvertedSalary_LG']].hist()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  How to remove outliers\n",
      "1. incorrect data report\n",
      "2. negative impacting the model\n",
      "\n",
      "strategy one\n",
      "removing the smallest quantile\n",
      "removing the top 5 percent\n",
      "\n",
      "q_cutoff = df['column_name'].quantile(0.95)\n",
      "mask=df['column_name] < q_cutoff\n",
      "trimmed_df=df[mask]\n",
      "\n",
      "or use standard deviations\n",
      "\n",
      "3 standard deviations expected as outliers\n",
      "\n",
      "mean=df['col_name'].mean()\n",
      "std=df['col_name'].std()\n",
      "\n",
      "cut_off=std * 3\n",
      "\n",
      "lower, upper  = mean-cut_off, mean+cut_off\n",
      "\n",
      "new_df= df[(df['col_name']< upper) & (df['col_name']>lower)]\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Find the 95th quantile\n",
      "quantile = so_numeric_df['ConvertedSalary'].quantile(0.95)\n",
      "\n",
      "# Trim the outliers\n",
      "trimmed_df = so_numeric_df[so_numeric_df['ConvertedSalary'] < quantile]\n",
      "\n",
      "# The original histogram\n",
      "so_numeric_df[['ConvertedSalary']].hist()\n",
      "plt.show()\n",
      "plt.clf()\n",
      "\n",
      "# The trimmed histogram\n",
      "trimmed_df[['ConvertedSalary']].hist()\n",
      "plt.show()\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Find the mean and standard dev\n",
      "std = so_numeric_df['ConvertedSalary'].std()\n",
      "mean = so_numeric_df['ConvertedSalary'].mean()\n",
      "\n",
      "# Calculate the cutoff\n",
      "cut_off = std * 3\n",
      "lower, upper = mean - cut_off, mean+cut_off\n",
      "\n",
      "# Trim the outliers\n",
      "trimmed_df = so_numeric_df[(so_numeric_df['ConvertedSalary'] < upper)     & (so_numeric_df['ConvertedSalary'] >lower)]\n",
      "\n",
      "# The trimmed box plot\n",
      "trimmed_df[['ConvertedSalary']].boxplot()\n",
      "plt.show()\n",
      "\n",
      "   >Scaling and Transforming new data\n",
      "\n",
      "1. apply the same scaler to training data and test data\n",
      "a. fit and transform the training data\n",
      "b. only transform the test data\n",
      "c. use the threshholds on the train set to remove outliers in the test set\n",
      "d. it is in rare cases that you want to remove outliers in the data\n",
      "\n",
      "\n",
      " >Sample (scaling)\n",
      "\n",
      "# Import StandardScaler\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Apply a standard scaler to the data\n",
      "SS_scaler = StandardScaler()\n",
      "\n",
      "\n",
      "# Fit the standard scaler to the data\n",
      "SS_scaler.fit(so_train_numeric[['Age']])\n",
      "\n",
      "# Transform the test data using the fitted scaler\n",
      "so_test_numeric['Age_ss'] = SS_scaler.transform(so_test_numeric[['Age']])\n",
      "print(so_test_numeric[['Age', 'Age_ss']].head())\n",
      "\n",
      "  Sample (excluding)\n",
      "\n",
      "train_std = so_train_numeric['ConvertedSalary'].std()\n",
      "train_mean = so_train_numeric['ConvertedSalary'].mean()\n",
      "\n",
      "cut_off = train_std * 3\n",
      "train_lower, train_upper = train_mean-cut_off, train_mean + cut_off\n",
      "\n",
      "# Trim the test DataFrame\n",
      "trimmed_df = so_test_numeric[(so_test_numeric['ConvertedSalary'] < train_upper) \\\n",
      "                             & (so_test_numeric['ConvertedSalary'] > train_lower)]\n",
      "\n",
      "\n",
      "       Introduction to Text Encoding\n",
      "\n",
      "1. Data not in a predetermine form is called unstructured data\n",
      "a. Text is an example of unstructured data\n",
      "b. words into columns and vectors\n",
      "\n",
      "[a-zA-Z] : all characters\n",
      "[^a-zA-Z]: all non letter characters\n",
      "\n",
      "\n",
      "speech_df['text']=speech_df['text'].str.replace('[^a-zA-Z]',' ')\n",
      "\n",
      "replacing using regular expressions\n",
      "\n",
      "speech_df['text']=speech_df['text'].lower()\n",
      "\n",
      "speech_df['word_counts']=speech_df['word_count'].str.split().str.len()\n",
      "\n",
      "speech_df['avg_word_len']= speech_df['char_cnt']/speech_df['word_cnt']\n",
      "\n",
      " Sample\n",
      "# Print the first 5 rows of the text column\n",
      "print(speech_df['text'].head())\n",
      "\n",
      " Sample\n",
      "# Replace all non letter characters with a whitespace\n",
      "speech_df['text_clean'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')\n",
      "\n",
      "\n",
      "# Change to lower case\n",
      "speech_df['text_clean'] = speech_df['text_clean'].str.lower()#\n",
      "\n",
      "# Print the first 5 rows of the text_clean column\n",
      "print(speech_df['text_clean'].head())\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Find the length of each text\n",
      "speech_df['char_cnt'] = speech_df['text_clean'].str.len()\n",
      "\n",
      "# Count the number of words in each text\n",
      "speech_df['word_cnt'] = speech_df['text_clean'].str.split().str.len()\n",
      "\n",
      "# Find the average length of word\n",
      "speech_df['avg_word_length'] = speech_df['char_cnt']/speech_df['word_cnt']\n",
      "\n",
      "# Print the first 5 rows of these columns\n",
      "print(speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']])\n",
      "\n",
      "\n",
      "        Word Count representation\n",
      "\n",
      "1. Create a list of unique words as columns in the dataframe with counts for each column\n",
      "\n",
      "from skearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "cv=CountVectorizer()\n",
      "\n",
      "print(cv)\n",
      "\n",
      "min_df: minimum fraction of documents the word must occur in\n",
      "max_df: maximum fraction of documents the word can occur in\n",
      "\n",
      "cv=CountVectorizer(min_df=0.1, max_df=0.9)\n",
      "\n",
      "cv.fit(speech_df['text_clean'])\n",
      "cv_transformed=cv.transform(speech_df['text_clean'])\n",
      "print(cv_transformed)\n",
      "\n",
      "cv_transformed.toarray() \n",
      "\n",
      "#transforms from a spare array to a regular array\n",
      "\n",
      "feature_names= cv.get_feature_names()\n",
      "\n",
      "print(feature_names)\n",
      "\n",
      "\n",
      "cv_df=pd.DataFrame(cv_transformed.toarray(),\n",
      "columns=cv.get_features_names()).add_prefix('Counts_')\n",
      "\n",
      "\n",
      " >Update your dataframe\n",
      "\n",
      "speech_df= pd.concat([speech_df,cv_df], axis=1, sort=False)\n",
      "\n",
      "print(speech_df.shape)\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Import CountVectorizer\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Instantiate CountVectorizer\n",
      "cv = CountVectorizer()\n",
      "\n",
      "# Fit the vectorizer\n",
      "cv.fit(speech_df['text_clean'])\n",
      "\n",
      "# Print feature names\n",
      "print(cv.get_feature_names())\n",
      "\n",
      " >Sample (transform)\n",
      "\n",
      "# Apply the vectorizer\n",
      "cv_transformed = cv.transform(speech_df['text_clean'])\n",
      "\n",
      "# Print the full array\n",
      "cv_array = cv_transformed.toarray()\n",
      "print(cv_array)\n",
      "\n",
      " >Sample (shape)\n",
      "# Apply the vectorizer\n",
      "cv_transformed = cv.transform(speech_df['text_clean'])\n",
      "\n",
      "# Print the full array\n",
      "cv_array = cv_transformed.toarray()\n",
      "\n",
      "# Print the shape of cv_array\n",
      "print(cv_array.shape)\n",
      "\n",
      "  Sample ( word counting)\n",
      "\n",
      "# Create a DataFrame with these features\n",
      "cv_df = pd.DataFrame(cv_array, \n",
      "                     columns=cv.get_feature_names()).add_prefix('Counts_')\n",
      "\n",
      "# Add the new columns to the original DataFrame\n",
      "speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\n",
      "print(speech_df_new.head())\n",
      "\n",
      " >Sample (fit and transform)\n",
      "\n",
      "# Import CountVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Specify arguements to limit the number of features generated\n",
      "cv =CountVectorizer(min_df=0.2, max_df=0.8)\n",
      "\n",
      "# Fit, transform, and convert into array\n",
      "cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
      "cv_array = cv_transformed.toarray()\n",
      "\n",
      "# Print the array shape\n",
      "print(cv_array.shape)\n",
      "\n",
      "\n",
      "        >Term frequency - inverse document frequency TF-IDF\n",
      "\n",
      "TF-ID= count of word occurances/total words in the document\n",
      "/\n",
      "log(Number of occurrences the word occurs in all docs/Total number of docs)\n",
      "\n",
      "Has the affect of reducing the weight of common words\n",
      "\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "tv=TfidVectorizer()\n",
      "print(tv)\n",
      "\n",
      "tv=TfidVectorizer(max_features=100, stop_words='english')\n",
      "\n",
      "max_features: maximum number of columns created by tf-idf (top 100 most common words)\n",
      "stop_words: list of common words to omit\n",
      "\n",
      "tv.fit(train_speech_df['text'])\n",
      "train_tv_transformed=tv.transform(train_speech_df['text'])\n",
      "\n",
      "train_tv_df = pd.DataFrame(train_tv_trasformed.toarray(),\n",
      "\t\t\tcolumns=tv.get_feature_names())\\\n",
      "\t\t\t\t.add_prefix('TFIDF_')\n",
      "\n",
      "train_speech_df= pd.concat([train_speech_df, train_tv_df], axis=1, sort=False)\n",
      "\n",
      "examine_row=train_tv_df.iloc[0]\n",
      "\n",
      "\n",
      "print(examine_row.sort_values(ascending=False))\n",
      "\n",
      "\n",
      " >Sample (examine row)\n",
      "\n",
      "# Import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Instantiate TfidfVectorizer\n",
      "tv = TfidfVectorizer()\n",
      "\n",
      "# Fit the vectroizer and transform the data\n",
      "tv_transformed = tv.fit_transform(speech_df['text_clean'])\n",
      "\n",
      "# Create a DataFrame with these features\n",
      "tv_df = pd.DataFrame(tv_transformed.toarray(), \n",
      "                     columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
      "print(tv_df.head())\n",
      "\n",
      "examine_row=tv_df.iloc[0]\n",
      "\n",
      "print(examine_row.sort_values(ascending=False))\n",
      "\n",
      "\n",
      " >Sample (test)\n",
      "\n",
      "# Instantiate TfidfVectorizer\n",
      "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
      "\n",
      "# Fit the vectroizer and transform the data\n",
      "tv_transformed = tv.fit_transform(train_speech_df['text_clean'])\n",
      "\n",
      "# Transform test data\n",
      "test_tv_transformed = tv.transform(test_speech_df['text_clean'])\n",
      "\n",
      "# Create new features for the test set\n",
      "test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), \n",
      "                          columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
      "print(test_tv_df.head())\n",
      "\n",
      "\n",
      "       >Bag of Words\n",
      "bag of words - no concept of order or grammer\n",
      "\n",
      "positive meaning: happy\n",
      "\n",
      "negative meaning: not happy\n",
      "\n",
      "positive meaning: never not happy\n",
      "\n",
      "\n",
      "tv_bi_gram_vec = TfidfVectorizer(ngram_range= (2,2))\n",
      "\n",
      "tv_bi_gram = tv_bi_gram_vec\\.fit_transform(speech_df['text'])\n",
      "\n",
      "print (tv_bi_gram_vec.get_feature_names())\n",
      "\n",
      "\n",
      "tv_df=pd.DataFrame(tv_bi_gram.toarray(), columns=tv_bi_gram_vec.get_feature_names())\\\n",
      "\t.add_prefix('Counts_')\n",
      "\n",
      "tv_sums=tv_df.sum()\n",
      "print(tv_sums.head())\n",
      "\n",
      "print(tv_sums.sort_values(ascending=False)).head()\n",
      "\n",
      "\n",
      " >Sample (trigrams)\n",
      "\n",
      "# Import CountVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Instantiate a trigram vectorizer\n",
      "cv_trigram_vec = CountVectorizer(max_features=100, \n",
      "                                 stop_words='english', \n",
      "                                 ngram_range= (3,3))\n",
      "\n",
      "# Fit and apply trigram vectorizer\n",
      "cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])\n",
      "(speech_df['text_clean'])\n",
      "\n",
      "# Print the trigram features\n",
      "print(cv_trigram_vec.get_feature_names())\n",
      "\n",
      "\n",
      "# Create a DataFrame of the features\n",
      "cv_tri_df = pd.DataFrame(cv_trigram.toarray(), \n",
      "                 columns=cv_trigram_vec.get_feature_names()).add_prefix('Counts_')\n",
      "\n",
      "# Print the top 5 words in the sorted output\n",
      "print(cv_tri_df.sum().sort_values(ascending=False).head())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\image processing.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\image processing.txt\n",
      "# Import the modules from skimage\n",
      "from skimage import data, color\n",
      "\n",
      "# Load the rocket image\n",
      "rocket = data.rocket()\n",
      "\n",
      "# Convert the image to grayscale\n",
      "gray_scaled_rocket = color.rgb2gray(rocket)\n",
      "\n",
      "# Show the original image\n",
      "show_image(rocket, 'Original RGB image')\n",
      "\n",
      "# Show the grayscale image\n",
      "show_image(gray_scaled_rocket, 'Grayscale image')\n",
      "\n",
      "\n",
      "    numpy for images\n",
      "\n",
      "flipping\n",
      "extracting and analyzing features\n",
      "\n",
      "\n",
      "madrid_image = plt.imread('madrid.jpeg')\n",
      "\n",
      "type(madrid_image)\n",
      "\n",
      "output:ndarray object\n",
      "\n",
      "red=image[:,:,0]\n",
      "green=image[:,:,1]\n",
      "blue=image[:,:,2]\n",
      "\n",
      "plt.imshow(red,cmap=\"gray\")\n",
      "plt.('Red')\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "you can see the intensities of each color\n",
      "\n",
      "madrid_image.shape\n",
      "\n",
      "(426,640,3)\n",
      "\n",
      "madrid_image.size\n",
      "\n",
      "vertically_flipped=np.flipud(madrid_image)\n",
      "\n",
      "show_image(vertically_flipped,'Vertically flipped image')\n",
      "\n",
      "horizontal flip\n",
      "\n",
      "horizontal_flipped-np.fliplr(madrid_image)\n",
      "\n",
      "show_image(horizontally_flipped,'Horizontally flipped image')\n",
      "\n",
      "a histogram is a visual representation of the intensity of each color intensity\n",
      "\n",
      "histograms are used to transform images\n",
      "1. analysis\n",
      "2. thresholding\n",
      "3. brightness and contrast\n",
      "4. equalizing\n",
      "\n",
      "red=image[:,:,0]\n",
      "plt.hist(red.ravel(),bins=256)\n",
      "\n",
      "\n",
      "ravel returns a contguous flatten array\n",
      "\n",
      "\n",
      "  sample\n",
      "\n",
      "\n",
      "# Flip the image vertically\n",
      "seville_vertical_flip = np.flipud(flipped_seville)\n",
      "\n",
      "# Flip the image horizontally\n",
      "seville_horizontal_flip = np.fliplr(seville_vertical_flip)\n",
      "\n",
      "# Show the resulting image\n",
      "show_image(seville_horizontal_flip, 'Seville')\n",
      "\n",
      "   sample red histogram\n",
      "\n",
      "# Obtain the red channel\n",
      "red_channel = image[:, :, 0]\n",
      "\n",
      "# Plot the red histogram with bins in a range of 256\n",
      "plt.hist(red_channel.ravel(), bins=256)\n",
      "\n",
      "# Set title and show\n",
      "plt.title('Red Histogram')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "       >Thresholding\n",
      "\n",
      "thresholding is partitioning an image into a foreground and background making them black and white\n",
      "\n",
      "255(white) if pixel > thresh value\n",
      "\n",
      "0 black if pixel < thresh value\n",
      "\n",
      "thresholding is use during\n",
      "1. object detection\n",
      "2. face detection\n",
      "etc\n",
      "\n",
      "convert to grayscal\n",
      "\n",
      "thresh=127\n",
      "\n",
      "binary= image > thresh\n",
      "\n",
      "show_image(image,'Original)\n",
      "show_image(binary,'Thresholded')\n",
      "\n",
      "inverted_binary = image <=thresh\n",
      "\n",
      "     Categories\n",
      "\n",
      "global or histogram based: good for uniform backgrounds\n",
      "\n",
      "local or adaptive: for uneven background illumination\n",
      "\n",
      "    all threshold\n",
      "\n",
      "from skimage.filters import try_all_threshold\n",
      "\n",
      "fig, ax = try_all_threshold(image, verbose=False)\n",
      "\n",
      "show_plot(fig,ax)\n",
      "\n",
      "      finding the optimal thresh value\n",
      "\n",
      "from skimage.filters import threshold_otsu\n",
      "\n",
      "#obtain the optimal threshold value\n",
      "thresh=threshold_otsu(red)\n",
      "?\n",
      "print(thresh)\n",
      "#apply the thresholding to the image\n",
      "binary_global = red> thresh\n",
      "?\n",
      "plt.imshow(binary_global,cmap='gray')\n",
      "plt.title('Thresholded')\n",
      "plt.show()\n",
      "\n",
      "output:\n",
      "threshold=123\n",
      "\n",
      "        >Local threshold\n",
      "\n",
      "use when the background is uneven\n",
      "\n",
      "block size to surround each pixel\n",
      "\n",
      "from skimage.filters import threshold_local\n",
      "\n",
      "block_size=35\n",
      "\n",
      "local_tresh= threshold_local(text_image, block_size,offset=10)\n",
      "\n",
      "\n",
      "     sample   > otsu to gray scale\n",
      "\n",
      "# Import the otsu threshold function\n",
      "from skimage.filters import threshold_otsu\n",
      "\n",
      "# Make the image grayscale using rgb2gray\n",
      "chess_pieces_image_gray = rgb2gray(chess_pieces_image)\n",
      "\n",
      "# Obtain the optimal threshold value with otsu\n",
      "thresh = threshold_otsu(chess_pieces_image_gray)\n",
      "\n",
      "# Apply thresholding to the image\n",
      "binary = chess_pieces_image_gray > thresh\n",
      "\n",
      "# Show the image\n",
      "show_image(binary, 'Binary image')\n",
      "\n",
      "    sample  > otsu to image   global thresh\n",
      "\n",
      "# Import the otsu threshold function\n",
      "c\n",
      "\n",
      "# Obtain the optimal otsu global thresh value\n",
      "global_thresh = threshold_otsu(page_image)\n",
      "\n",
      "# Obtain the binary image by applying global thresholding\n",
      "binary_global = page_image > global_thresh\n",
      "\n",
      "# Show the binary image obtained\n",
      "show_image(binary_global, 'Global thresholding')\n",
      "\n",
      "  > sample  > local thresh\n",
      "\n",
      "# Import the local threshold function\n",
      "from skimage.filters import threshold_local\n",
      "\n",
      "# Set the block size to 35\n",
      "block_size = 35\n",
      "\n",
      "# Obtain the optimal local thresholding\n",
      "local_thresh = threshold_local(page_image, block_size, offset=10)\n",
      "\n",
      "# Obtain the binary image by applying local thresholding\n",
      "binary_local = page_image > local_thresh\n",
      "\n",
      "# Show the binary image\n",
      "show_image(binary_local, 'Local thresholding')\n",
      "\n",
      "\n",
      "    sample   > all threshold\n",
      "\n",
      "# Import the try all function\n",
      "from skimage.filters import try_all_threshold\n",
      "\n",
      "# Import the rgb to gray convertor function \n",
      "from skimage.color import rgb2gray\n",
      "\n",
      "# Turn the fruits_image to grayscale\n",
      "grayscale = rgb2gray(fruits_image)\n",
      "\n",
      "# Use the try all method on the resulting grayscale image\n",
      "fig, ax = try_all_threshold(grayscale, verbose=False)\n",
      "\n",
      "# Show the resulting plots\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  > sample  > optimum threshold\n",
      "\n",
      "# Import threshold and gray convertor functions\n",
      "from skimage.filters import threshold_otsu\n",
      "from skimage.color import rgb2gray\n",
      "\n",
      "# Turn the image grayscale\n",
      "gray_tools_image = rgb2gray(tools_image)\n",
      "\n",
      "# Obtain the optimal thresh\n",
      "thresh = threshold_otsu(gray_tools_image)\n",
      "\n",
      "# Obtain the binary image by applying thresholding\n",
      "binary_image = gray_tools_image > thresh\n",
      "\n",
      "# Show the resulting binary image\n",
      "show_image(binary_image, 'Binarized image')\n",
      "\n",
      "\n",
      "        Filtering\n",
      "\n",
      "contrast\n",
      "morphology\n",
      "\n",
      "filter: enhancing an image\n",
      "emphasize or remove features\n",
      "smoothing\n",
      "sharpening\n",
      "\n",
      "neighborhoods are blocks of pixels\n",
      "\n",
      "with filtering we can detect edges\n",
      "\n",
      "sobel is a common edge detection algorithm\n",
      "\n",
      "   edge detection with sobel\n",
      "\n",
      "from skimage.filters import sobel\n",
      "\n",
      "image_coins=plt.imread('coins.jpg')\n",
      "plt.imshow(image_coins)\n",
      "edge=sobel=sobel(image_coins)\n",
      "\n",
      "\n",
      "\n",
      "     Function plot_comparision\n",
      "\n",
      "def plot_comparision(original,filtered, title_filtered):\n",
      "        fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(8,6), sharex=True, sharey=True)\n",
      "        \n",
      "        ax1.imshow(original,cmap=plt.cm.gray)\n",
      "        ax1.set_title('original')\n",
      "        ax1.axis('off')\n",
      "\n",
      "        ax2.imshow(filtered,cmap=plt.cm.gray)\n",
      "        ax2.set_title(title_filtered)\n",
      "        ax2.axis('off')\n",
      "\n",
      "\n",
      "     gaussian smoothing\n",
      "\n",
      "original vs blurred with gaussian filter\n",
      "\n",
      "reduces contrast\n",
      "\n",
      "from skimage.filters import gaussian\n",
      "\n",
      "gaussian_image = gaussian(amsterdam_pic, multichannel=True)\n",
      "\n",
      "\n",
      "     sobel edge detection\n",
      "\n",
      "# Import the color module\n",
      "from skimage import color\n",
      "\n",
      "# Import the filters module and sobel function\n",
      "from skimage.filters import sobel\n",
      "\n",
      "# Make the image grayscale\n",
      "soaps_image_gray = color.rgb2gray(soaps_image)\n",
      "\n",
      "# Apply edge detection filter\n",
      "edge_sobel = sobel(soaps_image_gray)\n",
      "\n",
      "# Show original and resulting image to compare\n",
      "show_image(soaps_image, \"Original\")\n",
      "show_image(edge_sobel, \"Edges with Sobel\")\n",
      "\n",
      "    guassian image\n",
      "\n",
      "# Import Gaussian filter \n",
      "from skimage.filters import gaussian\n",
      "\n",
      "# Apply filter\n",
      "gaussian_image = gaussian(building_image, multichannel=True)\n",
      "\n",
      "# Show original and resulting image to compare\n",
      "show_image(building_image, \"Original\")\n",
      "show_image(gaussian_image, \"Reduced sharpness Gaussian\")\n",
      "\n",
      "\n",
      "       > Contrast enhancement\n",
      "\n",
      "from skimage import exposure\n",
      "\n",
      "when we improve the contrast the details become more visible\n",
      "\n",
      "contrast is the measure of the dynamic range\n",
      "\n",
      "contrast is the difference between the maximum and minimum pixel intensity\n",
      "\n",
      "   enhance contrast\n",
      "\n",
      "1. contrast stretching\n",
      "2. histogram equalization\n",
      "\n",
      "types:\n",
      "histogram equalization\n",
      "adaptive histogram equalization\n",
      "limited adaptive histogram equalization\n",
      "\n",
      "\n",
      "from skimage import exposure\n",
      "\n",
      "image_eq= exposure.equalize_hist(image)\n",
      "\n",
      " > sample  > histogram of the chest xray\n",
      "\n",
      "# Import the required module\n",
      "from skimage import exposure\n",
      "\n",
      "# Show original x-ray image and its histogram\n",
      "show_image(chest_xray_image, 'Original x-ray')\n",
      "\n",
      "plt.title('Histogram of image')\n",
      "plt.hist(chest_xray_image.ravel(), bins=256)\n",
      "plt.show()\n",
      "\n",
      "# Use histogram equalization to improve the contrast\n",
      "xray_image_eq =  exposure.equalize_hist(chest_xray_image)\n",
      "\n",
      "# Show the resulting image\n",
      "show_image(xray_image_eq, 'Resulting image')\n",
      "\n",
      "\n",
      "  > sample adaptive contrast\n",
      "\n",
      "# Import the necessary modules\n",
      "from skimage import data, exposure\n",
      "\n",
      "# Load the image\n",
      "original_image = data.coffee()\n",
      "\n",
      "# Apply the adaptive equalization on the original image\n",
      "adapthist_eq_image = exposure.equalize_adapthist(original_image, clip_limit=0.03)\n",
      "\n",
      "# Compare the original image to the equalized\n",
      "show_image(original_image)\n",
      "show_image(adapthist_eq_image, '#ImageProcessingDatacamp')\n",
      "\n",
      "\n",
      "     > transformations\n",
      "\n",
      "1. preparing images for classification machine learning models\n",
      "\n",
      "2. optimization and compression of images\n",
      "\n",
      "3. save images with same proportions\n",
      "\n",
      "\n",
      "from skimage.transform import rotate\n",
      "\n",
      "image_rotated = rotate(image, -90)\n",
      "\n",
      "rescaling\n",
      "\n",
      "from skimage.transform import rescale\n",
      "\n",
      "image_rescaled=rescale(image,1/4, anti_aliasing=True, multichannel=True\n",
      "\n",
      "alias makes the pixel look like waves\n",
      "\n",
      "anti alias makes the image softer\n",
      "\n",
      "\n",
      "from skimage.transform import resize\n",
      "\n",
      "height=400\n",
      "width=600\n",
      "\n",
      "image_resize=resize(image,(height,width), anti-aliasing=True)\n",
      "\n",
      "#set proportional height so its 4 times its size\n",
      "\n",
      "height=image.shape[0]/4\n",
      "width=image.shape[1]/4\n",
      "\n",
      "image_resized=resize(image,(height,width), anti-aliasing=True)\n",
      "\n",
      "   sample resize and rotate\n",
      "\n",
      "from skimage.transform import rotate, rescale\n",
      "\n",
      "# Rotate the image 90 degrees clockwise \n",
      "rotated_cat_image = rotate(image_cat, -90)\n",
      "\n",
      "# Rescale with anti aliasing\n",
      "rescaled_with_aa = rescale(rotated_cat_image, 1/4, anti_aliasing=True, multichannel=True)\n",
      "\n",
      "# Rescale without anti aliasing\n",
      "rescaled_without_aa = rescale(rotated_cat_image, 1/4, anti_aliasing=False, multichannel=True)\n",
      "\n",
      "# Show the resulting images\n",
      "show_image(rescaled_with_aa, \"Transformed with anti aliasing\")\n",
      "show_image(rescaled_without_aa, \"Transformed without anti aliasing\")\n",
      "\n",
      "\n",
      "   > sample  > 3x rocket\n",
      "\n",
      "# Import the module and function to enlarge images\n",
      "from skimage.transform import rescale\n",
      "\n",
      "# Import the data module\n",
      "from skimage import data\n",
      "\n",
      "# Load the image from data\n",
      "rocket_image = data.rocket()\n",
      "\n",
      "# Enlarge the image so it is 3 times bigger\n",
      "enlarged_rocket_image = rescale(rocket_image, 3, anti_aliasing=True, multichannel=True)\n",
      "\n",
      "# Show original and resulting image\n",
      "show_image(rocket_image)\n",
      "show_image(enlarged_rocket_image, \"3 times enlarged image\")\n",
      "\n",
      "\n",
      "    sample 1/2 the image rescaling\n",
      "\n",
      "# Import the module and function\n",
      "from skimage.transform import resize\n",
      "\n",
      "# Set proportional height so its half its size\n",
      "height = int(dogs_banner.shape[0] / 2)\n",
      "width = int(dogs_banner.shape[1] / 2)\n",
      "\n",
      "# Resize using the calculated proportional height and width\n",
      "image_resized = resize(dogs_banner, (height,width),\n",
      "                       anti_aliasing=True)\n",
      "\n",
      "# Show the original and rotated image\n",
      "show_image(dogs_banner, 'Original')\n",
      "show_image(image_resized, 'Resized image')\n",
      "\n",
      "\n",
      "     > Morphology\n",
      "\n",
      "binary images can be distorted by thresholding images.\n",
      "\n",
      "morphological filtering account for form in the image.\n",
      "\n",
      "types:\n",
      "dilation: adds pixels to boundaries\n",
      "erosion: removes pixels on the object boundaries\n",
      "\n",
      "shapes: squares, diamond, cross\n",
      "\n",
      "from skimage import morphology\n",
      "\n",
      "square=morphology.square(4)\n",
      "\n",
      "rectangle = morphology.rectangle(4,2)\n",
      "\n",
      "   erosion\n",
      "\n",
      "from skimage import morphology\n",
      "\n",
      "selem = rectangle(12,6)\n",
      "\n",
      "erode_image = morphology.binary_erosion(image_horse, selem=selem)\n",
      "\n",
      "   dilation\n",
      "\n",
      "dilated_image = morphology.binary_dilation(binary_global)\n",
      "\n",
      "\n",
      "  > sample  > erosion\n",
      "\n",
      "#erosion is useful for removing minor white noise\n",
      "\n",
      "# Import the morphology module\n",
      "from skimage import morphology\n",
      "\n",
      "# Obtain the eroded shape \n",
      "eroded_image_shape = morphology.binary_erosion(upper_r_image) \n",
      "\n",
      "# See results\n",
      "show_image(upper_r_image, 'Original')\n",
      "show_image(eroded_image_shape, 'Eroded image')\n",
      "\n",
      " > sample   dilation\n",
      "\n",
      "# Import the module\n",
      "from skimage import morphology\n",
      "\n",
      "# Obtain the dilated image \n",
      "dilated_image = morphology.binary_dilation(world_image)\n",
      "\n",
      "# See results\n",
      "show_image(world_image, 'Original')\n",
      "show_image(dilated_image, 'Dilated image')\n",
      "\n",
      "       Image restoration\n",
      "\n",
      "Inpainting is reconstructing lost parts of images\n",
      "\n",
      "looking at the non-damaged regions\n",
      "\n",
      "Damaged pixels set as a mask\n",
      "\n",
      "mask are pixels with values that are zero\n",
      "\n",
      "from skimage.restoration import inpaint\n",
      "\n",
      "mask = get_mask(defect_image)\n",
      "\n",
      "restored_image = inpaint.inpaint_biharmonic(defect_image, mask, multichannel=True)\n",
      "\n",
      "show_image(restored_image)\n",
      "\n",
      "def get_mask(image):\n",
      "\tmask=np.zeros(image.shape[:-1])\n",
      "\t\n",
      "\tmask[101:106,0:240]=1\n",
      "\tmask[152:154,0:60]=1\n",
      "\tmask[154:156,100:120]=1\n",
      "\tmask[155:156,120:140]=1\n",
      "\n",
      "\tmask[212:217,0:150]=1\n",
      "\tmask[217:222,150:256]=1\t\n",
      "\treturn mask\n",
      "\n",
      "\n",
      "   repair a defective image\n",
      "\n",
      "# Import the module from restoration\n",
      "from skimage.restoration import inpaint\n",
      "\n",
      "# Show the defective image\n",
      "show_image(defect_image, 'Image to restore')\n",
      "\n",
      "# Apply the restoration function to the image using the mask\n",
      "restored_image = inpaint.inpaint_biharmonic(defect_image, mask,multichannel=True)\n",
      "show_image(restored_image)\n",
      "\n",
      "    removing a logo\n",
      "\n",
      "# Initialize the mask\n",
      "mask = np.zeros(image_with_logo.shape[:-1])\n",
      "\n",
      "# Set the pixels where the logo is to 1\n",
      "mask[210:272, 360:425] = 1\n",
      "\n",
      "# Apply inpainting to remove the logo\n",
      "image_logo_removed = inpaint.inpaint_biharmonic(image_with_logo,\n",
      "                                  mask,\n",
      "                                  multichannel=True)\n",
      "\n",
      "# Show the original and logo removed images\n",
      "show_image(image_with_logo, 'Image with logo')\n",
      "show_image(image_logo_removed, 'Image with logo removed')\n",
      "\n",
      "\n",
      "      >Noise\n",
      "\n",
      "images are signals\n",
      "\n",
      "noise is the result of errors in image processing that do not reflect the true intensities.\n",
      "\n",
      "\n",
      "from skimage.util import random_noise\n",
      "\n",
      "noisy_image = random_noise(dog_image)\n",
      "\n",
      "randomly distributed noise\n",
      "\n",
      "Denoising the image\n",
      "1. total variation filter (tv)\n",
      "2. bilateral filtering (replace each pixel with a value of weighted values preserving edges)\n",
      "3. wavelet denoising\n",
      "\n",
      "from skimage.restoration import denoise_tv_chambolle\n",
      "\n",
      "denoise_image = denoise_tv_chambolle(noisy_image, weight=0.1, multichannel=True)\n",
      "\n",
      "\n",
      "  >bilateral\n",
      "\n",
      "from skimage.restoration import denoise_bilateral\n",
      "\n",
      "denoised_image=denoise_bilateral(noisy_image,multichannel=True)\n",
      "\n",
      "\n",
      "   sample  > add noise\n",
      "\n",
      "# Import the module and function\n",
      "from skimage.util import random_noise\n",
      "\n",
      "# Add noise to the image\n",
      "noisy_image = random_noise(fruit_image)\n",
      "\n",
      "# Show original and resulting image\n",
      "show_image(fruit_image, 'Original')\n",
      "show_image(noisy_image, 'Noisy image')\n",
      "\n",
      "\n",
      "  > sample denoised with tv chambolle\n",
      "\n",
      "# Import the module and function\n",
      "from skimage.restoration import denoise_tv_chambolle\n",
      "\n",
      "# Apply total variation filter denoising\n",
      "denoised_image = denoise_tv_chambolle(noisy_image, \n",
      "                                      multichannel=True)\n",
      "\n",
      "# Show the noisy and denoised images\n",
      "show_image(noisy_image, 'Noisy')\n",
      "show_image(denoised_image, 'Denoised image')\n",
      "\n",
      "  > sample denoised  > bilateral\n",
      "\n",
      "# Import bilateral denoising function\n",
      "from skimage.restoration import denoise_bilateral\n",
      "\n",
      "# Apply bilateral filter denoising\n",
      "denoised_image = denoise_bilateral(landscape_image, \n",
      "                                   multichannel=True)\n",
      "\n",
      "# Show original and resulting images\n",
      "show_image(landscape_image, 'Noisy image')\n",
      "show_image(denoised_image, 'Denoised image')\n",
      "\n",
      "\n",
      "   >Super pixels and segmentation\n",
      "\n",
      "break the image into segments\n",
      "\n",
      "super pixels create segments in the image of pixels with similar pixel color ranges.\n",
      "\n",
      "super pixels have similar levels of grayscale\n",
      "\n",
      "super pixels have been applied to image tracking\n",
      "\n",
      "super pixels are represented as boundarie\n",
      "\n",
      "unsupervised segmentation\n",
      "1. simple linear iterative clustering (slic)\n",
      "\n",
      "\n",
      "from skimage.segmentation import slic\n",
      "from skimage.color import label2rgb\n",
      "\n",
      "segments= slic(coins_image,n_segments=600)\n",
      "\n",
      "segmented_image=label2rgb(segments,coins_image,kind='avg')\n",
      "\n",
      "\n",
      "    Sample    face reduced to 400 super pixels\n",
      "\n",
      "# Import the slic function from segmentation module\n",
      "from skimage.segmentation import slic\n",
      "\n",
      "# Import the label2rgb function from color module\n",
      "from skimage.color import label2rgb\n",
      "\n",
      "# Obtain the segmentation with 400 regions\n",
      "segments=slic(face_image,n_segments=400)\n",
      "\n",
      "# Put segments on top of original image to compare\n",
      "segmented_image = label2rgb(segments, face_image, kind='avg')\n",
      "\n",
      "# Show the segmented image\n",
      "show_image(segmented_image, \"Segmented image, 400 superpixels\")\n",
      "\n",
      "\n",
      "      find contours\n",
      "\n",
      "1. represents the boundaries of the objects\n",
      "\n",
      "measure size\n",
      "classify shapes\n",
      "determine the number of objects\n",
      "\n",
      "we can obtain a binary image applying thresholding or using edge detection\n",
      "\n",
      "preparing the image\n",
      "1. transform the image to 2d grayscale\n",
      "\n",
      "image=color.rgb2gray(image)\n",
      "\n",
      "thresh=threshold_otsu(image)\n",
      "\n",
      "threshholded_image=image>thresh\n",
      "\n",
      "use find_contours()\n",
      "\n",
      "from skimage import measure\n",
      "\n",
      "contours=measure.find_contours(threshold_image,0.8)\n",
      "\n",
      "the closer the level value is to 1 the more sensitive the it is to finding the boundary\n",
      "\n",
      "for contour in contours:\n",
      "    print(contour.shape)\n",
      "\n",
      "\n",
      "     sample  > horse contours\n",
      "\n",
      "# Import the modules\n",
      "from skimage import data, measure\n",
      "\n",
      "# Obtain the horse image\n",
      "horse_image = data.horse()\n",
      "\n",
      "# Find the contours with a constant level value of 0.8\n",
      "contours = measure.find_contours(horse_image, 0.8)\n",
      "\n",
      "# Shows the image with contours found\n",
      "show_image_contour(horse_image, contours)\n",
      "\n",
      "   > sample  > contours dice\n",
      "\n",
      "# Make the image grayscale\n",
      "image_dices = color.rgb2gray(image_dices)\n",
      "\n",
      "# Obtain the optimal thresh value\n",
      "thresh = filters.threshold_otsu(image_dices)\n",
      "\n",
      "# Apply thresholding\n",
      "binary = image_dices > thresh\n",
      "\n",
      "# Find contours at a constant value of 0.8\n",
      "contours = measure.find_contours(binary, 0.8)\n",
      "\n",
      "# Show the image\n",
      "show_image_contour(image_dices, contours)\n",
      "\n",
      "  sample  > count dots\n",
      "\n",
      "# Create list with the shape of each contour\n",
      "shape_contours = [cnt.shape[0] for cnt in contours]\n",
      "\n",
      "# Set 50 as the maximum size of the dots shape\n",
      "max_dots_shape = 50\n",
      "\n",
      "# Count dots in contours excluding bigger than dots size\n",
      "dots_contours = [cnt for cnt in contours if np.shape(cnt)[0] < max_dots_shape]\n",
      "\n",
      "# Shows all contours found \n",
      "show_image_contour(binary, contours)\n",
      "\n",
      "# Print the dice's number\n",
      "print(\"Dice's dots number: {}. \".format(len(dots_contours)))\n",
      "\n",
      "     >edges\n",
      "\n",
      "reduces information\n",
      "\n",
      "edges with canny\n",
      "\n",
      "from skimage.feature import canny\n",
      "\n",
      "dominos_grayscale_image=rgb2gray(dominos_image)\n",
      "\n",
      "fig,ax=plt.subplots(figsize=(20,12))\n",
      "\n",
      "ax.imshow(dominos_grayscale_image)\n",
      "plt.show()\n",
      "\n",
      "canny_edges=canny(dominos_grayscale_image,sigma=0.5)\n",
      "\n",
      "sigma removes noise\n",
      "\n",
      "guassian filter function\n",
      "\n",
      "the lower the sigma value the less the guassian filter applies to the image\n",
      "\n",
      "\n",
      " > sample  > canny for edge detection\n",
      "\n",
      "# Import the canny edge detector \n",
      "from skimage.feature import canny\n",
      "\n",
      "# Convert image to grayscale\n",
      "grapefruit = color.rgb2gray(grapefruit)\n",
      "\n",
      "# Apply canny edge detector\n",
      "canny_edges = canny(grapefruit)\n",
      "\n",
      "# Show resulting image\n",
      "show_image(canny_edges, \"Edges with Canny\")\n",
      "\n",
      "\n",
      "     right around the corner\n",
      "\n",
      "corner detection\n",
      "\n",
      "a corner is the intersection of two edges\n",
      "\n",
      "match points between corners of an image.\n",
      "\n",
      "\n",
      "from skimage.feature import corner_harris\n",
      "\n",
      "image=rgb2gray(image)\n",
      "\n",
      "measure_image = corner_harris(image)\n",
      "\n",
      "coords = corner_peaks(corner_harris(image), min_distance=5)\n",
      "\n",
      "print(\"A total of \", len(coords), \"corners were detected\")\n",
      "\n",
      "\n",
      "ax.plot(coords[:,1],coords[:,0],'+r',markersize=15)\n",
      "\n",
      "\n",
      "  > sample corner\n",
      "\n",
      "\n",
      "# Import the corner detector related functions and module\n",
      "from skimage.feature import corner_harris, corner_peaks\n",
      "\n",
      "# Convert image from RGB-3 to grayscale\n",
      "building_image_gray = color.rgb2gray(building_image)\n",
      "\n",
      "# Apply the detector  to measure the possible corners\n",
      "measure_image = corner_harris(building_image_gray)\n",
      "\n",
      "# Find the peaks of the corners using the Harris detector\n",
      "coords = corner_peaks(measure_image, min_distance=2)\n",
      "\n",
      "# Show original and resulting image with corners detected\n",
      "show_image(building_image, \"Original\")\n",
      "show_image_with_corners(building_image, coords)\n",
      "\n",
      "# Find the peaks with a min distance of 2 pixels\n",
      "coords_w_min_2 = corner_peaks(measure_image, min_distance=2)\n",
      "print(\"With a min_distance set to 2, we detect a total\", len(coords_w_min_2), \"corners in the image.\")\n",
      "\n",
      "# Find the peaks with a min distance of 40 pixels\n",
      "coords_w_min_40 = corner_peaks(measure_image, min_distance=40)\n",
      "print(\"With a min_distance set to 40, we detect a total\", len(coords_w_min_40), \"corners in the image.\")\n",
      "\n",
      "output:\n",
      "With a min_distance set to 2, we detect a total 98 corners in the image.\n",
      "With a min_distance set to 40, we detect a total 36 corners in the image.\n",
      "\n",
      "      >Face detection\n",
      "\n",
      "from skimage.feature import Cascade\n",
      "\n",
      "trained_file=data.lbp_frontal_face_cascade_filename()\n",
      "\n",
      "detector = Cascade(trained_file)\n",
      "\n",
      "search for a face\n",
      "the window will have a minimum size\n",
      "\n",
      "detected = detector.detect_multi_scale(img=image,  \n",
      "\tscale_factor=1.2, \n",
      "\tstep_ratio=1,\n",
      "\tmin_size=(10,10),\n",
      "\tmax_size=(200,200))\n",
      "\n",
      "\n",
      "def show_detected_face(result, detected, title=\"Face image\"):\n",
      "    plt.imshow(result)\n",
      "    img_desc=plt.gca()\n",
      "    plt.set_cmap('gray')\n",
      "    plt.title(title)\n",
      "    plt.axis('off')\n",
      "\n",
      "    for patch in detected:\n",
      "        img_desc.add_patch(\n",
      "            patches.Rectangle(\n",
      "                (patch['c'],patch['r']),patch['width'],patch['height'],\\\n",
      "                fill=False,color='r',linewidth=2))\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "\t\n",
      "r is the row position of the top left corner\n",
      "c is the column position fo the top left corner\n",
      "width: width of the rectangle\n",
      "height: height of the rectangle\n",
      "\n",
      "\n",
      "  > sample  > face detection\n",
      "\n",
      "print(type(data))\n",
      "# Load the trained file from data\n",
      "trained_file = data.lbp_frontal_face_cascade_filename()\n",
      "\n",
      "# Initialize the detector cascade\n",
      "detector = Cascade(trained_file)\n",
      "\n",
      "# Detect faces with min and max size of searching window\n",
      "detected = detector.detect_multi_scale(img = night_image,\n",
      "                                       scale_factor=1.2,\n",
      "                                       step_ratio=1,\n",
      "                                       min_size=(10,10),\n",
      "                                       max_size=(200,20))\n",
      "\n",
      "# Show the detected faces\n",
      "show_detected_face(night_image, detected)\n",
      "\n",
      "\n",
      "    sample  face detection using segments\n",
      "\n",
      "# Obtain the segmentation with default 100 regions\n",
      "segments = slic(profile_image)\n",
      "\n",
      "# Obtain segmented image using label2rgb\n",
      "segmented_image = label2rgb(segments, profile_image, kind='avg')\n",
      "\n",
      "# Detect the faces with multi scale method\n",
      "detected = detector.detect_multi_scale(img=segmented_image, \n",
      "                                       scale_factor=1.2, \n",
      "                                       step_ratio=1, \n",
      "                                       min_size=(10, 10), max_size=(1000, 1000))\n",
      "\n",
      "# Show the detected faces\n",
      "show_detected_face(segmented_image, detected)\n",
      "\n",
      "\n",
      "    Privacy protection\n",
      "\n",
      "from skimage.feature import Cascade\n",
      "from skimage.feature import guassian\n",
      "\n",
      "\n",
      "detected = detector. detect_multi_scale(img=image, \tscale_factor=1.2, \n",
      "\tstep_ratio=1,\n",
      "\tmin_size(50,50), max_size(100,100))\n",
      "\n",
      "for d in detected:\n",
      "\tface=getFace(d)\n",
      "\n",
      "\tguassian_face= gaussian(face, multichannel=True, sigma=10)\n",
      "\n",
      "\tresulting_image= mergeBlurryFace(image, gaussian_face)\n",
      "\n",
      "def getFace(d):\n",
      "\tx, y= d['r'],d['c']\n",
      "\twidth,height=d['r']+d['width'],d['c']+d['height']\n",
      "\n",
      "\tface=image[x:width, y:height]\n",
      "\treturn face\n",
      "\n",
      "\n",
      "def mergeBlurryFace(original,gaussian_image):L\n",
      "\n",
      "\tx, y= d['r'],d['c']\n",
      "\twidth,height=d['r']+d['width'],d['c']+d['height']\n",
      "\t\n",
      "\toriginal[x:width,y:height]=gaussian_image\n",
      "\n",
      "\treturn original\n",
      "\n",
      "\n",
      "   > sample    > gaussian\n",
      "\n",
      "# Detect the faces\n",
      "detected = detector. detect_multi_scale(img=group_image, \n",
      "                                       scale_factor=1.2, step_ratio=1, \n",
      "                                       min_size=(10,10), max_size=(100, 100))\n",
      "# For each detected face\n",
      "for d in detected:  \n",
      "    # Obtain the face rectangle from detected coordinates\n",
      "    face = getFaceRectangle(d)\n",
      "    \n",
      "    # Apply gaussian filter to extracted face\n",
      "    blurred_face = gaussian(face, multichannel=True, sigma = 8)\n",
      "    \n",
      "    # Merge this blurry face to our final image and show it\n",
      "    resulting_image = mergeBlurryFace(group_image, blurred_face) \n",
      "show_image(resulting_image, \"Blurred faces\")\n",
      "\n",
      "\n",
      "    sample   > denoise\n",
      "\n",
      "# Import the necessary modules\n",
      "from skimage.restoration import denoise_tv_chambolle, inpaint\n",
      "from skimage.transform import rotate\n",
      "\n",
      "# Transform the image so it's not rotated\n",
      "upright_img = rotate(damaged_image, 20)\n",
      "\n",
      "# Remove noise from the image, using the chambolle method\n",
      "upright_img_without_noise = denoise_tv_chambolle(upright_img,weight=0.1, multichannel=True)\n",
      "\n",
      "# Reconstruct the image missing parts\n",
      "mask = get_mask(upright_img)\n",
      "result = inpaint.inpaint_biharmonic(upright_img_without_noise, mask, multichannel=True)\n",
      "\n",
      "show_image(result)\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\machine learning in marketing.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\machine learning in marketing.txt\n",
      "predict customer purchases\n",
      "customer segmentation\n",
      "agents maximizing rewards\n",
      "\n",
      "predicting the target variable\n",
      "predicting how much the customers will spend\n",
      "customer churn\n",
      "defaults on loans\n",
      "\n",
      "customer segmentation by product purchase history\n",
      "\n",
      "\n",
      " Preparation of data\n",
      "\n",
      "explore the data sample\n",
      "\n",
      "telco_raw.dtypes\n",
      "\n",
      "custid=['customerID]\n",
      "target=['Churn']\n",
      "\n",
      "#Separate categorical and numeric column names as lists\n",
      "\n",
      "categorical=telco_raw.nunique()[telcom.nunique()<10].keys().tolist()\n",
      "\n",
      "\n",
      "#remove church\n",
      "categorical.remove(target[0])\n",
      "\n",
      "numerical = [ col for col in telco_raw.columns\n",
      "\tif col not in custid+target+categorical]\n",
      "\n",
      "\n",
      "color\n",
      "red\n",
      "white\n",
      "blue\n",
      "red\n",
      "\n",
      "one hot encode\n",
      "\n",
      "color \tred  \twhite \tblue\n",
      "red\t1\t0\t0\n",
      "white\t0\t1\t0\n",
      "blue\t0\t0\t1\n",
      "\n",
      "\n",
      "telco_raw=pd.get_dummies(data=telco_raw, columns=categorical, drop_first=True)\n",
      "\n",
      "# drop_first column because it is redundant and can be inferred from the other columns\n",
      "\n",
      "\n",
      " Scale the numerical features\n",
      "\n",
      "divide by standard deviation \n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler = StandardScaler()\n",
      "\n",
      "scaled_numerical = scaler.fit_transform(telco_raw[numerical])\n",
      "\n",
      "run on the numerical columns of the dataset\n",
      "\n",
      "#build dataframe\n",
      "\n",
      "scaled_numerical = pd.DataFrame(scaled_numerical, columns=numerical)\n",
      "\n",
      "#drop non-scaled numerical columns\n",
      "telco_raw = telco_raw.drop(columns=numerical, axis=1)\n",
      "\n",
      "#merge the non-numerical with the scaled numerical data\n",
      "\n",
      "telco= telco_raw.merge(right=scaled_nuemrical,\n",
      "\thow='left',\n",
      "\tleft_index=True,\n",
      "\tright_index=True\n",
      "\t)\n",
      "\n",
      " Sample\n",
      "\n",
      "\n",
      "# Print the data types of telco_raw dataset\n",
      "print(telco_raw.dtypes)\n",
      "\n",
      "# Print the header of telco_raw dataset\n",
      "print(telco_raw.head())\n",
      "\n",
      "# Print the number of unique values in each telco_raw column\n",
      "print(telco_raw.nunique())\n",
      "\n",
      "\n",
      " Sample\n",
      "#You will now separate categorical and numerical variables from the telco_raw DataFrame with a customized categorical vs. numerical unique value count threshold.\n",
      "\n",
      "# Store customerID and Churn column names\n",
      "custid = ['customerID']\n",
      "target = ['Churn']\n",
      "\n",
      "# Store categorical column names\n",
      "categorical = telco_raw.nunique()[telco_raw.nunique() < 5].keys().tolist()\n",
      "\n",
      "# Remove target from the list of categorical variables\n",
      "categorical.remove(target[0])\n",
      "\n",
      "# Store numerical column names\n",
      "numerical = [x for x in telco_raw.columns if x not in custid + target + categorical]\n",
      "\n",
      " Encoded categorical and scale numerical values\n",
      "\n",
      "# Perform one-hot encoding to categorical variables \n",
      "telco_raw = pd.get_dummies(data = telco_raw, columns = categorical, drop_first=True)\n",
      "\n",
      "# Initialize StandardScaler instance\n",
      "scaler = StandardScaler()\n",
      "\n",
      "# Fit and transform the scaler on numerical columns\n",
      "scaled_numerical = scaler.fit_transform(telco_raw[numerical])\n",
      "\n",
      "# Build a DataFrame from scaled_numerical\n",
      "scaled_numerical = pd.DataFrame(scaled_numerical, columns=numerical)\n",
      "\n",
      " ML modeling steps\n",
      "\n",
      "1. split data to training and testing\n",
      "2. initialize the model\n",
      "3. fit the model on the testing data\n",
      "4. predict values on the testing data\n",
      "5. measure model performance on testing data\n",
      "\n",
      "\n",
      "from sklearn import tree\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "train_X, test_X, train_Y, test_Y=train_test_split(X,Y, test_size=0.25)\n",
      "\n",
      "mytree= tree.DecisionTreeClassifier()\n",
      "\n",
      "treemodel=mytree.fit(train_X, train_Y)\n",
      "\n",
      "pred_Y=treemodel.predict(test_X)\n",
      "\n",
      "accuracy_score(test_Y,pred_Y)\n",
      "\n",
      "\n",
      " Unsupervised learning steps\n",
      "\n",
      "1. Initialize the model\n",
      "2. Fit the model\n",
      "3. Assign cluster values\n",
      "4. Explore results\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "import pandas as pd\n",
      "\n",
      "kmeans=KMeans(n_cluster=3)\n",
      "\n",
      "kmeans.fit(data)\n",
      "\n",
      "data.assign(Cluster=kmeans.labels_)\n",
      "\n",
      "data.groupby('Cluster').mean()\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "# Split X and Y into training and testing datasets\n",
      "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.25)\n",
      "\n",
      "#print(X.columns)\n",
      "\n",
      "# Ensure training dataset has only 75% of original X data\n",
      "print(train_X.shape[0] / X.shape[0])\n",
      "\n",
      "# Ensure testing dataset has only 25% of original X data\n",
      "print(test_Y.shape[0] / Y.shape[0])\n",
      "\n",
      "\n",
      " >Tree classifier\n",
      "\n",
      "# Initialize the model with max_depth set at 5\n",
      "mytree = tree.DecisionTreeClassifier(max_depth = 5)\n",
      "\n",
      "# Fit the model on the training data\n",
      "treemodel = mytree.fit(train_X, train_Y)\n",
      "\n",
      "# Predict values on the testing data\n",
      "pred_Y = treemodel.predict(test_X)\n",
      "\n",
      "# Measure model performance on testing data\n",
      "accuracy_score(test_Y,pred_Y)\n",
      "\n",
      " >Predicting Churn\n",
      "\n",
      "# Initialize the Decision Tree\n",
      "clf = tree.DecisionTreeClassifier(max_depth = 7, \n",
      "               criterion = 'gini', \n",
      "               splitter  = 'best')\n",
      "\n",
      "# Fit the model to the training data\n",
      "clf = clf.fit(train_X, train_Y)\n",
      "\n",
      "# Predict the values on test dataset\n",
      "pred_Y = clf.predict(test_X)\n",
      "\n",
      "# Print accuracy values\n",
      "print(\"Training accuracy: \", np.round(clf.score(train_X, train_Y), 3)) \n",
      "print(\"Test accuracy: \", np.round(accuracy_score(test_Y, pred_Y), 3))\n",
      "\n",
      "\n",
      "  items to research\n",
      "\n",
      "1. churn prediction fundamentals\n",
      "2. exploring churn rates\n",
      "3. predicting churn with logistic regression\n",
      "4. fit logistic regression with L1 regularization\n",
      "5. identify optimal L1 penalty coefficient\n",
      "6. predict churn with decision trees\n",
      "7. fit decision tree model\n",
      "8. identify optimal tree depth\n",
      "9. identify and interpret churn drivers\n",
      "10. explore logistic regression coeffients\n",
      "11. break down decision tree rules\n",
      "\n",
      "\n",
      "  Build customer and product segmentation\n",
      "1. determine the optimal number of clusters\n",
      "2. build segmentation using kmeans clustering\n",
      "3. alternative segmentation with NMF\n",
      "4. k-means segmentation averages\n",
      "5. NMF segmentation averages\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\nlp bag of words.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\nlp bag of words.txt\n",
      "bag of words is a method for finding topics in text\n",
      "\n",
      "need to first create tokens using tokenization\n",
      "\n",
      "count up all the tokens\n",
      "\n",
      "the frequent a word, the more important it might be\n",
      "\n",
      "lower case all the words in the text\n",
      "\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "from collections import Counter\n",
      "\n",
      "Counter(word_tokenize(\"\"\"The cat is in the box.  The cat likes the box.  The box is over the cat\"\"\"))\n",
      "\n",
      "counter.most_common(2)\n",
      "\n",
      "result is a series of tuples\n",
      "\n",
      "\n",
      "  >Sample   > find the top 10 most common words in an article to discover the topic\n",
      "\n",
      "# Import Counter\n",
      "from collections import Counter\n",
      "\n",
      "# Tokenize the article: tokens\n",
      "tokens = word_tokenize(article)\n",
      "\n",
      "# Convert the tokens into lowercase: lower_tokens\n",
      "lower_tokens = [token.lower() for token in tokens]\n",
      "\n",
      "# Create a Counter with the lowercase tokens: bow_simple\n",
      "bow_simple = Counter(lower_tokens)\n",
      "\n",
      "# Print the 10 most common tokens\n",
      "print(bow_simple.most_common(10))\n",
      "\n",
      "\n",
      "        Simple Text preprocessing\n",
      "\n",
      "helps for making better input data\n",
      "\n",
      "when performing machine learning or other statistical methods\n",
      "\n",
      "other common techniquest are lemmatization or stemming\n",
      "1. shorten the words to their root stems\n",
      "2. remove stop words, punctuation or unwanted tokens\n",
      "\n",
      "input: Cats, dogs, and birds are common pets. So are fish.\n",
      "\n",
      "output: cat, dog, bird, common, pet, fish\n",
      "\n",
      "\n",
      "from ntlk.corpus import stopwords\n",
      "\n",
      "text=\"\"\"The cat is in the box.  The cat likes the box.  The box is over the cat\"\"\"\n",
      "\n",
      "\n",
      "tokens=[w for w in word_tokenizer(text.lower()) if w.isalpha()]\n",
      "\n",
      "no_stops=[t for t in tokens if t not in stopwords.words('english')]\n",
      "\n",
      "Counter(no_stop).most_common(2)\n",
      "\n",
      "\n",
      " >sample   > lematize\n",
      "\n",
      "\n",
      "# Import WordNetLemmatizer\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "\n",
      "# Retain alphabetic words: alpha_only\n",
      "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
      "\n",
      "# Remove all stop words: no_stops\n",
      "no_stops = [t for t in alpha_only if t not in english_stops]\n",
      "\n",
      "# Instantiate the WordNetLemmatizer\n",
      "wordnet_lemmatizer = WordNetLemmatizer()\n",
      "\n",
      "# Lemmatize all tokens into a new list: lemmatized\n",
      "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
      "\n",
      "# Create the bag-of-words: bow\n",
      "bow = Counter(lemmatized)\n",
      "\n",
      "# Print the 10 most common tokens\n",
      "print(bow.most_common(10))\n",
      "\n",
      "\n",
      "       gensim nlp library\n",
      "\n",
      "\n",
      "popular open-solurce nlp library\n",
      "uses top academic models to perform complex tasks\n",
      "\n",
      "perform topic identification and document comparison\n",
      "\n",
      "a vector is a multi dimensional representation of a word.  it is trained from a large corpus of words\n",
      "\n",
      "male-female: king and queen, man and woman\n",
      "verb tense: walking and walked,swimming and swam\n",
      "country-capital: spain and madrid, italy and rome\n",
      "\n",
      "we can find comparisons between the words dependant on how near or far the words are.\n",
      "\n",
      "\n",
      "from gensim.corpora.dictionary import Dictionary\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "tokenized_docs=[ word_tokenize(doc.lower()) for doc in sentences]\n",
      "dictionary=Dictionary(tokenized_docs)\n",
      "print('This will create an id for each token in the corpus')\n",
      "#print(dictionary.token2id)\n",
      "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
      "#print(corpus)\n",
      "print(\"Each document is converted into a bag of words indicating the frequency of each token\")\n",
      "\n",
      "The tuple list has the first element as the token id and the second element in the tuple is the frequency\n",
      "\n",
      "  sample  > create a corpus\n",
      "\n",
      "# Import Dictionary\n",
      "from gensim.corpora.dictionary import Dictionary\n",
      "\n",
      "# Create a Dictionary from the articles: dictionary\n",
      "dictionary = Dictionary(articles)\n",
      "\n",
      "# Select the id for \"computer\": computer_id\n",
      "computer_id = dictionary.token2id.get(\"computer\")\n",
      "\n",
      "# Use computer_id with the dictionary to print the word\n",
      "print(dictionary.get(computer_id))\n",
      "\n",
      "# Create a MmCorpus: corpus\n",
      "corpus = [dictionary.doc2bow(article) for article in articles]\n",
      "\n",
      "# Print the first 10 word ids with their frequency counts from the fifth document\n",
      "print(corpus[4][:10])\n",
      "\n",
      "  sample  > get the top 5 words in the corpus\n",
      "\n",
      "# Save the fifth document: doc\n",
      "doc = corpus[4]\n",
      "\n",
      "# Sort the doc for frequency: bow_doc\n",
      "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
      "\n",
      "# Print the top 5 words of the document alongside the count\n",
      "for word_id, word_count in bow_doc[:5]:\n",
      "    print(dictionary.get(word_id), word_count)\n",
      "    \n",
      "# Create the defaultdict: total_word_count\n",
      "total_word_count = defaultdict(int)\n",
      "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
      "    total_word_count[word_id] += word_count\n",
      "    \n",
      "# Create a sorted list from the defaultdict: sorted_word_count\n",
      "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
      "\n",
      "# Print the top 5 words across all documents alongside the count\n",
      "for word_id, word_count in sorted_word_count[:5]:\n",
      "    print(dictionary.get(word_id),word_count)\n",
      "\n",
      "        tf-idf\n",
      "\n",
      "term frequency - inverse document frequency\n",
      "\n",
      "allows you to determine the most important words in each document.\n",
      "\n",
      "each corpus may have shared words beyond just stop words\n",
      "\n",
      "some words should be downweight\n",
      "\n",
      "ensures common words don't show up as key words\n",
      "\n",
      "j is the row and i is the column\n",
      "\n",
      "weight (i,j) = tf(i,j) * log(N/df(i))\n",
      "\n",
      "weight(i,j) tf-idf weight for token i in the document j\n",
      "\n",
      "tf(i,j) = number of occurrences of token i in the document j\n",
      "\n",
      "df(i)= number of documents that contain token i\n",
      "\n",
      "N= total number of documents\n",
      "\n",
      "\n",
      "the weight will be low if the term does not appear often in the document because the tf variable will be low.  the weight will be low if the log is near zero meaning (N/df(i)) is close to 1. log of 1 is 0\n",
      "\n",
      "\n",
      "from gensim.models.tfidfmodel import TfidfModel\n",
      "\n",
      "tfidf=TfidfModel(corpus)\n",
      "\n",
      "  sample  > get calculate the tfidf weights\n",
      "\n",
      "# Create a new TfidfModel using the corpus: tfidf\n",
      "tfidf = TfidfModel(corpus)\n",
      "\n",
      "# Calculate the tfidf weights of doc: tfidf_weights\n",
      "tfidf_weights = tfidf[doc]\n",
      "\n",
      "# Print the first five weights\n",
      "print(tfidf_weights[:5] )\n",
      "\n",
      "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
      "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
      "\n",
      "# Print the top 5 weighted words\n",
      "for term_id, weight in sorted_tfidf_weights[:5]:\n",
      "    print(dictionary.get(term_id), weight)\n",
      "\n",
      "print(doc)\n",
      "\n",
      "        Named Entity\n",
      "\n",
      "NER nlp to identify people, place, organizations\n",
      "date, states works of art\n",
      "\n",
      "answering who, what, when and where\n",
      "\n",
      "named_entity=[(entity.text,entity.label_) for entity in doc.ents]\n",
      "print(named_entity)\n",
      "\n",
      "def find_persons(text):\n",
      "    # Create Doc object\n",
      "    doc2 = nlp(text)\n",
      "  \n",
      "    # Identify the persons\n",
      "    persons = [ent.text for ent in doc2.ents if ent.label_ == 'PERSON']\n",
      "  \n",
      "    # Return persons\n",
      "    return persons\n",
      "\n",
      "persons=find_persons(paragraph)\n",
      "print(persons)\n",
      "\n",
      "PERSON, DATE, WORK_OF_ART, ORG, GPS,CARDINAL\n",
      "GPE: country city states\n",
      "LAW\n",
      "LANGUAGE\n",
      "\n",
      "use to extract facts\n",
      "\n",
      "CoreNLP integrates with nltk\n",
      "CoreNLP runs on java\n",
      "\n",
      "    using nltk for Name Entity Recognition\n",
      "\n",
      "sentence=\"\"\"In New York, I like to ride the Metro to visit MOMA and some restaurants rated well by Ruth Reichl\"\"\"\n",
      "\n",
      "tokenized_sent=nltk.word_tokenize(sentence)\n",
      "tagged_sent=nltk.pos_tag(tokenized_sent)\n",
      "\n",
      "nouns\n",
      "pronouns\n",
      "adjectives\n",
      "verbs\n",
      "\n",
      "NNP - proper noun singular\n",
      "PRP - proper noun\n",
      "VB - verb\n",
      "DT - determinant\n",
      "\n",
      "\n",
      "print(nltk.ne_chunk(tagged_sent))\n",
      "\n",
      "returns the sentence as a tree\n",
      "\n",
      "\n",
      "  Sample   > 1. create sentences 2. tokenize, 3. parts of speech 4. chunk sentences.  5. output where chunk label is NE\n",
      "\n",
      "# Tokenize the article into sentences: sentences\n",
      "sentences = nltk.sent_tokenize(article)\n",
      "print(sentences)\n",
      "\n",
      "# Tokenize each sentence into words: token_sentences\n",
      "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
      "#print(token_sentences)\n",
      "\n",
      "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
      "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
      "\n",
      "print(pos_sentences)\n",
      "\n",
      "# Create the named entity chunks: chunked_sentences\n",
      "chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n",
      "\n",
      "# Test for stems of the tree with 'NE' tags\n",
      "for sent in chunked_sentences:\n",
      "    for chunk in sent:\n",
      "        if hasattr(chunk, \"label\") and chunk.label()== \"NE\":\n",
      "            print(chunk)\n",
      "\n",
      "# Create the defaultdict: ner_categories\n",
      "ner_categories = defaultdict(int)\n",
      "\n",
      "# Create the nested for loop\n",
      "for sent in chunked_sentences:\n",
      "    for chunk in sent:\n",
      "        if hasattr(chunk, 'label'):\n",
      "            ner_categories[chunk.label()] += 1\n",
      "            \n",
      "# Create a list from the dictionary keys for the chart labels: labels\n",
      "labels = list(ner_categories)\n",
      "print(labels)\n",
      "\n",
      "# Create the defaultdict: ner_categories\n",
      "ner_categories = defaultdict(int)\n",
      "\n",
      "# Create the nested for loop\n",
      "for sent in chunked_sentences:\n",
      "    for chunk in sent:\n",
      "        if hasattr(chunk, 'label'):\n",
      "            ner_categories[chunk.label()] += 1\n",
      "            \n",
      "# Create a list from the dictionary keys for the chart labels: labels\n",
      "labels = list(ner_categories.keys())\n",
      "\n",
      "# Create a list of the values: values\n",
      "values = [ner_categories.get(v) for v in labels]\n",
      "\n",
      "# Create the pie chart\n",
      "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
      "\n",
      "# Display the chart\n",
      "plt.show()\n",
      "\n",
      "['\\ufeffThe taxi-hailing company Uber brings into very sharp focus the question of whether corporations can be said to have a moral character.', 'If any human being were to behave with the single-minded and ruthless greed of the company, we would consider them sociopathic.', 'Uber wanted to know as much as possible about the people who use its service, and those who donâ€™t.', 'It has an arrangement with unroll.me, a company which offered a free service for unsubscribing from junk mail, to buy the contacts unroll.me customers had had with rival taxi companies.', 'Even if their email was notionally anonymised, this use of it was not something the users had bargained for.', 'Beyond that, it keeps track of the phones that have been used to summon its services even after the original owner has sold them, attempting this with Appleâ€™s phones even thought it is forbidden by the company.', 'Uber has also tweaked its software so that regulatory agencies that the company regarded as hostile would, when they tried to hire a driver, be given false reports about the location of its cars.', 'Uber management booked and then cancelled rides with a rival taxi-hailing company which took their vehicles out of circulation.', 'Uber deny this was the intention.', 'The punishment for this behaviour was negligible.', 'Uber promised not to use this â€œgreyballâ€ software against law enforcement â€“ one wonders what would happen to someone carrying a knife who promised never to stab a policeman with it.', 'Travis Kalanick of Uber got a personal dressing down from Tim Cook, who runs Apple, but the company did not prohibit the use of the app.', 'Too much money was at stake for that.', 'Millions of people around the world value the cheapness and convenience of Uberâ€™s rides too much to care about the lack of driversâ€™ rights or pay.', 'Many of the users themselves are not much richer than the drivers.', 'The â€œsharing economyâ€ encourages the insecure and exploited to exploit others equally insecure to the profit of a tiny clique of billionaires.', 'Silicon Valleyâ€™s culture seems hostile to humane and democratic values.', 'The outgoing CEO of Yahoo, Marissa Mayer, who is widely judged to have been a failure, is likely to get a $186m payout.', 'This may not be a cause for panic, any more than the previous hero worship should have been a cause for euphoria.', 'Yet thereâ€™s an urgent political task to tame these companies, to ensure they are punished when they break the law, that they pay their taxes fairly and that they behave responsibly.']\n",
      "\n",
      "[['\\ufeffThe', 'taxi-hailing', 'company', 'Uber', 'brings', 'into', 'very', 'sharp', 'focus', 'the', 'question', 'of', 'whether', 'corporations', 'can', 'be', 'said', 'to', 'have', 'a', 'moral', 'character', '.'], ['If', 'any', 'human', 'being', 'were', 'to', 'behave', 'with', 'the', 'single-minded', 'and', 'ruthless', 'greed', 'of', 'the', 'company', ',', 'we', 'would', 'consider', 'them', 'sociopathic', '.'], ['Uber', 'wanted', 'to', 'know', 'as', 'much', 'as', 'possible', 'about', 'the', 'people', 'who', 'use', 'its', 'service', ',', 'and', 'those', 'who', 'don', 'â€™', 't', '.'], ['It', 'has', 'an', 'arrangement', 'with', 'unroll.me', ',', 'a', 'company', 'which', 'offered', 'a', 'free', 'service', 'for', 'unsubscribing', 'from', 'junk', 'mail', ',', 'to', 'buy', 'the', 'contacts', 'unroll.me', 'customers', 'had', 'had', 'with', 'rival', 'taxi', 'companies', '.'], ['Even', 'if', 'their', 'email', 'was', 'notionally', 'anonymised', ',', 'this', 'use', 'of', 'it', 'was', 'not', 'something', 'the', 'users', 'had', 'bargained', 'for', '.'], ['Beyond', 'that', ',', 'it', 'keeps', 'track', 'of', 'the', 'phones', 'that', 'have', 'been', 'used', 'to', 'summon', 'its', 'services', 'even', 'after', 'the', 'original', 'owner', 'has', 'sold', 'them', ',', 'attempting', 'this', 'with', 'Apple', 'â€™', 's', 'phones', 'even', 'thought', 'it', 'is', 'forbidden', 'by', 'the', 'company', '.'], ['Uber', 'has', 'also', 'tweaked', 'its', 'software', 'so', 'that', 'regulatory', 'agencies', 'that', 'the', 'company', 'regarded', 'as', 'hostile', 'would', ',', 'when', 'they', 'tried', 'to', 'hire', 'a', 'driver', ',', 'be', 'given', 'false', 'reports', 'about', 'the', 'location', 'of', 'its', 'cars', '.'], ['Uber', 'management', 'booked', 'and', 'then', 'cancelled', 'rides', 'with', 'a', 'rival', 'taxi-hailing', 'company', 'which', 'took', 'their', 'vehicles', 'out', 'of', 'circulation', '.'], ['Uber', 'deny', 'this', 'was', 'the', 'intention', '.'], ['The', 'punishment', 'for', 'this', 'behaviour', 'was', 'negligible', '.'], ['Uber', 'promised', 'not', 'to', 'use', 'this', 'â€œ', 'greyball', 'â€', 'software', 'against', 'law', 'enforcement', 'â€“', 'one', 'wonders', 'what', 'would', 'happen', 'to', 'someone', 'carrying', 'a', 'knife', 'who', 'promised', 'never', 'to', 'stab', 'a', 'policeman', 'with', 'it', '.'], ['Travis', 'Kalanick', 'of', 'Uber', 'got', 'a', 'personal', 'dressing', 'down', 'from', 'Tim', 'Cook', ',', 'who', 'runs', 'Apple', ',', 'but', 'the', 'company', 'did', 'not', 'prohibit', 'the', 'use', 'of', 'the', 'app', '.'], ['Too', 'much', 'money', 'was', 'at', 'stake', 'for', 'that', '.'], ['Millions', 'of', 'people', 'around', 'the', 'world', 'value', 'the', 'cheapness', 'and', 'convenience', 'of', 'Uber', 'â€™', 's', 'rides', 'too', 'much', 'to', 'care', 'about', 'the', 'lack', 'of', 'drivers', 'â€™', 'rights', 'or', 'pay', '.'], ['Many', 'of', 'the', 'users', 'themselves', 'are', 'not', 'much', 'richer', 'than', 'the', 'drivers', '.'], ['The', 'â€œ', 'sharing', 'economy', 'â€', 'encourages', 'the', 'insecure', 'and', 'exploited', 'to', 'exploit', 'others', 'equally', 'insecure', 'to', 'the', 'profit', 'of', 'a', 'tiny', 'clique', 'of', 'billionaires', '.'], ['Silicon', 'Valley', 'â€™', 's', 'culture', 'seems', 'hostile', 'to', 'humane', 'and', 'democratic', 'values', '.'], ['The', 'outgoing', 'CEO', 'of', 'Yahoo', ',', 'Marissa', 'Mayer', ',', 'who', 'is', 'widely', 'judged', 'to', 'have', 'been', 'a', 'failure', ',', 'is', 'likely', 'to', 'get', 'a', '$', '186m', 'payout', '.'], ['This', 'may', 'not', 'be', 'a', 'cause', 'for', 'panic', ',', 'any', 'more', 'than', 'the', 'previous', 'hero', 'worship', 'should', 'have', 'been', 'a', 'cause', 'for', 'euphoria', '.'], ['Yet', 'there', 'â€™', 's', 'an', 'urgent', 'political', 'task', 'to', 'tame', 'these', 'companies', ',', 'to', 'ensure', 'they', 'are', 'punished', 'when', 'they', 'break', 'the', 'law', ',', 'that', 'they', 'pay', 'their', 'taxes', 'fairly', 'and', 'that', 'they', 'behave', 'responsibly', '.']]\n",
      "\n",
      "output\n",
      "[[('\\ufeffThe', 'JJ'), ('taxi-hailing', 'JJ'), ('company', 'NN'), ('Uber', 'NNP'), ('brings', 'VBZ'), ('into', 'IN'), ('very', 'RB'), ('sharp', 'JJ'), ('focus', 'VB'), ('the', 'DT'), ('question', 'NN'), ('of', 'IN'), ('whether', 'IN'), ('corporations', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('said', 'VBD'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('moral', 'JJ'), ('character', 'NN'), ('.', '.')], [('If', 'IN'), ('any', 'DT'), ('human', 'JJ'), ('being', 'VBG'), ('were', 'VBD'), ('to', 'TO'), ('behave', 'VB'), ('with', 'IN'), ('the', 'DT'), ('single-minded', 'JJ'), ('and', 'CC'), ('ruthless', 'JJ'), ('greed', 'NN'), ('of', 'IN'), ('the', 'DT'), ('company', 'NN'), (',', ','), ('we', 'PRP'), ('would', 'MD'), ('consider', 'VB'), ('them', 'PRP'), ('sociopathic', 'JJ'), ('.', '.')], [('Uber', 'NNP'), ('wanted', 'VBD'), ('to', 'TO'), ('know', 'VB'), ('as', 'RB'), ('much', 'JJ'), ('as', 'IN'), ('possible', 'JJ'), ('about', 'IN'), ('the', 'DT'), ('people', 'NNS'), ('who', 'WP'), ('use', 'VBP'), ('its', 'PRP$'), ('service', 'NN'), (',', ','), ('and', 'CC'), ('those', 'DT'), ('who', 'WP'), ('don', 'VBP'), ('â€™', 'JJ'), ('t', 'NN'), ('.', '.')], [('It', 'PRP'), ('has', 'VBZ'), ('an', 'DT'), ('arrangement', 'NN'), ('with', 'IN'), ('unroll.me', 'JJ'), (',', ','), ('a', 'DT'), ('company', 'NN'), ('which', 'WDT'), ('offered', 'VBD'), ('a', 'DT'), ('free', 'JJ'), ('service', 'NN'), ('for', 'IN'), ('unsubscribing', 'VBG'), ('from', 'IN'), ('junk', 'NN'), ('mail', 'NN'), (',', ','), ('to', 'TO'), ('buy', 'VB'), ('the', 'DT'), ('contacts', 'NNS'), ('unroll.me', 'JJ'), ('customers', 'NNS'), ('had', 'VBD'), ('had', 'VBN'), ('with', 'IN'), ('rival', 'JJ'), ('taxi', 'NN'), ('companies', 'NNS'), ('.', '.')], [('Even', 'RB'), ('if', 'IN'), ('their', 'PRP$'), ('email', 'NN'), ('was', 'VBD'), ('notionally', 'RB'), ('anonymised', 'VBN'), (',', ','), ('this', 'DT'), ('use', 'NN'), ('of', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('not', 'RB'), ('something', 'NN'), ('the', 'DT'), ('users', 'NNS'), ('had', 'VBD'), ('bargained', 'VBN'), ('for', 'IN'), ('.', '.')], [('Beyond', 'NN'), ('that', 'IN'), (',', ','), ('it', 'PRP'), ('keeps', 'VBZ'), ('track', 'NN'), ('of', 'IN'), ('the', 'DT'), ('phones', 'NNS'), ('that', 'WDT'), ('have', 'VBP'), ('been', 'VBN'), ('used', 'VBN'), ('to', 'TO'), ('summon', 'VB'), ('its', 'PRP$'), ('services', 'NNS'), ('even', 'RB'), ('after', 'IN'), ('the', 'DT'), ('original', 'JJ'), ('owner', 'NN'), ('has', 'VBZ'), ('sold', 'VBN'), ('them', 'PRP'), (',', ','), ('attempting', 'VBG'), ('this', 'DT'), ('with', 'IN'), ('Apple', 'NNP'), ('â€™', 'NNP'), ('s', 'VBP'), ('phones', 'NNS'), ('even', 'RB'), ('thought', 'VBD'), ('it', 'PRP'), ('is', 'VBZ'), ('forbidden', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('company', 'NN'), ('.', '.')], [('Uber', 'NNP'), ('has', 'VBZ'), ('also', 'RB'), ('tweaked', 'VBN'), ('its', 'PRP$'), ('software', 'NN'), ('so', 'IN'), ('that', 'DT'), ('regulatory', 'JJ'), ('agencies', 'NNS'), ('that', 'IN'), ('the', 'DT'), ('company', 'NN'), ('regarded', 'VBD'), ('as', 'IN'), ('hostile', 'NN'), ('would', 'MD'), (',', ','), ('when', 'WRB'), ('they', 'PRP'), ('tried', 'VBD'), ('to', 'TO'), ('hire', 'VB'), ('a', 'DT'), ('driver', 'NN'), (',', ','), ('be', 'VB'), ('given', 'VBN'), ('false', 'JJ'), ('reports', 'NNS'), ('about', 'IN'), ('the', 'DT'), ('location', 'NN'), ('of', 'IN'), ('its', 'PRP$'), ('cars', 'NNS'), ('.', '.')], [('Uber', 'NNP'), ('management', 'NN'), ('booked', 'VBD'), ('and', 'CC'), ('then', 'RB'), ('cancelled', 'VBD'), ('rides', 'NNS'), ('with', 'IN'), ('a', 'DT'), ('rival', 'JJ'), ('taxi-hailing', 'JJ'), ('company', 'NN'), ('which', 'WDT'), ('took', 'VBD'), ('their', 'PRP$'), ('vehicles', 'NNS'), ('out', 'IN'), ('of', 'IN'), ('circulation', 'NN'), ('.', '.')], [('Uber', 'NNP'), ('deny', 'NN'), ('this', 'DT'), ('was', 'VBD'), ('the', 'DT'), ('intention', 'NN'), ('.', '.')], [('The', 'DT'), ('punishment', 'NN'), ('for', 'IN'), ('this', 'DT'), ('behaviour', 'NN'), ('was', 'VBD'), ('negligible', 'JJ'), ('.', '.')], [('Uber', 'NNP'), ('promised', 'VBD'), ('not', 'RB'), ('to', 'TO'), ('use', 'VB'), ('this', 'DT'), ('â€œ', 'NN'), ('greyball', 'NN'), ('â€', 'NNP'), ('software', 'NN'), ('against', 'IN'), ('law', 'NN'), ('enforcement', 'NN'), ('â€“', 'NNP'), ('one', 'NN'), ('wonders', 'VBZ'), ('what', 'WDT'), ('would', 'MD'), ('happen', 'VB'), ('to', 'TO'), ('someone', 'NN'), ('carrying', 'VBG'), ('a', 'DT'), ('knife', 'NN'), ('who', 'WP'), ('promised', 'VBD'), ('never', 'RB'), ('to', 'TO'), ('stab', 'VB'), ('a', 'DT'), ('policeman', 'NN'), ('with', 'IN'), ('it', 'PRP'), ('.', '.')], [('Travis', 'NNP'), ('Kalanick', 'NNP'), ('of', 'IN'), ('Uber', 'NNP'), ('got', 'VBD'), ('a', 'DT'), ('personal', 'JJ'), ('dressing', 'VBG'), ('down', 'RP'), ('from', 'IN'), ('Tim', 'NNP'), ('Cook', 'NNP'), (',', ','), ('who', 'WP'), ('runs', 'VBZ'), ('Apple', 'NNP'), (',', ','), ('but', 'CC'), ('the', 'DT'), ('company', 'NN'), ('did', 'VBD'), ('not', 'RB'), ('prohibit', 'VB'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('the', 'DT'), ('app', 'NN'), ('.', '.')], [('Too', 'RB'), ('much', 'JJ'), ('money', 'NN'), ('was', 'VBD'), ('at', 'IN'), ('stake', 'NN'), ('for', 'IN'), ('that', 'DT'), ('.', '.')], [('Millions', 'NNS'), ('of', 'IN'), ('people', 'NNS'), ('around', 'IN'), ('the', 'DT'), ('world', 'NN'), ('value', 'NN'), ('the', 'DT'), ('cheapness', 'NN'), ('and', 'CC'), ('convenience', 'NN'), ('of', 'IN'), ('Uber', 'NNP'), ('â€™', 'NNP'), ('s', 'VBD'), ('rides', 'NNS'), ('too', 'RB'), ('much', 'RB'), ('to', 'TO'), ('care', 'VB'), ('about', 'IN'), ('the', 'DT'), ('lack', 'NN'), ('of', 'IN'), ('drivers', 'NNS'), ('â€™', 'NNP'), ('rights', 'NNS'), ('or', 'CC'), ('pay', 'NN'), ('.', '.')], [('Many', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('users', 'NNS'), ('themselves', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('much', 'RB'), ('richer', 'JJR'), ('than', 'IN'), ('the', 'DT'), ('drivers', 'NNS'), ('.', '.')], [('The', 'DT'), ('â€œ', 'JJ'), ('sharing', 'VBG'), ('economy', 'NN'), ('â€', 'JJ'), ('encourages', 'VBZ'), ('the', 'DT'), ('insecure', 'NN'), ('and', 'CC'), ('exploited', 'VBD'), ('to', 'TO'), ('exploit', 'VB'), ('others', 'NNS'), ('equally', 'RB'), ('insecure', 'VBP'), ('to', 'TO'), ('the', 'DT'), ('profit', 'NN'), ('of', 'IN'), ('a', 'DT'), ('tiny', 'JJ'), ('clique', 'NN'), ('of', 'IN'), ('billionaires', 'NNS'), ('.', '.')], [('Silicon', 'NNP'), ('Valley', 'NNP'), ('â€™', 'NNP'), ('s', 'JJ'), ('culture', 'NN'), ('seems', 'VBZ'), ('hostile', 'JJ'), ('to', 'TO'), ('humane', 'NN'), ('and', 'CC'), ('democratic', 'JJ'), ('values', 'NNS'), ('.', '.')], [('The', 'DT'), ('outgoing', 'VBG'), ('CEO', 'NNP'), ('of', 'IN'), ('Yahoo', 'NNP'), (',', ','), ('Marissa', 'NNP'), ('Mayer', 'NNP'), (',', ','), ('who', 'WP'), ('is', 'VBZ'), ('widely', 'RB'), ('judged', 'VBN'), ('to', 'TO'), ('have', 'VB'), ('been', 'VBN'), ('a', 'DT'), ('failure', 'NN'), (',', ','), ('is', 'VBZ'), ('likely', 'JJ'), ('to', 'TO'), ('get', 'VB'), ('a', 'DT'), ('$', '$'), ('186m', 'CD'), ('payout', 'NN'), ('.', '.')], [('This', 'DT'), ('may', 'MD'), ('not', 'RB'), ('be', 'VB'), ('a', 'DT'), ('cause', 'NN'), ('for', 'IN'), ('panic', 'NN'), (',', ','), ('any', 'DT'), ('more', 'JJR'), ('than', 'IN'), ('the', 'DT'), ('previous', 'JJ'), ('hero', 'NN'), ('worship', 'NN'), ('should', 'MD'), ('have', 'VB'), ('been', 'VBN'), ('a', 'DT'), ('cause', 'NN'), ('for', 'IN'), ('euphoria', 'NN'), ('.', '.')], [('Yet', 'RB'), ('there', 'EX'), ('â€™', 'NNP'), ('s', 'VBD'), ('an', 'DT'), ('urgent', 'JJ'), ('political', 'JJ'), ('task', 'NN'), ('to', 'TO'), ('tame', 'VB'), ('these', 'DT'), ('companies', 'NNS'), (',', ','), ('to', 'TO'), ('ensure', 'VB'), ('they', 'PRP'), ('are', 'VBP'), ('punished', 'VBN'), ('when', 'WRB'), ('they', 'PRP'), ('break', 'VBP'), ('the', 'DT'), ('law', 'NN'), (',', ','), ('that', 'IN'), ('they', 'PRP'), ('pay', 'VBP'), ('their', 'PRP$'), ('taxes', 'NNS'), ('fairly', 'RB'), ('and', 'CC'), ('that', 'IN'), ('they', 'PRP'), ('behave', 'VBP'), ('responsibly', 'RB'), ('.', '.')]]\n",
      "\n",
      "\n",
      "\n",
      "   spacy a library for natural language processing\n",
      "\n",
      "import spacy\n",
      "\n",
      "nlp=spacy.load('en')\n",
      "\n",
      "nlp.entity\n",
      "\n",
      "\n",
      "doc=nlp(\"\"\"Berlin is the capital of Germany; and the residence of Chancellor Angela Merkel.\"\"\")\n",
      "\n",
      "doc.ents\n",
      "\n",
      "output: Berlin, Germany, Angela Merkel\n",
      "\n",
      "print(doc.ents[0], doc.ents[0].label_)\n",
      "\n",
      "\n",
      "   Sample  > spacy to parse entities\n",
      "\n",
      "# Import spacy\n",
      "import spacy\n",
      "\n",
      "# Instantiate the English model: nlp\n",
      "nlp = spacy.load('en',tagger=False, parser=False, matcher=False)\n",
      "\n",
      "# Create a new document: doc\n",
      "doc = nlp(article)\n",
      "\n",
      "# Print all of the found entities and their labels\n",
      "for ent in doc.ents:\n",
      "    print(ent.label_, ent.text)\n",
      "\n",
      "\n",
      "  > Multilingual NER with polyglot\n",
      "\n",
      "nlp library which use word vectors\n",
      "\n",
      "gensim nlp\n",
      "\n",
      "polygot supports a 130 languages\n",
      "\n",
      "transliteration is swapping characters from one language to another\n",
      "\n",
      "Spanish NER with polyglot\n",
      "\n",
      "from polyglot.text import Text\n",
      "\n",
      "text=\"\"\"El presidente de la Generalitat de Cataluna,\n",
      "Carles Puigdemont, ha afirmado hoy a la alcaldesa de Madrid, Manuel Carmena, que en su etapa de alcalde de Girona (de julio de 2011 a enero de 2016) hjzo una gran promocion de madrid\"\"\"\"\n",
      "\n",
      "ptext=Text(text)\n",
      "\n",
      "conda install -c syllabs_admin polyglot\n",
      "\n",
      "ptext.entities\n",
      "\n",
      "\n",
      "   > sample    load the text and print the entities\n",
      "\n",
      "# Create a new text object using Polyglot's Text class: txt\n",
      "\n",
      "txt = Text(article)\n",
      "\n",
      "# Print each of the entities found\n",
      "for ent in txt.entities:\n",
      "    print(ent)\n",
      "    \n",
      "print(type(ent))\n",
      "# Print the type of ent\n",
      "print(type(txt.entities))\n",
      "\n",
      "\n",
      "polyglot.text.Chunk\n",
      "\n",
      "   sample   join the entities elements into a string and a entity tag tuple\n",
      "\n",
      "# Create the list of tuples: entities\n",
      "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
      "\n",
      "# Print entities\n",
      "print(entities)\n",
      "\n",
      "  sample   find specific entities in the text\n",
      "\n",
      "# Initialize the count variable: count\n",
      "count = 0\n",
      "\n",
      "# Iterate over all the entities\n",
      "for ent in txt.entities:\n",
      "    # Check whether the entity contains 'MÃ¡rquez' or 'Gabo'\n",
      "    if (\"MÃ¡rquez\" in ent) or (\"Gabo\" in ent):\n",
      "        # Increment count\n",
      "        count+=1\n",
      "\n",
      "# Print count\n",
      "print(count)\n",
      "\n",
      "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
      "percentage = count / len(txt.entities)\n",
      "print(percentage)\n",
      "\n",
      "    classifying fake news\n",
      "\n",
      "training data\n",
      "label or outcome to learn\n",
      "\n",
      "classification problem\n",
      "intelligent hypothesis\n",
      "use language to find features.\n",
      "\n",
      "\n",
      "\n",
      "based on the movie Plot predict if it is sci-fi or action\n",
      "\n",
      "supervised learning steps\n",
      "1. collect and preprocess the data\n",
      "2. determine a label\n",
      "3. split data into training and testing sets\n",
      "4. extract features from text to help predict the label\n",
      "5. evaluate trained model using the test set\n",
      "\n",
      "y=df['SciFile]\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "\n",
      "X=df['Comment']\n",
      "\n",
      "df['IsSafety']=0\n",
      "df['IsSafety']=[1 if item=='Safety' else 0  for item in df['Label']]\n",
      "y=df['IsSafety']\n",
      "\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.33, random_state=42)\n",
      "\n",
      "  sample   > Count vectorizer a binary classification\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Print the head of df\n",
      "print(df.head())\n",
      "\n",
      "# Create a series to store the labels: y\n",
      "y = df.label\n",
      "\n",
      "# Create training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"],y,test_size=.33,random_state=53)\n",
      "\n",
      "# Initialize a CountVectorizer object: count_vectorizer\n",
      "count_vectorizer = CountVectorizer(stop_words='english')\n",
      "\n",
      "# Transform the training data using only the 'text' column values: count_train \n",
      "count_train = count_vectorizer.fit_transform(X_train.values)\n",
      "\n",
      "# Transform the test data using only the 'text' column values: count_test \n",
      "count_test = count_vectorizer.transform(X_test.values)\n",
      "\n",
      "# Print the first 10 features of the count_vectorizer\n",
      "print(count_vectorizer.get_feature_names()[:10])\n",
      "\n",
      "\n",
      "   >tfidfVectorizer\n",
      "\n",
      "creating tf-idf vectors for your documents.\n",
      "\n",
      "# Import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
      "tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7)\n",
      "\n",
      "# Transform the training data: tfidf_train \n",
      "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
      "\n",
      "# Transform the test data: tfidf_test \n",
      "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
      "\n",
      "# Print the first 10 features\n",
      "print(tfidf_vectorizer.get_feature_names()[:10])\n",
      "\n",
      "# Print the first 5 vectors of the tfidf training data\n",
      "print(tfidf_train.A[:5])\n",
      "\n",
      "  > sample count tfidvectorizer equals count vectorizer\n",
      "\n",
      "# Create the CountVectorizer DataFrame: count_df\n",
      "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
      "\n",
      "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
      "tfidf_df = pd.DataFrame(tfidf_train.A,columns=tfidf_vectorizer.get_feature_names())\n",
      "\n",
      "# Print the head of count_df\n",
      "print(count_df.head())\n",
      "\n",
      "# Print the head of tfidf_df\n",
      "print(tfidf_df.head())\n",
      "\n",
      "# Calculate the difference in columns: difference\n",
      "difference = set(tfidf_df.columns) - set(count_df.columns)\n",
      "print(difference)\n",
      "\n",
      "# Check whether the DataFrames are equal\n",
      "print(count_df.equals(tfidf_df))\n",
      "\n",
      "output: False\n",
      "\n",
      "        naive bayes classifier\n",
      "\n",
      "naive bayes model\n",
      "commonly used for testing nlp classification problems\n",
      "basis in probability\n",
      "\n",
      "given a particular piece of data, how likely is a particular outcome?\n",
      "\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import metrics\n",
      "\n",
      "nb_classifier=MultinomialNB()\n",
      "nb_classifier.fit(count_train, y_train)\n",
      "pred=nb_classifier.predict(count_test)\n",
      "\n",
      "print(metrics.accuracy_score(y_test,pred))\n",
      "\n",
      "metrics.confusion_matrix(y_test,pred, labels=[0,1])\n",
      "\n",
      "   > sample     multinomialNB classification\n",
      "\n",
      "# Import the necessary modules\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import metrics\n",
      "\n",
      "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
      "nb_classifier=MultinomialNB()\n",
      "\n",
      "# Fit the classifier to the training data\n",
      "nb_classifier.fit(count_train, y_train)\n",
      "\n",
      "# Create the predicted tags: pred\n",
      "pred=nb_classifier.predict(count_test)\n",
      "\n",
      "# Calculate the accuracy score: score\n",
      "score = metrics.accuracy_score(y_test,pred)\n",
      "print(score)\n",
      "\n",
      "# Calculate the confusion matrix: cm\n",
      "cm = metrics.confusion_matrix(y_test,pred, labels=['FAKE','REAL'])\n",
      "print(cm)\n",
      "\n",
      "   sample using tfidf train data\n",
      "\n",
      "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
      "nb_classifier=MultinomialNB()\n",
      "\n",
      "# Fit the classifier to the training data\n",
      "nb_classifier.fit(tfidf_train, y_train)\n",
      "\n",
      "# Create the predicted tags: pred\n",
      "pred=nb_classifier.predict(tfidf_test)\n",
      "\n",
      "# Calculate the accuracy score: score\n",
      "score = metrics.accuracy_score(y_test,pred)\n",
      "print(score)\n",
      "\n",
      "# Calculate the confusion matrix: cm\n",
      "cm = metrics.confusion_matrix(y_test,pred, labels=['FAKE','REAL'])\n",
      "print(cm)\n",
      "\n",
      "\n",
      "   > Simple NLP, complex problems.\n",
      "\n",
      "\n",
      "<<<<sample <<< alpha of .1 performs at 89%\n",
      "\n",
      "# Create the list of alphas: alphas\n",
      "alphas = np.arange(0,1,0.1)\n",
      "\n",
      "# Define train_and_predict()\n",
      "def train_and_predict(alpha):\n",
      "    # Instantiate the classifier: nb_classifier\n",
      "    nb_classifier = MultinomialNB(alpha=alpha)\n",
      "    # Fit to the training data\n",
      "    nb_classifier.fit(tfidf_train, y_train)\n",
      "    # Predict the labels: pred\n",
      "    pred=nb_classifier.predict(tfidf_test)\n",
      "    # Compute accuracy: score\n",
      "    score = metrics.accuracy_score(y_test,pred)\n",
      "    return score\n",
      "\n",
      "# Iterate over the alphas and print the corresponding score\n",
      "for alpha in alphas:\n",
      "    print('Alpha: ', alpha)\n",
      "    print('Score: ', train_and_predict(alpha))\n",
      "    print()\n",
      "\n",
      "   > sample the fake news\n",
      "\n",
      "You can map the important vector weights back to actual words using some simple inspection techniques.\n",
      "\n",
      "# Get the class labels: class_labels\n",
      "class_labels = nb_classifier.classes_\n",
      "print(class_labels)\n",
      "\n",
      "# Extract the features: feature_names\n",
      "feature_names = tfidf_vectorizer.get_feature_names()\n",
      "print(feature_names)\n",
      "\n",
      "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
      "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
      "\n",
      "print(feat_with_weights)\n",
      "\n",
      "# Print the first class label and the top 20 feat_with_weights entries\n",
      "print(class_labels[0], feat_with_weights[:20])\n",
      "\n",
      "# Print the second class label and the bottom 20 feat_with_weights entries\n",
      "print(class_labels[1], feat_with_weights[-20:])\n",
      "\n",
      "output:\n",
      "\n",
      "\n",
      "   sample  multiple output text classifier using MultinomialNB\n",
      "\n",
      "\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "stop_words=spacy.lang.en.stop_words.STOP_WORDS\n",
      "\n",
      "df2=df.copy()\n",
      "\n",
      "LABELS=['IsSafety','IsDocumentation','IsChangeOrder','IsOperations','IsAdministration','IsGeneral']\n",
      "\n",
      "train, test = train_test_split(df2, random_state=42, test_size=0.33, shuffle=True)\n",
      "\n",
      "pl = Pipeline([\n",
      "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
      "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
      "                    fit_prior=True, class_prior=None))),\n",
      "            ])\n",
      "\n",
      "X_train = train.Comment\n",
      "X_test = test.Comment\n",
      "\n",
      "for category in LABELS:\n",
      "    print('... Processing {}'.format(category))\n",
      "    # train the model using X_dtm & y\n",
      "    pl.fit(X_train, train[category])\n",
      "    # compute the testing accuracy\n",
      "    prediction = pl.predict(X_test)\n",
      "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
      "\n",
      "\n",
      "input_text=[\"the ladder was not safety anchored to the wall\"]\n",
      "\n",
      "pl.fit(X_train, train['IsSafety'])\n",
      "\n",
      "#tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
      "#tfidf_test = tfidf_vectorizer.fit_transform(input_text)\n",
      "\n",
      "prediction = pl.predict(input_text)\n",
      "print(\"Predict if the comment is safety related\", prediction)\n",
      "\n",
      "\n",
      "\n",
      "model = Pipeline([\n",
      "    ('parser', HTMLParser()),\n",
      "    ('text_union', FeatureUnion(\n",
      "        transformer_list = [\n",
      "            ('entity_feature', Pipeline([\n",
      "                ('entity_extractor', EntityExtractor()),\n",
      "                ('entity_vect', CountVectorizer()),\n",
      "            ])),\n",
      "            ('keyphrase_feature', Pipeline([\n",
      "                ('keyphrase_extractor', KeyphraseExtractor()),\n",
      "                ('keyphrase_vect', TfidfVectorizer()),\n",
      "            ])),\n",
      "        ],\n",
      "        transformer_weights= {\n",
      "            'entity_feature': 0.6,\n",
      "            'keyphrase_feature': 0.2,\n",
      "        }\n",
      "    )),\n",
      "    ('clf', LogisticRegression()),\n",
      "])\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\nlp pipeline for multiple labels output.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\nlp pipeline for multiple labels output.txt\n",
      "budgets for school are complex\n",
      "\n",
      "are spending more on text books\n",
      "\n",
      "hundreds of hours each year are spent manually labeling\n",
      "\n",
      "expense line\n",
      "1. labels: Textbooks, Math, Middle School\n",
      "\n",
      "Supervised learning problem\n",
      "\n",
      "groupings of line items that go together\n",
      "\n",
      "100 target variables\n",
      "1. Expense for Pre kindergarden (different funding)\n",
      "2. student_type\n",
      "\n",
      "9 categories per column\n",
      "\n",
      "prioritizing time\n",
      "1. human in the loop (proability of likely)\n",
      "\n",
      "df.info()\n",
      "\n",
      "tells us the data types\n",
      "tells us the missing values\n",
      "\n",
      "df.describe()\n",
      "gives us summary statistics\n",
      "\n",
      "   Project\n",
      "\n",
      "df=pd.read_csv('TrainingData.csv',index_col=0)\n",
      "\n",
      "df.info()\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1560 entries, 198 to 101861\n",
      "Data columns (total 25 columns):\n",
      "Function                  1560 non-null object\n",
      "Use                       1560 non-null object\n",
      "Sharing                   1560 non-null object\n",
      "Reporting                 1560 non-null object\n",
      "Student_Type              1560 non-null object\n",
      "Position_Type             1560 non-null object\n",
      "Object_Type               1560 non-null object\n",
      "Pre_K                     1560 non-null object\n",
      "Operating_Status          1560 non-null object\n",
      "Object_Description        1461 non-null object\n",
      "Text_2                    382 non-null object\n",
      "SubFund_Description       1183 non-null object\n",
      "Job_Title_Description     1131 non-null object\n",
      "Text_3                    296 non-null object\n",
      "Text_4                    193 non-null object\n",
      "Sub_Object_Description    364 non-null object\n",
      "Location_Description      874 non-null object\n",
      "FTE                       449 non-null float64\n",
      "Function_Description      1340 non-null object\n",
      "Facility_or_Department    252 non-null object\n",
      "Position_Extra            1026 non-null object\n",
      "Total                     1542 non-null float64\n",
      "Program_Description       1192 non-null object\n",
      "Fund_Description          819 non-null object\n",
      "Text_1                    1132 non-null object\n",
      "dtypes: float64(2), object(23)\n",
      "\n",
      "\n",
      "df.describe()\n",
      "\n",
      "\n",
      "FET Total\n",
      "count  449.000000  1.542000e+03\n",
      "mean     0.493532  1.446867e+04\n",
      "std      0.452844  7.916752e+04\n",
      "min     -0.002369 -1.044084e+06\n",
      "25%      0.004310  1.108111e+02\n",
      "50%      0.440000  7.060299e+02\n",
      "75%      1.000000  5.347760e+03\n",
      "max      1.047222  1.367500e+06\n",
      "\n",
      "\n",
      "Job_Title_Description column. The values in this column tell us if a budget item is for a teacher, custodian, or other employee.\n",
      "\n",
      " For example, the Object_Type column describes whether the budget item is related classroom supplies, salary, travel expenses, etc. \n",
      "\n",
      "\n",
      "   project    build a histogram\n",
      "\n",
      "# Print the summary statistics\n",
      "print(df.describe())\n",
      "\n",
      "# Import matplotlib.pyplot as plt\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Create the histogram\n",
      "plt.hist(df['FTE'].dropna())\n",
      "\n",
      "# Add title and labels\n",
      "plt.title('Distribution of %full-time \\n employee works')\n",
      "plt.xlabel('% of full-time')\n",
      "plt.ylabel('num employees')\n",
      "\n",
      "# Display the histogram\n",
      "plt.show()\n",
      "\n",
      "FTE: Stands for \"full-time equivalent\". If the budget item is associated to an employee, this number tells us the percentage of full-time that the employee works. A value of 1 means the associated employee works for the school full-time. A value close to 0 means the item is associated to a part-time or contracted employee.\n",
      "\n",
      "Total: Stands for the total cost of the expenditure. This number tells us how much the budget item cost.\n",
      "\n",
      "There are some full time employees and some part time employees\n",
      "\n",
      "       Looking at the datatypes\n",
      "\n",
      "ml algorithms work on numbers, not strings\n",
      "\n",
      "strings are slow to compare versus numbers\n",
      "\n",
      "category encodes categorical data\n",
      "\n",
      "astype('category')\n",
      "\n",
      "sample_df=sample_df.label.astype('category')\n",
      "\n",
      "dummies = pd.get_dummies(sample[['label']],prefix_sep='_')\n",
      "\n",
      "also called a binary indicator representation\n",
      "\n",
      "lambda are simple online functions\n",
      "\n",
      "square = lambda x: x*x\n",
      "\n",
      "categorized_label = lambda x: x.astype('category')\n",
      "sample_df.label=sample_df[['label']].apply(categorize_label,axis=0)\n",
      "sample_df.info()\n",
      "\n",
      "\n",
      "  practice exploring datatypes\n",
      "\n",
      "df.dtypes.value_counts()\n",
      "\n",
      "LABELS: ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']\n",
      "\n",
      "# Define the lambda function: categorize_label\n",
      "categorize_label = lambda x: x.astype('category')\n",
      "\n",
      "print(LABELS)\n",
      "# Convert df[LABELS] to a categorical type\n",
      "df[LABELS] = df[LABELS].apply(categorize_label, axis=0)\n",
      "\n",
      "# Print the converted dtypes\n",
      "print(df[LABELS].dtypes)\n",
      "\n",
      "Function            category\n",
      "Use                 category\n",
      "Sharing             category\n",
      "Reporting           category\n",
      "Student_Type        category\n",
      "Position_Type       category\n",
      "Object_Type         category\n",
      "\n",
      "\n",
      "  > practice   lambda to find unique labels and then plot their frequency\n",
      "\n",
      "# Import matplotlib.pyplot\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Calculate number of unique values for each label: num_unique_labels\n",
      "num_unique_labels = df[LABELS].apply( lambda x: pd.Series.nunique(x))\n",
      "\n",
      "# Plot number of unique values for each label\n",
      "num_unique_labels.plot(kind='bar')\n",
      "\n",
      "# Label the axes\n",
      "plt.xlabel('Labels')\n",
      "plt.ylabel('Number of unique values')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "       >how do we measure success\n",
      "\n",
      "how accurate is your model\n",
      "\n",
      "legitimate email is 99% and spam 1%\n",
      "\n",
      "a model must be useful\n",
      "\n",
      "   Log loss\n",
      "\n",
      "log loss is the measurement of error\n",
      "we want to minimize error\n",
      "\n",
      "y=probability is between 0 and 1\n",
      "p=actual value is binary (1=yes, 0=no)\n",
      "\n",
      "\n",
      "logloss=-1/N sum(1-yi)log(1-pi)\n",
      "\n",
      "where i is each row\n",
      "\n",
      "The goal is to minimize logloss\n",
      "1. better to be less confident than confident and wrong\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def compute_log_loss(predicted, actual, eps(1e-14):\n",
      "\n",
      "\"\"\" computes the logarithmic loss between predicted and actual when these are 1D arrays\n",
      "\t:param predicted: The predicted probabilities as floats between 0-1\n",
      "\t:param actual: The actual binary labels. Either 0 or 1\n",
      "\t:param eps(optional) : log(0) is inf, so we need to offset our predicted values slightly by eps from 0 or 1\n",
      "\"\"\"\n",
      "\tpredicted=np.clip(predicted,eps,1-eps)\n",
      "\tloss=-1 * np.mean(actual*np.log(predicted)+(1-actual) * np.log(1-predicted))\n",
      "\treturn loss\n",
      "\n",
      "\n",
      "clip sets a maximum and minimum value of an array\n",
      "\n",
      "compute_log_loss(predicted=0.9, actual=0)\n",
      "2.3\n",
      "compute_log_loss(predicted=0.5, actual=1)\n",
      "0.69\n",
      "\n",
      "\n",
      "  practice\n",
      "\n",
      "# Compute and print log loss for 1st case\n",
      "correct_confident_loss = compute_log_loss(correct_confident, actual_labels)\n",
      "print(\"Log loss, correct and confident: {}\".format(correct_confident_loss)) \n",
      "\n",
      "# Compute log loss for 2nd case\n",
      "correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)\n",
      "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident_loss)) \n",
      "\n",
      "# Compute and print log loss for 3rd case\n",
      "wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)\n",
      "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident_loss)) \n",
      "\n",
      "# Compute and print log loss for 4th case\n",
      "wrong_confident_loss = compute_log_loss(wrong_confident,actual_labels)\n",
      "print(\"Log loss, wrong and confident: {}\".format(wrong_confident_loss)) \n",
      "\n",
      "# Compute and print log loss for actual labels\n",
      "actual_labels_loss = compute_log_loss(actual_labels, actual_labels)\n",
      "print(\"Log loss, actual labels: {}\".format(actual_labels_loss)) \n",
      "\n",
      "\n",
      "      >Start with a simple model\n",
      "how much signal you can pull out as quickly as possible\n",
      "\n",
      "train basic model on numeric data \n",
      "\n",
      "multi-class logistic regression\n",
      "\n",
      "StratifiedShuffleSplit\n",
      "only works with a single target variable\n",
      "\n",
      "multilabel_train_test_split()\n",
      "\n",
      "data_to_train=df[NUMERIC_COLUMNS].fillna(-1000)\n",
      "\n",
      "we want our model to respond to nan differently than real values\n",
      "\n",
      "\n",
      "X_train, X_test, y_train,y_test= multilabel_train_test_split(data_to_train, label_to_use, size=0.2, seed=123)\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "\n",
      "treats each column of y independently\n",
      "fits a separate classifere for of the columns\n",
      "\n",
      "clf=OneVsRestClassifier(LogisticRegression())\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "  sample  > multi class classification to create train and test sets\n",
      "\n",
      "print(df.columns)\n",
      "# Create the new DataFrame: numeric_data_only\n",
      "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
      "\n",
      "# Get labels and convert to dummy variables: label_dummies\n",
      "label_dummies = pd.get_dummies(df[LABELS])\n",
      "\n",
      "# Create training and test sets\n",
      "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
      "                                                               label_dummies,\n",
      "                                                               size=0.2, \n",
      "                                                               seed=123)\n",
      "\n",
      "# Print the info\n",
      "print(\"X_train info:\")\n",
      "print(X_train.info())\n",
      "print(\"\\nX_test info:\")  \n",
      "print(X_test.info())\n",
      "print(\"\\ny_train info:\")  \n",
      "print(y_train.info())\n",
      "print(\"\\ny_test info:\")  \n",
      "print(y_test.info()) \n",
      "\n",
      "output:\n",
      "y_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 520 entries, 209 to 448628\n",
      "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
      "dtypes: uint8(104)\n",
      "memory usage: 56.9 KB\n",
      "None\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "\n",
      "# Instantiate the classifier: clf\n",
      "clf=OneVsRestClassifier(LogisticRegression())\n",
      "\n",
      "# Fit the classifier to the training data\n",
      "\n",
      "clf.fit(X_train,y_train)\n",
      "# Print the accuracy\n",
      "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))\n",
      "\n",
      "\n",
      "       Making predictions\n",
      "\n",
      "predicting on holdout data\n",
      "\n",
      "holdout=pd.read_csv('HoldoutData.csv',index_col=0)\n",
      "holdout=holdout[NUMERIC_COLUMNS].fillna(-1000)\n",
      "\n",
      "predictions=clf.predict_proba(holdout)\n",
      "\n",
      "#calculate the probablity for each label\n",
      "#log loss penalize being confident and wrong\n",
      "#.predict would result in worst performance compared to .predict_proba()\n",
      "\n",
      "#submit your predictions as a csv\n",
      "\n",
      "#with to_csv function\n",
      "\n",
      "#proba needs to be convert into a data frame\n",
      "\n",
      "predictions_df=pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns,\n",
      "index=holdout.index,\n",
      "data=predictions)\n",
      "\n",
      "predictions_df.to_csv('predictions.csv')\n",
      "\n",
      "score=score_submission(pred_path='predictions.csv')\n",
      "\n",
      "\n",
      "#The point of the holdout data is to provide a fair test for machine learning competitions.\n",
      "\n",
      "  > sample  > creating predictions of probabilities of the features using holdout data\n",
      "\n",
      "# Instantiate the classifier: clf\n",
      "clf = OneVsRestClassifier(LogisticRegression())\n",
      "\n",
      "# Fit it to the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Load the holdout data: holdout\n",
      "holdout=pd.read_csv('HoldoutData.csv',index_col=0)\n",
      "holdout=holdout[NUMERIC_COLUMNS].fillna(-1000)\n",
      "\n",
      "print(holdout.columns)\n",
      "# Generate predictions: predictions\n",
      "predictions=clf.predict_proba(holdout)\n",
      "\n",
      "\n",
      "labels output:\n",
      "['Function',\n",
      " 'Use',\n",
      " 'Sharing',\n",
      " 'Reporting',\n",
      " 'Student_Type',\n",
      " 'Position_Type',\n",
      " 'Object_Type',\n",
      " 'Pre_K',\n",
      " 'Operating_Status']\n",
      "\n",
      "\n",
      "# Format predictions in DataFrame: prediction_df\n",
      "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
      "                             index=holdout.index,\n",
      "                             data=predictions)\n",
      "\n",
      "\n",
      "# Save prediction_df to csv\n",
      "predictions_df.to_csv('predictions.csv')\n",
      "\n",
      "# Submit the predictions for scoring: score\n",
      "score = score_submission(pred_path='predictions.csv')\n",
      "\n",
      "# Print score\n",
      "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))\n",
      "\n",
      "\n",
      "Your model, trained with numeric data only, yields logloss score: 1.9067227623381413\n",
      "\n",
      "       Natural language processing\n",
      "\n",
      "basic techniques for processing text data\n",
      "\n",
      "Tokenization is splitting a long string into segments\n",
      "store segments as lists\n",
      "\n",
      "['natural','language','processing']\n",
      "\n",
      "bag of word representation\n",
      "1. count the number of times a particular token appears\n",
      "\n",
      "bag of words\n",
      "\n",
      "1-gram\n",
      "petro vend fuel and fluids\n",
      "\n",
      "2-gram\n",
      "petro vend\n",
      "vend fuel\n",
      "fuel and\n",
      "and fluids\n",
      "\n",
      "3-gram\n",
      "petro vend fuel\n",
      "vend fuel and\n",
      "fuel and fluids\n",
      "\n",
      "   > representing text numerically\n",
      "\n",
      "represent text numerically\n",
      "\n",
      "text as bag of words\n",
      "\n",
      "CountVectorizer() tokenizes all the strings\n",
      "builds a volcabulary\n",
      "\n",
      "1. Converting text into vectors\n",
      "2. bag of words model is extracting word tokens\n",
      "a. computing the frequency of word tokens\n",
      "b. computing a word vector out of these frequencies and volcabulary of corpus\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "TOKEN_BASICS='\\\\\\\\s+(?=\\\\\\\\s+)'\n",
      "df.Program_Description.fillna('',inplace=True)\n",
      "\n",
      "vec_basic=CountVectorizer(token_pattern=TOKEN_BASICS)\n",
      "\n",
      "vec_basic.fit(df.Program_Description)\n",
      "\n",
      "\n",
      "print(vec_basics.get_feature_names())\n",
      "\n",
      "\n",
      "     >Representing text numerically\n",
      "\n",
      "\n",
      "bag of words counts occurrences\n",
      "\n",
      "CountVectorizer()\n",
      "\n",
      "\n",
      "\n",
      "  Practice  > tokenizing position_extra\n",
      "\n",
      "# Import CountVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
      "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
      "\n",
      "# Fill missing values in df.Position_Extra\n",
      "df.Position_Extra.fillna('',inplace=True)\n",
      "\n",
      "# Instantiate the CountVectorizer: vec_alphanumeric\n",
      "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
      "\n",
      "# Fit to the data\n",
      "vec_alphanumeric.fit(df.Position_Extra)\n",
      "\n",
      "# Print the number of tokens and first 15 tokens\n",
      "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
      "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
      "print(vec_alphanumeric.get_feature_names()[:15])\n",
      "\n",
      "output: ['1st', '2nd', '3rd', 'a', 'ab', 'additional', 'adm', 'administrative', 'and', 'any', 'art', 'assessment', 'assistant', 'asst', 'athletic']\n",
      "\n",
      "# Define combine_text_columns()\n",
      "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
      "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
      "    \n",
      "    # Drop non-text columns that are in the df\n",
      "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
      "    text_data =data_frame.drop(to_drop,axis=1)\n",
      "    \n",
      "    # Replace nans with blanks\n",
      "    text_data.fillna(\"\",inplace=True)\n",
      "    \n",
      "    # Join all text items in a row that have a space in between\n",
      "    return text_data.apply(lambda x: \" \".join(x), axis=1)\n",
      "\n",
      "\n",
      "  >combine text columns and tokenizing using regular expressions\n",
      "\n",
      "# Import the CountVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Create the basic token pattern\n",
      "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
      "\n",
      "# Create the alphanumeric token pattern\n",
      "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
      "\n",
      "# Instantiate basic CountVectorizer: vec_basic\n",
      "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
      "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
      "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
      "\n",
      "# Create the text vector\n",
      "text_vector = combine_text_columns(df)\n",
      "\n",
      "# Fit and transform vec_basic\n",
      "vec_basic.fit_transform(text_vector)\n",
      "\n",
      "# Print number of tokens of vec_basic\n",
      "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
      "\n",
      "# Fit and transform vec_alphanumeric\n",
      "vec_alphanumeric.fit_transform(text_vector)\n",
      "\n",
      "# Print number of tokens of vec_alphanumeric\n",
      "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))\n",
      "\n",
      "  >Pipelines, features \n",
      "1. a pipelineis a repeatable way to go from raw data to trained model\n",
      "\n",
      "2. pipeline object takes sequential list of steps where the output of one step is teh input to the next step.\n",
      "\n",
      "3. each step is a tuple with two elements\n",
      "tranform: obj implementing .fit() and .transform()\n",
      "\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "\n",
      "pl=Pipeline([\n",
      "\t(\n",
      "'imp', Imputer()),\n",
      "'clf', OneVsRestClassifier(LogisticRegression())\n",
      ")\n",
      "])\n",
      "\n",
      "\n",
      "we pass the pipeline a series of named steps\n",
      "\n",
      "sample data:\n",
      "label, numeric, text, with_missing\n",
      "\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test= train_test_split(\n",
      "\tsample_df[['numeric','with_missing']],\n",
      "\tpd.get_dummies(sample_df['label']),\n",
      "\trandom_state=2)\n",
      "\n",
      "pl.fit(X_train, y_train)\n",
      "\n",
      "accuracy = pl.score(X_test,y_test)\n",
      "\n",
      "print('accuracy on numeric data, no nans',accuracy)\n",
      "\n",
      "   Practice   > build the pipeline\n",
      "\n",
      "# Import Pipeline\n",
      "from sklearn.pipeline import Pipeline\n",
      "# Import other necessary modules\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "\n",
      "# Split and select numeric data only, no nans \n",
      "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],\n",
      "                                                    pd.get_dummies(sample_df['label']), \n",
      "                                                    random_state=22)\n",
      "\n",
      "# Instantiate Pipeline object: pl\n",
      "pl = Pipeline([\n",
      "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
      "    ])\n",
      "\n",
      "# Fit the pipeline to the training data\n",
      "pl.fit(X_train, y_train)\n",
      "\n",
      "# Compute and print accuracy\n",
      "accuracy = pl.score(X_test, y_test)\n",
      "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)\n",
      "\n",
      "   practice    > add the imputer\n",
      "\n",
      "# Import the Imputer object\n",
      "from sklearn.preprocessing import Imputer\n",
      "\n",
      "# Create training and test sets using only numeric data\n",
      "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],\n",
      "                                                    pd.get_dummies(sample_df['label']), \n",
      "                                                    random_state=456)\n",
      "\n",
      "# Insantiate Pipeline object: pl\n",
      "pl = Pipeline([\n",
      "    \n",
      "        ('imp', Imputer()),\n",
      "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
      "    ])\n",
      "\n",
      "# Fit the pipeline to the training data\n",
      "pl.fit(X_train,y_train)\n",
      "\n",
      "# Compute and print accuracy\n",
      "accuracy =pl.score(X_test,y_test)\n",
      "print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)\n",
      "\n",
      "     Adding text features to the pipeline\n",
      "\n",
      "\n",
      "pl=Pipeline([\n",
      "\t('vec', CountVectorizer()),\n",
      "\t('clf', OneVsRestClassifier(LogisticRegression()))\n",
      "])\n",
      "\n",
      "\n",
      "Preprocessing multiple dtypes\n",
      "\n",
      "want to use all available features in one pipeline\n",
      "\n",
      "problem: pipeline steps for numeric and text preprocessing can't follow each other\n",
      "\n",
      "the output of CountVectorizer can't be input to imputer\n",
      "\n",
      "Solution: FunctionTransformer() and FeatureUnion()\n",
      "\n",
      "FunctionTransformer turns a python function into an object that scikit-learn pipeline can understand\n",
      "\n",
      "two functions for the pipeline preprocessing\n",
      "1. function 1 return numeric columns\n",
      "2. function 2 return text columns\n",
      "\n",
      "can then preprocess numeric and text data in separate pipelines\n",
      "\n",
      "from sklearn.preprocessing import FunctionTransformer\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "\n",
      "\n",
      "get_text_data=FunctionTransformer(lambda x: x['text'],validate=False)\n",
      "\n",
      "get_numeric_data=FunctionTransformer(lambda x: x[['numeric','with_missing']],validate=False\n",
      "\n",
      "validate=False means don't check for nans\n",
      "\n",
      "union=FeatureUnion([\n",
      "  \t('numeric',numeric_pipeline),\n",
      "\t('text',text_pipeline)\n",
      "]\n",
      "\n",
      "      Entire pipeline\n",
      "\n",
      "numeric_pipeline=Pipeline([\n",
      "    ('selector',get_numeric_data),\n",
      "    ('imputer',SimpleImputer())\n",
      "    \n",
      "])\n",
      "\n",
      "text_pipeline=Pipeline([\n",
      "    ('selector',get_text_data),\n",
      "    ('vectorizer',CountVectorizer())\n",
      "])\n",
      "\n",
      "pl = Pipeline([\n",
      "        ('union',FeatureUnion([\n",
      "            ('numeric',numeric_pipeline),\n",
      "            ('text',text_pipeline)\n",
      "        ])\n",
      "        )\n",
      "        ,\n",
      "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
      "    ])\n",
      "\n",
      "\n",
      " >Practice  <<<< vectorize text and dummies\n",
      "\n",
      "\n",
      "# Import the CountVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "print(sample_df['text'])\n",
      "# Split out only the text data\n",
      "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],pd.get_dummies(sample_df['label']), \n",
      "                                                    random_state=456)\n",
      "\n",
      "# Instantiate Pipeline object: pl\n",
      "pl = Pipeline([\n",
      "        ('vec', CountVectorizer()),\n",
      "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
      "    ])\n",
      "\n",
      "# Fit to the training data\n",
      "pl.fit(X_train,y_train)\n",
      "\n",
      "# Compute and print accuracy\n",
      "accuracy = pl.score(X_test,y_test)\n",
      "print(\"\\nAccuracy on sample data - just text data: \", accuracy)\n",
      "\n",
      "\n",
      "  Practice  > creating two pipelines using Function Transformer\n",
      "\n",
      "# Import FunctionTransformer\n",
      "from sklearn.preprocessing import FunctionTransformer\n",
      "\n",
      "# Obtain the text data: get_text_data\n",
      "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
      "\n",
      "# Obtain the numeric data: get_numeric_data\n",
      "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
      "\n",
      "# Fit and transform the text data: just_text_data\n",
      "just_text_data = get_text_data.fit_transform(sample_df)\n",
      "\n",
      "# Fit and transform the numeric data: just_numeric_data\n",
      "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
      "\n",
      "# Print head to check results\n",
      "print('Text Data')\n",
      "print(just_text_data.head())\n",
      "print('\\nNumeric Data')\n",
      "print(just_numeric_data.head())\n",
      "\n",
      "\n",
      "  > Practice  > create the union\n",
      "\n",
      "# Import FeatureUnion\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "\n",
      "# Split using ALL data in sample_df\n",
      "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],\n",
      "                                                    pd.get_dummies(sample_df['label']), \n",
      "                                                    random_state=22)\n",
      "\n",
      "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
      "process_and_join_features = FeatureUnion(\n",
      "            transformer_list = [\n",
      "                ('numeric_features', Pipeline([\n",
      "                    ('selector', get_numeric_data),\n",
      "                    ('imputer', Imputer())\n",
      "                ])),\n",
      "                ('text_features', Pipeline([\n",
      "                    ('selector', get_text_data),\n",
      "                    ('vectorizer', CountVectorizer())\n",
      "                ]))\n",
      "             ]\n",
      "        )\n",
      "\n",
      "# Instantiate nested pipeline: pl\n",
      "pl = Pipeline([\n",
      "        ('union', process_and_join_features),\n",
      "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
      "    ])\n",
      "\n",
      "\n",
      "# Fit pl to the training data\n",
      "pl.fit(X_train, y_train)\n",
      "\n",
      "# Compute and print accuracy\n",
      "accuracy = pl.score(X_test, y_test)\n",
      "print(\"\\nAccuracy on sample data - all data: \", accuracy)\n",
      "\n",
      "\n",
      "         Choosing a classification model\n",
      "\n",
      "\n",
      "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
      "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
      "    \n",
      "    # Drop non-text columns that are in the df\n",
      "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
      "    text_data =data_frame.drop(to_drop,axis=1)\n",
      "    \n",
      "    # Replace nans with blanks\n",
      "    text_data.fillna(\"\",inplace=True)\n",
      "    \n",
      "    # Join all text items in a row that have a space in between\n",
      "    return text_data.apply(lambda x: \" \".join(x), axis=1)\n",
      "\n",
      "# Obtain the text data: get_text_data\n",
      "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
      "\n",
      "# Obtain the numeric data: get_numeric_data\n",
      "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
      "\n",
      "\n",
      "\n",
      " >change the model\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "pl = Pipeline([\n",
      "        ('union', process_and_join_features),\n",
      "        ('clf', OneVsRest(RandomForestClassifier()))\n",
      "    ])\n",
      "\n",
      "\n",
      "   practice  > setup the pipeline and add the combine text into one column function\n",
      "\n",
      "# Import FunctionTransformer\n",
      "from sklearn.preprocessing import FunctionTransformer\n",
      "\n",
      "# Get the dummy encoding of the labels\n",
      "dummy_labels = pd.get_dummies(df[LABELS])\n",
      "\n",
      "# Get the columns that are features in the original df\n",
      "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
      "\n",
      "# Split into training and test sets\n",
      "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
      "                                                               dummy_labels,\n",
      "                                                               0.2, \n",
      "                                                               seed=123)\n",
      "\n",
      "# Preprocess the text data: get_text_data\n",
      "get_text_data = FunctionTransformer(combine_text_columns,validate=False)\n",
      "\n",
      "# Preprocess the numeric data: get_numeric_data\n",
      "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
      "\n",
      " > create the pipeline\n",
      "\n",
      "# Complete the pipeline: pl\n",
      "pl = Pipeline([\n",
      "        ('union', FeatureUnion(\n",
      "            transformer_list = [\n",
      "                ('numeric_features', Pipeline([\n",
      "                    ('selector', get_numeric_data),\n",
      "                    ('imputer', Imputer())\n",
      "                ])),\n",
      "                ('text_features', Pipeline([\n",
      "                    ('selector', get_text_data),\n",
      "                    ('vectorizer', CountVectorizer())\n",
      "                ]))\n",
      "             ]\n",
      "        )),\n",
      "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
      "    ])\n",
      "\n",
      "# Fit to the training data\n",
      "pl.fit(X_train,y_train)\n",
      "\n",
      "# Compute and print accuracy\n",
      "accuracy = pl.score(X_test, y_test)\n",
      "print(\"\\nAccuracy on budget dataset: \", accuracy)\n",
      "\n",
      "  switch to random forest\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Import random forest classifer\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Edit model step in pipeline\n",
      "pl = Pipeline([\n",
      "        ('union', FeatureUnion(\n",
      "            transformer_list = [\n",
      "                ('numeric_features', Pipeline([\n",
      "                    ('selector', get_numeric_data),\n",
      "                    ('imputer', Imputer())\n",
      "                ])),\n",
      "                ('text_features', Pipeline([\n",
      "                    ('selector', get_text_data),\n",
      "                    ('vectorizer', CountVectorizer())\n",
      "                ]))\n",
      "             ]\n",
      "        )),\n",
      "        ('clf', RandomForestClassifier(n_estimators=15))\n",
      "    ])\n",
      "\n",
      "# Fit to the training data\n",
      "pl.fit(X_train, y_train)\n",
      "\n",
      "# Compute and print accuracy\n",
      "accuracy = pl.score(X_test, y_test)\n",
      "print(\"\\nAccuracy on budget dataset: \", accuracy)\n",
      "\n",
      "\n",
      "  Learning from an expert\n",
      "\n",
      "\n",
      "1. Text processing\n",
      "2. statistical methods\n",
      "3. computational efficiency\n",
      "\n",
      "text preprocessing\n",
      "Tokenize on punctuation to avoid hyphens, underscore, etc\n",
      "\n",
      "include unigrams and bi-grams in the model to capture important information in the text.\n",
      "\n",
      "vec = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
      "ngram_range=(1,2))\n",
      "\n",
      "accept alphanumeric tokens and use one-grams and bi-grams in the tokens\n",
      "\n",
      "\n",
      "  > getting predictions\n",
      "\n",
      "holdout= pd.read_csv('HoldOutData.csv', index_col=0)\n",
      "\n",
      "predictions=pl.predict_proba(holdout)\n",
      "prediction_df=pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns, index=holdout.index, data=predictions)\n",
      "\n",
      "prediction_df.to_csv('predictions.csv')\n",
      "score=score_submission(pred_path='predictions.csv')\n",
      "\n",
      "\n",
      "     >Practice    combine text columns,  tokenize only alphanumeric values\n",
      "\n",
      "# Import the CountVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Create the text vector\n",
      "text_vector = combine_text_columns(X_train)\n",
      "\n",
      "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
      "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
      "\n",
      "# Instantiate the CountVectorizer: text_features\n",
      "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
      "\n",
      "# Fit text_features to the text vector\n",
      "text_features.fit(text_vector)\n",
      "\n",
      "# Print the first 10 tokens\n",
      "print(text_features.get_feature_names()[:10])\n",
      "\n",
      "\n",
      "   setup the pipeline   > countvectorizer with n-grams\n",
      "\n",
      "# Import pipeline\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "# Import classifiers\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "\n",
      "# Import CountVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Import other preprocessing modules\n",
      "from sklearn.preprocessing import Imputer\n",
      "from sklearn.feature_selection import chi2, SelectKBest\n",
      "\n",
      "# Select 300 best features\n",
      "chi_k = 300\n",
      "\n",
      "# Import functional utilities\n",
      "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "\n",
      "# Perform preprocessing\n",
      "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
      "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
      "\n",
      "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
      "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
      "\n",
      "# Instantiate pipeline: pl\n",
      "pl = Pipeline([\n",
      "        ('union', FeatureUnion(\n",
      "            transformer_list = [\n",
      "                ('numeric_features', Pipeline([\n",
      "                    ('selector', get_numeric_data),\n",
      "                    ('imputer', Imputer())\n",
      "                ])),\n",
      "                ('text_features', Pipeline([\n",
      "                    ('selector', get_text_data),\n",
      "                    ('vectorizer', CountVectorizer   \n",
      "                    (token_pattern=TOKENS_ALPHANUMERIC,\n",
      "                    ngram_range=(1,2))),\n",
      "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
      "                ]))\n",
      "             ]\n",
      "        )),\n",
      "        ('scale', MaxAbsScaler()),\n",
      "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
      "    ])\n",
      "\n",
      "    Statistical tools\n",
      "\n",
      "interaction terms\n",
      "\n",
      "B1X1+B2X2+B3(x1*x2)\n",
      "\n",
      "\n",
      "beta are the coeffients\n",
      "X are 0 or 1 in the row for a column\n",
      "\n",
      "\n",
      "X1 and X2 are 0 or 1\n",
      "X3 is X1*X2 and 1 if X1 and X2 are in the row\n",
      "\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "\n",
      "interaction=PolynomialFeatures(degree=2,\n",
      "interaction_only=True,\n",
      "include_bias=False)\n",
      "\n",
      "degree=2 multiples two columns together\n",
      "bias term is an offset for a model\n",
      "bias term allows the model to have a non-zero y value when x value is zero\n",
      "\n",
      "SparseInteractions(degree=2).fit_transform(x).toarray()\n",
      "\n",
      "PolynomialFeatures does not support sparse matrices\n",
      "\n",
      "SparseInteractions work the the sparse matrices\n",
      "\n",
      "  practice   > implement sparse interactions\n",
      "\n",
      "# Instantiate pipeline: pl\n",
      "pl = Pipeline([\n",
      "        ('union', FeatureUnion(\n",
      "            transformer_list = [\n",
      "                ('numeric_features', Pipeline([\n",
      "                    ('selector', get_numeric_data),\n",
      "                    ('imputer', Imputer())\n",
      "                ])),\n",
      "                ('text_features', Pipeline([\n",
      "                    ('selector', get_text_data),\n",
      "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
      "                                                   ngram_range=(1, 2))),  \n",
      "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
      "                ]))\n",
      "             ]\n",
      "        )),\n",
      "        ('int', SparseInteractions(degree=2)),\n",
      "        ('scale', MaxAbsScaler()),\n",
      "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
      "    ])\n",
      "\n",
      "\n",
      "#https://gist.github.com/pjbull/063a9b4e4f9cfcc4d03cba18fee63de7\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from scipy import sparse\n",
      "from itertools import combinations\n",
      "\n",
      "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
      "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
      "        self.degree = degree\n",
      "        self.feature_name_separator = feature_name_separator\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "    def transform(self, X):\n",
      "        if not sparse.isspmatrix_csc(X):\n",
      "            X = sparse.csc_matrix(X)\n",
      "\n",
      "        if hasattr(X, \"columns\"):\n",
      "            self.orig_col_names = X.columns\n",
      "        else:\n",
      "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
      "          \n",
      "        spi = self._create_sparse_interactions(X)\n",
      "        return spi\n",
      "\n",
      "    def get_feature_names(self):\n",
      "        return self.feature_names\n",
      "\n",
      "    def _create_sparse_interactions(self, X):\n",
      "        out_mat = []\n",
      "        self.feature_names = self.orig_col_names.tolist()\n",
      "\n",
      "        for sub_degree in range(2, self.degree + 1):\n",
      "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
      "                # add name for new column\n",
      "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
      "                self.feature_names.append(name)\n",
      "                # get column multiplications value\n",
      "                out = X[:, col_ixs[0]]    \n",
      "                for j in col_ixs[1:]:\n",
      "                    out = out.multiply(X[:, j])\n",
      "                out_mat.append(out)\n",
      "        return sparse.hstack([X] + out_mat)\n",
      "\n",
      "        Leaning from the expert\n",
      "\n",
      "as the array grows we need more computational power\n",
      "\n",
      "hashing is a way of increasing memory efficiency\n",
      "\n",
      "\n",
      "hashing limits the size of the matrice\n",
      "\n",
      "a hash out may be a integer\n",
      "\n",
      "the hashing tokenizer maps the token to the value\n",
      "\n",
      "dimensionality reduction\n",
      "\n",
      "instead of using the countvectorizer for tokenizing words we change to the hashingvectorizer\n",
      "\n",
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "\n",
      "vec=HashingVectorizer(norm=None,\n",
      "\tnon_negative=True,\n",
      "\ttoken_pattern=TOKENS_ALPHANUMERIC,\n",
      "\tngram_range=(1,2))\n",
      "\n",
      "\n",
      "   >practice using HashingVectorizer instead of countVectorizer\n",
      "\n",
      "# Import HashingVectorizer\n",
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "\n",
      "# Get text data: text_data\n",
      "text_data = combine_text_columns(X_train)\n",
      "\n",
      "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
      "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
      "\n",
      "# Instantiate the HashingVectorizer: hashing_vec\n",
      "hashing_vec = HashingVectorizer(norm=None,\n",
      "\tnon_negative=True,\n",
      "\ttoken_pattern=TOKENS_ALPHANUMERIC,\n",
      "\tngram_range=(1,2))\n",
      "# Fit and transform the Hashing Vectorizer\n",
      "hashed_text = hashing_vec.fit_transform(text_data)\n",
      "\n",
      "# Create DataFrame and print the head\n",
      "hashed_df = pd.DataFrame(hashed_text.data)\n",
      "print(hashed_df.head())\n",
      "\n",
      "   practice building the winning pipeline\n",
      "\n",
      "# Import the hashing vectorizer\n",
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "\n",
      "# Instantiate the winning model pipeline: pl\n",
      "pl = Pipeline([\n",
      "        ('union', FeatureUnion(\n",
      "            transformer_list = [\n",
      "                ('numeric_features', Pipeline([\n",
      "                    ('selector', get_numeric_data),\n",
      "                    ('imputer', Imputer())\n",
      "                ])),\n",
      "                ('text_features', Pipeline([\n",
      "                    ('selector', get_text_data),\n",
      "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
      "                                                     non_negative=True, norm=None, binary=False,\n",
      "                                                     ngram_range=(1,2))),\n",
      "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
      "                ]))\n",
      "             ]\n",
      "        )),\n",
      "        ('int', SparseInteractions(degree=2)),\n",
      "        ('scale', MaxAbsScaler()),\n",
      "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
      "    ])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\Non-negative matrix factorization (NMF).txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\Non-negative matrix factorization (NMF).txt\n",
      " Non-negative matrix factorization (NMF)\n",
      "1. Dimension reduction technique\n",
      "2. NMF models are interpretable unlike PCA\n",
      "3. All the sample features must be non-ngeative >=0\n",
      "4. The desired number of components must always be specified\n",
      "5. Works with numpy arrays and with csr_matrix (sparse arrays)\n",
      "\n",
      "sample >\n",
      "\n",
      "1. word frequency array, 4 words, many documents\n",
      "2. measure presence of words in each document using tf-idf where tf=frequency of word in document.  the measurement is percentage of words in the document.\n",
      "\n",
      "tf=frequency of word in document\n",
      "idf=reduces influence of frequent words\n",
      "\n",
      "3. The dimension of components is equal to the number of dimensions in the samples\n",
      "4. nmf feature values are non-negative\n",
      "5. the features and the components can be combined to get the original data sample.\n",
      "\n",
      "\n",
      "from sklearn.decomposition import NMF\n",
      "model= NMF(n_components=2)\n",
      "\n",
      "model.fit(samples)\n",
      "\n",
      "nmf_features=model.transform(samples)\n",
      "\n",
      "if we multiple the nmf_features by the model.components_ we reconstruct the sample and adding up\n",
      "\n",
      "The matrix factorization of NMF is the reconstruction of the original sample\n",
      "\n",
      "images encoded as arrays\n",
      "audio spectrograms\n",
      "purchase histories on e-commerce sites\n",
      "\n",
      "\n",
      "  >sample\n",
      "\n",
      "from sklearn.decomposition import NMF\n",
      "\n",
      "# Create an NMF instance: model\n",
      "model = NMF(n_components=6)\n",
      "\n",
      "# Fit the model to articles\n",
      "model.fit(articles)\n",
      "\n",
      "# Transform the articles: nmf_features\n",
      "nmf_features = model.transform(articles)\n",
      "\n",
      "\n",
      "# Print the NMF features\n",
      "print(nmf_features)\n",
      "\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Create a pandas DataFrame: df\n",
      "df = pd.DataFrame(nmf_features, index=titles)\n",
      "\n",
      "# Print the row for 'Anne Hathaway'\n",
      "print(df.loc['Anne Hathaway'])\n",
      "\n",
      "# Print the row for 'Denzel Washington'\n",
      "print(df.loc['Denzel Washington'])\n",
      "\n",
      "\n",
      " NMF interpretable parts\n",
      "\n",
      "1. NMF represent patterns\n",
      "\n",
      "word-frequency array articles(tf-idf)\n",
      "a. 20,000 articles (rows)\n",
      "b. 800 words columns\n",
      "\n",
      "from sklearn.decomposition import NMF\n",
      "nmf= NMF(n_components=10)\n",
      "nmf.fit(articles)\n",
      "\n",
      "#shape is 10 rows with 800 columns in 2d numpy array\n",
      "\n",
      "1. nmf components represent topics\n",
      "2. nmf features combine topics into documents\n",
      "\n",
      "for images, nmf components are parts of the image\n",
      "8x8 is an array with values of 0 to 1\n",
      "\n",
      "collection of images of the same size\n",
      "a. encode as 2D array\n",
      "b. each row corresponds to an image\n",
      "c. each column corresponds to a pixel\n",
      "\n",
      "bitmap = sample.reshape((2,3))\n",
      "print(bitmap)\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Create a DataFrame: components_df\n",
      "components_df = pd.DataFrame(model.components_,columns=words)\n",
      "\n",
      "# Print the shape of the DataFrame\n",
      "print(components_df.shape)\n",
      "\n",
      "# Select row 3: component\n",
      "component = components_df.iloc[3]\n",
      "\n",
      "# Print result of nlargest\n",
      "print(component.nlargest())\n",
      "\n",
      "\n",
      " sample digit\n",
      "\n",
      "# Import pyplot\n",
      "from matplotlib import pyplot as pltim\n",
      "\n",
      "# Select the 0th row: digit\n",
      "digit = samples[0,:]\n",
      "\n",
      "# Print digit\n",
      "print(digit)\n",
      "\n",
      "# Reshape digit to a 13x8 array: bitmap\n",
      "bitmap = digit.reshape(13,8)\n",
      "\n",
      "# Print bitmap\n",
      "print(bitmap)\n",
      "\n",
      "# Use plt.imshow to display bitmap\n",
      "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
      "plt.colorbar()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Import NMF\n",
      "from sklearn.decomposition import NMF\n",
      "\n",
      "# Create an NMF model: model\n",
      "model = NMF(n_components=7)\n",
      "\n",
      "# Apply fit_transform to samples: features\n",
      "features = model.fit_transform(samples)\n",
      "\n",
      "# Call show_as_image on each component\n",
      "for component in model.components_:\n",
      "    show_as_image(component)\n",
      "\n",
      "# Assign the 0th row of features: digit_features\n",
      "digit_features = features[7,:]\n",
      "\n",
      "# Print digit_features\n",
      "print(digit_features)\n",
      "\n",
      " >Sample import using PCA\n",
      "\n",
      "# Import PCA\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# Create a PCA instance: model\n",
      "model = PCA(n_components=7)\n",
      "\n",
      "# Apply fit_transform to samples: features\n",
      "features = model.fit_transform(samples)\n",
      "\n",
      "# Call show_as_image on each component\n",
      "for component in model.components_:\n",
      "    show_as_image(component)\n",
      "\n",
      " Building recommender systems using nmf\n",
      "\n",
      "1. recommend articles with similar articles\n",
      "2. apply nmf to the word frequency array\n",
      "3. compare two articles with the NMF feature values\n",
      "\n",
      "\n",
      "\n",
      "from sklearn.decomposition import NMF\n",
      "nmf=NMF(n_components=6)\n",
      "nmf_features = nmf.fit_transform(articles)\n",
      "\n",
      "versions of articles\n",
      "1. similar articels have similar topics\n",
      "\n",
      "compare the cosine similarity\n",
      "1. use the angle between the lines\n",
      "2. higher values indicate a higher similarity\n",
      "\n",
      "from sklearn.preprocessing import normalize\n",
      "\n",
      "norm_features = normalize(nmf_features)\n",
      "\n",
      "current_article = norm_features[23,:]\n",
      "\n",
      "similarities=norm_features.dot(current_article)\n",
      "\n",
      "print(similarities)\n",
      "\n",
      "df = pd.DataFrame(norm_features, index=titles)\n",
      "\n",
      "current_article= df.loc['Dog bites man']\n",
      "\n",
      "simiarities = df.dot(current_article)\n",
      "\n",
      "print(similarities.nlargest())\n",
      "\n",
      "\n",
      " >Sample\n",
      "\n",
      "# Perform the necessary imports\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import normalize\n",
      "\n",
      "# Normalize the NMF features: norm_features\n",
      "norm_features = normalize(nmf_features)\n",
      "\n",
      "# Create a DataFrame: df\n",
      "df = pd.DataFrame(norm_features,index=titles)\n",
      "\n",
      "# Select the row corresponding to 'Cristiano Ronaldo': article\n",
      "article = df.loc['Cristiano Ronaldo']\n",
      "\n",
      "# Compute the dot products: similarities\n",
      "similarities = df.dot(article)\n",
      "\n",
      "# Display those with the largest cosine similarity\n",
      "print(similarities.nlargest())\n",
      "\n",
      "\n",
      " Sample\n",
      "\n",
      "In this exercise and the next, you'll use what you've learned about NMF to recommend popular music artists! You are given a sparse array artists whose rows correspond to artists and whose columns correspond to users. The entries give the number of times each artist was listened to by each user.\n",
      "\n",
      "\n",
      "artists\n",
      "\n",
      "(0, 2)\t105.0\n",
      "  (0, 15)\t165.0\n",
      "  (0, 20)\t91.0\n",
      "  (0, 21)\t98.0\n",
      "  (0, 29)\t120.0\n",
      "  (0, 48)\t236.0\n",
      "  (0, 70)\t67.0\n",
      "\n",
      "# Perform the necessary imports\n",
      "from sklearn.decomposition import NMF\n",
      "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "# Create a MaxAbsScaler: scaler\n",
      "scaler = MaxAbsScaler()\n",
      "\n",
      "# Create an NMF model: nmf\n",
      "nmf = NMF(n_components=20)\n",
      "\n",
      "# Create a Normalizer: normalizer\n",
      "normalizer = Normalizer()\n",
      "\n",
      "# Create a pipeline: pipeline\n",
      "pipeline = make_pipeline(scaler,nmf,normalizer)\n",
      "\n",
      "# Apply fit_transform to artists: norm_features\n",
      "norm_features = pipeline.fit_transform(artists)\n",
      "\n",
      "print(artists)\n",
      "\n",
      " >Sample\n",
      "\n",
      "uppose you were a big fan of Bruce Springsteen - which other musicial artists might you like? Use your NMF features from the previous exercise and the cosine similarity to find similar musical artists. A solution to the previous exercise has been run, so norm_features is an array containing the normalized NMF features as rows. The names of the musical artists are available as the list artist_names.\n",
      "\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Create a DataFrame: df\n",
      "df = pd.DataFrame(norm_features,index=artist_names)\n",
      "\n",
      "# Select row of 'Bruce Springsteen': artist\n",
      "artist = df.loc['Bruce Springsteen']\n",
      "\n",
      "# Compute cosine similarities: similarities\n",
      "similarities = df.dot(artist)\n",
      "\n",
      "# Display those with highest cosine similarity\n",
      "print(similarities.nlargest())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pandas time series.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pandas time series.txt\n",
      "How to plot data without a date\n",
      "\n",
      "data={'key':[0,1,2,3],'data_values':[100,150,400,200]}\n",
      "data=pd.DataFrame(data)\n",
      "data2={'key':[0,1,2,3],'data_values':[45,98,200,300]}\n",
      "data2=pd.DataFrame(data2)\n",
      "data.set_index('key')\n",
      "data2.set_index('key')\n",
      "print(data)\n",
      "\n",
      "fig, axs = plt.subplots(2,1, figsize=(5,10))\n",
      "data.iloc[:,1].plot(y='data_values',ax=axs[0])\n",
      "data2.iloc[:,1].plot(y='data_values',ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "\n",
      " adding an x time stamp\n",
      "\n",
      "data={'time':['2020-04-01','2020-04-02','2020-04-03','2020-04-04'],'data_values':[100,150,400,200]}\n",
      "data=pd.DataFrame(data)\n",
      "data2={'time':['2020-04-01','2020-04-02','2020-04-03','2020-04-04'],'data_values':[45,98,200,300]}\n",
      "\n",
      "data2=pd.DataFrame(data2)\n",
      "data.set_index('time')\n",
      "data2.set_index('time')\n",
      "print(data)\n",
      "\n",
      "fig, axs = plt.subplots(2,1, figsize=(5,10))\n",
      "data.plot(x='time',y='data_values',ax=axs[0])\n",
      "data2.plot(x='time', y='data_values',ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "\n",
      " linear regression\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "# Prepare input and output DataFrames\n",
      "X = boston[['AGE']]\n",
      "y = boston[['RM']]\n",
      "\n",
      "# Fit the model\n",
      "model = linear_model.LinearRegression()\n",
      "model.fit(X, y)\n",
      "\n",
      "print(new_inputs.reshape(-1,1))\n",
      "predictions = model.predict(new_inputs.reshape(-1,1))\n",
      "\n",
      "# Visualize the inputs and predicted values\n",
      "plt.scatter(new_inputs, predictions, color='r', s=3)\n",
      "plt.xlabel('inputs')\n",
      "plt.ylabel('predictions')\n",
      "plt.show()\n",
      "\n",
      "\n",
      " generating time\n",
      "\n",
      "generates 11 numbers \n",
      "\n",
      "indices=np.arange(0,10)  \n",
      "print(indices)\n",
      "\n",
      "creates 10 evenly spaced numbers starting with 1 and ending with 10\n",
      "print(np.linspace(1,10,10))\n",
      "\n",
      "\n",
      "import librosa as lr\n",
      "from glob import glob\n",
      "\n",
      "# List all the wav files in the folder\n",
      "audio_files = glob(data_dir + '/*.wav')\n",
      "\n",
      "# Read in the first audio file, create the time array\n",
      "audio, sfreq = lr.load(audio_files[0])\n",
      "time = np.arange(0, len(audio)) / sfreq\n",
      "\n",
      "\n",
      "# Plot audio over time\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(time, audio)\n",
      "ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      " >read in stock prices per year\n",
      "\n",
      "# Read in the data\n",
      "data = pd.read_csv('prices.csv', index_col=0)\n",
      "\n",
      "# Convert the index of the DataFrame to datetime\n",
      "data.index = pd.to_datetime(data.index)\n",
      "print(data.head())\n",
      "\n",
      "# Loop through each column, plot its values over time\n",
      "fig, ax = plt.subplots()\n",
      "for column in data:\n",
      "    data[column].plot(ax=ax, label=column)\n",
      "ax.legend()\n",
      "plt.show()\n",
      "\n",
      " classification and feature engineering\n",
      "\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "X=np.column_stack([means,maxs,stds])\n",
      "y=labels.reshape([-1,1])\n",
      "model=LinearSVC()\n",
      "model.fit(X,y)\n",
      "\n",
      "predictions=model.predict(X_test)\n",
      "\n",
      "percent_score= sum(predictions==labels_test)/len(labels_test)\n",
      "percent_score= accuracy_score(labels_test,predictions)\n",
      "\n",
      "\n",
      " heartbeat analysis\n",
      "\n",
      "fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True)\n",
      "\n",
      "# Calculate the time array\n",
      "time = np.arange(normal.shape[0]) / sfreq\n",
      "\n",
      "# Stack the normal/abnormal audio so you can loop and plot\n",
      "stacked_audio = np.hstack([normal, abnormal]).T\n",
      "\n",
      "# Loop through each audio file / ax object and plot\n",
      "# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
      "for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
      "    ax.plot(time, iaudio)\n",
      "show_plot_and_make_titles()\n",
      "\n",
      " using the mean to smooth the noise\n",
      "\n",
      "mean_normal = np.mean(normal, axis=1)\n",
      "mean_abnormal = np.mean(abnormal, axis=1)\n",
      "\n",
      "# Plot each average over time\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
      "ax1.plot(time, mean_normal)\n",
      "ax1.set(title=\"Normal Data\")\n",
      "ax2.plot(time, mean_abnormal)\n",
      "ax2.set(title=\"Abnormal Data\")\n",
      "plt.show()\n",
      "\n",
      " Test the data\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "# Initialize and fit the model\n",
      "model = LinearSVC()\n",
      "model.fit(X_train,y_train)\n",
      "\n",
      "# Generate predictions and score them manually\n",
      "predictions = model.predict(X_test)\n",
      "print(sum(predictions == y_test.squeeze()) / len(y_test))\n",
      "\n",
      " Smoothing signal\n",
      "\n",
      "# Rectify the audio signal\n",
      "audio_rectified = audio.apply(np.abs)\n",
      "\n",
      "# Plot the result\n",
      "# figsize parameter 1 is the width and parameter 2 is the height\n",
      "audio_rectified.plot(figsize=(10, 5))\n",
      "plt.show() \n",
      "\n",
      " Rolling and mean\n",
      "\n",
      "# Smooth by applying a rolling mean\n",
      "audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
      "\n",
      "# Plot the result\n",
      "audio_rectified_smooth.plot(figsize=(10, 5))\n",
      "plt.show()\n",
      "\n",
      "\n",
      " cross_val_score\n",
      "\n",
      "means = np.mean(audio_rectified_smooth, axis=0)\n",
      "stds = np.std(audio_rectified_smooth, axis=0)\n",
      "maxs = np.max(audio_rectified_smooth, axis=0)\n",
      "\n",
      "# Create the X and y arrays\n",
      "X = np.column_stack([means, stds, maxs])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "from sklearn.model_selection import cross_val_score\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "print(percent_score)\n",
      "\n",
      " tempo\n",
      "\n",
      "tempos = []\n",
      "for col, i_audio in audio.items():\n",
      "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
      "\n",
      "# Convert the list to an array so you can manipulate it more easily\n",
      "tempos = np.array(tempos)\n",
      "\n",
      "# Calculate statistics of each tempo\n",
      "tempos_mean = tempos.mean(axis=-1)\n",
      "tempos_std = tempos.std(axis=-1)\n",
      "tempos_max = tempos.max(axis=-1)\n",
      "\n",
      "  column_stack\n",
      "\n",
      "# Create the X and y arrays\n",
      "X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "\n",
      "\n",
      "\n",
      "  converting date index to a datetime\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "  grid=True\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "# Plot the entire time series diet and show gridlines\n",
      "diet.plot(grid=True)\n",
      "plt.show()\n",
      "\n",
      " filter on index\n",
      "\n",
      "diet.index = pd.to_datetime(diet.index)\n",
      "\n",
      "# Slice the dataset to keep only 2012\n",
      "diet2012 = diet['2012']\n",
      "\n",
      "# Plot 2012 data\n",
      "diet2012.plot(grid=True)\n",
      "plt.show()\n",
      "\n",
      " Differences between two sets of dates\n",
      "\n",
      "set_stock_dates = set(stocks.index)\n",
      "set_bond_dates = set(bonds.index)\n",
      "\n",
      "\n",
      "differences=set_stock_dates - set_bond_dates\n",
      "# Take the difference between the sets and print\n",
      "print(differences)\n",
      "\n",
      "# Merge stocks and bonds DataFrames using join()\n",
      "stocks_and_bonds = stocks.join(bonds, how='inner')\n",
      "\n",
      "\n",
      " Correlation between two variables\n",
      "\n",
      "# Compute percent change using pct_change()\n",
      "returns = stocks_and_bonds.pct_change()\n",
      "\n",
      "# Compute correlation using corr()\n",
      "correlation = returns['SP500'].corr(returns['US10Y'])\n",
      "print(\"Correlation of stocks and interest rates: \", correlation)\n",
      "\n",
      "# Make scatter plot\n",
      "plt.scatter(returns['SP500'],returns['US10Y'])\n",
      "plt.show()\n",
      "\n",
      "        Spectrogram\n",
      "\n",
      "fourier transforms\n",
      "fast or slow moving waves\n",
      "fft show a series of fast and slow wave osciliations in a time series.\n",
      "\n",
      "short-time fourier transform is calculating a fft over a time frame then sliding the window over by one\n",
      "\n",
      "The spectrogram is the square of each sfft\n",
      "\n",
      "we can calculate the stft with librosa\n",
      "\n",
      "the sound frequencies are converted in to decibels which normalizes the average values of all the frequencies\n",
      "\n",
      "we can visualize it with specshow() function\n",
      "\n",
      "from librosa.core import stft, amplitude_to_db\n",
      "from librosa.display import specshow\n",
      "\n",
      "HOP_LENGTH = 2**4\n",
      "SIZE_WINDOW= 2**7\n",
      "\n",
      "#calculate the short fast fourier transform\n",
      "\n",
      "audio_spec = stft(audio, hop_length=HOP_LENGTH, n_fft=SIZE_WINDOW)\n",
      "\n",
      "#convert into decibels\n",
      "spec_db=amplitude_to_db(audio_spec)\n",
      "\n",
      "#visualize\n",
      "specshow(spec_db, sr=sfreq, x_axis='time',\n",
      "\ty_axis='hz', hop_length=HOP_LENGTH)\n",
      "\n",
      "bandwidths=lr.feature.spectral_bandwidth(S=spec)[0]\n",
      "centroids=lr.feature.spectral_centroid(S=spec)[0]\n",
      "\n",
      "ax= spectshow(spec, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "ax.plot(times_spec, centroids)\n",
      "ax.fill_between(times_spec, centroids-bandwidths/2, centroids+bandwidths/2, alpha=0.5)\n",
      "\n",
      "\n",
      "each spectral has different patterns\n",
      "we can use these patterns to distinquish spectrals from one another\n",
      "\n",
      "for example spectral bandwidth and spectral centroids describe where most of the energy is at each moment in time.\n",
      "\n",
      "centroids_all=[]\n",
      "bandwidths_all=[]\n",
      "\n",
      "for spec in spectrograms:\n",
      "\tbandwidths=lr.feature.spectral_bandwidth(S=lr.db_to_amplitude(spec))\n",
      "\tcentroids=lr.feature.spectral_centroid(S=lr.db_to_amplitude(spec))\n",
      "\tbandwidths_all.append(np.mean(bandwidths))\n",
      "\tcentroids_all.appen(np.mean(centroids))\n",
      "\n",
      "#input matrix\n",
      "\n",
      "X= np.column_stack([means,stds,maxs,tempo_mean,tempo_max,tempo_std, bandwidths_all, centroids_all])\n",
      "\n",
      "\n",
      "\n",
      "   sample   >  short term fourier transform heart beat audio\n",
      "#Spectral engineering is one of the most common techniques in machine learning for time series data\n",
      "\n",
      "# Import the stft function\n",
      "from librosa.core import stft\n",
      "\n",
      "# Prepare the STFT\n",
      "HOP_LENGTH = 2**4\n",
      "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n",
      "\n",
      " > sample  > convert the spectral to decibals\n",
      "\n",
      "# Convert into decibels\n",
      "spec_db = amplitude_to_db(spec)\n",
      "\n",
      "# Compare the raw audio to the spectrogram of the audio\n",
      "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
      "axs[0].plot(time, audio)\n",
      "\n",
      "specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "plt.show()\n",
      "\n",
      "#the heartbeats come in pairs as seen by the vertical lines in the spectrogram\n",
      "\n",
      "\n",
      "   >sample  > calculate the bandwidths and centroids\n",
      "\n",
      "# By computing the spectral features, you have a much better idea of what's going on. \t\n",
      "\n",
      "import librosa as lr\n",
      "from librosa.core import amplitude_to_db\n",
      "from librosa.display import specshow\n",
      "\n",
      "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
      "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
      "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
      "\n",
      "\n",
      "# Convert spectrogram to decibels for visualization\n",
      "spec_db = amplitude_to_db(spec)\n",
      "\n",
      "# Display these features on top of the spectrogram\n",
      "fig, ax = plt.subplots(figsize=(10, 5))\n",
      "ax = specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
      "ax.plot(times_spec, centroids)\n",
      "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
      "ax.set(ylim=[None, 6000])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#You've spent this lesson engineering many features from the audio data - some contain information about how the audio changes in time\n",
      "\n",
      "#Combine all of them into an array that can be fed into the classifier, and see how it does.\n",
      "\n",
      " > sample build the final array for the classifier\n",
      "\n",
      "# Loop through each spectrogram\n",
      "bandwidths = []\n",
      "centroids = []\n",
      "\n",
      "for spec in spectrograms:\n",
      "    # Calculate the mean spectral bandwidth\n",
      "    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
      "    # Calculate the mean spectral centroid\n",
      "    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
      "    # Collect the values\n",
      "    bandwidths.append(this_mean_bandwidth)  \n",
      "    centroids.append(this_mean_centroid)\n",
      "\n",
      "# Create X and y arrays\n",
      "X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids])\n",
      "y = labels.reshape([-1, 1])\n",
      "\n",
      "# Fit the model and score on testing data\n",
      "percent_score = cross_val_score(model, X, y, cv=5)\n",
      "print(np.mean(percent_score))\n",
      "\n",
      "output .48\n",
      "\n",
      "#To improve the accuracy, you want to find the right features that provide relevant information and also build models on much larger data\n",
      "\n",
      "   >Regression\n",
      "\n",
      "regression model predict continueous models\n",
      "\n",
      "regression: a process that results in a formal model of the data\n",
      "correlation: a statistic that describe the data.  how two features correlate between each other.\n",
      "\n",
      "down or up together or an inverse relationship\n",
      "\n",
      "timeseries often have patterns that change over time\n",
      "\n",
      "two timeseries that seem correlated at one moment may not remain so over time\n",
      "\n",
      "fig, axs=plt.subplots(1,2)\n",
      "\n",
      "axs[0].plot(x,c='k',lw=3,alpha=.2)\n",
      "axs[0].plot(y)\n",
      "axs[0].set(xlabel='time',title='X values=time')\n",
      "\n",
      "#encode time as acolor in a scatterplot\n",
      "\n",
      "axs[1].scatter(x_long,y_long, c=np.arange(len(x_long)),cmap='viridis')\n",
      "axs[1].set(xlabel='x',ylabel='y',title='Color=time')\n",
      "\n",
      "  >regression models\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "model=LinearRegression()\n",
      "model.fit(X,y)\n",
      "model.predict(X)\n",
      "\n",
      "  >Ridge (see introductory course on skilearn)\n",
      "\n",
      "alphas=[.1,1e2,1e3]\n",
      "\n",
      "ax.plot(y_test,color='k', alpha=.3,lw=3)\n",
      "\n",
      "for ii, alpha in enumerate(alphas):\n",
      "\ty_predicted=Ridge(alpha=alpha).fit(X_train,y_train).predict(X_test)\n",
      "\tax.plot(y_predict, c=cmap(ii/len(alphas)))\n",
      "\n",
      "ax.legend(['True values','Model 1', 'Model 2', 'Model 3'])\n",
      "ax.set(xlabel='Time')\n",
      "\n",
      "  >Scoring a regression model\n",
      "\n",
      "Correlation (r)\n",
      "Coefficient of Determination(R2)\n",
      "\n",
      "\n",
      "Coefficient of Determination R2\n",
      "1- error(model)/variance(testdata)\n",
      "\n",
      "Error is actual - predicted of the model\n",
      "\n",
      "Variance is the mean squared distance of the data from their mean\n",
      "(x-x_mean) ** 2 / n\n",
      "\n",
      "or\n",
      "\n",
      "np.var(versicolor_petal_length)\n",
      "\n",
      "\n",
      "deviations = np.mean(y_data) - y_data\n",
      "\n",
      "VAR = np.sum(np.square(deviations))\n",
      "\n",
      "R-Squared : what fraction of variation is linear\n",
      "\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "print(r2_score(y_predicted, y_test))\n",
      "\n",
      "\n",
      "   Sample  > plot the prices for Ebay and Yhoo over time\n",
      "\n",
      "# Plot the raw values over time\n",
      "print(prices.columns)\n",
      "prices.plot()\n",
      "plt.show()\n",
      "\n",
      "   SAMPLE  > plot a scatter plot for ebay and yahoo prices\n",
      "\n",
      "# Scatterplot with one company per axis\n",
      "prices.plot.scatter('EBAY', 'YHOO')\n",
      "plt.show()\n",
      "\n",
      "   sample  > plot the scatter plot with color showing the price index\n",
      "# Scatterplot with color relating to time\n",
      "prices.plot.scatter('EBAY', 'YHOO', c=prices.index, \n",
      "                    cmap=plt.cm.viridis, colorbar=False)\n",
      "plt.show()\n",
      "\n",
      "The prices.index are dates.   The color changes over time.\n",
      "\n",
      "<<<<<sample  > add a ridge regressor and do 3 fold cross validation to check the accuracy.\n",
      "\n",
      "\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "# Use stock symbols to extract training data\n",
      "X = all_prices[['EBAY','NVDA','YHOO']]\n",
      "y = all_prices[['AAPL']]\n",
      "\n",
      "# Fit and score the model with cross-validation\n",
      "scores = cross_val_score(Ridge(), X, y, cv=3)\n",
      "print(scores)\n",
      "\n",
      "If Measure of fit of the model is a small value that means model is well fit to the data.\n",
      "\n",
      "If Measure of magnitude of coefficient is a small value that means model is not overfit.\n",
      "\n",
      "  >sample  > calculating the R2 factor\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "# Split our data into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, \n",
      "                                                    train_size=.8, shuffle=False, random_state=1)\n",
      "\n",
      "# Fit our model and generate predictions\n",
      "model = Ridge()\n",
      "model.fit(X_train,y_train)\n",
      "predictions = model.predict(X_test)\n",
      "score = r2_score(y_test, predictions)\n",
      "print(score)\n",
      "\n",
      "output: -5.70939901949\n",
      "\n",
      "ebay, nvda, yhoo are not linear predicters for apple prices\n",
      "\n",
      "  >sample    y_test is falling then predicted apple price is climbing\n",
      "\n",
      "# Visualize our predictions along with the \"true\" values, and print the score\n",
      "fig, ax = plt.subplots(figsize=(15, 5))\n",
      "ax.plot(y_test, color='k', lw=3)\n",
      "ax.plot(predictions, color='r', lw=2)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "The poor r2 score reflects a deviation between the predicted and true time series values.\n",
      "\n",
      "\n",
      "       cleaning and improving the data\n",
      "\n",
      "time data errors often happens because of human error, machine sensor malfunctions, and database failures.\n",
      "\n",
      "\n",
      "interpolation is using time to fill in missing data\n",
      "\n",
      "you can use time to assist in interpolation\n",
      "\n",
      "interpolation means using known values on either side of a gap in the data to make assumptions about what missing\n",
      "\n",
      "#create a boolean mask to find where the missing values are\n",
      "\n",
      "missing= prices.isna()\n",
      "\n",
      "#create a list of interpolated missing values\n",
      "prices_interp = prices.interpolate('linear')\n",
      "\n",
      "ax=prices_interp.plot(c='r')\n",
      "prices.plot(c='k',ax=ax, lw=2)\n",
      "\n",
      "    another way to fix missing data is to transform it so it more better behaved\n",
      "\n",
      "1. smooth the data\n",
      "2. use more complex transformations\n",
      "\n",
      "a common transformation is to standardize the mean and variance over time\n",
      "\n",
      "1. convert the dataset so each point represents the % change over a previous window\n",
      "2. this makes timepoints more comparable to one another if absolute values of data change a lot\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def percent_change(values):\n",
      "\t\"\"\"Calculates the % change between the last value and the mean of previous values\"\"\"\n",
      "\n",
      "\tprevious_values=values[:-1]\n",
      "\tlast_value=values[-1]\n",
      "\tpercent_change=(last_value-np.mean(previous_values))/np.mean(previous_values)\n",
      "\treturn percent_change\n",
      "\n",
      "\n",
      " > pass the percent_change function as a input to rolling prices\n",
      "\n",
      "fig,axs=plt.subplots(1,2,figsize=(10,5))\n",
      "ax=prices.plot(ax=axs[0])\n",
      "\n",
      "ax=prices.rolling(window=20).aggregate(percent_change).plot(ax=axs[1])\n",
      "ax.legend_.set_visible(False)\n",
      "\n",
      "#periods of high or low changes are easier to spot.\n",
      "\n",
      "    Outliers\n",
      "\n",
      "outliers are datapoints that are significantly statistically different from the dataset\n",
      "\n",
      "they have negative effects on the predictive power of your model, biasing it away from its true value\n",
      "\n",
      "One solution: remove or replace outliers with a more representative vlaue\n",
      "\n",
      "** there can be legitimate extreme values\n",
      "\n",
      "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
      "\n",
      "for data, ax in zip([prices,prices_perc_change],axs):\n",
      "\tthis_mean=data.mean()\n",
      "\tthis.std=data.std()\n",
      "\n",
      "\tdata.plot(ax=ax)\n",
      "\tax.axhline(this_mean+this_std*3, ls='--',c='r')\n",
      "\tax.axhline(this_mean-this_std*3, ls='--',c='r')\n",
      "\n",
      "\n",
      "find price outside of 3 std range from the mean\n",
      "\n",
      "#replace the outliers with the median\n",
      "\n",
      "prices_outlier_centered=prices_outlier_perc - prices_outlier_perc.mean()\n",
      "\n",
      "std=prices_outlier_perc.std()\n",
      "\n",
      "outliers=np.abs(prices_outlier_centered)>(std*3)\n",
      "\n",
      "prices_outlier_fixed=prices_outlier_centered.copy()\n",
      "prices_outlier_fixed[outliers]=np.nanmedian(prices_outlier_fixed)\n",
      "\n",
      "fig,axs = plt.subplots(1,2,figsize=(10,5))\n",
      "prices_outlier_centered.plot(ax=axs[0])\n",
      "prices_outlier_fixed.plot(ax=axs[1])\n",
      "\n",
      "\n",
      "   >sample      visualize missing values in continueous data\n",
      "\n",
      "# Visualize the dataset\n",
      "prices.plot(legend=False)\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "# Count the missing values of each time series\n",
      "missing_values = prices.isna().sum()\n",
      "print(missing_values)\n",
      "\n",
      "   sample  > interpolate\n",
      "\n",
      "# Create a function we'll use to interpolate and plot\n",
      "def interpolate_and_plot(prices, interpolation_type):\n",
      "\n",
      "    # Create a boolean mask for missing values\n",
      "    missing_values = prices.isna()\n",
      "\n",
      "    # Interpolate the missing values\n",
      "    prices_interp = prices.interpolate(interpolation_type)\n",
      "\n",
      "    # Plot the results, highlighting the interpolated values in black\n",
      "    fig, ax = plt.subplots(figsize=(10, 5))\n",
      "    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
      "    \n",
      "    # Now plot the interpolated values on top in red\n",
      "    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "# Interpolate using the latest non-missing value\n",
      "interpolation_type = 'zero' #'linear' or 'quadratic'\n",
      "interpolate_and_plot(prices, interpolation_type)\n",
      "\n",
      "\n",
      "  > sample  > rolling percent change\n",
      "\n",
      "# Your custom function\n",
      "def percent_change(series):\n",
      "    # Collect all *but* the last value of this window, then the final value\n",
      "    previous_values = series[:-1]\n",
      "    last_value = series[-1]\n",
      "\n",
      "    # Calculate the % difference between the last value and the mean of earlier values\n",
      "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
      "    return percent_change\n",
      "\n",
      "# Apply your custom function and plot\n",
      "prices_perc = prices.rolling(20).aggregate(percent_change)\n",
      "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
      "plt.show()\n",
      "\n",
      "    sample  > apply  replace_outliers\n",
      "\n",
      "def replace_outliers(series):\n",
      "    # Calculate the absolute difference of each timepoint from the series mean\n",
      "    absolute_differences_from_mean = np.abs(series - np.mean(series))\n",
      "\n",
      "    # Calculate a mask for the differences that are > 3 standard deviations from the mean\n",
      "    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n",
      "    \n",
      "    # Replace these values with the median accross the data\n",
      "    series[this_mask] = np.nanmedian(series)\n",
      "    return series\n",
      "\n",
      "# Apply your preprocessing function to the timeseries and plot the results\n",
      "prices_perc = prices_perc.apply(replace_outliers)\n",
      "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "          creating features over time\n",
      "\n",
      "extract features as they change over time\n",
      "\n",
      "feats=prices.rolling(20).aggregate([np.std,np.max]).dropna()\n",
      "print(feats.head())\n",
      "\n",
      "\n",
      "\n",
      "rolling is a rolling window\n",
      "\n",
      "AIG: std, amax\n",
      "ABt: std, amax\n",
      "\n",
      "always plot properties of your features\n",
      "it will help you spot noise data and outliers\n",
      "\n",
      "    >partial function\n",
      "\n",
      "lets you define a new function with parts of the old one\n",
      "\n",
      "\n",
      "from functools import partial\n",
      "\n",
      "mean_over_first_axis = partial(np.mean, axis=0)\n",
      "\n",
      "print(mean_over_first_axis(a))\n",
      "\n",
      "#the mean function always operates on the first axis\n",
      "\n",
      "percentiles give fine grained summaries of your data\n",
      "\n",
      "print(np.percentile(np.linespace(0,200),q=20))\n",
      "\n",
      "percentiles first input is an array\n",
      "q, the second input is an integer between 0 and 100\n",
      "\n",
      "returns the values of the first input as a percentile of the second input\n",
      "\n",
      "\n",
      "data = np.linspace(0,100)\n",
      "\n",
      "percentile_funcs= [partial(np.percentile, q=ii) for ii in [20,40,60]]\n",
      "\n",
      "percentiles = [i_func(data) for i_func in percentile_funcs]\n",
      "print(percentiles)\n",
      "\n",
      "output: [20,40,60]\n",
      "\n",
      "data.rolling(20).aggregrate(percentiles)\n",
      "\n",
      "         Calculating date-based features      \n",
      "\n",
      "statistical features: are numerical features like mean and standard deviation.\n",
      "human features like days of the week, holidays\n",
      "these features can span multiple years.\n",
      "\n",
      "\n",
      "#ensure index is datetime\n",
      "prices.index=pd.to_datetime(prices.index)\n",
      "\n",
      "#extract datetime features\n",
      "\n",
      "day_of_week_num = prices.index.weekday\n",
      "print(day_of_week_num[:10])\n",
      "output:[0 1 2 3 4 0 1 2 3 4]\n",
      "\n",
      "day_of_week = prices.index.weekday_name\n",
      "print(day_of_week[:10])\n",
      "output:['Monday','Tuesday'...'Friday']\n",
      "\n",
      "\n",
      "   >sample  > rolling window   > visualize min, max,mean, std for ebay\n",
      "\n",
      "# Define a rolling window with Pandas, excluding the right-most datapoint of the window\n",
      "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
      "\n",
      "# Define the features you'll calculate for each window\n",
      "features_to_calculate = [np.min, np.max, np.mean, np.std]\n",
      "\n",
      "# Calculate these features for your rolling window object\n",
      "features = prices_perc_rolling.aggregate(features_to_calculate)\n",
      "\n",
      "# Plot the results\n",
      "ax = features.loc[:\"2011-01\"].plot()\n",
      "prices_perc.loc[:\"2011-01\"].plot(ax=ax, color='k', alpha=.2, lw=3)\n",
      "ax.legend(loc=(1.01, .6))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   percentiles and partial functions\n",
      "\n",
      "# Import partial from functools\n",
      "from functools import partial\n",
      "percentiles = [1, 10, 25, 50, 75, 90, 99]\n",
      "\n",
      "# Use a list comprehension to create a partial function for each quantile\n",
      "percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n",
      "\n",
      "# Calculate each of these quantiles on the data using a rolling window\n",
      "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
      "features_percentiles = prices_perc_rolling.aggregate(percentile_functions)\n",
      "\n",
      "# Plot a subset of the result\n",
      "ax = features_percentiles.loc[:\"2011-01\"].plot(cmap=plt.cm.viridis)\n",
      "ax.legend(percentiles, loc=(1.01, .5))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Extract date features from the data, add them as columns\n",
      "prices_perc['day_of_week'] = prices_perc.index.dayofweek\n",
      "prices_perc['week_of_year'] = prices_perc.index.weekofyear\n",
      "prices_perc['month_of_year'] = prices_perc.index.month\n",
      "\n",
      "# Print prices_perc\n",
      "print(prices_perc)\n",
      "\n",
      "\n",
      "       >Feature extraction\n",
      "\n",
      "time series has a linear flow with relationships between the data\n",
      "\n",
      "information in the past can help predict what happens in the future\n",
      "\n",
      "often the features best-suited to predict a timeseries are previous values of the same timeseries\n",
      "\n",
      "the smoothness of the data help determine how correlated a timepoint is with its neighboring timepoints\n",
      "\n",
      "the amount of auto-correlation in data will impact your models\n",
      "\n",
      "data= pd.Series()\n",
      "\n",
      "shifts=[0,1,2,3,4,5,6,7]\n",
      "\n",
      "many_shifts={'lag_{}'.format(ii): data.shift(ii) for ii in shifts}\n",
      "\n",
      "many_shifts=pd.DataFrame(many_shifts)\n",
      "\n",
      "model=Ridge()\n",
      "model.fit(many_shifts,data)\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots()\n",
      "ax.bar(many_shifts.columns, model_coef_)\n",
      "ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
      "\n",
      "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "\n",
      "    Sample  > time shifted features\n",
      "In machine learning for time series, it's common to use information about previous time points to predict a subsequent time point.\n",
      "\n",
      "# These are the \"time lags\"\n",
      "shifts = np.arange(1, 11).astype(int)\n",
      "\n",
      "print(prices_perc)\n",
      "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
      "shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
      "\n",
      "# Convert into a DataFrame for subsequent use\n",
      "prices_perc_shifted = pd.DataFrame(shifted_data)\n",
      "\n",
      "# Plot the first 100 samples of each\n",
      "ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis)\n",
      "prices_perc.iloc[:100].plot(color='r', lw=2)\n",
      "ax.legend(loc='best')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Replace missing values with the median for each column\n",
      "X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
      "y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
      "\n",
      "# Fit the model\n",
      "model = Ridge()\n",
      "model.fit(X, y)\n",
      "\n",
      "def visualize_coefficients(coefs, names, ax):\n",
      "    # Make a bar plot for the coefficients, including their names on the x-axis\n",
      "    ax.bar(names, coefs)\n",
      "    ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
      "    \n",
      "    # Set formatting so it looks nice\n",
      "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "    return ax\n",
      "\n",
      "# Visualize the output data up to \"2011-01\"\n",
      "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
      "y.loc[:'2011-01'].plot(ax=axs[0])\n",
      "\n",
      "# Run the function to visualize model's coefficients\n",
      "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
      "plt.show()\n",
      "\n",
      "Increase the data window from 20 to 40\n",
      "\n",
      "As you can see here, by transforming your data with a larger window, you've also changed the relationship between each timepoint and the ones that come just before it. This model's coefficients gradually go down to zero, which means that the signal itself is smoother over time.\n",
      "\n",
      "\n",
      "     >Cross validating time series data\n",
      "\n",
      "KFold is the most common cross validation\n",
      "\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "cv=KFold(n_splits=5)\n",
      "for tr,tt in cv.split(X,y):\n",
      "\n",
      "always visualize your models behavior during cross validation\n",
      "\n",
      "fig, axs = plt.subplots(2,1)\n",
      "\n",
      "axs[0].scatter(tt,[0]*len(tt),marker='_',s=2,lw=40)\n",
      "axs[0].set(ylim=[-.1,.1],title='Test set indices (color=CV loop)', xlabel='Index of raw data')\n",
      "\n",
      "axs[1].plot(model.predict(X[tt]))\n",
      "axs[1].set(title='Test set predictions on each CV loop', xlabel('Prediction index')\n",
      "\n",
      "\n",
      "from sklearn.model_selection import ShuffleSplit\n",
      "\n",
      "cv=ShuffleSplit(n_splits=3)\n",
      "for tr,tt in cv.split(X,y):\n",
      "\n",
      "\n",
      "    >time series cv iterator  (use only the past to validate)\n",
      "\n",
      "1. generally you should not use datapoints in the future to predict data in the past\n",
      "\n",
      "2. Always use training data from the past to predict the future\n",
      "\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "cv=TimeSeriesSplit(n_splits=10)\n",
      "\n",
      "fig,ax=plt.subplots(figsize=(10,5))\n",
      "\n",
      "for ii,(tr,tt) in enumerate(cv.split(X,y)):\n",
      "\tl1=ax.scatter(tr,[ii]*len(tr), c=[plt.cm.coolwarm(.1)],marker='_',lw=6)\n",
      "\n",
      "\tl2=ax.scatter(tt,[ii]*len(tt), c=[plt.cm.coolwarm(.9)],marker='_',lw=6)\n",
      "\n",
      "\tax.set(ylim[10,-1],title='TimeSeriesSplit behavior', xlabel='data index', ylabel='CV iteration')\n",
      "\tax.legend([l1,l2],['Training','Validation'])\n",
      "\n",
      "\n",
      "only the past is use to validate the data\n",
      "\n",
      "def myfunction(estimator, X,y):\n",
      "\ty_pred=estimator.predict(X)\n",
      "\tmy_custom_score=my_custom_function(y_pred,y)\n",
      "\treturn my_custom_score\n",
      "\n",
      "def my_pearsonr(est,X,y):\n",
      "\ty_pred=est.predict(X).squeeze()\n",
      "\tmy_corrcoef_matrix=np.corrcoef(y_pred,y.squeeze())\n",
      "\tmy_corrcoef = my_corrcoef[1,0]\n",
      "\treturn my_corrcoef\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Sample     Shufflesplit    > visualization by time\n",
      "\n",
      "# Import ShuffleSplit and create the cross-validation object\n",
      "from sklearn.model_selection import ShuffleSplit\n",
      "cv = ShuffleSplit(n_splits=3, random_state=1)\n",
      "\n",
      "# Iterate through CV splits\n",
      "results = []\n",
      "\n",
      "for tr, tt in cv.split(X, y):\n",
      "    # Fit the model on training data\n",
      "    model.fit(X[tr], y[tr])\n",
      "    \n",
      "    # Generate predictions on the test data, score the predictions, and collect\n",
      "    prediction = model.predict(X[tt])\n",
      "    score = r2_score(y[tt], prediction)\n",
      "    results.append((prediction, score, tt))\n",
      "\n",
      "# Custom function to quickly visualize predictions\n",
      "visualize_predictions(results)\n",
      "\n",
      "https://goodboychan.github.io/chans_jupyter/python/datacamp/time_series_analysis/machine_learning/2020/06/18/02-Validating-and-Inspecting-Time-Series-Models.html\n",
      "\n",
      "def visualize_predictions(results):\n",
      "    fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
      "\n",
      "    # Loop through our model results to visualize them\n",
      "    for ii, (prediction, score, indices) in enumerate(results):\n",
      "        # Plot the predictions of the model in the order they were generated\n",
      "        offset = len(prediction) * ii\n",
      "        axs[0].scatter(np.arange(len(prediction)) + offset, prediction, \n",
      "                       label='Iteration {}'.format(ii))\n",
      "\n",
      "        # Plot the predictions of the model according to how time was ordered\n",
      "        axs[1].scatter(indices, prediction)\n",
      "    axs[0].legend(loc=\"best\")\n",
      "    axs[0].set(xlabel=\"Test prediction number\", title=\"Predictions ordered by test prediction number\")\n",
      "    axs[1].set(xlabel=\"Time\", title=\"Predictions ordered by time\")\n",
      "\n",
      "\n",
      "            Sample    using KFold\n",
      "\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "# Create KFold cross-validation object\n",
      "from sklearn.model_selection import KFold\n",
      "cv = KFold(n_splits=10, shuffle=False, random_state=1)\n",
      "\n",
      "# Iterate through CV splits\n",
      "results = []\n",
      "for tr, tt in cv.split(X, y):\n",
      "    # Fit the model on training data\n",
      "    model.fit(X[tr],y[tr])\n",
      "    \n",
      "    # Generate predictions on the test data and collect\n",
      "    prediction = model.predict(X[tt])\n",
      "    results.append((prediction, tt))\n",
      "    \n",
      "# Custom function to quickly visualize predictions\n",
      "visualize_predictions(results)\n",
      "\n",
      "#This time, the predictions generated within each CV loop look 'smoother' than they were before - they look more like a real time series because you didn't shuffle the data\n",
      "\n",
      "\n",
      "     Sample   > Timeseries Split\n",
      "\n",
      "# Import TimeSeriesSplit\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "\n",
      "# Create time-series cross-validation object\n",
      "cv = TimeSeriesSplit(n_splits=10)\n",
      "\n",
      "# Iterate through CV splits\n",
      "fig, ax = plt.subplots()\n",
      "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
      "    # Plot the training data on each iteration, to see the behavior of the CV\n",
      "    ax.plot(tr, ii + y[tr])\n",
      "\n",
      "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
      "plt.show()\n",
      "\n",
      "#Note that the size of the training set grew each time when you used the time series cross-validation object\n",
      "\n",
      "\n",
      "      >Stationary and stability\n",
      "\n",
      "a stationary time series is one that does not change their statistical properties over time.\n",
      "\n",
      "most time series are non-stationary to some extent\n",
      "\n",
      "it has the same mean, standard deviation, and trends\n",
      "\n",
      "cross validation to quantify parameter stability\n",
      "\n",
      "calculate model parameter on each iteration\n",
      "\n",
      "assess parameter stability across all cv split\n",
      "\n",
      "bootstrapping is a way to estimate the confidence using the mean of a group of numbers\n",
      "\n",
      "1. take a random sample of data with replacement\n",
      "2. calculate the mean of the sample\n",
      "3. repeat the process many times\n",
      "4. caclulate the percentiles of the result\n",
      "\n",
      "the result is a 95% confidence interval of the mean of each coefficent\n",
      "\n",
      "from sklearn.utils import resample\n",
      "\n",
      "n_boots=100\n",
      "\n",
      "bootstrap_means=np.zeros(n_boots, n_coefficients)\n",
      "for ii in range(n_boots):\n",
      "\trandom_sample=resample(cv_coefficients)\n",
      "\tbootstrap_means[ii]=random_sample.mean(axis=0)\n",
      "\n",
      "percentiles=np.percentiles(bootstrap_means,(2.5,97.5),axis=0)\n",
      "\n",
      "fig,ax=plt.subplots()\n",
      "\n",
      "ax.scatter(many_shifts.columns,percentiles[0], marker='_',s=200)\n",
      "ax.scatter(many_shifts.columns,percentiles[1], marker='_',s=200)\n",
      "\n",
      "this gives an idea of the variability of the mean across all cross validation iterations\n",
      "\n",
      "      Assessing model performance stability\n",
      "\n",
      "if your using the TimeSeriesSplit, you can plot the models score over time.\n",
      "\n",
      "This is helpful to find certain regions of time that hurt the score\n",
      "\n",
      "it is also import to find non-stationary signals\n",
      "\n",
      "\n",
      "def my_corrcoef(est,X,y):\n",
      "\t\"\"\"return the correlation coefficient between model predictions and a validation set\"\"\"\n",
      "\treturn np.corrcoef(y,est,predict(X))[1,0]\n",
      "\n",
      "first_indices=[data.index[tt[0]] for tr,tt in cv.split(X,y)]\n",
      "\n",
      "cv_scores=cross_val_score(model, X,y,cv=cv, scoring=my_corrcoef)\n",
      "cv_scores=pd.Series(cv_scores, index=first_indices)\n",
      "\n",
      "\n",
      "cv.split rturns a ndarray train set indices and test ndarray set indices\n",
      "\n",
      "find the beginning of each validation block using a list comprehension\n",
      "\n",
      "collect the score and convert them into a pandas series\n",
      "\n",
      "visualize the results as a timeseries\n",
      "\n",
      "fig, axs=plt.subplots(2,1, figsize=(10,5), sharex=True)\n",
      "\n",
      "cv_scores_mean=cv_scores.rolling(10, min_periods=1).mean()\n",
      "\n",
      "cv_scores.plot(ax=axs[0])\n",
      "axs[0].set(title='Validation scores (correlation)', ylim=[0,1])\n",
      "\n",
      "data.plot(ax=axs[1])\n",
      "axs[1].set(title='Validation data')\n",
      "\n",
      "   >restrict to the latest time points to be used in training\n",
      "\n",
      "window=100\n",
      "\n",
      "cv=TimeSeries(n_splits=10, max_train_size=window)\n",
      "\n",
      "\n",
      "      >Sample  > boot strap the data    build the function\n",
      "\n",
      "from sklearn.utils import resample\n",
      "\n",
      "def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
      "    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
      "    # Create our empty array to fill the results\n",
      "    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
      "    for ii in range(n_boots):\n",
      "        # Generate random indices for our data *with* replacement, then take the sample mean\n",
      "        random_sample = resample(data)\n",
      "        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
      "        \n",
      "    # Compute the percentiles of choice for the bootstrapped means\n",
      "    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
      "    return percentiles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pre-processing techniques.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pre-processing techniques.txt\n",
      "df.describe()\n",
      "\n",
      "removing missing data\n",
      "\n",
      "df.dropna()\n",
      "\n",
      "df.drop(['col1'],axis=1)\n",
      "\n",
      "print(df[df['b']==7])\n",
      "\n",
      "print(df['B'].isnull().sum())\n",
      "\n",
      "print(df[df['B'].notnull()])\n",
      "\n",
      "\n",
      "  >sample  > find the number nulls in the category_desc column\n",
      "\n",
      "# Check how many values are missing in the category_desc column\n",
      "print(volunteer['category_desc'].isnull().sum())\n",
      "\n",
      "# Subset the volunteer dataset\n",
      "volunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n",
      "\n",
      "# Print out the shape of the subset\n",
      "print(volunteer_subset.shape)\n",
      "\n",
      "\n",
      "      Working with data types\n",
      "\n",
      "how to convert data types in the dataset\n",
      "\n",
      "print(volunteer.dtypes)\n",
      "\n",
      "\n",
      "object\n",
      "int64\n",
      "float64\n",
      "datetime64 or timedelta\n",
      "\n",
      "df['C']=df['C'].,astype('float')\n",
      "\n",
      "  >sample    converting to int\n",
      "\n",
      " # Print the head of the hits column\n",
      "print(volunteer[\"hits\"].dtypes)\n",
      "\n",
      "# Convert the hits column to type int\n",
      "volunteer[\"hits\"] = volunteer['hits'].astype(int)\n",
      "\n",
      "# Look at the dtypes of the dataset\n",
      "print(volunteer.dtypes)\n",
      "\n",
      "\n",
      "       >Training and test sets\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, x_train, y_test=train_test_split(X,y)\n",
      "\n",
      "75% and 25% by default\n",
      "\n",
      "Stratifed sampling\n",
      "100 samples, 80% class 1 and 20% class 2\n",
      "\n",
      "Training set: 75 samples, 60 class 1 and 15 class 2\n",
      "Test set: 25 samples, 20 class 1 and 5 class 2\n",
      "\n",
      "y['labels'].value_counts()\n",
      "\n",
      "\n",
      "    Sample  value_counts()\n",
      "\n",
      "volunteer['category_desc'].value_counts()\n",
      "\n",
      "Strengthening Communities    307\n",
      "Helping Neighbors in Need    119\n",
      "Education                     92\n",
      "Health                        52\n",
      "Environment                   32\n",
      "Emergency Preparedness        15\n",
      "Name: category_desc, dtype: int64\n",
      "\n",
      "    sample  > train_test_split using stratify\n",
      "\n",
      "\n",
      "# Create a data with all columns except category_desc\n",
      "volunteer_X = volunteer.drop(['category_desc'], axis=1)\n",
      "\n",
      "# Create a category_desc labels dataset\n",
      "volunteer_y = volunteer[['category_desc']]\n",
      "\n",
      "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
      "X_train, X_test,y_train,y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n",
      "\n",
      "# Print out the category_desc counts on the training y labels\n",
      "print(y_train['category_desc'].value_counts())\n",
      "\n",
      "\n",
      "y_train output\n",
      "\n",
      "Strengthening Communities    230\n",
      "Helping Neighbors in Need     89\n",
      "Education                     69\n",
      "Health                        39\n",
      "Environment                   24\n",
      "Emergency Preparedness        11\n",
      "Name: category_desc, dtype: int64\n",
      "\n",
      "\n",
      "    Standardizing data\n",
      "\n",
      "standardization is technique for taking contineously distributed data and make it look normally distributed\n",
      "\n",
      "Log normalization and feature scaling\n",
      "\n",
      "applied to contineous numerical data\n",
      "\n",
      "model in linear space\n",
      "\n",
      "dataset features have high variance\n",
      "\n",
      "dataset features are contineous and on different scales\n",
      "\n",
      "   > sample  > knn k nearest neighbor\n",
      "\n",
      "# Split the dataset and labels into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "\n",
      "# Fit the k-nearest neighbors model to the training data\n",
      "knn.fit(X_train,y_train)\n",
      "\n",
      "# Score the model on the test data\n",
      "print(knn.score(X_test,y_test))\n",
      "\n",
      "\n",
      "       >Log normalization\n",
      "\n",
      "log normalization can be helpful if you have a column with high variance\n",
      "\n",
      "applies log transformation\n",
      "\n",
      "natural log using the contant _e_ = 2.718\n",
      "\n",
      "log 30 is 3.4\n",
      "\n",
      "because 3.4 ** 2.718 = 30 or 3.4**_e_=30\n",
      "\n",
      "captures relative changes and captures the magnitude of change and keeps everything in positive space\n",
      "\n",
      "Log normalization reduces the variance in the model\n",
      "\n",
      "print(df.var())\n",
      "\n",
      "col1: 0.12858\n",
      "col2  1691.72167\n",
      "\n",
      "import numpy as np\n",
      "df['log_column2']= np.log(df['col2'])\n",
      "print(df)\n",
      "\n",
      "log_2 has scaled down the values\n",
      "\n",
      "\n",
      "  o sample  > log normalization\n",
      "\n",
      "# Print out the variance of the Proline column\n",
      "print(wine['Proline'].var())\n",
      "\n",
      "# Apply the log normalization function to the Proline column\n",
      "wine['Proline_log'] = np.log(wine['Proline'])\n",
      "\n",
      "\n",
      "# Check the variance of the normalized Proline column\n",
      "print(wine['Proline_log'])\n",
      "\n",
      "\n",
      "    scaling data for feature comparison\n",
      "\n",
      "1. features on different scales\n",
      "2. model with linear characteristics\n",
      "3. center features around 0 and transform to unit variance\n",
      "4. transforms to approximately normal distribution\n",
      "\n",
      "variance is low in the column but differs across columns\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler=StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(df),\n",
      "columns=df.columns)\n",
      "\n",
      "\n",
      "print(df_scaled)\n",
      "\n",
      "print(df.var())\n",
      "\n",
      "The variance on each column is the same\n",
      "\n",
      "\n",
      "    sample standardscaler normalization\n",
      "\n",
      "# Import StandardScaler from scikit-learn\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "\n",
      "# Create the scaler\n",
      "ss = StandardScaler()\n",
      "\n",
      "# Take a subset of the DataFrame you want to scale \n",
      "wine_subset = wine[['Ash','Alcalinity of ash','Magnesium']]\n",
      "\n",
      "# Apply the scaler to the DataFrame subset\n",
      "wine_subset_scaled = ss.fit_transform(wine_subset)\n",
      "\n",
      "\n",
      "  o sample  > log normalization\n",
      "\n",
      "# Print out the variance of the Proline column\n",
      "print(wine['Proline'].var())\n",
      "\n",
      "# Apply the log normalization function to the Proline column\n",
      "wine['Proline_log'] = np.log(wine['Proline'])\n",
      "\n",
      "\n",
      "# Check the variance of the normalized Proline column\n",
      "print(wine['Proline_log'])\n",
      "\n",
      "\n",
      "    scaling data for feature comparison\n",
      "\n",
      "1. features on different scales\n",
      "2. model with linear characteristics\n",
      "3. center features around 0 and transform to unit variance\n",
      "4. transforms to approximately normal distribution\n",
      "\n",
      "variance is low in the column but differs across columns\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler=StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(df),\n",
      "columns=df.columns)\n",
      "\n",
      "\n",
      "print(df_scaled)\n",
      "\n",
      "print(df.var())\n",
      "\n",
      "The variance on each column is the same\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "X_train,y_train, X_test,y_test = train_test_split(X,y)\n",
      "\n",
      "knn=KNeighborsClassifier()\n",
      "knn.fit(X_train, y_train)\n",
      "knn.score(X_test,y_test)\n",
      "\n",
      "\n",
      "  >sample  > knn\n",
      "\n",
      "# Split the dataset and labels into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
      "\n",
      "# Fit the k-nearest neighbors model to the training data\n",
      "knn.fit(X_train,y_train)\n",
      "\n",
      "# Score the model on the test data\n",
      "print(knn.score(X_test, y_test))\n",
      "\n",
      "   >sample  > apply standard scaler fit_transform\n",
      "\n",
      "# Create the scaling method.\n",
      "ss = StandardScaler()\n",
      "\n",
      "# Apply the scaling method to the dataset used for modeling.\n",
      "X_scaled = ss.fit_transform(X)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
      "\n",
      "# Fit the k-nearest neighbors model to the training data.\n",
      "knn.fit(X_train,y_train)\n",
      "\n",
      "# Score the model on the test data.\n",
      "print(knn.score(X_test,y_test))\n",
      "\n",
      "\n",
      "     >Feature Engineering\n",
      "\n",
      "creation of new features based on existing features\n",
      "insight into relationships between features\n",
      "extract and expand data\n",
      "dataset-dependent\n",
      "\n",
      "   >Encoding categorical variables\n",
      "\n",
      "the classifiers require numeric input\n",
      "\n",
      "encoding is required\n",
      "\n",
      "fav_color=blue,green,orange, green\n",
      "\n",
      "  encode binary as 1 and 0\n",
      "\n",
      "users['sub_enc']=users['subscribed'].apply(lambda val: 1 if val=='y' else 0)\n",
      "\n",
      "   >Label encoding\n",
      "\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "le=LabelEncoder()\n",
      "users['sub_enc_le']=le.fit_tranform(user['subscribed'])\n",
      "\n",
      "   one hot-encoding\n",
      "\n",
      "pd.get_dummies(users['fav_colors'])\n",
      "\n",
      "        >Sample   > label encoder\n",
      "\n",
      "# Set up the LabelEncoder object\n",
      "enc = LabelEncoder()\n",
      "\n",
      "# Apply the encoding to the \"Accessible\" column\n",
      "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
      "\n",
      "# Compare the two columns\n",
      "print(hiking[['Accessible', 'Accessible_enc']].head())\n",
      "\n",
      "    Sample  > get_dummies\n",
      "\n",
      "# Transform the category_desc column\n",
      "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
      "\n",
      "# Take a look at the encoded columns\n",
      "print(category_enc.head())\n",
      "\n",
      "\n",
      "  >Engineering numerical features\n",
      "\n",
      "\n",
      "columns=['day1','day2','day3']\n",
      "df['mean']=df.apply(lambda row: row[columns].mean(), axis=1)\n",
      "\n",
      "print(df)\n",
      "\n",
      "\n",
      "  > Dates\n",
      "\n",
      "df['date_converted']=pd.to_datetime(df['date'])\n",
      "\n",
      "df['month']=df['date_converted'].apply(lambda row: row.month)\n",
      "\n",
      "  > sample   > creating an average column\n",
      "\n",
      "# Create a list of the columns to average\n",
      "run_columns = ['run1', 'run2', 'run3', 'run4', 'run5']\n",
      "\n",
      "#running_times_5k.columns\n",
      "\n",
      "\n",
      "# Use apply to create a mean column\n",
      "running_times_5k[\"mean\"] = running_times_5k.apply(lambda x: x[run_columns].mean(), axis=1)\n",
      "\n",
      "# Take a look at the results\n",
      "print(running_times_5k[\"mean\"])\n",
      "\n",
      "   sample  > extract the month\n",
      "\n",
      "# First, convert string column to date column\n",
      "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
      "\n",
      "# Extract just the month from the converted column\n",
      "volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].apply(lambda row: row.month)\n",
      "\n",
      "# Take a look at the converted and new month columns\n",
      "print(volunteer[[\"start_date_converted\", \"start_date_month\"]].head())\n",
      "\n",
      "\n",
      "   >Feature engineering from text\n",
      "\n",
      "\n",
      "import re\n",
      "\n",
      "my_string=\"temperature: 75.6 F\"\n",
      "\n",
      "pattern=re.compile(\"\\d+\\.\\d+\")\n",
      "\n",
      "look for the float value in the string\n",
      "\\d digits\n",
      "\\. decimal period\n",
      "\n",
      "temp = re.match(pattern, my_string)\n",
      "\n",
      "print(float(temp.group(0))\n",
      "\n",
      "Vectorize the text\n",
      "\n",
      "tfidf vector term frequency inverse document frequency\n",
      "\n",
      "put weights on words that are more significant\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "print(documents.head())\n",
      "\n",
      "\n",
      "tfidf_vec = TfidfVectorizer()\n",
      "text_tfidf = tfidf_vec.fit_transform(documents)\n",
      "\n",
      "naives bayes classifier\n",
      "each feature is independent of others\n",
      "\n",
      "\n",
      "  > sample  > extract miles from a string\n",
      "\n",
      "# Write a pattern to extract numbers and decimals\n",
      "def return_mileage(length):\n",
      "    pattern = re.compile(\"\\d+\\.\\d+\")\n",
      "    \n",
      "    # Search the text for matches\n",
      "    mile = re.match(pattern,length)\n",
      "    \n",
      "    # If a value is returned, use group(0) to return the found value\n",
      "    if mile is not None:\n",
      "        return float(mile.group(0))\n",
      "        \n",
      "# Apply the function to the Length column and take a look at both columns\n",
      "hiking[\"Length_num\"] = hiking['Length'].apply(lambda row: return_mileage(row))\n",
      "print(hiking[[\"Length\", \"Length_num\"]].head())\n",
      "\n",
      "  > sample  > vectorize text\n",
      "\n",
      "# Take the title text\n",
      "title_text = volunteer['title']\n",
      "\n",
      "# Create the vectorizer method\n",
      "tfidf_vec = TfidfVectorizer()\n",
      "\n",
      "# Transform the text into tf-idf vectors\n",
      "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
      "print(text_tfidf)\n",
      "\n",
      "   sample    naive bayes\n",
      "\n",
      "# Split the dataset according to the class distribution of category_desc\n",
      "print(text_tfidf)\n",
      "y = volunteer[\"category_desc\"]\n",
      "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)\n",
      "\n",
      "# Fit the model to the training data\n",
      "nb.fit(X_train,y_train)\n",
      "\n",
      "# Print out the model's accuracy\n",
      "print(nb.score(X_test,y_test))\n",
      "\n",
      "\n",
      "       Feature selection\n",
      "\n",
      "removing unnecessary features that might create noise\n",
      "remove correlated features\n",
      "remove duplicate features\n",
      "\n",
      "it is an iterative process\n",
      "\n",
      "depends on the end goal\n",
      "\n",
      "the feature move to together \n",
      "use pearson correlation coefient (-1 and 1)\n",
      "\n",
      "df.corr()\n",
      "\n",
      "\n",
      "   sample  > df.corr   > drop columns\n",
      "\n",
      "# Create a list of redundant column names to drop\n",
      "to_drop = [\"category_desc\", \"locality\", \"region\", \"postalcode\", \"vol_requests\"]\n",
      "\n",
      "# Drop those columns from the dataset\n",
      "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
      "\n",
      "# Print out the head of the new dataset\n",
      "print(volunteer_subset.head())\n",
      "\n",
      "volunteer.corr()\n",
      "\n",
      "\n",
      "  >sample  > drop columns with corr > .75\n",
      "\n",
      "import numpy as np\n",
      "# Print out the column correlations of the wine dataset\n",
      "print(wine.corr())\n",
      "\n",
      "corr=wine.corr()\n",
      "\n",
      "m = ~(corr.mask(np.eye(len(corr), dtype=bool)).abs() > 0.75).any()\n",
      "\n",
      "raw = corr.loc[m, m]\n",
      "print(raw)\n",
      "\n",
      "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\n",
      "to_drop = \"Flavanoids\"\n",
      "\n",
      "# Drop that column from the DataFrame\n",
      "wine = wine.drop(to_drop, axis=1)\n",
      "\n",
      "       >Selecting features using text vectors\n",
      "\n",
      "looking at word weights\n",
      "print(tfidf_vec.vocabulary_)\n",
      "\n",
      "word weight and index of te word\n",
      "\n",
      "3 is the third row\n",
      "get the data and the indices\n",
      "text_tfidf[3].data\n",
      "text_tfidf[3].indices\n",
      "\n",
      "vocab= {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
      "\n",
      "reverses the indice and the word\n",
      "\n",
      "combine the vocabulary with the word vectorized weights\n",
      "\n",
      "zipped_row=dict(zip(text_tfidf[3].indices,\n",
      "text_tfidf[3].data))\n",
      "\n",
      "for index,item in zipped_row.items():\n",
      "    print(vocab.get(index),item)\n",
      "\n",
      "now you can see word importance\n",
      "\n",
      "\n",
      "# Add in the rest of the parameters\n",
      "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
      "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
      "    \n",
      "    # Let's transform that zipped dict into a series\n",
      "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
      "    \n",
      "    # Let's sort the series to pull out the top n weighted words\n",
      "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
      "    return [original_vocab[i] for i in zipped_index]\n",
      "\n",
      "original_vocab=tfidf_vec.vocabulary_\n",
      "\n",
      "# Print out the weighted words\n",
      "print(return_weights(vocab,original_vocab, text_tfidf, 8, 3))\n",
      "\n",
      "\n",
      "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
      "    filter_list = []\n",
      "    for i in range(0, vector.shape[0]):\n",
      "    \n",
      "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
      "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
      "        filter_list.extend(filtered)\n",
      "    # Return the list in a set, so we don't get duplicate word indices\n",
      "    return set(filter_list)\n",
      "\n",
      "# Call the function to get the list of word indices\n",
      "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
      "\n",
      "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
      "filtered_text = text_tfidf[:, list(filtered_words)]\n",
      "\n",
      "\n",
      "# Split the dataset according to the class distribution of category_desc\n",
      "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
      "\n",
      "# Fit the model to the training data\n",
      "nb.fit(train_X, train_y)\n",
      "\n",
      "# Print out the model's accuracy\n",
      "print(nb.score(test_X, test_y))\n",
      "\n",
      "the set remove the duplicate word weights\n",
      "\n",
      "\n",
      "      >Dimension reduction\n",
      "\n",
      "Principal component analysis\n",
      "\n",
      "linear transformation to uncorrelated space\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "pca=PCA()\n",
      "df_pca=pca.fit_transform(df)\n",
      "\n",
      "print(df_pca)\n",
      "\n",
      "end of preprocessing journey\n",
      "\n",
      "\n",
      "    >sample    pca\n",
      "\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "# Set up PCA and the X vector for diminsionality reduction\n",
      "pca = PCA()\n",
      "wine_X = wine.drop(\"Type\", axis=1)\n",
      "\n",
      "# Apply PCA to the wine dataset X vector\n",
      "transformed_X = pca.fit_transform(wine_X)\n",
      "\n",
      "# Look at the percentage of variance explained by the different components\n",
      "print(pca.explained_variance_ratio_)\n",
      "\n",
      "\n",
      "   > sample    pca transform  > knn.fit and score\n",
      "\n",
      "\n",
      "# Split the transformed X and the y labels into training and test sets\n",
      "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X,y)\n",
      "\n",
      "# Fit knn to the training data\n",
      "knn.fit(X_wine_train,y_wine_train)\n",
      "\n",
      "# Score knn on the test data and print it out\n",
      "knn.score(X_wine_test,y_wine_test)\n",
      "\n",
      "\n",
      "    >UFO preprocessing\n",
      "\n",
      "\n",
      "missing data: dropna() and notnull()\n",
      "type: astype()\n",
      "\n",
      "train_test_split(X,y,stratify=y)\n",
      "\n",
      "\n",
      "   > sample change the data types for two features\n",
      "\n",
      "# Check the column types\n",
      "print(ufo.dtypes)\n",
      "\n",
      "# Change the type of seconds to float\n",
      "ufo[\"seconds\"] = ufo[\"seconds\"].astype(float)\n",
      "\n",
      "# Change the date column to type datetime\n",
      "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
      "\n",
      "# Check the column types\n",
      "print(ufo[[\"seconds\",\"date\"]].dtypes)\n",
      "\n",
      "  > sample   > find no missing\n",
      "\n",
      "# Check how many values are missing in the length_of_time, state, and type columns\n",
      "print(ufo[[\"length_of_time\", \"state\", \"type\"]].isnull().sum())\n",
      "\n",
      "# Keep only rows where length_of_time, state, and type are not null\n",
      "ufo_no_missing = ufo[ufo[\"length_of_time\"].notnull() & \n",
      "          ufo[\"state\"].notnull() & \n",
      "          ufo[\"type\"].notnull()]\n",
      "\n",
      "# Print out the shape of the new dataset\n",
      "print(ufo_no_missing.shape)\n",
      "\n",
      "output:\n",
      "length_of_time    143\n",
      "state             419\n",
      "type              159\n",
      "\n",
      "\n",
      "   > categorical variables\n",
      "\n",
      "pd.get_dummies\n",
      "\n",
      "standardization\n",
      "var()\n",
      "np.log()\n",
      "\n",
      "    sample    return minutes\n",
      "\n",
      "def return_minutes(time_string):\n",
      "\n",
      "    # Use \\d+ to grab digits\n",
      "    pattern = re.compile(r\"\\d+\")\n",
      "    \n",
      "    # Use match on the pattern and column\n",
      "    num = re.match(pattern, time_string)\n",
      "    if num is not None:\n",
      "        return int(num.group(0))\n",
      "        \n",
      "# Apply the extraction to the length_of_time column\n",
      "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(lambda x: return_minutes(x))\n",
      "\n",
      "# Take a look at the head of both of the columns\n",
      "print(ufo[[\"minutes\",\"length_of_time\"]].head())\n",
      "\n",
      "\n",
      "output:\n",
      "minutes   length_of_time\n",
      "2    None  about 5 minutes\n",
      "4    None       10 minutes\n",
      "7    None        2 minutes\n",
      "8    None        2 minutes\n",
      "9    None        5 minutes\n",
      "In [1]:\n",
      ";\n",
      "\n",
      "  > sample  > ufo var\n",
      "\n",
      "print(ufo.var())\n",
      "\n",
      "seconds    424087.417474\n",
      "lat            39.757593\n",
      "long          379.590986\n",
      "minutes       117.546372\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "# Check the variance of the seconds and minutes columns\n",
      "print(ufo.var())\n",
      "\n",
      "# Log normalize the seconds column\n",
      "ufo[\"seconds_log\"] = np.log(ufo[\"seconds\"])\n",
      "\n",
      "# Print out the variance of just the seconds_log column\n",
      "print(ufo[\"seconds_log\"].var())\n",
      "\n",
      "\n",
      "       Engineering new features\n",
      "\n",
      "Month of the sighting\n",
      "description and vectorize\n",
      "\n",
      ".month and .hour\n",
      "\n",
      "regex\n",
      "\n",
      "\n",
      "   >Sample     encode country and type\n",
      "\n",
      "# Use Pandas to encode us values as 1 and others as 0\n",
      "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda x: 1 if x==\"us\" else 0)\n",
      "\n",
      "# Print the number of unique type values\n",
      "print(len(ufo['type'].unique()))\n",
      "\n",
      "# Create a one-hot encoded set of the type values\n",
      "type_set = pd.get_dummies(ufo['type'])\n",
      "\n",
      "# Concatenate this set back to the ufo DataFrame\n",
      "ufo = pd.concat([ufo, type_set], axis=1)\n",
      "\n",
      "\n",
      "  > sample    set the month and year of the date\n",
      "\n",
      "# Look at the first 5 rows of the date column\n",
      "print(ufo['date'].head())\n",
      "\n",
      "# Extract the month from the date column\n",
      "ufo[\"month\"] = ufo[\"date\"].apply(lambda x: x.month)\n",
      "\n",
      "# Extract the year from the date column\n",
      "ufo[\"year\"] = ufo[\"date\"].apply(lambda x:x.year)\n",
      "\n",
      "# Take a look at the head of all three columns\n",
      "print(ufo[[\"date\",\"month\",\"year\"]].head())\n",
      "\n",
      "\n",
      "  Feature selection and modeling\n",
      "\n",
      "* redundant features\n",
      "* text vector\n",
      "* know your dataset\n",
      "\n",
      "\n",
      "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
      "print(ufo[['seconds','seconds_log','minutes']].corr())\n",
      "\n",
      "seconds      1.000000     0.853371  0.980341\n",
      "seconds_log  0.853371     1.000000  0.824493\n",
      "minutes      0.980341     0.824493  1.000000\n",
      "\n",
      "# Make a list of features to drop\n",
      "to_drop = ['seconds','minutes','city','country']\n",
      "\n",
      "# Drop those features\n",
      "ufo_dropped = ufo.drop(to_drop,axis=1)\n",
      "\n",
      "# Let's also filter some words out of the text vector we created\n",
      "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)\n",
      "\n",
      "   sample  one hot encoding\n",
      "The y labels are the encoded country column, where 1 is us and 0 is ca\n",
      "\n",
      "# Take a look at the features in the X set of data\n",
      "print(X.columns)\n",
      "print(y)\n",
      "# Split the X and y sets using train_test_split, setting stratify=y\n",
      "train_X, test_X, train_y, test_y = train_test_split(X,y)\n",
      "\n",
      "# Fit knn to the training sets\n",
      "knn.fit(train_X,train_y)\n",
      "\n",
      "# Print the score of knn on the test sets\n",
      "print(knn.score(test_X,test_y))\n",
      "\n",
      "   >sample  use a list of filter words\n",
      "\n",
      "# Use the list of filtered words we created to filter the text vector\n",
      "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
      "print(filtered_text)\n",
      "\n",
      "# Split the X and y sets using train_test_split, setting stratify=y \n",
      "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
      "\n",
      "# Fit nb to the training sets\n",
      "nb.fit(train_X,train_y)\n",
      "\n",
      "# Print the score of nb on the test sets\n",
      "print(nb.score(test_X,test_y))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pyspark and airflow and bash.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pyspark and airflow and bash.txt\n",
      "   >bash operator\n",
      "BashOperators are used to execute any bash commands that could be run in a bash shell.\n",
      "\n",
      "the task id is an unique identifier\n",
      "\n",
      "dag inherits from the base operator class\n",
      "\n",
      "bash_command is the bash commands to be operated as a string.\n",
      "\n",
      "bash_task=BashOperator(\n",
      "\ttask_id=\"greet_world\",\n",
      "\tdag=dag,\n",
      "\tbash_command='echo \"Hello,world!\"'\n",
      ")\n",
      "\n",
      "    Python Operator\n",
      "\n",
      "from airflow.operators.python_operator import PythonOperator\n",
      "from my_library import my_magic_function\n",
      "\n",
      "python_task = PythonOperator(\n",
      "\tdag=dag,\n",
      "\ttask_id='perform_magic',\n",
      "\tpython_callable=my_magic_function,\n",
      "\top_kwargs={\"snowflake\":\"*\",\"amount\":42}\n",
      ")\n",
      "\n",
      "      running pyspark with bash operator\n",
      "spark_master=(\n",
      "\t\"spark://\",\n",
      "\t\"spark_standalone_cluster_ip\"\n",
      "\t\":7077\")\n",
      "\n",
      "command=(\n",
      "\t\"spark-submit \"\n",
      "\t\"--master {master} \"\n",
      "\t\"--py-files package1.zip \"\n",
      "\t\"/path/to/app.py\"\n",
      ").format(master=spark_master)\n",
      "\n",
      "BashOperator(bash_command=command, _)\n",
      "\n",
      "  > running pyspark with SSHOperator\n",
      "\n",
      "from airflow.contrib.operators\\\n",
      "\t.ssh_operator import SSHOperator\n",
      "\n",
      "task=SSHOperator(\n",
      "\ttask_id=\"ssh_spark_submit\",\n",
      "\tdaq=daq,\n",
      "\tcommand=command,\n",
      "\tsssh_conn_id=\"spark_master_ssh\"\n",
      ")\n",
      "\n",
      "\n",
      "  > SparkSubmitOperator\n",
      "\n",
      "from airflow.contrib.operators \\\n",
      "\t.spark_submit_operator \\\n",
      "\timport SparkSubmitOperator\n",
      "\n",
      "spark_task=SparkSubmitOperator(\n",
      "\ttask_id='spark_submit_id',\n",
      "\tdag=dag,\n",
      "\tapplication='/path/toapp.py',\n",
      "\tpy_files='package1.zip',\n",
      "\tconn_id='spark_default'\n",
      ")\n",
      "\n",
      "     creating the Dag object\n",
      "\n",
      "Directed Acyclic Graph (DAG).\n",
      "\n",
      "When thinking about a workflow, you should think of individual tasks that can be executed independently. This is also a very resilient design, as each task could be retried multiple times if an error occurs.\n",
      "\n",
      "# Create a DAG object\n",
      "dag = DAG(\n",
      "  dag_id='optimize_diaper_purchases',\n",
      "  default_args={\n",
      "    # Don't email on failure\n",
      "    'email_on_failure': False,\n",
      "    # Specify when tasks should have started earliest\n",
      "    'start_date': datetime(2019, 6, 25)\n",
      "  },\n",
      "  # Run the DAG daily\n",
      "  schedule_interval='@daily')\n",
      "\n",
      "\n",
      "config = os.path.join(os.environ[\"AIRFLOW_HOME\"], \n",
      "                      \"scripts\",\n",
      "                      \"configs\", \n",
      "                      \"data_lake.conf\")\n",
      "\n",
      "ingest = BashOperator(\n",
      "  # Assign a descriptive id\n",
      "  task_id=\"ingest_data\", \n",
      "  # Complete the ingestion pipeline\n",
      "   bash_command='tap-marketing-api | target-csv --config %s' % config,\n",
      "  dag=dag)\n",
      "\n",
      "\n",
      "This bash operator will indeed call our ingestion pipeline. \n",
      "\n",
      "# Import the operator\n",
      "from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator\n",
      "\n",
      "# Set the path for our files.\n",
      "entry_point = os.path.join(os.environ[\"AIRFLOW_HOME\"], \"scripts\", \"clean_ratings.py\")\n",
      "dependency_path = os.path.join(os.environ[\"AIRFLOW_HOME\"], \"dependencies\", \"pydiaper.zip\")\n",
      "\n",
      "with DAG('data_pipeline', start_date=datetime(2019, 6, 25),\n",
      "         schedule_interval='@daily') as dag:\n",
      "  \t# Define task clean, running a cleaning job.\n",
      "    clean_data = SparkSubmitOperator(\n",
      "        application=entry_point, \n",
      "        py_files=dependency_path,\n",
      "        task_id='clean_data',\n",
      "        conn_id='spark_default')\n",
      "\n",
      "\n",
      "   >sample Python operator\n",
      "\n",
      "spark_args = {\"py_files\": dependency_path,\n",
      "              \"conn_id\": \"spark_default\"}\n",
      "# Define ingest, clean and transform job.\n",
      "with dag:\n",
      "    ingest = BashOperator(task_id='Ingest_data', bash_command='tap-marketing-api | target-csv --config %s' % config)\n",
      "    clean =  SparkSubmitOperator(application=clean_path, task_id='clean_data', **spark_args)\n",
      "    insight =  SparkSubmitOperator(application=transform_path, task_id='show_report', **spark_args)\n",
      "    \n",
      "    # set triggering sequence\n",
      "    ingest   clean   insight\n",
      "\n",
      "   > deploying airflow\n",
      "\n",
      "installing and configuring airflow\n",
      "\n",
      "install on linux\n",
      "\n",
      "export AIRFLOW_HOME=~/airflow\n",
      "\n",
      "pip install apache-airflow\n",
      "\n",
      "airflow initdb\n",
      "\n",
      "executor = SequentialExecutor\n",
      "\n",
      "airflow /\n",
      "\tconnections\n",
      "\tdags\n",
      "\tlogs\n",
      "\tplugins\n",
      "\tpools\n",
      "\tscript\n",
      "\ttests\n",
      "\tvariables\n",
      "\tairflow.cfg\n",
      "\treadme.md\n",
      "\trequirements.txt\n",
      "\tunittests.cfg\n",
      "\tunittests.db\n",
      "\n",
      "\n",
      "from airflow.models import DagBag\n",
      "\n",
      "def test_dagbag_import():\n",
      "\tdagbag=DagBag()\n",
      "\n",
      "\tnumber_of_failures = len(dagbag.import_errors)\n",
      "\tassert number_of_failures==0,\n",
      "\t\"There should be no DAG failures. Got: %s\" % dagbag.import_errors\n",
      "\n",
      "\n",
      " > how to get dags uploaded to the server\n",
      "\n",
      "dag.py to the airflow repo     copy unpacked artifact to the airflow server\n",
      "\n",
      "\n",
      "airflow.cfg\n",
      "\n",
      "[core]\n",
      "# The home folder for airflow, default is ~/airflow\n",
      "airflow_home = /home/repl/workspace/airflow\n",
      "\n",
      "# The folder where your airflow pipelines live, most likely a\n",
      "# subfolder in a code repository\n",
      "# This path must be absolute\n",
      "dags_folder = /home/repl/workspace/airflow/dags\n",
      "\n",
      "# The folder where airflow should store its log files\n",
      "# This path must be absolute\n",
      "base_log_folder = /home/repl/workspace/airflow/logs\n",
      "\n",
      "# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.\n",
      "# Users must supply an Airflow connection id that provides access to the storage\n",
      "# location. If remote_logging is set to true, see UPDATING.md for additional\n",
      "# configuration requirements.\n",
      "remote_logging = False\n",
      "remote_log_conn_id =\n",
      "remote_base_log_folder =\n",
      "encrypt_s3_logs = False\n",
      "\n",
      "# Logging level\n",
      "logging_level = INFO\n",
      "fab_logging_level = WARN\n",
      "\n",
      "# Logging class\n",
      "# Specify the class that will specify the logging configuration\n",
      "# This class has to be on the python classpath\n",
      "# logging_config_class = my.path.default_local_settings.LOGGING_CONFIG\n",
      "logging_config_class =\n",
      "\n",
      "# Log format\n",
      "log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s\n",
      "\n",
      "      Celery Executor\n",
      "\n",
      "The CeleryExecutor is a good choice for distributing tasks across multiple worker machines. \n",
      "\n",
      "\n",
      "     > Greetings DAG\n",
      "\n",
      "\n",
      "dag = DAG(\n",
      "    \"cleaning\",\n",
      "    default_args=default_args,\n",
      "    user_defined_macros={\"env\": Variable.get(\"environment\")},\n",
      "    schedule_interval=\"0 5 */2 * *\"\n",
      ")\n",
      "\n",
      "\n",
      "def say(what):\n",
      "    print(what)\n",
      "\n",
      "\n",
      "with dag:\n",
      "    say_hello = BashOperator(task_id=\"say-hello\", bash_command=\"echo Hello,\")\n",
      "    say_world = BashOperator(task_id=\"say-world\", bash_command=\"echo World\")\n",
      "    shout = PythonOperator(task_id=\"shout\",\n",
      "                           python_callable=say,\n",
      "                           op_kwargs={'what': '!'})\n",
      "\n",
      "    say_hello   say_world   shout\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pyspark building machine learning models.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pyspark building machine learning models.txt\n",
      "https://github.com/conwayyao/Recipe-Analysis/blob/master/CuisineAnalyzer/cuisinedata/indian.csv\n",
      "\n",
      "Indian food recipes (tangent)\n",
      "\n",
      "more data is the objective\n",
      "\n",
      "data is divided into partitions\n",
      "the partition can fit into ram\n",
      "spark does most processing in memory\n",
      "\n",
      "the cluster composed of one or more nodes.\n",
      "\n",
      "each node is a computer with ram and physical storage.\n",
      "\n",
      "a cluster manager allocates.\n",
      "\n",
      "resources and coordinates activity across the cluster. \n",
      "\n",
      "using the spark api the driver communicates with the cluster manager.\n",
      "\n",
      "on each node, spark launches and executor tasks application.  Work is divided into tasks which are units of computation.  tasks run multiple threads across the cores in a node.\n",
      "\n",
      "Interaction with spark can be written in java, scala, python, or r\n",
      "\n",
      "\n",
      "import pyspark\n",
      "\n",
      "pyspark.__version__\n",
      "'2.4.1'\n",
      "\n",
      "Structured Data - pyspark.sql\n",
      "Streaming Data - pyspark.streaming\n",
      "Machine Learning - pyspark.ml\n",
      "\n",
      "connect to spark\n",
      "\n",
      "remote cluster\n",
      "spark://<IP address | DNS name>:<port>\n",
      "spark://13.59.151.161:7077\n",
      "\n",
      "7077 is the default port\n",
      "\n",
      "Local cluster:\n",
      "local\n",
      "local[4]\n",
      "local[*]\n",
      "\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark= SparkSession.builder\n",
      "\t.master('local[*]')\n",
      "\t.appName('first_spark_application')\n",
      "\t.getOrCreate()\n",
      "\n",
      "\n",
      "\n",
      "spark.stop()\n",
      "\n",
      "\n",
      "  sample create an spark session\n",
      "\n",
      "# Import the PySpark module\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "# Create SparkSession object\n",
      "spark = SparkSession.builder \\\n",
      "                    .master('local[*]') \\\n",
      "                    .appName('test') \\\n",
      "                    .getOrCreate()\n",
      "\n",
      "# What version of Spark?\n",
      "# (Might be different to what you saw in the presentation!)\n",
      "print(spark.version)\n",
      "\n",
      "# Terminate the cluster\n",
      "spark.stop()\n",
      "\n",
      "     >Loading data into a dataframe\n",
      "1. count() #number of rows\n",
      "2. show()\n",
      "3. printSchema()\n",
      "4. dtypes\n",
      "\n",
      "cars.csv\n",
      "1. mfr\n",
      "2. mod\n",
      "3. org\n",
      "4. type\n",
      "5. cyl\n",
      "6. size\n",
      "7. weight\n",
      "8. len\n",
      "9. rpm\n",
      "10. cons\n",
      "\n",
      "spark.read.csv parameters:\n",
      "\n",
      "1. Header=True #tells if the first row is a header row\n",
      "2. sep=','\n",
      "3. schema - explicit column data types\n",
      "4. inferSchema - deduced column data types from data (two passes over the data to infer the data types of the columns)\n",
      "5. nullValue - placeholder for missing data\n",
      "\n",
      "\n",
      "cars.printSchema()\n",
      "\n",
      "\n",
      "read.csv treats all columns as strings by default\n",
      "\n",
      "cars=spark.read.csv('cars.csv',header=True, inferSchema=True, nullValue='NA')\n",
      "\n",
      "\n",
      "schema = StructType)[\n",
      "\tStructField(\"maker\",StringType()),\n",
      "\tStructField(\"cyl\",IntegerType()),\n",
      "\tStructField(\"size\",DoubleType())\n",
      "])\n",
      "\n",
      "cars=spark.read.csv('cars.csv',header=True, schema=schema, nullValue='NA')\n",
      "\n",
      "\n",
      "  > read.csv\n",
      "\n",
      "# Read data from CSV file\n",
      "flights = spark.read.csv('flights.csv',\n",
      "                         sep=',',\n",
      "                         header=True,\n",
      "                         inferSchema=True,\n",
      "                         nullValue='NA')\n",
      "\n",
      "# Get number of records\n",
      "print(\"The data contain %d records.\" % flights.count())\n",
      "\n",
      "# View the first five records\n",
      "flights.show(5)\n",
      "\n",
      "# Check column data types\n",
      "print(flights.printSchema())\n",
      "print(flights.dtype)\n",
      "\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n",
      "\n",
      "  >sample  > create a schema for the read.csv\n",
      "\n",
      "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
      "\n",
      "# Specify column names and types\n",
      "schema = StructType([\n",
      "    StructField(\"id\", IntegerType()),\n",
      "    StructField(\"text\", StringType()),\n",
      "    StructField(\"label\", IntegerType())\n",
      "])\n",
      "\n",
      "# Load data from a delimited file\n",
      "sms = spark.read.csv('sms.csv', sep=';', header=True, schema=schema)\n",
      "\n",
      "# Print schema of DataFrame\n",
      "sms.printSchema()\n",
      "\n",
      "\n",
      "       Data preparation     >\n",
      "\n",
      "cars.csv:\n",
      "\n",
      "maker\n",
      "model\n",
      "origin\n",
      "type\n",
      "cyl\n",
      "size\n",
      "weight\n",
      "length\n",
      "rpm\n",
      "consumption\n",
      "\n",
      "cars=cars.drop('maker','model')\n",
      "cars=cars.select('origin, 'type','cyl','size','weight','length','rpm','consumption')\n",
      "\n",
      "\n",
      "cars.filter('cyl IS NULL').count()\n",
      "\n",
      "or \n",
      "\n",
      "drop all records with missing values in any column\n",
      "\n",
      "cars=cars.dropna()\n",
      "\n",
      "\n",
      "from pyspark.sql.functions import round\n",
      "\n",
      "#kilograms conversion\n",
      "cars = cars.withColumn('mass',round(cars.weight/2.205,0))\n",
      "\n",
      "#meters conversion\n",
      "cars= cars.withColumn('length',round(cars.length * 0.0254,3))\n",
      "\n",
      "   Indexing categorical data\n",
      "\n",
      "from pyspark.ml.feature import StringIndexer\n",
      "\n",
      "indexer=StringIndexer(inputCol='type',\n",
      "\t\t\toutputCol='type_idx')\n",
      "\n",
      "indexer=indexer.fit(cars)\n",
      "\n",
      "cars=indexer.transform(cars)\n",
      "\n",
      "use StringOrderType to change order\n",
      "\n",
      "cars=StringIndexer(inputCol=\"origin\", outputCol=\"label\").fit(cars).transform(cars)\n",
      "\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "\n",
      "assembler=VectorAssembler(inputCols=['cyl','size'], outputCol='features')\n",
      "\n",
      "assembler.transform(cars)\n",
      "\n",
      "All the features are assembled into a single column\n",
      "\n",
      "\n",
      "     sample     determine delay vs missing flights\n",
      "\n",
      "# Remove the 'flight' column\n",
      "flights_drop_column = flights.drop('flight')\n",
      "\n",
      "# Number of records with missing 'delay' values\n",
      "flights_drop_column.filter('delay IS NULL').count()\n",
      "\n",
      "# Remove records with missing 'delay' values\n",
      "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
      "\n",
      "# Remove records with missing values in any column and get the number of remaining rows\n",
      "flights_none_missing = flights_valid_delay.dropna()\n",
      "print(flights_none_missing.count())\n",
      "\n",
      "\n",
      "   sample  > create a new column and drop the old column\n",
      "\n",
      "# Import the required function\n",
      "from pyspark.sql.functions import round\n",
      "\n",
      "# Convert 'mile' to 'km' and drop 'mile' column\n",
      "flights_km = flights.withColumn('km', round(flights.mile * 1.60934, 0)) \\\n",
      "                    .drop('mile')\n",
      "\n",
      "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
      "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
      "\n",
      "# Check first five records\n",
      "flights_km.show(5)\n",
      "\n",
      "\n",
      "    sample  > transforming categorical string data\n",
      "\n",
      "from pyspark.ml.feature import StringIndexer\n",
      "\n",
      "# Create an indexer\n",
      "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
      "\n",
      "# Indexer identifies categories in the data\n",
      "indexer_model = indexer.fit(flights)\n",
      "\n",
      "# Indexer creates a new column with numeric index values\n",
      "flights_indexed = indexer_model.transform(flights)\n",
      "\n",
      "# Repeat the process for the other categorical feature\n",
      "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
      "\n",
      "\n",
      "   create the vector assembler\n",
      "\n",
      "# Import the necessary class\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "\n",
      "# Create an assembler object\n",
      "assembler = VectorAssembler(inputCols=[\n",
      "'mon', 'dom', 'dow',\n",
      "'carrier_idx',\n",
      "'org_idx', \n",
      "'km',\n",
      "'depart',\n",
      "'duration'\n",
      "], outputCol='features')\n",
      "\n",
      "# Consolidate predictor columns\n",
      "flights_assembled = assembler.transform(flights)\n",
      "\n",
      "# Check the resulting column\n",
      "flights_assembled.select('features', 'delay').show(5, truncate=False)\n",
      "\n",
      "\n",
      "        >Decision tree\n",
      "\n",
      "recursive partition\n",
      "\n",
      "first split is a dominate class and the non dominate class\n",
      "\n",
      "the depth of the tree along a branch need not be the same\n",
      "\n",
      "\n",
      "0 for usa manufactured cars\n",
      "1 for manufactured elsewhere\n",
      "\n",
      "called label\n",
      "\n",
      "cars_train, cars_test=cars.randomSplit([.8,.2], seed=23)\n",
      "\n",
      "cars_train.count()\n",
      "cars_test.count()\n",
      "\n",
      "\n",
      "from pyspark.ml.classification import DecisionTreeClassifier\n",
      "\n",
      "tree=DecisionTreeClassifier()\n",
      "\n",
      "\n",
      "tree.fit(cars_train)\n",
      "\n",
      "prediction=tree_model.transform(cars_test)\n",
      "1. label\n",
      "2. prediction\n",
      "3. probability\n",
      "\n",
      "   > create the confusion matrix\n",
      "\n",
      "prediction.groupBy(\"label\",\"prediction\").count().show()\n",
      "\n",
      "True positive\n",
      "False positive\n",
      "False negative\n",
      "True negative\n",
      "\n",
      "Accuracy=(TN+TP)/(TN+TP+FN+FP)\n",
      "\n",
      "  sample  > train test split\n",
      "\n",
      "# Split into training and testing sets in a 80:20 ratio\n",
      "flights_train, flights_test = flights.randomSplit([0.8,0.2],seed=17)\n",
      "\n",
      "# Check that training set has around 80% of records\n",
      "training_ratio = flights_train.count() / flights_test.count()\n",
      "print(training_ratio)\n",
      "\n",
      "\n",
      "   >sample  > Decision Tree Classifier\n",
      "\n",
      "# Import the Decision Tree Classifier class\n",
      "from pyspark.ml.classification import DecisionTreeClassifier\n",
      "\n",
      "# Create a classifier object and fit to the training data\n",
      "tree =DecisionTreeClassifier()\n",
      "tree_model = tree.fit(flights_train)\n",
      "\n",
      "# Create predictions for the testing data and take a look at the predictions\n",
      "prediction = tree_model.transform(flights_test)\n",
      "prediction.select('label', 'prediction', 'probability').show(5, False)\n",
      "\n",
      "\n",
      "  > sample  > create a confusion matrix\n",
      "\n",
      "# Create a confusion matrix\n",
      "prediction.groupBy(\"label\", 'prediction').count().show()\n",
      "\n",
      "# Calculate the elements of the confusion matrix\n",
      "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
      "TP = prediction.filter('prediction=1 AND label=prediction').count()\n",
      "FN = prediction.filter('prediction=0 AND label!=prediction').count()\n",
      "FP = prediction.filter('prediction=1 AND label!=prediction').count()\n",
      "\n",
      "# Accuracy measures the proportion of correct predictions\n",
      "accuracy = (TP+TN)/(TN+TP+FN+FP)\n",
      "print(accuracy)\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<logistic regression\n",
      "\n",
      "logistic curve models 0 or 1\n",
      "\n",
      "threshhold above .5 then predictive state is 1 else 0\n",
      "\n",
      "the curve can be more steep or more gradual or shift left or right\n",
      "\n",
      "\n",
      "from pyspark.ml.classification import LogisticRegression\n",
      "\n",
      "\n",
      "logistic=LogisticRegression()\n",
      "\n",
      "logistic = logistic.fit(cars_train)\n",
      "\n",
      "prediction=logistics.transform(car_test)\n",
      "\n",
      "precision=  tp/ (tp+fp)\n",
      "\n",
      "recall= tp/(tp+fn)\n",
      "\n",
      "from pyspark.ml.evaluation import MulticlassClassficationEvaluator\n",
      "\n",
      "evaluator=MulticlassClassificationEvaluator()\n",
      "evaluator.evaluate(prediction,{evaluator.metricName:'weightedPrecision'})\n",
      "\n",
      "1. weightedRecall\n",
      "2. accuracy\n",
      "3. f1\n",
      "\n",
      "roc and auc\n",
      "plots the true positive rate by the false positive rate\n",
      "\n",
      "  sample   > predict using logistic regression\n",
      "\n",
      "# Import the logistic regression class\n",
      "from pyspark.ml.classification import LogisticRegression\n",
      "\n",
      "\n",
      "# Create a classifier object and train on training data\n",
      "logistic = LogisticRegression().fit(flights_train)\n",
      "\n",
      "# Create predictions for the testing data and show confusion matrix\n",
      "prediction = logistic.transform(flights_test)\n",
      "prediction.groupBy(\"label\", \"prediction\").count().show()\n",
      "\n",
      "   sample  > get the roc auc value\n",
      "\n",
      "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
      "\n",
      "# Calculate the elements of the confusion matrix\n",
      "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
      "TP = prediction.filter('prediction=1 AND label=prediction').count()\n",
      "FN = prediction.filter('prediction=0 AND label!=prediction').count()\n",
      "FP = prediction.filter('prediction=1 AND label!=prediction').count()\n",
      "\n",
      "# Calculate precision and recall\n",
      "precision = TP/(TP+FP)\n",
      "recall = TP/(TP+FN)\n",
      "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
      "\n",
      "# Find weighted precision\n",
      "multi_evaluator = MulticlassClassificationEvaluator()\n",
      "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
      "\n",
      "# Find AUC\n",
      "binary_evaluator = BinaryClassificationEvaluator()\n",
      "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName:'areaUnderROC'})\n",
      "\n",
      "print(auc) \n",
      "\n",
      "       Turning text into tables\n",
      "\n",
      "80% of machine learning is data preparation\n",
      "\n",
      "one record per document.\n",
      "\n",
      "collection of documents\n",
      "\n",
      "tokenize the documents as columns in the table\n",
      "\n",
      "remove the stop words\n",
      "\n",
      "The table indicates the frequency of the word\n",
      "\n",
      "the table is known as a term document matrix\n",
      "\n",
      "remove punctuation\n",
      "\n",
      "      regular expressions\n",
      "\n",
      "from pyspark.sql.functions import regexp_replace\n",
      "\n",
      "REGEX='[,\\\\-]'\n",
      "\n",
      "escape the -\n",
      "\n",
      "books=books.withColumn('text',regexp_replace(books.text, REGEX,' '))\n",
      "\n",
      "\n",
      "books= Tokenizer(inputCol=\"text\", outputCol=\"tokens\").transform(books)\n",
      "\n",
      "stop words are common words adding very little information\n",
      "\n",
      "from pyspark.ml.feature import StopWordsRemover\n",
      "\n",
      "stopwords=StopWordsRemover()\n",
      "\n",
      "stopwords.getStopWords()\n",
      "\n",
      "stopwords=stopwords.setInputCol('tokens').setOutputCol('words')\n",
      "\n",
      "books=stopwords.transform(books)\n",
      "\n",
      "from pyspark.ml.feature import HashingTF\n",
      "\n",
      "hasher=HashingTF(inputCol=\"words\", outputCol=\"hash\", numFeatures=32)\n",
      "\n",
      "books=hasher.transform(books)\n",
      "\n",
      "from pyspark.ml.feature import IDF\n",
      "\n",
      "books=IDF(inputCol=\"hash\", outputCol=\"features\").fit(books).transform(books)\n",
      "\n",
      "IDF measure the frequency of the word across all documents\n",
      "\n",
      "inverse document frequency\n",
      "\n",
      "    sample  > Tokenize text\n",
      "\n",
      "# Import the necessary functions\n",
      "from pyspark.sql.functions import regexp_replace\n",
      "from pyspark.ml.feature import Tokenizer\n",
      "\n",
      "# Remove punctuation (REGEX provided) and numbers\n",
      "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
      "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
      "\n",
      "# Merge multiple spaces\n",
      "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
      "\n",
      "# Split the text into words\n",
      "wrangled = Tokenizer(inputCol='text', outputCol=\"words\").transform(wrangled)\n",
      "\n",
      "wrangled.show(4, truncate=False)\n",
      "\n",
      "\n",
      " > sample  > building the [[idf features]]\n",
      "\n",
      "from pyspark.ml.feature import StopWordsRemover,HashingTF,IDF\n",
      "\n",
      "# Remove stop words.\n",
      "wrangled = StopWordsRemover(inputCol=\"words\", outputCol=\"terms\")\\\n",
      "      .transform(sms)\n",
      "\n",
      "# Apply the hashing trick\n",
      "wrangled = HashingTF(inputCol=\"terms\", outputCol=\"hash\", numFeatures=1024)\\\n",
      "      .transform(wrangled)\n",
      "\n",
      "# Convert hashed symbols to TF-IDF\n",
      "tf_idf = IDF(inputCol=\"hash\", outputCol=\"features\")\\\n",
      "      .fit(wrangled).transform(wrangled)\n",
      "      \n",
      "tf_idf.select('terms', 'features').show(4, truncate=False)\n",
      "\n",
      "  > sample  > logistic regression prediction\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "sms_train, sms_test = sms.randomSplit([0.8,0.2], seed=13)\n",
      "\n",
      "# Fit a Logistic Regression model to the training data\n",
      "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
      "\n",
      "# Make predictions on the testing data\n",
      "prediction = logistic.transform(sms_test)\n",
      "\n",
      "# Create a confusion matrix, comparing predictions to known labels\n",
      "prediction.groupBy(\"label\",\"prediction\").count().show()\n",
      "\n",
      "\n",
      "\n",
      "         One-Hot Encoding\n",
      "\n",
      "categorical data\n",
      "\n",
      "create a column for each of the categorical levels\n",
      "\n",
      "dummy variables\n",
      "\n",
      "the sparse form records the column number and the value 1 for the categorical data\n",
      "\n",
      "the process of creating dummy variables is called one hot encoding because only one column is active or hot\n",
      "\n",
      "from pyspark.ml.feature import OneHotEncoderEstimator\n",
      "\n",
      "\n",
      "indexer=StringIndexer(inputCol='type',\n",
      "\t\t\toutputCol='type_idx')\n",
      "\n",
      "indexer=indexer.fit(cars)\n",
      "\n",
      "cars=indexer.transform(cars)\n",
      "\n",
      "onehot = OneHotEncoderEstimator(inputCols=['type_idx'], outputCols=['type_dummy']\n",
      "\n",
      "onehot=onehot.fit(cars)\n",
      "\n",
      "onehot.categorySizes\n",
      "\n",
      "cars=onehot.transform(cars)\n",
      "\n",
      "cars.select('type,'type_idx','type_dummy').distinct().sort('type_idx').show\n",
      "\n",
      "\n",
      "DenseVector([1,0,0,0,0,7,0,0])\n",
      "represented as\n",
      "SparseVector(8,[0,5],[1,7])\n",
      "\n",
      "\n",
      "8 items, non zero in position 0 and 5 with values 1 and 7\n",
      "\n",
      "   >sample  OneHotEncoderEstimator\n",
      "\n",
      "# Import the one hot encoder class\n",
      "from pyspark.ml.feature import OneHotEncoderEstimator\n",
      "\n",
      "# Create an instance of the one hot encoder\n",
      "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
      "\n",
      "# Apply the one hot encoder to the flights data\n",
      "onehot = onehot.fit(flights)\n",
      "flights_onehot = onehot.transform(flights)\n",
      "\n",
      "# Check the results\n",
      "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()\n",
      "\n",
      "\n",
      "          Regression\n",
      "\n",
      "scatter plot to visualize consumption verus mass\n",
      "\n",
      "\n",
      "residuals are the difference between the observed value and the corresponding value.  Vertical distance between the points and the model line\n",
      "\n",
      "The best model is found by minimizing a loss function\n",
      "\n",
      "Mean squared error 1/N (yi-y^i)**2\n",
      "\n",
      "yi=observed values\n",
      "y^i=model values\n",
      "\n",
      "predict consumption using mass, cyl, and type_dummy\n",
      "\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "\n",
      "regression = LinearRegression(labelCol=\"consumption\")\n",
      "\n",
      "regression=regression.fit(cars_train)\n",
      "\n",
      "predictions = regression.transform(cars_test)\n",
      "\n",
      "\n",
      "        >RegressionEvaluator\n",
      "\n",
      "from pyspark.ml.evalution import RegressionEvaluator\n",
      "\n",
      "RegressionEvaluator(labelCol='consumption').evalulate(predictions)\n",
      "\n",
      "the square root of the mean square error is the RMSE\n",
      "\n",
      "RMSE is the standard deviation of the residuals\n",
      "\n",
      "RegressionEvaluator\n",
      "1. mae (mean absolute error)\n",
      "2. r2\n",
      "3. mse (mean squared error)\n",
      "\n",
      "\n",
      "regression.intercept\n",
      "\n",
      "slope with each mass and consumption combination\n",
      "slope indicates how rapidly the model changes when mass and consumption change\n",
      "\n",
      "regression.coefficients\n",
      "\n",
      "there is a coefficient for each of the predictors\n",
      "mass\n",
      "cyl\n",
      "midsize\n",
      "small\n",
      "compact\n",
      "sporty\n",
      "large\n",
      "\n",
      "DenseVector([.0027,.1897,-1.309,-1.7933,-1.3594,-1.2917,-1.9693])\n",
      "\n",
      "\n",
      "mass=.0027\n",
      "cyl=.1897\n",
      "\n",
      "midsize=-1.3\n",
      "small=-1.79\n",
      "compact=-1.35\n",
      "sporty=-1.29\n",
      "large=-1.9\n",
      "\n",
      "large vehicles are the most fuel efficient for their mass.\n",
      "\n",
      "all other types consume less fuel than a large vehicle\n",
      "\n",
      "   sample build a regressor\n",
      "\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "from pyspark.ml.evaluation import RegressionEvaluator\n",
      "\n",
      "# Create a regression object and train on training data\n",
      "regression =LinearRegression(labelCol=\"duration\").fit(flights_train)\n",
      "\n",
      "# Create predictions for the testing data and take a look at the predictions\n",
      "predictions = regression.transform(flights_test)\n",
      "predictions.select('duration', 'prediction').show(5, False)\n",
      "\n",
      "# Calculate the RMSE\n",
      "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
      "\n",
      "\n",
      "   sample  > making sense of the coefficients\n",
      "\n",
      "# Intercept (average minutes on ground)\n",
      "inter = regression.intercept\n",
      "print(inter)\n",
      "\n",
      "# Coefficients\n",
      "coefs = regression.coefficients\n",
      "print(coefs)\n",
      "\n",
      "# Average minutes per km\n",
      "minutes_per_km = regression.coefficients[0]\n",
      "print(minutes_per_km)\n",
      "\n",
      "# Average speed in km per hour\n",
      "avg_speed = 60 / minutes_per_km\n",
      "print(avg_speed)\n",
      "\n",
      "\n",
      "regression\n",
      "duration= intercept + coefficient * distance\n",
      "\n",
      " > sample\n",
      "\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "from pyspark.ml.evaluation import RegressionEvaluator\n",
      "\n",
      "# Create a regression object and train on training data\n",
      "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
      "\n",
      "# Create predictions for the testing data\n",
      "predictions = regression.transform(flights_test)\n",
      "\n",
      "# Calculate the RMSE on testing data\n",
      "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
      "\n",
      "\n",
      "         intrepreting coefficients\n",
      "\n",
      "The coefficients attribute is a list, where the first element indicates how flight duration changes with flight distance\n",
      "\n",
      "0 â€” km\n",
      "1 â€” ORD\n",
      "2 â€” SFO\n",
      "3 â€” JFK\n",
      "4 â€” LGA\n",
      "5 â€” SMF\n",
      "6 â€” SJC and\n",
      "7 â€” TUS.\n",
      "\n",
      "# Average speed in km per hour\n",
      "avg_speed_hour = 60/regression.coefficients[0]\n",
      "print(avg_speed_hour)\n",
      "\n",
      "# Average minutes on ground at OGG\n",
      "inter = regression.intercept\n",
      "print(inter)\n",
      "\n",
      "# Average minutes on ground at JFK\n",
      "avg_ground_jfk = regression.intercept + regression.coefficients[3]\n",
      "print(avg_ground_jfk)\n",
      "\n",
      "# Average minutes on ground at LGA\n",
      "avg_ground_lga = regression.intercept + regression.coefficients[4]\n",
      "print(avg_ground_lga)\n",
      "\n",
      "\n",
      "output:\n",
      "807.3336599681242\n",
      "15.856628374450773\n",
      "68.53550999587868\n",
      "62.56747182033072\n",
      "\n",
      "intercept\n",
      "15.856628374450773\n",
      "\n",
      "coefficients\n",
      "[0.07431871477075411,28.399568722791717,20.55190513998231,52.678881621427905,46.710843445879945,18.28741662016716,15.721837765620768,17.737941505895947]\n",
      "\n",
      "\n",
      "        carefully manipulating features\n",
      "\n",
      "bucketing:  assigning features to buckets or bins with well defined boundaries\n",
      "\n",
      "heights in meters\n",
      "\n",
      "defined as short, average, tall\n",
      "\n",
      "from pyspark.ml.feature import Bucketizer\n",
      "\n",
      "bucketizer=Bucketizer(splits=[3500,4500,6000,6500],\n",
      "\tinputCol=\"rpm\",\n",
      "\toutputCol=\"rpm_bin\")\n",
      "\n",
      "cars=bucketizer.transform(cars)\n",
      "\n",
      "bucketed.select('rpm','rpm_bin').show(5)\n",
      "\n",
      "cars.groupBy('rpm_bin).count().show()\n",
      "\n",
      "low, medium, high [no sparse array]\n",
      "\n",
      "\n",
      "regression.coefficients\n",
      "[1.3814,0.1433])\n",
      "regression.intercept\n",
      "8.1835\n",
      "\n",
      "low RPM\n",
      "consumption=8.1835+1.3814\n",
      "\n",
      "medium RPM\n",
      "consumption=8.1835+0.1433\n",
      "\n",
      "operations on a single column\n",
      "\n",
      "log()\n",
      "sqrt()\n",
      "pow()\n",
      "\n",
      "operations on two columns\n",
      "product\n",
      "ratio\n",
      "\n",
      "\n",
      "\n",
      "bmi = mass / height**2\n",
      "\n",
      "cars = cars.withColumn('density_line', cars.mass/cars.length)\n",
      "\n",
      "cars = cars.withColumn('density_quad', cars.mass/cars.length**2)\n",
      "\n",
      "cars = cars.withColumn('density_cube', cars.mass/cars.length**3)\n",
      "\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator\n",
      "\n",
      "# Create buckets at 3 hour intervals through the day\n",
      "buckets =Bucketizer(splits=[0,3,6,9,12,15,18,21,24], inputCol='depart', outputCol='depart_bucket')\n",
      "\n",
      "# Bucket the departure times\n",
      "bucketed = buckets.transform(flights)\n",
      "bucketed.select('depart','depart_bucket').show(5)\n",
      "\n",
      "# Create a one-hot encoder\n",
      "onehot = OneHotEncoderEstimator(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
      "\n",
      "# One-hot encode the bucketed departure times\n",
      "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
      "flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').show(5)\n",
      "\n",
      "\n",
      "\n",
      "   adding departure time\n",
      "\n",
      "# Find the RMSE on testing data\n",
      "from pyspark.ml.evaluation import RegressionEvaluator\n",
      "\n",
      "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
      "\n",
      "# Average minutes on ground at OGG for flights departing between 00:00 and 03:00\n",
      "avg_night_ogg = regression.intercept + regression.coefficients[8]\n",
      "print(avg_night_ogg)\n",
      "\n",
      "# Average minutes on ground at JFK for flights departing between 00:00 and 03:00\n",
      "avg_night_jfk = regression.intercept + regression.coefficients[8] + regression.coefficients[3]\n",
      "print(avg_night_jfk)\n",
      "\n",
      "\n",
      "        Regularization\n",
      "\n",
      "\n",
      "penalized regression: model is punished for having too many coefficients.\n",
      "\n",
      "mse chooses coefficients that minimize the loss or the residual.\n",
      "\n",
      "regularization term\n",
      "Lasso - absolute value of the coefficients\n",
      "Ridge - square of the coefficients\n",
      "\n",
      "both will shrink the coefficients of non contributing coefficients\n",
      "lasso moves those coefficients to 0\n",
      "\n",
      "strength of regularization is determined by parameter alpha\n",
      "\n",
      "alpha=0 - no regularization (standard regression)\n",
      "alpha=infinity - complete regularation (all coefficients zero)\n",
      "\n",
      "\n",
      "assembler = VectorAssembler(inputCols=[\n",
      "\t'mass','cyl','type_dummy','density_line','density_quad','density_cube'], outputCol='features')\n",
      "\n",
      "cars=assembler.transform(cars)\n",
      "\n",
      "regression = LinearRegression(labelCol='consumption').fit(cars_train)\n",
      "\n",
      "regression.coefficients\n",
      "\n",
      "DenseVector([-0.012,0.174,-0.897,-1.445,-0.985,-1.071,-1.335,0.189,-0.780,1.160])\n",
      "\n",
      "every predictor is contributing to the model\n",
      "however it is unlike that all the feature are equally important in predicting consumption\n",
      "\n",
      "ridge= LinearRegression(labelCol='consumption', elasticNetParam=0, regParam=0.1)\n",
      "ridge.fit(cars_train)\n",
      "\n",
      "#RMSE\n",
      "0.72453\n",
      "\n",
      "\n",
      "lasso = LinearRegression(labelCol='consumption', elasticNetParam=1, regParam=0.1)\n",
      "lasso.fit(cars_train)\n",
      "\n",
      "\n",
      "DenseVector[0,0,0,-.056,0,0,0,0.026,0,0])\n",
      "\n",
      "all but two features are 0\n",
      "\n",
      "small type car and the linear density\n",
      "\n",
      "\n",
      "  features\n",
      "\n",
      "km\n",
      "org (origin airport, one-hot encoded, 8 levels)\n",
      "depart (departure time, binned in 3 hour intervals, one-hot encoded, 8 levels)\n",
      "dow (departure day of week, one-hot encoded, 7 levels) and\n",
      "mon (departure month, one-hot encoded, 12 levels).\n",
      "\n",
      "\n",
      "   >Sample    > create a linear Regression and a Regression Evaluator\n",
      "\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "from pyspark.ml.evaluation import RegressionEvaluator\n",
      "\n",
      "\n",
      "# Fit linear regression model to training data\n",
      "regression = LinearRegression(labelCol=\"duration\").fit(flights_train)\n",
      "\n",
      "# Make predictions on testing data\n",
      "predictions = regression.transform(flights_test)\n",
      "\n",
      "# Calculate the RMSE on testing data\n",
      "rmse = RegressionEvaluator(labelCol=\"duration\").evaluate(predictions)\n",
      "print(\"The test RMSE is\", rmse)\n",
      "\n",
      "# Look at the model coefficients\n",
      "coeffs = regression.coefficients\n",
      "print(coeffs)\n",
      "\n",
      "\n",
      "\n",
      "   > sample    lasso\n",
      "\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "from pyspark.ml.evaluation import RegressionEvaluator\n",
      "\n",
      "# Fit Lasso model (a = 1) to training data\n",
      "regression = LinearRegression(labelCol='duration', regParam=1, elasticNetParam=1).fit(flights_train)\n",
      "\n",
      "# Calculate the RMSE on testing data\n",
      "rmse = RegressionEvaluator(labelCol='duration').evaluate(regression.transform(flights_test))\n",
      "print(\"The test RMSE is\", rmse)\n",
      "\n",
      "# Look at the model coefficients\n",
      "coeffs = regression.coefficients\n",
      "print(coeffs)\n",
      "\n",
      "# Number of zero coefficients\n",
      "zero_coeff = sum([beta == 0 for beta in regression.coefficients])\n",
      "print(\"Number of coefficients equal to 0:\", zero_coeff)\n",
      "\n",
      "output\n",
      "\n",
      "The test RMSE is 11.221618112066176\n",
      "[0.07326284332459325,0.26927242574175647,-4.213823507520847,23.31411303902282,16.924833465407964,-7.538366699625629,-5.04321753247765,-20.348693139176927,0.0,0.0,0.0,0.0,0.0,1.199161974782719,0.43548357163388335,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "Number of coefficients equal to 0: 22\n",
      "\n",
      "\n",
      "         >Pipeline\n",
      "\n",
      "make it better\n",
      "\n",
      "leakage occurrs when fit is applied to any testing data\n",
      "a pipeline combines a series of steps\n",
      "\n",
      "\n",
      "manual sequence of steps\n",
      "\n",
      "#one hot encode the type categorical data\n",
      "indexer=StringIndexer(inputCol='type', outputCol='type_idx')\n",
      "\n",
      "onehot = OneHotEncoderEstimator(inputCols=['type_idx'), outputCols=['type_dummy'])\n",
      "\n",
      "#create a single features column\n",
      "\n",
      "assemble=VectorAssembler(inputCols=['mass','cyl','type_dummy'],outputCol='features')\n",
      "\n",
      "#build the regression model\n",
      "\n",
      "regression=LinearRegression(labelCol='consumption')\n",
      "\n",
      "indexer=indexer.fit(cars_train)\n",
      "cars_train=indexer.transform(cars_train)\n",
      "cars_test=indexer.transform(cars_test)\n",
      "\n",
      "cars_train=onehot.transform(cars_train)\n",
      "cars_tst=onehot.transform(cars_test)\n",
      "\n",
      "cars_train=assemble.transform(cars_train)\n",
      "cars_test=assemble.transform(cars_test)\n",
      "\n",
      "regression=regression.fit(cars_train)\n",
      "\n",
      "predictions=regression.transform(cars_test)\n",
      "\n",
      "\n",
      "from pyspark.ml import Pipeline\n",
      "\n",
      "#sequence of stages\n",
      "pipeline= Pipeline(stages=[indexer, onehot, assemble, regression])\n",
      "\n",
      "pipeline=pipeline.fit(cars_train)\n",
      "\n",
      "predictions=pipeline.transform(cars_test)\n",
      "\n",
      "pipeline.stages\n",
      "pipeline.stages[3]\n",
      "\n",
      "pipeline.stages[3].intercept\n",
      "pipeline.stages[3].coefficients\n",
      "\n",
      "\n",
      "      >sample   > setup for the pipeline\n",
      "\n",
      "# Convert categorical strings to index values\n",
      "indexer =StringIndexer(inputCol='org',outputCol='org_idx')\n",
      "\n",
      "# One-hot encode index values\n",
      "onehot =OneHotEncoderEstimator(\n",
      "    inputCols=['org_idx','dow'],\n",
      "    outputCols=['org_dummy','dow_dummy']\n",
      ")\n",
      "\n",
      "# Assemble predictors into a single column\n",
      "assembler = VectorAssembler(inputCols=['km','org_dummy','dow_dummy'], outputCol='features')\n",
      "\n",
      "# A linear regression object\n",
      "regression = LinearRegression(labelCol='duration')\n",
      "\n",
      "   Add the pipeline\n",
      "\n",
      "# Import class for creating a pipeline\n",
      "from pyspark.ml import Pipeline\n",
      "\n",
      "# Construct a pipeline\n",
      "pipeline = Pipeline(stages=[indexer,onehot,assembler,regression])\n",
      "\n",
      "# Train the pipeline on the training data\n",
      "pipeline = pipeline.fit(flights_train)\n",
      "\n",
      "# Make predictions on the testing data\n",
      "predictions = pipeline.transform(flights_test)\n",
      "\n",
      "\n",
      "      sample     setup for words for a pipeline\n",
      "\n",
      "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
      "\n",
      "# Break text into tokens at non-word characters\n",
      "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
      "\n",
      "# Remove stop words\n",
      "remover = StopWordsRemover(inputCol=\"words\", outputCol='terms')\n",
      "\n",
      "# Apply the hashing trick and transform to TF-IDF\n",
      "hasher = HashingTF(inputCol=\"terms\", outputCol=\"hash\")\n",
      "idf = IDF(inputCol=\"hash\", outputCol=\"features\")\n",
      "\n",
      "# Create a logistic regression object and add everything to a pipeline\n",
      "logistic = LogisticRegression()\n",
      "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])\n",
      "\n",
      "\n",
      "        >Cross validation\n",
      "\n",
      "data, training, testing\n",
      "\n",
      "training is split into fold called cross validation\n",
      "\n",
      "regression=LinearRegression(labelCol='consumption')\n",
      "\n",
      "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
      "\n",
      "\n",
      "params = ParamGridBuilder().build()\n",
      "\n",
      "cv=CrossValidator(estimator=regression,\n",
      "\testimatorParamMaps = params,\n",
      "\tevaluator=evaluator,\n",
      "\tnumFolds=10, seed=13)\n",
      "\n",
      "folds default to 3\n",
      "\n",
      "cv=cv.fit(cars_train)\n",
      "\n",
      "cv.avgMetrics\n",
      "\n",
      "predictions=cv.transform(cars_test)\n",
      "rmse = evaluator.evaluate(predictions)\n",
      "\n",
      "   sample     setup Cross Validation with 5 fold\n",
      "\n",
      "# Create an empty parameter grid\n",
      "params = ParamGridBuilder().build()\n",
      "\n",
      "# Create objects for building and evaluating a regression model\n",
      "regression = LinearRegression(labelCol='duration')\n",
      "evaluator = RegressionEvaluator(labelCol=\"duration\")\n",
      "\n",
      "# Create a cross validator\n",
      "cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
      "\n",
      "# Train and test model on multiple folds of the training data\n",
      "cv = cv.fit(flights_train)\n",
      "\n",
      "# NOTE: Since cross-valdiation builds multiple models, the fit() method can take a little while to complete.\n",
      "\n",
      "predictions=cv.transform(flights_test)\n",
      "rmse = evaluator.evaluate(predictions)\n",
      "print(rmse)\n",
      "\n",
      "output: 16\n",
      "\n",
      "\n",
      "\n",
      "    sample cross validating with a pipeline\n",
      "\n",
      "# Create an indexer for the org field\n",
      "indexer = StringIndexer(inputCol='org',outputCol='org_idx')\n",
      "\n",
      "# Create an one-hot encoder for the indexed org field\n",
      "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
      "\n",
      "# Assemble the km and one-hot encoded fields\n",
      "assembler = VectorAssembler(inputCols=['km','org_dummy'],outputCol='features')\n",
      "\n",
      "# Create a pipeline and cross-validator.\n",
      "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
      "\n",
      "cv = CrossValidator(estimator=pipeline,\n",
      "          estimatorParamMaps=params,\n",
      "          evaluator=evaluator)\n",
      "\n",
      "   sorting a groupby\n",
      "\n",
      "res = g.apply(lambda x: x.sort_values(ascending=False).head(3))\n",
      "          >Grid Search\n",
      "\n",
      "\n",
      "regression=LinearRegression(labelCol='consumption', fitIntercept=True)\n",
      "\n",
      "regression = regression.fit(cars_train)\n",
      "\n",
      "rmse = evaluator.evaluate(regression.transform(cars_test))\n",
      "\n",
      "output.745\n",
      "\n",
      "set fitInercept=False\n",
      "output.65\n",
      "\n",
      "from pyspark.ml.tuning import ParamGridBuilder\n",
      "\n",
      "params=ParamGridBuilder()\n",
      "\n",
      "#add grid points\n",
      "\n",
      "params=params.addGrid(regression.fitIntercept,[True,False])\n",
      "\n",
      "params=params.build()\n",
      "\n",
      "print('Number of models to be tested:', len(params))\n",
      "\n",
      "output:2\n",
      "\n",
      "\n",
      "cv=CrossValidator(estimator=regression,\n",
      "\t\testimatorParamsMaps=params,\n",
      "\t\tevaluator=evaluator)\n",
      "\n",
      "cv=cv.setNumFolds(10).setSeed(13).fit(cars_train)\n",
      "\n",
      "20 models\n",
      "\n",
      "cv.avgMetrics\n",
      "\n",
      "output:[.8006,0.9079] \n",
      "\n",
      "the model that includes an intercept does better than one without\n",
      "\n",
      "cv.bestModel\n",
      "\n",
      "predictions = cv.tranform(cars_test)\n",
      "\n",
      "cv.bestModel.explainParam('fitIntercept')\n",
      "\n",
      "output: current is True\n",
      "\n",
      "#add multiple grid points\n",
      "\n",
      "params=params.addGrid(regression.fitIntercept,[True,False]) \\\n",
      "\t.addGrid(regression.regParam,[0.001,0.01,0.1,1,10])\\\n",
      "\t.addGrid(regression.elasticNetParam,[0,0.25,0.5,0.75,1])\\\n",
      "\t.build()\n",
      "\n",
      "\n",
      "    sample    > Grid builder    CrossValidator 5 folds\n",
      "\n",
      "# Create parameter grid\n",
      "params = ParamGridBuilder()\n",
      "\n",
      "# Add grids for two parameters\n",
      "params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
      "               .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])\n",
      "\n",
      "# Build the parameter grid\n",
      "params = params.build()\n",
      "print('Number of models to be tested: ', len(params))\n",
      "\n",
      "# Create cross-validator\n",
      "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
      "\n",
      "output: number of models to be tested: 12\n",
      "\n",
      "\n",
      "    sample  > make predictions with the best model\n",
      "\n",
      "# Get the best model from cross validation\n",
      "best_model = cv.bestModel\n",
      "\n",
      "# Look at the stages in the best model\n",
      "print(best_model.stages)\n",
      "\n",
      "# Get the parameters for the LinearRegression object in the best model\n",
      "best_model.stages[3].extractParamMap()\n",
      "\n",
      "# Generate predictions on testing data using the best model then calculate RMSE\n",
      "predictions = best_model.transform(flights_test)\n",
      "evaluator.evaluate(predictions)\n",
      "\n",
      "output:\n",
      "[StringIndexer_14299b2d5472, OneHotEncoderEstimator_9a650c117f1d, VectorAssembler_933acae88a6e, LinearRegression_9f5a93965597]\n",
      "In [1]:\n",
      "\n",
      "\n",
      "        >sample build the params grid for logistic regression\n",
      "\n",
      "# Create parameter grid\n",
      "params = ParamGridBuilder()\n",
      "\n",
      "# Add grid for hashing trick parameters\n",
      "params = params.addGrid(hasher.numFeatures,[1024, 4096,16384]) \\\n",
      "               .addGrid(hasher.binary, [True,False])\n",
      "\n",
      "# Add grid for logistic regression parameters\n",
      "params = params.addGrid(logistic.regParam,[0.01, 0.1, 1.0, 10.0]) \\\n",
      "               .addGrid(logistic.elasticNetParam, [0.0, 0.5, 1.0])\n",
      "\n",
      "# Build parameter grid\n",
      "params = params.build()\n",
      "\n",
      "        >Ensemble\n",
      "\n",
      "it is a collection of models\n",
      "\n",
      "collective opinion of a group is better\n",
      "\n",
      "there must be diversity and independence \n",
      "\n",
      "\n",
      "Random Forest is a collection of trees\n",
      "\n",
      "no two trees are the same\n",
      "\n",
      "from pyspark.ml.classification import RandomForestClassifier\n",
      "\n",
      "forest=RandomForestClassifier(numTrees=5)\n",
      "\n",
      "forest=forest.fit(cars_train)\n",
      "\n",
      "forest.trees\n",
      "\n",
      "\n",
      "the model uses : cyl, size, mass, length, rpm, and consumption\n",
      "\n",
      "forest.featureImportances\n",
      "\n",
      "sparseVector(6,{0:0.0205,1:0.2701,2:0.108,3:0.1895,4:0.2939,5:0.1181})\n",
      "\n",
      "rpm is the most important\n",
      "cyl is the least important\n",
      "\n",
      "\n",
      "Gradient-Boost Trees\n",
      "\n",
      "trees working in series\n",
      "\n",
      "1. build a decision tree and add to the ensemble\n",
      "2. predict label for each training instance using ensemble\n",
      "3. compare predictions with known labels\n",
      "4. emphasize training instances with incorrect predictions\n",
      "\n",
      "5. return to 1 and train another tree which works to improve the incorrect predictions.\n",
      "\n",
      "each new tree attempts to correct the errors of the proceeding trees\n",
      "\n",
      "from pyspark.ml.classification import GBTClassifier\n",
      "\n",
      "gbt = GBTClassifier(maxIter=10)\n",
      "\n",
      "gbt=gbt.fit(cars_train)\n",
      "\n",
      "\n",
      "   sample  compare the AUC for the Decision Tree, Random Forest, and Gradient Boosted Tree\n",
      "\n",
      "\n",
      "\n",
      "# Import the classes required\n",
      "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier\n",
      "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
      "\n",
      "# Create model objects and train on training data\n",
      "tree =DecisionTreeClassifier().fit(flights_train)\n",
      "gbt = GBTClassifier().fit(flights_train)\n",
      "\n",
      "# Compare AUC on testing data\n",
      "evaluator = BinaryClassificationEvaluator()\n",
      "evaluator.evaluate(tree.transform(flights_test))\n",
      "evaluator.evaluate(gbt.transform(flights_test))\n",
      "\n",
      "# Find the number of trees and the relative importance of features\n",
      "print(gbt.getNumTrees)\n",
      "print(gbt.featureImportances)\n",
      "\n",
      "\n",
      "output\n",
      "20 (trees)\n",
      "(3,[0,1,2],[0.27857733519498645,0.3517987451488248,0.36962391965618874])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      RandomForestClassifier\n",
      "\n",
      "You'll find good values for the following parameters:\n",
      "\n",
      "featureSubsetStrategy â€” the number of features to consider for splitting at each node and\n",
      "maxDepth â€” the maximum number of splits along any branch.\n",
      "\n",
      "# Create a random forest classifier\n",
      "forest = RandomForestClassifier()\n",
      "\n",
      "# Create a parameter grid\n",
      "params = ParamGridBuilder() \\\n",
      "            .addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']) \\\n",
      "            .addGrid(forest.maxDepth, [2, 5, 10]) \\\n",
      "            .build()\n",
      "\n",
      "# Create a binary classification evaluator\n",
      "evaluator = BinaryClassificationEvaluator()\n",
      "\n",
      "# Create a cross-validator\n",
      "cv = CrossValidator(estimator=forest, estimatorParamMaps=params,evaluator=evaluator, numFolds=5)\n",
      "\n",
      "\n",
      "   random forest   > cross validation   > evaluate auc\n",
      "\n",
      "# Average AUC for each parameter combination in grid\n",
      "avg_auc = cv.avgMetrics\n",
      "\n",
      "# Average AUC for the best model\n",
      "best_model_auc =  max(avg_auc)\n",
      "\n",
      "print(avg_auc)\n",
      "print(best_model_auc)\n",
      "\n",
      "# What's the optimal parameter value?\n",
      "print(cv.bestModel.params)\n",
      "opt_max_depth = cv.bestModel.explainParam(\"maxDepth\")\n",
      "opt_feat_substrat = cv.bestModel.explainParam('featureSubsetStrategy')\n",
      "\n",
      "# AUC for best model on testing data\n",
      "best_auc = evaluator.evaluate(cv.transform(flights_test))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pyspark pipelines.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pyspark pipelines.txt\n",
      "components of a data platform\n",
      "\n",
      "ingest using singer\n",
      "\n",
      "deploy spark transformation pipelines\n",
      "\n",
      "test your code automatically\n",
      "\n",
      "apply common data cleaning operations\n",
      "\n",
      "gain insights by combining data with pyspark\n",
      "\n",
      "operational systems:\n",
      "1. messaging services\n",
      "2. payment systems\n",
      "3. google analytics\n",
      "4. crm\n",
      "5. clickstream\n",
      "6. location services\n",
      "\n",
      "data lake\n",
      "1. organized by zones\n",
      "(operational data is the landing zone)\n",
      "2. services to clean (clean zone)\n",
      "3. business zone\n",
      "\n",
      "extract transform and load\n",
      "\n",
      "      Singer\n",
      "\n",
      "1. connecting to multiple data sources\n",
      "2. open source\n",
      "\n",
      "communicates using json\n",
      "\n",
      "tap and targets\n",
      "\n",
      "1. schema\n",
      "2. state\n",
      "3. record\n",
      "\n",
      "import singer\n",
      "singer.write_schema(schema=json_schema,\n",
      "\tstream_name='DC_employees',\n",
      "\tkey_properties=[\"id\"]\n",
      "\n",
      "import json\n",
      "\n",
      "json.dumps(json_schema['properties']['age'])\n",
      "\n",
      "with open('foo.json', mode='w') as fh:\n",
      "\tjson.dump(obj=json_schema, fp=fh)\n",
      "\n",
      "#writes the json-serialized object to the open file handle\n",
      "\n",
      "  > sample  > json.dump\n",
      "\n",
      "# Import json\n",
      "import json\n",
      "\n",
      "database_address = {\n",
      "  \"host\": \"10.0.0.5\",\n",
      "  \"port\": 8456\n",
      "}\n",
      "\n",
      "# Open the configuration file in writable mode\n",
      "with open(\"database_config.json\", mode='w') as fh:\n",
      "  # Serialize the object in this file handle\n",
      "  json.dump(obj=database_address, fp=fh) \n",
      "\n",
      "\n",
      "    > sample singer write_schema\n",
      "\n",
      "# Complete the JSON schema\n",
      "schema = {'properties': {\n",
      "    'brand': {'type': 'string'},\n",
      "    'model': {'type': 'string'},\n",
      "    'price': {'type': 'number'},\n",
      "    'currency': {'type': 'string'},\n",
      "    'quantity': {'type': 'integer', 'minimum': 1},  \n",
      "    'date': {'type': 'string', 'format': 'date'},\n",
      "    'countrycode': {'type': 'string', 'pattern': \"^[A-Z]{2}$\"}, \n",
      "    'store_name': {'type': 'string'}}}\n",
      "\n",
      "\n",
      "# Write the schema\n",
      "singer.write_schema(stream_name='products', schema=schema, key_properties=[])\n",
      "\n",
      "\n",
      "     >ingestion pipeline with Singer\n",
      "\n",
      "columns=(\"id\",\"name\",\"age\",\"has_children\")\n",
      "\n",
      "users={\n",
      "(1,\"adrian\",32,False),\n",
      "(2,\"ruanne\",28,True)\n",
      "}\n",
      "\n",
      "\n",
      "singer.write_record(stream_name=\"DC_employees\",\n",
      "record=dict(zip(columns,users.pop())))\n",
      "\n",
      "fixed_dict = {\"type\":\"RECORD\",\"stream\":\"DC_employees\"}\n",
      "\n",
      "record_msg={**fixed_dict,\"record\":dict(zip(columns, users.pop()))}\n",
      "\n",
      "print(json.dumps(record_msg))\n",
      "\n",
      "** unpacks the dictionary\n",
      "\n",
      "\n",
      "import singer\n",
      "\n",
      "singer.write_schema(stream_name=\"foo\", schema=...)\n",
      "\n",
      "singer.write_records(stream_name=\"foo\", records=...)\n",
      "\n",
      "     introduction to pipes\n",
      "\n",
      "python my_tap.py | target-csv\n",
      "\n",
      "python my_tap.py | target-csv --config userconfig.cfg\n",
      "\n",
      "      >keeping track with state messages\n",
      "\n",
      "last_update_on\n",
      "\n",
      "extract after last_update_on\n",
      "\n",
      "update after tap\n",
      "\n",
      "singer.write_state(value={\"max-last-updated-on\": some_variable})\n",
      "\n",
      "\n",
      "Youâ€™re running a Singer tap daily at midnight, to synchronize changes between databases. Your tap, called tap-mydelta, extracts only the records that were updated in this database since your last retrieval. To do so, your tap keeps state: it keeps track of the last record it reported on, which can be derived from the tableâ€™s last_updated_on field.\n",
      "\n",
      "   sample   get from local host rest end point\n",
      "\n",
      "endpoint = \"http://localhost:5000\"\n",
      "\n",
      "# Fill in the correct API key\n",
      "api_key = \"scientist007\"\n",
      "\n",
      "# Create the web APIâ€™s URL\n",
      "authenticated_endpoint = \"{}/{}\".format(\"http://localhost:5000\", api_key)\n",
      "\n",
      "print(authenticated_endpoint)\n",
      "\n",
      "# Get the web APIâ€™s reply to the endpoint\n",
      "api_response = requests.get(authenticated_endpoint).json()\n",
      "pprint.pprint(api_response)\n",
      "\n",
      "output:\n",
      "\n",
      "<script.py> output:\n",
      "    http://localhost:5000/scientist007\n",
      "    {'apis': [{'description': 'list the shops available',\n",
      "               'url': '<api_key>/diaper/api/v1.0/shops'},\n",
      "              {'description': 'list the items available in shop',\n",
      "               'url': '<api_key>/diaper/api/v1.0/items/<shop_name>'}]}\n",
      "    {'apis': [{'url': '<api_key>/diaper/api/v1.0/shops', 'description': 'list the shops available'}, {'url': '<api_key>/diaper/api/v1.0/items/<shop_name>', 'description': 'list the items available in shop'}]}\n",
      "\n",
      "/shops\n",
      "items/<shop_name>\n",
      "\n",
      "\n",
      "   sample get a list of shops\n",
      "\n",
      "# Create the APIâ€™s endpoint for the shops\n",
      "shops_endpoint = \"{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"shops\")\n",
      "\n",
      "shops = requests.get(shops_endpoint).json()\n",
      "print(shops)\n",
      "\n",
      "{'shops': ['Aldi', 'Kruidvat', 'Carrefour', 'Tesco', 'DM']}\n",
      "\n",
      "items_endpoint = \"{}/{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"items\",\"Aldi\")\n",
      "items = requests.get(items_endpoint).json()\n",
      "\n",
      "\n",
      "{'items': [{'countrycode': 'BE', 'brand': 'Diapers-R-Us', 'model': '6months', 'price': 6.8, 'currency': 'EUR', 'quantity': 40, 'date': '2019-02-03'}]}\n",
      "\n",
      "\n",
      "\n",
      "dm\n",
      "\n",
      "{'items': [{'brand': 'Huggies',\n",
      "            'countrycode': 'DE',\n",
      "            'currency': 'EUR',\n",
      "            'date': '2019-02-01',\n",
      "            'model': 'newborn',\n",
      "            'price': 6.8,\n",
      "            'quantity': 40},\n",
      "           {'brand': 'Huggies',\n",
      "            'countrycode': 'AT',\n",
      "            'currency': 'EUR',\n",
      "            'date': '2019-02-01',\n",
      "            'model': 'newborn',\n",
      "            'price': 7.2,\n",
      "            'quantity': 40}]}\n",
      "\n",
      "\n",
      "    sample   > extract the schema\n",
      "\n",
      "\n",
      "# Use the convenience function to query the API\n",
      "tesco_items = retrieve_products(\"Tesco\")\n",
      "\n",
      "singer.write_schema(stream_name=\"products\", schema=schema,\n",
      "                    key_properties=[])\n",
      "\n",
      "\n",
      "# Write a single record to the stream, that adheres to the schema\n",
      "singer.write_record(stream_name=\"products\", \n",
      "            record={**tesco_items[0], 'store_name': \"Tesco\"})\n",
      "\n",
      "\n",
      "      >sample  write the store_name to the database\n",
      "\n",
      "\n",
      "# Use the convenience function to query the API\n",
      "tesco_items = retrieve_products(\"Tesco\")\n",
      "\n",
      "singer.write_schema(stream_name=\"products\", schema=schema,\n",
      "                    key_properties=[])\n",
      "\n",
      "# Write a single record to the stream, that adheres to the schema\n",
      "singer.write_record(stream_name=\"products\", \n",
      "                    record={**tesco_items[0], \"store_name\": \"Tesco\"})\n",
      "\n",
      "for shop in requests.get(SHOPS_URL).json()[\"shops\"]:\n",
      "    # Write all of the records that you retrieve from the API\n",
      "    singer.write_records(\n",
      "      stream_name=\"products\", # Use the same stream name that you used in the schema\n",
      "      records=({**tesco_items[0], \"store_name\": shop}\n",
      "               for item in retrieve_products(shop))\n",
      "    )  \n",
      "\n",
      "    tap to drain\n",
      "\n",
      "tap-marketing-api | target-csv --config data_lake.conf\n",
      "\n",
      "\n",
      "     > spark \n",
      "\n",
      "1. spark sql\n",
      "2. spark streaming\n",
      "3. MLlib\n",
      "4. GraphX\n",
      "\n",
      "api is pyspark\n",
      "\n",
      "data processing at scale\n",
      "interactive analytics\n",
      "\n",
      "validate hypothesis\n",
      "machine learning and score models\n",
      "\n",
      "spark is not used for little data\n",
      "\n",
      "prices.csv\n",
      "\n",
      "ratings.csv\n",
      "\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.getOrCreate()\n",
      "\n",
      "prices-spark.read.\n",
      "options(header=\"true\").\n",
      "csv('mnt/data_lake/landing/prices.csv\")\n",
      "\n",
      "prices.show()\n",
      "\n",
      "\n",
      "from pprint import pprint\n",
      "\n",
      "pprint(prices.dtypes)\n",
      "\n",
      "\n",
      "schema=StructType(\n",
      "StructField(\"store\",StringType(),nullable=False),\n",
      "\n",
      "StructField(\"price\",FloatType(), nullable=False)\n",
      "\n",
      "StructField(\"date\",DateType(),nullable=False\n",
      ")\n",
      "\n",
      "\n",
      "prices-spark.read.\n",
      "options(header=\"true\").\n",
      "schema(schema).\n",
      "csv('mnt/data_lake/landing/prices.csv\")\n",
      "\n",
      "\n",
      "\n",
      "# Read a csv file and set the headers\n",
      "df = (spark.read\n",
      "      .options(header=True)\n",
      "      .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
      "\n",
      "df.show()\n",
      "\n",
      "brand\n",
      "model\n",
      "absorption_rate\n",
      "comfort\n",
      "\n",
      "ham or spam csv\n",
      "\n",
      "https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/sms_spam.csv\n",
      "\n",
      "\n",
      "        read.csv\n",
      "\n",
      "# Read a csv file and set the headers\n",
      "df = (spark.read\n",
      "      .options(header=True)\n",
      "      .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
      "\n",
      "df.show()\n",
      "\n",
      "  >schema\n",
      "\n",
      "# Define the schema\n",
      "schema = StructType([\n",
      "  StructField(\"brand\", StringType(), nullable=False),\n",
      "  StructField(\"model\", StringType(), nullable=False),\n",
      "  StructField(\"absorption_rate\", ByteType(), nullable=True),\n",
      "  StructField(\"comfort\", ByteType(), nullable=True)\n",
      "])\n",
      "\n",
      "better_df = (spark\n",
      "             .read\n",
      "             .options(header=\"true\")\n",
      "             # Pass the predefined schema to the Reader\n",
      "             .schema(schema)\n",
      "             .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
      "pprint(better_df.dtypes)\n",
      "\n",
      "\n",
      "     >cleaning data\n",
      "\n",
      "handling invalid rows\n",
      "\n",
      "prices=(spark.read\n",
      "\t.options(header='true',mode'DROPMALFORMED')\n",
      "\t.csv('landing/prices.csv'))\n",
      "\n",
      "the significance of null\n",
      "1. keep the row\n",
      "2. fill the blanks with null\n",
      "\n",
      "prices.fillna(25, subset=['quantity']).show()\n",
      "\n",
      "\n",
      "   >badly chosen placeholders\n",
      "\n",
      "employees= spark.read.options(header='true').\n",
      "schema(schema).csv('employees.csv')\n",
      "\n",
      "   replace with condition in the when function\n",
      "\n",
      "from pyspark.sql.functions import col, when\n",
      "from datetime import date, timedelta\n",
      "\n",
      "one_year_from_now = date.today().replace(year=date.today().year+1)\n",
      "better_frame = employees.withColumn('end_date',\n",
      "\twhen(col('end_date')> one_year_from_now,None).otherwise(col('end_date')))\n",
      "\n",
      "better_frame.show()\n",
      "\n",
      "none is translated to null\n",
      "\n",
      "bytetype range is -128 to 127\n",
      "\n",
      "\n",
      " > fillna to replace missing values\n",
      "\n",
      "print(\"BEFORE\")\n",
      "ratings.show()\n",
      "\n",
      "print(\"AFTER\")\n",
      "# Replace nulls with arbitrary value on column subset\n",
      "ratings = ratings.fillna(4, subset=[\"comfort\"])\n",
      "ratings.show()\n",
      "\n",
      "   > drop invalid rows\n",
      "PERMISSIVE is the default mode\n",
      "\n",
      "# Specify the option to drop invalid rows\n",
      "ratings = (spark\n",
      "           .read\n",
      "           .options(header=True, mode='DROPMALFORMED')\n",
      "           .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings_with_invalid_rows.csv\"))\n",
      "ratings.show()\n",
      "\n",
      "  >  conditionally replacing data\n",
      "\n",
      "from pyspark.sql.functions import col, when\n",
      "\n",
      "# Add/relabel the column\n",
      "categorized_ratings = ratings.withColumn(\n",
      "    \"comfort\",\n",
      "    # Express the condition in terms of column operations\n",
      "    when(col(\"comfort\") > 3, \"sufficient\").otherwise(\"insufficient\"))\n",
      "\n",
      "categorized_ratings.show()\n",
      "\n",
      "\n",
      "           >Transforming data with spark\n",
      "deriving insights\n",
      "\n",
      "standardizing names and normalizing numerical data\n",
      "\n",
      "filtering rows\n",
      "selecting and renaming columns\n",
      "grouping and aggregation\n",
      "ordering results\n",
      "\n",
      "prices=spark.read.options(header='true').schema(schema).csv('landing/prices.csv')\n",
      "\n",
      "filter is passed boolean values\n",
      "\n",
      "prices_in_beligium = prices.filter(col('countrycode')=='BE).orderBy(col('date'))\n",
      "\n",
      "col creates column objects\n",
      "\n",
      "prices.select(\n",
      "\n",
      "\tcol('store'),\n",
      "\tcol('brand').alias('brandname')\n",
      ").distinct()\n",
      "\n",
      "  > grouping and aggregating\n",
      "\n",
      "(prices\n",
      "\t.groupBy(col('brand'))\n",
      "\t.mean('price')\n",
      ").show()\n",
      "\n",
      "(prices\n",
      "\t.groupBy(col('brand'))\n",
      "\t.agg(\n",
      "\t\tavg('price').alias('average_price')\n",
      "\t\tcount('brand').alias('number_of_times')\n",
      "\t)\n",
      ")\n",
      "\n",
      "\n",
      "  > joining data\n",
      "\n",
      "ratings_with_prices=ratings.join(prices,['brand','model'])\n",
      "\n",
      "     sample    select distinct columns\n",
      "\n",
      "from pyspark.sql.functions import col\n",
      "\n",
      "# Select the columns and rename the \"absorption_rate\" column\n",
      "result = ratings.select([col(\"brand\"),\n",
      "                       col(\"model\"),\n",
      "                       col(\"absorption_rate\").alias('absorbency')])\n",
      "\n",
      "# Show only unique values\n",
      "result.distinct().show()\n",
      "\n",
      "\n",
      "    sample  > agg\n",
      "\n",
      "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
      "\n",
      "aggregated = (purchased\n",
      "              # Group rows by 'Country'\n",
      "              .groupBy(col('Country'))\n",
      "              .agg(\n",
      "                # Calculate the average salary per group and rename\n",
      "                avg('Salary').alias('average_salary'),\n",
      "                # Calculate the standard deviation per group\n",
      "                stddev_samp('Salary'),\n",
      "                # Retain the highest salary per group and rename\n",
      "                sfmax('Salary').alias('highest_salary')\n",
      "              )\n",
      "             )\n",
      "\n",
      "aggregated.show()\n",
      "\n",
      "\n",
      "+-------+--------------+-------------------+--------------+\n",
      "    |Country|average_salary|stddev_samp(Salary)|highest_salary|\n",
      "    +-------+--------------+-------------------+--------------+\n",
      "    |Germany|       63000.0|                NaN|         63000|\n",
      "    | France|       48000.0|                NaN|         48000|\n",
      "    |  Spain|       62000.0| 12727.922061357855|         71000|\n",
      "    +-------+--------------+-------------------+--------------+\n",
      "    \n",
      "\n",
      "Note that the standard deviation column has returned NaN in a few cases. Thatâ€™s because there werenâ€™t enough data points for these countries (only one record, so you canâ€™t compute a meaningful sample standard deviation), as weâ€™re only loading a small file in this exercise\n",
      "\n",
      "\n",
      "    >Packaging your application\n",
      "\n",
      "\n",
      "python my_pyspark_data_pipeline.py\n",
      "\n",
      "spark-submit\n",
      "\n",
      "1. sets up launch environment for use with the cluster manager and the selected deploy mode\n",
      "\n",
      "spark-submit\n",
      "\n",
      "\t--master \"local[*]\" \\\n",
      "\t--py-files PY_FILES \\\n",
      "\tMAIN_PYTHON_FILE \\\n",
      "\tapp_arguments\n",
      "\n",
      "zip files\n",
      "\tzip \\\n",
      "\t\t--recurse-path\\\n",
      "\t\tdependencies.zip\n",
      "\t\tpydiaper\n",
      "\n",
      "spark-submit \\\n",
      "\t--py-files dependencies.zip \\\n",
      "\tpydiaper/cleaning/clean_prices.py\n",
      "\n",
      "\n",
      "        >importance of tests\n",
      "\n",
      "1. new functionality desired\n",
      "2. bugs need to get squashed\n",
      "\n",
      "written expectations of the code\n",
      "\n",
      "raises confidence that the code is correct now\n",
      "\n",
      "tests are the most up-to-date form of documentation\n",
      "\n",
      "testing takes time\n",
      "testing have a high return on investment\n",
      "\n",
      "unit tests\n",
      "service tests\n",
      "ui test (end to end tests)\n",
      "\n",
      "   writing unit tests\n",
      "\n",
      "1. Extract\n",
      "2. Transform\n",
      "3. Load\n",
      "\n",
      "transformation is where we add the business logic\n",
      "\n",
      "\n",
      "prices_with_ratings=spark.read.csv()\n",
      "exchange_rates=spark.read.csv()\n",
      "\n",
      "unit_prices_with_ratings = (prices_with_ratings.join() #transform\n",
      ".withColumn())\n",
      "\n",
      "transformations operate on dataframes\n",
      "\n",
      "     >dataframes in memory\n",
      "\n",
      "from pyspark.sql import Row\n",
      "\n",
      "purchase=Row(\"price\",\n",
      "\t\"quantity\",\n",
      "\t\"product\")\n",
      "\n",
      "record=purchase(12.99,1,\"cake\")\n",
      "\n",
      "df=spark.createDataFrame((record,))\n",
      "\n",
      "unit_prices_with_ratings=(prices_with_ratings\n",
      "\t.join(exchange_rates,['currency','date'])\n",
      "\t.withColumn('unit_price_in_euro',\n",
      "\tcol('price')/col('quantity')\n",
      "\t*col('exchange_rate_to_euro'))\n",
      "\n",
      "\n",
      "     create reusable well name functions\n",
      "\n",
      "def link_with_exchange_rates(prices,rates):\n",
      "\treturn prices.join(rates,['currency','date'])\n",
      "\n",
      "def calculate_unit_price_in_euro(df):\n",
      "\treturn\n",
      "\tdf.withColumn('unit_price_in_euro',\n",
      "\tcol('price')/col('quantity')\n",
      "\t*col('exchange_rate_to_euro'))\n",
      "\n",
      "\n",
      "unit_price_with_ratings=(\n",
      "\tcalculate_unit_price_in_euro(\n",
      "\tlink_with_exchange_rates(prices,exchange_rates)\n",
      "\t)\n",
      ")\n",
      "\n",
      "***each transformation can be tested and reduced\n",
      "\n",
      "def test_calculate_unit_price_in_euro():\n",
      "\trecord=dict(price=10,\n",
      "\t\tquantity=5,\n",
      "\t\texchange_rate_to_euro=2.)\n",
      "\n",
      "\tdf=spark.createDataFrame([Row(**record)])\n",
      "\n",
      "\tresult=calculate_unit_price_in_euro(df)\n",
      "\n",
      "\texpected_record=Row(**record, unit_price_in_euro=4.)\n",
      "\texpected=spark.createDateFrame([expected_record])\n",
      "\tassertDataFrameEqual(result,expected)\n",
      "\n",
      "testing framework: pytest\n",
      "\n",
      "create in-memory dataframes makes testing easier because the data is in plain sight\n",
      "focus is on a small number of examples\n",
      "\n",
      "    sample  > in memory dataframe\n",
      "\n",
      "\n",
      "from datetime import date\n",
      "from pyspark.sql import Row\n",
      "\n",
      "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
      "\n",
      "# Create a tuple of records\n",
      "data = (\n",
      "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
      "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
      "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
      ")\n",
      "\n",
      "# Create a DataFrame from these records\n",
      "frame = spark.createDataFrame(data)\n",
      "frame.show()\n",
      "\n",
      "\n",
      "\n",
      "script.py\n",
      "\n",
      "\n",
      "\n",
      "Light mode\n",
      "\n",
      "\n",
      "\n",
      "from datetime import date\n",
      "from pyspark.sql import Row\n",
      " \n",
      "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
      " \n",
      "# Create a tuple of records\n",
      "data = (\n",
      "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
      "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
      "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
      ")\n",
      " \n",
      "# Create a DataFrame from these records\n",
      "frame = spark.createDataFrame(data)\n",
      "frame.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Run Code\n",
      "Submit Answer\n",
      "\n",
      "\n",
      "IPython Shell\n",
      "\n",
      "\n",
      "Slides\n",
      "\n",
      "\n",
      "\n",
      "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
      "\n",
      "aggregated = (purchased\n",
      "              # Group rows by 'Country'\n",
      "              .groupBy(col('Country'))\n",
      "              .agg(\n",
      "                # Calculate the average salary per group and rename\n",
      "                avg('Salary').alias('average_salary'),\n",
      "                # Calculate the standard deviation per group\n",
      "                stddev_samp('Salary'),\n",
      "                # Retain the highest salary per group and rename\n",
      "                sfmax('Salary').alias('highest_salary')\n",
      "              )\n",
      "             )\n",
      "\n",
      "aggregated.show()\n",
      "from datetime import date\n",
      "from pyspark.sql import Row\n",
      "\n",
      "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
      "\n",
      "# Create a tuple of records\n",
      "data = (\n",
      "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
      "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
      "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
      ")\n",
      "\n",
      "# Create a DataFrame from these records\n",
      "frame = spark.createDataFrame(data)\n",
      "frame.show()\n",
      "+-------+------------+------------------+----------+----------+\n",
      "|country|utm_campaign|airtime_in_minutes|start_date|  end_date|\n",
      "+-------+------------+------------------+----------+----------+\n",
      "|    USA|DiapersFirst|                28|2017-01-20|2017-01-27|\n",
      "|Germany|  WindelKind|                31|2017-01-25|      null|\n",
      "|  India|CloseToCloth|                32|2017-01-25|2017-02-02|\n",
      "+-------+------------+------------------+----------+----------+\n",
      "\n",
      "    sample\t\n",
      "\n",
      "pipenv run pytest\n",
      "\n",
      "\n",
      "from .chinese_provinces_improved import \\\n",
      "    aggregate_inhabitants_by_province\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.types import StructType, \\\n",
      "    StructField, StringType, LongType, BooleanType\n",
      "\n",
      "\n",
      "def test_aggregate_inhabitants_by_province():\n",
      "    \"\"\"The number of inhabitants per province should be aggregated,\n",
      "    regardless of their distinctive features.\n",
      "    \"\"\"\n",
      "\n",
      "    spark = SparkSession.builder.getOrCreate()\n",
      "\n",
      "    fields = [\n",
      "        StructField(\"country\", StringType(), True),\n",
      "        StructField(\"province\", StringType(), True),\n",
      "        StructField(\"inhabitants\", LongType(), True),\n",
      "        StructField(\"foo\", BooleanType(), True),  # distinctive features\n",
      "    ]\n",
      "\n",
      "    frame = spark.createDataFrame({\n",
      "        (\"China\", \"A\", 3, False),\n",
      "        (\"China\", \"A\", 2, True),\n",
      "        (\"China\", \"B\", 14, False),\n",
      "        (\"US\", \"A\", 4, False)},\n",
      "        schema=StructType(fields)\n",
      "    )\n",
      "    actual = aggregate_inhabitants_by_province(frame).cache()\n",
      "\n",
      "    # In the older implementation, the data was first filtered for a specific\n",
      "    # country, after which you'd aggregate by province. The same province\n",
      "    # name could occur in multiple countries though.\n",
      "    # This test is expecting the data to be grouped by country,\n",
      "    # then province from aggregate_inhabitants_by_province()\n",
      "    expected = spark.createDataFrame(\n",
      "        {(\"China\", \"A\", 5), (\"China\", \"B\", 14), (\"US\", \"A\", 4)},\n",
      "        schema=StructType(fields[:3])\n",
      "    ).cache()\n",
      "\n",
      "    assert actual.schema == expected.schema, \"schemas don't match up\"\n",
      "    assert sorted(actual.collect()) == sorted(expected.collect()),\\\n",
      "        \"data isn't equal\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " > improvements\n",
      "\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import col, lower, sum\n",
      "\n",
      "from .catalog import catalog\n",
      "\n",
      "\n",
      "def extract_demographics(sparksession, catalog):\n",
      "    return sparksession.read.parquet(catalog[\"clean/demographics\"])\n",
      "\n",
      "\n",
      "def store_chinese_demographics(frame, catalog):\n",
      "    frame.write.parquet(catalog[\"business/chinese_demographics\"])\n",
      "\n",
      "\n",
      "# Improved aggregation function, grouped by country and province\n",
      "def aggregate_inhabitants_by_province(frame):\n",
      "    return (frame\n",
      "            .groupBy(\"province\")\n",
      "            .agg(sum(col(\"inhabitants\")).alias(\"inhabitants\"))\n",
      "            )\n",
      "\n",
      "\n",
      "def main():\n",
      "    spark = SparkSession.builder.getOrCreate()\n",
      "    frame = extract_demographics(spark, catalog)\n",
      "    chinese_demographics = frame.filter(lower(col(\"country\")) == \"china\")\n",
      "    aggregated_demographics = aggregate_inhabitants_by_province(\n",
      "        chinese_demographics\n",
      "    )\n",
      "    store_chinese_demographics(aggregated_demographics, catalog)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "\n",
      "      continuous testing     \n",
      "\n",
      "\n",
      "unittest\n",
      "pytest\n",
      "doctest\n",
      "nose\n",
      "\n",
      "assert or raise\n",
      "\n",
      "assert compute == expected\n",
      "\n",
      "report which test passed and which ones failed\n",
      "\n",
      "automation is one of the objectives of a data engineer\n",
      "\n",
      "ci/cd pipeline\n",
      "\n",
      "continuous integration\n",
      "1. get code changes integrated with the master branch regularly\n",
      "\n",
      "continuous delivery\n",
      "1. Create artifacts (deliverables like documentation, but also programs) that can be deployed into production without breaking things\n",
      "\n",
      "cicleci - run tests automatically for you\n",
      "\n",
      "circleci looks for .circleci/config.yml\n",
      "1. has a section called jobs\n",
      "\n",
      "jobs:\n",
      "\ttest:\n",
      "\t\tdocker:\n",
      "\t\t\t-image:circleci/python:3.6.4\n",
      "\t\tsteps:\n",
      "\t\t\t-checkout\n",
      "\t\t\t-run: pip install -r requirements.txt\n",
      "\t\t\t-run: pytest\n",
      "\n",
      "\n",
      "cicleci\n",
      "1. checkout code\n",
      "2. install test & build requirements\n",
      "3. run tests\n",
      "\n",
      "\n",
      "order\n",
      "1. check out your application from version control\n",
      "2. install your python application dependencies\n",
      "3. run the test suite of your application\n",
      "4. create artifacts\n",
      "5. save the artifacts to location accessible by your company's compute infrastructure\n",
      "\n",
      "\n",
      "\n",
      "Add flake8 to the development section in the Pipfile, which is in the projectâ€™s root folder. This file serves a similar purpose as the requirements.txt files you might have seen in other Python projects. It solves some problems with those though. To add flake8 correctly, look at the line that mentions pytest.\n",
      "\n",
      "\n",
      "      Modern day workflow management\n",
      "\n",
      "\n",
      "sequence of tasks scheduled to be run\n",
      "a task can be trigger by a sequence of event\n",
      "\n",
      "schedule or triggered\n",
      "\n",
      "scheduled with cron\n",
      "\n",
      "reads crontab files\n",
      "\n",
      "#Minutes hours Days Months Day of the week Command\n",
      "\n",
      "*/15 9-17 * * 1-3,5 log_my_activity\n",
      "1. one task per line\n",
      "2. launch my process, log my activity at a specific time\n",
      "3. every fifteen minutes between normal office hours, ever day of the month, for every month, \n",
      "Mon, tues, wednesday, and fridays\n",
      "\n",
      "your can add comments\n",
      "\n",
      "other tools\n",
      "1. luigi\n",
      "2. azkaban\n",
      "3. airflow\n",
      "\n",
      "apache airflow fulfills modern engineering needs\n",
      "1. create and visualize complex workflows\n",
      "2. monitor and log workflows\n",
      "3. scales horizontally (work with other machines)\n",
      "\n",
      "      >The directed Acyclic Graph (DAG)\n",
      "\n",
      "1. nodes are connected by edges\n",
      "2. the edge denote a sense of direction on the nodes\n",
      "3. Acyclic means there is no way to circle back to the same node\n",
      "4. The nodes are operators\n",
      "\n",
      "from airflow import DAG\n",
      "\n",
      "my_dag = DAG(\n",
      "\tdag_id=\"publish_logs\",\n",
      "\tschedule_interval=\"* * * * *\",\n",
      "\tstate_date=datetime(2010,1,1)\n",
      "\n",
      ")\n",
      "\n",
      "BashOperator (bash script)\n",
      "Pythonoperator (python script)\n",
      "SparkSubmitOperator\n",
      "\n",
      "    > defining dependencies between task is established using set_downstream and set_upstream operators\n",
      "\n",
      "task1.set_downstream(task2)\n",
      "task3.set_upstream(task2)\n",
      "\n",
      "\n",
      "\n",
      "    Dag schedule job\n",
      "\n",
      "schedule interval: * default\n",
      "\n",
      "minute\n",
      "hour\n",
      "day of the month\n",
      "day of the week\n",
      "\n",
      "\n",
      "from datetime import datetime\n",
      "from airflow import DAG\n",
      "\n",
      "reporting_dag = DAG(\n",
      "    dag_id=\"publish_EMEA_sales_report\", \n",
      "    # Insert the cron expression\n",
      "    schedule_interval=\"0 7 * * 1\",\n",
      "    start_date=datetime(2019, 11, 24),\n",
      "    default_args={\"owner\": \"sales\"}\n",
      ")\n",
      "\n",
      "# Specify direction using verbose method\n",
      "prepare_crust.set_downstream(apply_tomato_sauce)\n",
      "\n",
      "tasks_with_tomato_sauce_parent = [add_cheese, add_ham, add_olives, add_mushroom]\n",
      "\n",
      "for task in tasks_with_tomato_sauce_parent:\n",
      "    # Specify direction using verbose method on relevant task\n",
      "    apply_tomato_sauce.set_downstream(task)\n",
      "\n",
      "# Specify direction using bitshift operator\n",
      "tasks_with_tomato_sauce_parent   bake_pizza\n",
      "\n",
      "# Specify direction using verbose method\n",
      "bake_pizza.set_upstream(prepare_oven)\n",
      "\n",
      "a.set_downstream(b) means b must be executed after a.\n",
      "a   b also means b must be executed after a.\n",
      "b.set_upstream(a) means a must be executed before b.\n",
      "b << a also means a must be executed before b.\n",
      "\n",
      "Set prepare_crust to precede apply_tomato_sauce using the appropriate method.\n",
      "\n",
      "Set apply_tomato_sauceto precede each of tasks in tasks_with_tomato_sauce_parent using the appropriate method.\n",
      "Set the tasks_with_tomato_sauce_parent list to precede bake_pizza using either the bitshift operator   or <<.\n",
      "Set bake_pizza to succeed prepare_oven using the appropriate method.\n",
      "\n",
      "   >sample\n",
      "\n",
      "# Specify direction using verbose method\n",
      "prepare_crust.set_downstream(apply_tomato_sauce)\n",
      "\n",
      "tasks_with_tomato_sauce_parent = [add_cheese, add_ham, add_olives, add_mushroom]\n",
      "for task in tasks_with_tomato_sauce_parent:\n",
      "    # Specify direction using verbose method on relevant task\n",
      "    apply_tomato_sauce.set_downstream(task)\n",
      "\n",
      "# Specify direction using bitshift operator\n",
      "tasks_with_tomato_sauce_parent   bake_pizza\n",
      "\n",
      "# Specify direction using verbose method\n",
      "bake_pizza.set_upstream(prepare_oven)\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pyspark.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pyspark.txt\n",
      "Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n",
      "\n",
      "As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\n",
      "\n",
      "\n",
      "# Verify SparkContext\n",
      "sc=SparkContext\n",
      "print(sc)\n",
      "\n",
      "# Print Spark version\n",
      "print(sc.version)\n",
      "\n",
      "spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you'll be using the Spark DataFrame abstraction built on top of RDDs.\n",
      "\n",
      "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\n",
      "\n",
      "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\n",
      "\n",
      "\n",
      "# Import SparkSession from pyspark.sql\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "# Create my_spark\n",
      "my_spark = SparkSession.builder.getOrCreate()\n",
      "\n",
      "# Print my_spark\n",
      "print(my_spark)\n",
      "\n",
      "\n",
      "# Print the tables in the catalog\n",
      "print(spark.catalog.listTables())\n",
      "\n",
      "  > example  > sql like extraction\n",
      "\n",
      "# Don't change this query\n",
      "query = \"FROM flights SELECT * LIMIT 10\"\n",
      "\n",
      "# Get the first 10 rows of flights\n",
      "flights10 = spark.sql(query)\n",
      "\n",
      "# Show the results\n",
      "flights10.show()\n",
      "\n",
      "\n",
      "   > sample   > group by in sql\n",
      "\n",
      "# Don't change this query\n",
      "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
      "\n",
      "# Run the query\n",
      "flight_counts = spark.sql(query)\n",
      "\n",
      "# Convert the results to a pandas DataFrame\n",
      "pd_counts = flight_counts.toPandas()\n",
      "\n",
      "# Print the head of pd_counts\n",
      "print(pd_counts.head())\n",
      "\n",
      "\n",
      "In the last exercise, you saw how to move data from Spark to pandas. However, maybe you want to go the other direction, and put a pandas DataFrame into a Spark cluster! The SparkSession class has a method for this as well.\n",
      "\n",
      "The .createDataFrame() method takes a pandas DataFrame and returns a Spark DataFrame.\n",
      "\n",
      "The output of this method is stored locally, not in the SparkSession catalog. This means that you can use all the Spark DataFrame methods on it, but you can't access the data in other contexts.\n",
      "\n",
      "For example, a SQL query (using the .sql() method) that references your DataFrame will throw an error. To access the data in this way, you have to save it as a temporary table.\n",
      "\n",
      "You can do this using the .createTempView() Spark DataFrame method, which takes as its only argument the name of the temporary table you'd like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame.\n",
      "\n",
      "There is also the method .createOrReplaceTempView(). This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You'll use this method to avoid running into problems with duplicate tables.\n",
      "\n",
      "Check out the diagram to see all the different ways your Spark data structures interact with each other.\n",
      "\n",
      "\n",
      "# Create pd_temp\n",
      "pd_temp = pd.DataFrame(np.random.random(10))\n",
      "\n",
      "# Create spark_temp from pd_temp\n",
      "spark_temp = spark.createDataFrame(pd_temp)\n",
      "# Examine the tables in the catalog\n",
      "print(spark.catalog.listTables())\n",
      "\n",
      "# Add spark_temp to the catalog\n",
      "spark_temp.createOrReplaceTempView('temp')\n",
      "\n",
      "# Examine the tables in the catalog again\n",
      "print(spark.catalog.listTables())\n",
      "\n",
      "query='select * from temp'\n",
      "\n",
      "result=spark.sql(query).toPandas()\n",
      "\n",
      "print(result.head())\n",
      "\n",
      "\n",
      "\n",
      "   >sample   > read csv\n",
      "\n",
      "SparkSession has a .read attribute which has several methods for reading different data sources into Spark DataFrames. Using these you can create a DataFrame from a .csv file just like with regular pandas DataFrames!\n",
      "\n",
      "# Don't change this file path\n",
      "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
      "\n",
      "# Read in the airports data\n",
      "airports = spark.read.csv(file_path,header=True)\n",
      "\n",
      "# Show the data\n",
      "\n",
      "airports.show()\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "\n",
      "    >creating columns\n",
      "\n",
      "\n",
      "The new column must be an object of class Column. Creating one of these is as easy as extracting a column from your DataFrame using df.colName.\n",
      "\n",
      "Updating a Spark DataFrame is somewhat different than working in pandas because the Spark DataFrame is immutable. This means that it can't be changed, and so columns can't be updated in place.\n",
      "\n",
      "  sample  withColumn\n",
      "\n",
      "# Create the DataFrame flights\n",
      "flights = spark.table(\"flights\")\n",
      "\n",
      "# Show the head\n",
      "flights.show()\n",
      "\n",
      "# Add duration_hrs\n",
      "flights = flights.withColumn(\"duration_hrs\", flights.air_time/60)\n",
      "\n",
      "\n",
      "       .filter()\n",
      "\n",
      "Let's take a look at the .filter() method. As you might suspect, this is the Spark counterpart of SQL's WHERE clause. The .filter() method takes either an expression that would follow the WHERE clause of a SQL expression as a string, or a Spark Column of boolean (True/False) values.\n",
      "\n",
      "flights.filter(\"air_time > 120\").show()\n",
      "flights.filter(flights.air_time > 120).show()\n",
      "\n",
      "columns\n",
      "\n",
      "['year',\n",
      " 'month',\n",
      " 'day',\n",
      " 'dep_time',\n",
      " 'dep_delay',\n",
      " 'arr_time',\n",
      " 'arr_delay',\n",
      " 'carrier',\n",
      " 'tailnum',\n",
      " 'flight',\n",
      " 'origin',\n",
      " 'dest',\n",
      " 'air_time',\n",
      " 'distance',\n",
      " 'hour',\n",
      " 'minute']\n",
      "\n",
      "# Filter flights by passing a string\n",
      "long_flights1 = flights.filter(\"distance>1000\")\n",
      "\n",
      "# Filter flights by passing a column of boolean values\n",
      "long_flights2 = flights.filter(flights.distance > 1000)\n",
      "\n",
      "# Print the data to check they're equal\n",
      "long_flights1.show()\n",
      "long_flights2.show()\n",
      "\n",
      "output: same results\n",
      "\n",
      "         .select()\n",
      "\n",
      "# Select the first set of columns\n",
      "selected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n",
      "\n",
      "# Select the second set of columns\n",
      "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
      "\n",
      "# Define first filter\n",
      "filterA = flights.origin == \"SEA\"\n",
      "\n",
      "# Define second filter\n",
      "filterB = flights.dest == \"PDX\"\n",
      "\n",
      "# Filter the data, first by filterA then by filterB\n",
      "selected2 = temp.filter(filterA).filter(filterB)\n",
      "\n",
      "     select () .alias()\n",
      "\n",
      "# Define avg_speed\n",
      "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
      "\n",
      "# Select the correct columns\n",
      "speed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
      "\n",
      "# Create the same table using a SQL expression\n",
      "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")\n",
      "\n",
      "speed1.show()\n",
      "speed2.show()\n",
      "\n",
      "     >Aggregating \n",
      "\n",
      "All of the common aggregation methods, like .min(), .max(), and .count() are GroupedData methods. These are created by calling the .groupBy() DataFrame method\n",
      "\n",
      "   Group by\n",
      "\n",
      "# Find the shortest flight from PDX in terms of distance\n",
      "flights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()\n",
      "\n",
      "# Find the longest flight from SEA in terms of air time\n",
      "flights.filter(flights.origin == \"SEA\").groupBy().max(\"air_time\").show()\n",
      "\n",
      "\n",
      "    groupby withcolumns\n",
      "\n",
      "# Average duration of Delta flights\n",
      "flights.filter(flights.carrier==\"DL\").filter(flights.origin==\"SEA\").groupBy().avg(\"air_time\").show()\n",
      "\n",
      "# Total hours in the air\n",
      "flights.withColumn(\"duration_hours\", flights.air_time/60).groupBy().sum(\"duration_hours\").show()\n",
      "\n",
      "\n",
      "    groupby  count avg\n",
      "\n",
      "# Group by tailnum\n",
      "by_plane = flights.groupBy(\"tailnum\")\n",
      "\n",
      "# Number of flights each plane made\n",
      "by_plane.count().show()\n",
      "\n",
      "# Group by origin\n",
      "by_origin = flights.groupBy(\"origin\")\n",
      "\n",
      "# Average duration of flights from PDX and SEA\n",
      "by_origin.avg(\"air_time\").show()\n",
      "\n",
      "  > pyspark.sql.functions\n",
      "\n",
      "In addition to the GroupedData methods you've already seen, there is also the .agg() method. This method lets you pass an aggregate column expression that uses any of the aggregate functions from the pyspark.sql.functions submodule.\n",
      "\n",
      "# Import pyspark.sql.functions as F\n",
      "import pyspark.sql.functions as F\n",
      "\n",
      "# Group by month and dest\n",
      "by_month_dest = flights.groupBy(\"month\", \"dest\")\n",
      "\n",
      "# Average departure delay by month and destination\n",
      "by_month_dest.avg(\"dep_delay\").show()\n",
      "\n",
      "# Standard deviation of departure delay\n",
      "by_month_dest.agg(F.stddev(\"dep_delay\")).show()\n",
      "\n",
      "\n",
      "        Joining\n",
      "\n",
      "This column is called the key. Examples of keys here include the tailnum and carrier columns from the flights table.\n",
      "\n",
      "flights and planes\n",
      "\n",
      "\n",
      "In PySpark, joins are performed using the DataFrame method .join(). This method takes three arguments. The first is the second DataFrame that you want to join with the first one. The second argument, on, is the name of the key column(s) as a string. The names of the key column(s) must be the same in each table.\n",
      "\n",
      "\n",
      "# Examine the data\n",
      "airports.show()\n",
      "\n",
      "# Rename the faa column\n",
      "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
      "\n",
      "# Join the DataFrames\n",
      "flights_with_airports = flights.join(airports, on=\"dest\", how=\"leftouter\")\n",
      "\n",
      "# Examine the new DataFrame\n",
      "flights_with_airports.show()\n",
      "\n",
      "\n",
      "     >Machine Learning pipelines\n",
      "\n",
      "At the core of the pyspark.ml module are the Transformer and Estimator classes. Almost every other class in the module behaves similarly to these two basic classes.\n",
      "\n",
      "\n",
      "\n",
      "# Rename year column\n",
      "planes = planes.withColumnRenamed(\"year\",\"plane_year\")\n",
      "\n",
      "# Join the DataFrames\n",
      "model_data = flights.join(flights, on=[\"tailnum\"], how=\"leftouter\")\n",
      "\n",
      "model_data.show()\n",
      "\n",
      "['tailnum', 'year', 'month', 'day', 'dep_time', 'dep_delay', 'arr_time', 'arr_delay', 'carrier', 'flight', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute', 'year', 'month', 'day', 'dep_time', 'dep_delay', 'arr_time', 'arr_delay', 'carrier', 'flight', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute']\n",
      "\n",
      "\n",
      "# Rename year column\n",
      "planes = planes.withColumnRenamed(\"year\",\"plane_year\")\n",
      "\n",
      "# Join the DataFrames\n",
      "model_data = flights.join(planes, on=[\"tailnum\"], how=\"leftouter\")\n",
      "\n",
      "model_data.show()\n",
      "\n",
      "print(model_data.columns)\n",
      "\n",
      "     TRANSFORMING\n",
      "\n",
      "it's important to know that Spark only handles numeric data.\n",
      "\n",
      "DataFrame must be either integers or decimals\n",
      "\n",
      "To remedy this, you can use the .cast() method in combination with the .withColumn() method. It's important to note that .cast() works on columns, while .withColumn() works on DataFrames\n",
      "\n",
      "integer\n",
      "double\n",
      "\n",
      "\n",
      "# Cast the columns to integers\n",
      "model_data = model_data.withColumn(\"arr_delay\",  model_data.arr_delay.cast(\"integer\"))\n",
      "model_data = model_data.withColumn(\"air_time\", model_data.arr_time.cast(\"integer\"))\n",
      "model_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\n",
      "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))\n",
      "\n",
      "# Create the column plane_age\n",
      "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)\n",
      "\n",
      "\n",
      "    create new features  > check if is_late\n",
      "\n",
      "# Create is_late\n",
      "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay>0)\n",
      "\n",
      "\n",
      "# Convert to an integer\n",
      "model_data = model_data.withColumn(\"label\",model_data.is_late.cast('integer'))\n",
      "\n",
      "print(model_data.label)\n",
      "\n",
      "# Remove missing values\n",
      "model_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")\n",
      "\n",
      "model_data.show()\n",
      "\n",
      "    Strings and factors\n",
      " So far this hasn't been an issue; even boolean columns can easily be converted to integers without any trouble. But you'll also be using the airline and the plane's destination as features in your model.\n",
      "\n",
      "\n",
      "Fortunately, PySpark has functions for handling this built into the pyspark.ml.features submodule. You can create what are called 'one-hot vectors' to represent the carrier and the destination of each flight. A one-hot vector is a way of representing a categorical feature where every observation has a vector in which all elements are zero except for at most one element, which has a value of one\n",
      "\n",
      "The first step to encoding your categorical feature is to create a StringIndexer. Members of this class are Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column.\n",
      "\n",
      "The second step is to encode this numeric column as a one-hot vector using a OneHotEncoder. This works exactly the same way as the StringIndexer by creating an Estimator and then a Transformer. The end result is a column that encodes your categorical feature as a vector that's suitable for machine learning routines!\n",
      "\n",
      "\n",
      "All you have to remember is that you need to create a StringIndexer and a OneHotEncoder, and the Pipeline will take care of the rest\n",
      "\n",
      "\n",
      "n this exercise you'll create a StringIndexer and a OneHotEncoder to code the carrier column. To do this, you'll call the class constructors with the arguments inputCol and outputCol.\n",
      "\n",
      "The inputCol is the name of the column you want to index or encode, and the outputCol is the name of the new column that the Transformer should create.\n",
      "\n",
      "\n",
      "# Create a StringIndexer\n",
      "carr_indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n",
      "\n",
      "# Create a OneHotEncoder\n",
      "carr_encoder = OneHotEncoder(inputCol=\"carrier_index\",outputCol=\"carrier_fact\")\n",
      "\n",
      "\n",
      "# Create a StringIndexer\n",
      "dest_indexer = StringIndexer(inputCol=\"dest\", outputCol=\"dest_index\")\n",
      "\n",
      "# Create a OneHotEncoder\n",
      "dest_encoder = OneHotEncoder(inputCol=\"dest_index\",outputCol=\"dest_fact\")\n",
      "\n",
      "The last step in the Pipeline is to combine all of the columns containing our features into a single column. This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form. You can do this by storing each of the values from a column as an entry in a vector. Then, from the model's point of view, every observation is a vector that contains all of the information about it and a label that tells the modeler what value that observation corresponds to.\n",
      "\n",
      "Because of this, the pyspark.ml.feature submodule contains a class called VectorAssembler. This Transformer takes all of the columns you specify and combines them into a new vector column.\n",
      "\n",
      "\n",
      "# Make a VectorAssembler\n",
      "vec_assembler = VectorAssembler(inputCols=[\"month\",\"air_time\",\"carrier_fact\",\"dest_fact\",\"plane_age\"], outputCol=\"features\")\n",
      "\n",
      "\n",
      "Pipeline is a class in the pyspark.ml module that combines all the Estimators and Transformers that you've already created. This lets you reuse the same modeling process over and over again by wrapping it up in one simple object. Neat, right?\n",
      "\n",
      "# Import Pipeline\n",
      "from pyspark.ml import Pipeline\n",
      "\n",
      "# Make the pipeline\n",
      "flights_pipe = Pipeline(stages=[dest_indexer,dest_encoder,carr_indexer,carr_encoder,vec_assembler])\n",
      "\n",
      "# Fit and transform the data\n",
      "piped_data = flights_pipe.fit(model_data).transform(model_data)\n",
      "\n",
      "training, test = piped_data.randomSplit([.6,.4])\n",
      "\n",
      "        Logistic Regression\n",
      "\n",
      "# Import LogisticRegression\n",
      "from pyspark.ml.classification import LogisticRegression\n",
      "\n",
      "# Create a LogisticRegression Estimator\n",
      "lr = LogisticRegression()\n",
      "\n",
      "\n",
      "  > cross validation\n",
      "\n",
      "You'll be using cross validation to choose the hyperparameters by creating a grid of the possible pairs of values for the two hyperparameters, elasticNetParam and regParam, and using the cross validation error to compare all the different models so you can choose the best one!\n",
      "\n",
      "\n",
      "# Import the evaluation submodule\n",
      "import pyspark.ml.evaluation as evals\n",
      "\n",
      "# Create a BinaryClassificationEvaluator\n",
      "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
      "\n",
      "The submodule pyspark.ml.tuning includes a class called ParamGridBuilder that does just that (maybe you're starting to notice a pattern here; PySpark has a submodule for just about everything!).\n",
      "\n",
      "You'll need to use the .addGrid() and .build() methods to create a grid that you can use for cross validation. The .addGrid() method takes a model parameter (an attribute of the model Estimator, lr, that you created a few exercises ago) and a list of values that you want to try. The .build() method takes no arguments, it just returns the grid that you'll use later.\n",
      "\n",
      "# Import the tuning submodule\n",
      "import pyspark.ml.tuning as tune\n",
      "\n",
      "# Create the parameter grid\n",
      "grid = tune.ParamGridBuilder()\n",
      "\n",
      "# Add the hyperparameter\n",
      "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
      "grid = grid.addGrid(lr.elasticNetParam, [0,1])\n",
      "\n",
      "# Build the grid\n",
      "grid = grid.build()\n",
      "\n",
      "# Create the CrossValidator\n",
      "cv = tune.CrossValidator(estimator=lr,\n",
      "               estimatorParamMaps=grid,\n",
      "               evaluator=evaluator\n",
      "               )\n",
      "\n",
      "# Call lr.fit()\n",
      "best_lr = lr.fit(training)\n",
      "\n",
      "# Print best_lr\n",
      "print(best_lr)\n",
      "\n",
      "# Use the model to predict the test set\n",
      "test_results = best_lr.transform(test)\n",
      "\n",
      "# Evaluate the predictions\n",
      "print(evaluator.evaluate(test_results))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pytorch activation.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pytorch activation.txt\n",
      "\n",
      "input_layer=torch.tensor([2.,1.])\n",
      "weight_1 = torch.tensor([[0.45,0.32],\n",
      "[-0.12,0.29]])\n",
      "hidden_layer=torch.matmul(input_layer, weight_1)\n",
      "\n",
      "weight_2 = torch.tensor([[0.48,-0.12],\n",
      "[0.64,0.91]])\n",
      "\n",
      "output_layer=torch.matmul(hidden_layer, weight_2)\n",
      "\n",
      "   dimensions separated by a hyper plane\n",
      "\n",
      "activation functions:\n",
      "1.sigmoid 1/(1+e**-x)\n",
      "2. tanh tanh(x)\n",
      "3. relu max(0,x)\n",
      "4. leaky relu max(0.1x,x)\n",
      "5. maxout (max(w1t+b1w2tx+b2)\n",
      "6. elu x >=0 or alpha(e**x-1) for x<0\n",
      "\n",
      "neural networks can deal with non linear datasets\n",
      "\n",
      "\n",
      "relu -> retified linearu unit -> most popular\n",
      "\n",
      "it does not change the positive attributes and zeros out the negative attributes\n",
      "\n",
      "\n",
      "import torch.nn as nn\n",
      "\n",
      "relu=nn.ReLU()\n",
      "tensor_1=torch.tensor([2.,4.])\n",
      "print(relu(tensor_1))\n",
      "tensor_2=torch.tensor([[2.,4.],[1.2,0]])\n",
      "print(relu(tensor_2))\n",
      "\n",
      "\n",
      "\n",
      "    sample  > input layer and two hidden layers and a output layer\n",
      "\n",
      "input_layer=torch.tensor([[ 0.0401, -0.9005,  0.0397, -0.0876]])\n",
      "\n",
      "weight_1=torch.tensor([[-0.1094, -0.8285,  0.0416, -1.1222],\n",
      "        [ 0.3327, -0.0461,  1.4473, -0.8070],\n",
      "        [ 0.0681, -0.7058, -1.8017,  0.5857],\n",
      "        [ 0.8764,  0.9618, -0.4505,  0.2888]])\n",
      "\n",
      "\n",
      "weight_2=torch.tensor([[ 0.6856, -1.7650,  1.6375, -1.5759],\n",
      "        [-0.1092, -0.1620,  0.1951, -0.1169],\n",
      "        [-0.5120,  1.1997,  0.8483, -0.2476],\n",
      "        [-0.3369,  0.5617, -0.6658,  0.2221]])\n",
      "\n",
      "\n",
      "weight_3=torch.tensor([[ 0.8824,  0.1268,  1.1951,  1.3061],\n",
      "        [-0.8753, -0.3277, -0.1454, -0.0167],\n",
      "        [ 0.3582,  0.3254, -1.8509, -1.4205],\n",
      "        [ 0.3786,  0.5999, -0.5665, -0.3975]])\n",
      "\n",
      "\n",
      "# Calculate the first and second hidden layer\n",
      "hidden_1 = torch.matmul(input_layer, weight_1)\n",
      "hidden_2 = torch.matmul(hidden_1, weight_2)\n",
      "\n",
      "# Calculate the output\n",
      "print(torch.matmul(hidden_2, weight_3))\n",
      "\n",
      "# Calculate weight_composed_1 and weight\n",
      "weight_composed_1 = torch.matmul(weight_1, weight_2)\n",
      "weight = torch.matmul(weight_composed_1, weight_3)\n",
      "\n",
      "# Multiply input_layer with weight\n",
      "print(torch.matmul(input_layer, weight))\n",
      "\n",
      "\n",
      "tensor([[0.2655, 0.1311, 3.8221, 3.0032]])\n",
      "tensor([[0.2655, 0.1311, 3.8221, 3.0032]])\n",
      "\n",
      "\n",
      "     apply relu to the hidden layers\n",
      "\n",
      "\n",
      "# Apply non-linearity on hidden_1 and hidden_2\n",
      "hidden_1_activated = relu(torch.matmul(input_layer, weight_1))\n",
      "hidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))\n",
      "print(torch.matmul(hidden_2_activated, weight_3))\n",
      "\n",
      "# Apply non-linearity in the product of first two weights. \n",
      "weight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))\n",
      "\n",
      "# Multiply `weight_composed_1_activated` with `weight_3\n",
      "weight = torch.matmul(weight_composed_1_activated, weight_3)\n",
      "\n",
      "# Multiply input_layer with weight\n",
      "print(torch.matmul(input_layer, weight))\n",
      "\n",
      "    building the network\n",
      "\n",
      "# Instantiate ReLU activation function as relu\n",
      "relu = nn.ReLU()\n",
      "\n",
      "# Initialize weight_1 and weight_2 with random numbers\n",
      "weight_1 = torch.rand(4, 6)\n",
      "weight_2 = torch.rand(6, 2)\n",
      "\n",
      "# Multiply input_layer with weight_1\n",
      "hidden_1 = torch.matmul(input_layer, weight_1)\n",
      "\n",
      "# Apply ReLU activation function over hidden_1 and multiply with weight_2\n",
      "hidden_1_activated = relu(hidden_1)\n",
      "print(torch.matmul(hidden_1_activated, weight_2)) \n",
      "\n",
      "tensor([[0., 0.]])\n",
      "\n",
      "\n",
      "       Loss functions         >\n",
      "\n",
      "1. initialize the neural networks with random weights\n",
      "2. do a forward pass\n",
      "3. calculate loss function yielding a single number\n",
      "4. calculate the gradients\n",
      "5. change the weights based on gradients\n",
      "\n",
      "for regression : least squared loss\n",
      "for classification : softmax cross-entropy loss\n",
      "\n",
      "     softmax cross-entropy loss\n",
      "\n",
      "cat 3.2\n",
      "car 5.1\n",
      "frog -1.7\n",
      "\n",
      "probabilities must be >=0\n",
      "unnormalized probabilities\n",
      "cat 24.5\n",
      "car 164.0\n",
      "frog 0.18\n",
      "\n",
      "sum is 188\n",
      "\n",
      "probablies must sum to 1\n",
      "normalized probablities\n",
      "cat .13  (24/188)\n",
      "car .87  (164/188)\n",
      "frog 0.00  (.18/188)\n",
      "\n",
      "\n",
      "calculate the cross entropy loss\n",
      "\n",
      "cat L = -ln(0.13) = 2.0404\n",
      "car L = -ln(0.87) = .13826\n",
      "frog L = -ln(.18) = 1.714\n",
      "\n",
      "\n",
      "logits = torch.tensor([[3.2,5.1,-1.7]])\n",
      "ground_truth = torch.tensor([0]) #first class cat\n",
      "\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "\n",
      "loss=criterion(logits, ground_truth)\n",
      "print(loss)\n",
      "\n",
      "tensor (2.0404)\n",
      "\n",
      "#CrossEntropyLoss includes cross entropy and softmax together\n",
      "\n",
      "logits = torch.tensor([[10.2,5.1,-1.7]])\n",
      "\n",
      "tensor(0.0061)\n",
      "\n",
      "logits = torch.tensor([[-10.2,5.1,-1.7]])\n",
      "\n",
      "tensor(15.1011)\n",
      "\n",
      "\n",
      "    sample cross entropy loss\n",
      "\n",
      "The tensors for logits must have two dimensions\n",
      "\n",
      "\n",
      "# Initialize the scores and ground truth\n",
      "logits = torch.tensor([[-1.2, 0.12, 4.8]])\n",
      "ground_truth = torch.tensor([2])\n",
      "\n",
      "# Instantiate cross entropy loss\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "\n",
      "# Compute and print the loss\n",
      "loss = criterion(logits,ground_truth)\n",
      "print(loss)\n",
      "\n",
      "\n",
      "tensor(0.0117)\n",
      "\n",
      "\n",
      "   > sample loss for a tensor whose value is 111\n",
      "\n",
      "# Import torch and torch.nn\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "# Initialize logits and ground truth\n",
      "logits = torch.rand(1,1000)\n",
      "ground_truth = torch.tensor([111])\n",
      "\n",
      "# Instantiate cross-entropy loss\n",
      "criterion=nn.CrossEntropyLoss()\n",
      "\n",
      "# Calculate and print the loss\n",
      "loss = criterion(logits,ground_truth)\n",
      "print(loss)\n",
      "\n",
      "tensor(6.8696)\n",
      "\n",
      "\n",
      "   preparing a dataset in pytorch\n",
      "\n",
      "MNIST (70,000 hand written digits)\n",
      "28x28 images \n",
      "and CIFAR-10 (50,000 images with labels)\n",
      "32x32 red green and blue\n",
      "\n",
      "\n",
      "conda install -c pytorch torchvision\n",
      "\n",
      "import torchvision\n",
      "import torch.utils.data\n",
      "import torchvision.transforms as transforms\n",
      "\n",
      "transform=transforms.Compose(\n",
      "    [transforms.ToTensor(),\n",
      "        transforms.Normalize(\n",
      "                    (0.4914,0.48216,0.44653),\n",
      "                    (0.24703,0.24349,0.26159)\n",
      "                            )])\n",
      "\n",
      "#mean and std of each of the channels\n",
      "\n",
      "\n",
      "trainset=torchvision.datasets.CIFAR10(root='./data',train=True, download=True, transform=transform)\n",
      "\n",
      "testset=torchvision.datasets.CIFAR10(root='./data',train=False, download=True, transform=transform)\n",
      "\n",
      "\n",
      "testloader=torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
      "\n",
      "print(testloader.dataset.test_data.shape, trainload.dataset.train_data.shape)\n",
      "\n",
      "\n",
      "     sample  MNIST\n",
      "\n",
      "# Transform the data to torch tensors and normalize it \n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "\t\t\t\t\t\t\t\ttransforms.Normalize((0.1307), ((0.3081)))])\n",
      "\n",
      "# Prepare training set and testing set\n",
      "trainset = torchvision.datasets.MNIST('mnist', train=True, \n",
      "\t\t\t\t\t\t\t\t\t  download=True, transform=transform)\n",
      "testset = torchvision.datasets.MNIST('mnist', train=False,\n",
      "\t\t\t   download=True, transform=transform)\n",
      "\n",
      "# Prepare training loader and testing loader\n",
      "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
      "                                          shuffle=True, num_workers=0)\n",
      "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
      "\t\t\t\t\t\t\t\t\t\t shuffle=False, num_workers=0) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   >   torch.utils.data.DataLoader\n",
      "\n",
      "# Compute the shape of the training set and testing set\n",
      "trainset_shape = trainloader.dataset.train_data.shape\n",
      "testset_shape = testloader.dataset.test_data.shape\n",
      "\n",
      "# Print the computed shapes\n",
      "print(trainset_shape, testset_shape)\n",
      "\n",
      "# Compute the size of the minibatch for training set and testing set\n",
      "trainset_batchsize = trainloader.batch_size\n",
      "testset_batchsize = testloader.batch_size\n",
      "\n",
      "# Print sizes of the minibatch\n",
      "print(trainset_batchsize, testset_batchsize)\n",
      "\n",
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n",
      "32 32\n",
      "\n",
      "\n",
      "  > steps\n",
      "1. prepare the dataloaders\n",
      "2. build a neural network with random numbers\n",
      "\n",
      "Loop over:\n",
      "1. do a forward pass\n",
      "2. calculate the loss function, the number tell you how well the neural network is doing with the training set\n",
      "3. calculate the gradient\n",
      "4. change the weights based on the gradients\n",
      "a. weight-=weight_gradient * learning rate\n",
      "\n",
      "gradient is the steepness of the function\n",
      "\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.optim as optim\n",
      "\n",
      "class Net(nn.Module):\n",
      "\tdef __init__(self):\n",
      "\t\tsuper(Net,self).__init__()\n",
      "\t\tself.fc1=nn.Linear(32*32*3,500) #input layer of 32x32 pixel by 3 color channels with 500 neurons in the hidden layer\n",
      "\t\tself.fc2=nn.Linear(500,10)\n",
      "#10 output classes\n",
      "\n",
      "\tdef forward(self,x):\n",
      "\t\tx=F.relu(self.fc1(x))\n",
      "\t\treturn self.fc2(x)\n",
      "\n",
      "net=Net()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
      "\n",
      "correct,total=0,0\n",
      "predictions=[]\n",
      "for epoch in range(10):\n",
      "    for i, data in enumerate(trainloader,0):\n",
      "        inputs,labels=data\n",
      "        inputs=inputs.view(-1,32*32*3)\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        print(\"epoch\",epoch)\n",
      "        \n",
      "        outputs=net(inputs)\n",
      "        loss=criterion(outputs,labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()  #adjust weights\n",
      "\n",
      "        __,predicted=torch.max(outputs.data,1)\n",
      "        predictions.append(outputs)\n",
      "        total+=labels.size(0)\n",
      "        correct+=(predicted==labels).sum().item()\n",
      "\n",
      "print(\"The training set accuracy of the network is %d %%\" % (100*correct/total))\n",
      "\n",
      "\n",
      "\n",
      "correct,total=0,0\n",
      "predictions=[]\n",
      "for epoch in range(10):\n",
      "    for i, data in enumerate(testloader,0):\n",
      "        inputs,labels=data\n",
      "        inputs=inputs.view(-1,32*32*3)\n",
      "        outputs=net(inputs)\n",
      "        __,predicted=torch.max(outputs.data,1)\n",
      "        total += labels.size(0)\n",
      "        correct+=(predicted==labels).sum().item()\n",
      "\n",
      "print(\"The testing set accuracy of the network is %d %%\" % (100*correct/total))\n",
      "\n",
      "\n",
      "\n",
      "    sample \n",
      "\n",
      "# Define the class Net\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):    \n",
      "    \t# Define all the parameters of the net\n",
      "        super(Net, self).__init__()\n",
      "        self.fc1 = nn.Linear(28 * 28 * 1, 200)\n",
      "        self.fc2 = nn.Linear(200, 10)\n",
      "\n",
      "    def forward(self, x):    \n",
      "    \t# Do the forward pass\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "# Instantiate the Adam optimizer and Cross-Entropy loss function\n",
      "model = Net()   \n",
      "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "  \n",
      "for batch_idx, data_target in enumerate(train_loader):\n",
      "    data = data_target[0]\n",
      "    target = data_target[1]\n",
      "    data = data.view(-1, 28 * 28)\n",
      "    optimizer.zero_grad()\n",
      "\n",
      "    # Complete a forward pass\n",
      "    output = model(data)\n",
      "\n",
      "    # Compute the loss, gradients and change the weights\n",
      "    loss = criterion(output,target)\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "# Set the model in eval mode\n",
      "model.eval()\n",
      "\n",
      "for i, data in enumerate(test_loader, 0):\n",
      "    inputs, labels = data\n",
      "    \n",
      "    # Put each image into a vector\n",
      "    inputs = inputs.view(-1, 28*28)\n",
      "    \n",
      "    # Do the forward pass and get the predictions\n",
      "    outputs = model(inputs)\n",
      "    _, outputs = torch.max(outputs.data, 1)\n",
      "    total += labels.size(0)\n",
      "    correct += (outputs == labels).sum().item()\n",
      "print('The testing set accuracy of the network is: %d %%' % (100 * correct / total))\n",
      "\n",
      "\n",
      "      >Convolution operator\n",
      "\n",
      "1. do you need to consider all the relations between the features\n",
      "\n",
      "2. Fully connect neural networks are big and so very computationally inefficient\n",
      "\n",
      "3. they have so many parameters and so overfit\n",
      "\n",
      "convolutions\n",
      "1.slide over it and apply filter at each location. given the filter is 5x5x3\n",
      "2. dot product\n",
      "\n",
      "the convolution layer contains multiple activation maps\n",
      "\n",
      "padding allows the activation map to be the size of the image\n",
      "\n",
      "   two ways to use convolution neural networks\n",
      "\n",
      "******oop (torch.nn)*****\n",
      "in_channels (int) = number of channels in input\n",
      "\n",
      "out_channels(int0 number of channels produced by the convolution\n",
      "\n",
      "kernel_size(int or tuple) size of the convolving kernel\n",
      "\n",
      "stride(int or tuple,optional) stride of the convolution. default=1\n",
      "\n",
      "padding(int or tuple,optional) \n",
      "\n",
      "******functional(torch.nn.functional)*****\n",
      "\n",
      "inptu - input tensor of shape\n",
      "\n",
      "weight - filters of shape\n",
      "\n",
      "stride - the stride of the convolving kernel\n",
      "\n",
      "padding - implicit zero paddings on both sides of the input.\n",
      "\n",
      "\n",
      "\n",
      "   object oriented\n",
      "\n",
      "import torch\n",
      "import torch.nn\n",
      "\n",
      "image = torch.rand(16,3,32,32)\n",
      "conv_filter = torch.nn.Conv2d(in_channels=3,\n",
      "\tout_channels=1, kernel_size=5,\n",
      "\tstride=1, padding=0)\n",
      "\n",
      "output_feature = conv_filter(image)\n",
      "\n",
      "print(output_feature)\n",
      "\n",
      "\n",
      "   > functional\n",
      "\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "image = torch.rand(16,3,32,32)\n",
      "filter = torch.rand(1,3,5,5)\n",
      "\n",
      "out_feat_F = F.conv2d(image, filter, stride=1,\n",
      "padding=0)\n",
      "\n",
      "print(out_feat_F.shape)\n",
      "\n",
      "torch.Size([10, 6, 28, 28])\n",
      "\n",
      "     sample oop\n",
      "\n",
      "Create 10 images with shape (1, 28, 28).\n",
      "Build 6 convolutional filters of size (3, 3) with stride set to 1 and padding set to 1.\n",
      "Apply the filters in the image and print the shape of the feature map.\n",
      "\n",
      "\n",
      "\n",
      "# Create 10 random images of shape (1, 28, 28)\n",
      "images = torch.rand(10,1,28,28)\n",
      "\n",
      "# Build 6 conv. filters\n",
      "conv_filters = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, stride=1,padding=1)\n",
      "\n",
      "# Convolve the image with the filters \n",
      "output_feature = conv_filters(images)\n",
      "print(output_feature.shape)\n",
      "\n",
      " > sample function\n",
      "\n",
      "# Create 10 random images\n",
      "image = torch.rand(10,1,28,28)\n",
      "\n",
      "# Create 6 filters\n",
      "filters = torch.rand(6, 1, 3, 3)\n",
      "\n",
      "# Convolve the image with the filters\n",
      "output_feature =  F.conv2d(image, filters, stride=1,\n",
      "padding=1)\n",
      "print(output_feature.shape)\n",
      "\n",
      "torch.Size([10, 6, 28, 28])\n",
      "\n",
      "\n",
      "    >pooling operators\n",
      "\n",
      "pooling is a way of feature selection\n",
      "\n",
      "pooling lowers the spatial dimension\n",
      "\n",
      "224x224x64 pool -> 112x112x64\n",
      "\n",
      "max pool with stride=2\n",
      "\n",
      "3,1,3,5\n",
      "6,0,7,9\n",
      "3,2,1,4\n",
      "0,2,4,3\n",
      "\n",
      "\n",
      "3,1,6,0 ->6\n",
      "3,5,7,9 ->9\n",
      "3,2,0,2 ->3\n",
      "1,4,4,3 ->4\n",
      "\n",
      "pool output\n",
      "6,9\n",
      "3,4\n",
      "\n",
      "average-pooling\n",
      "2.5 (10/4),6 (24/4)\n",
      "1.75 (7/4),3 (12/4)\n",
      "\n",
      "\n",
      "\n",
      "dimensions: minimium size, height, depth, width\n",
      "\n",
      "\n",
      "import torch\n",
      "import torch.nn\n",
      "\n",
      "im = torch.Tensor([[[[3,1,3,5],[6,0,7,9],[3,2,1,4],[0,2,4,3]]]])\n",
      "\n",
      "max_pooling = torch.nn.MaxPool2d(2)\n",
      "output_features=max_pooling(im)\n",
      "print(output_features)\n",
      "\n",
      "\n",
      "\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "im = torch.Tensor([[[[3,1,3,5],[6,0,7,9],[3,2,1,4],[0,2,4,3]]]])\n",
      "\n",
      "\n",
      "output_feature_F=F.max_pool2d(im,2)\n",
      "print(output_feature_F)\n",
      "\n",
      "\n",
      "tensor([[[[6.,9.],[3.,4.]]]])\n",
      "\n",
      "\n",
      "output_feature_F=F.avg_pool2d(im,2)\n",
      "print(output_feature_F)\n",
      "\n",
      "tensor([[[[2.5.,6.],[1.75.,3.]]]])\n",
      "\n",
      "\n",
      "   > sample max pooling\n",
      "\n",
      "# Build a pooling operator with size `2`.\n",
      "max_pooling = torch.nn.MaxPool2d(2)\n",
      "# Apply the pooling operator\n",
      "output_feature = max_pooling(im)\n",
      "\n",
      "# Use pooling operator in the image\n",
      "output_feature_F = F.max_pool2d(im,2)\n",
      "\n",
      "# print the results of both cases\n",
      "print(output_feature)\n",
      "print(output_feature_F)\n",
      "\n",
      "tensor([[[[8., 5., 9.],\n",
      "          [9., 2., 6.],\n",
      "          [2., 9., 8.]]]])\n",
      "tensor([[[[8., 5., 9.],\n",
      "          [9., 2., 6.],\n",
      "          [2., 9., 8.]]]])\n",
      "\n",
      "\n",
      "  > sample  > avg pooling\n",
      "\n",
      "avg_pooling = torch.nn.AvgPool2d(2)\n",
      "# Apply the pooling operator\n",
      "output_feature = avg_pooling(im)\n",
      "\n",
      "# Use pooling operator in the image\n",
      "output_feature_F = F.avg_pool2d(im,2)\n",
      "\n",
      "# print the results of both cases\n",
      "print(output_feature)\n",
      "print(output_feature_F)\n",
      "\n",
      "tensor([[[[ 3.7500,  0.5000,  5.0000],\n",
      "          [ 3.5000, -1.0000,  3.7500],\n",
      "          [-0.2500,  4.2500,  0.5000]]]])\n",
      "tensor([[[[ 3.7500,  0.5000,  5.0000],\n",
      "          [ 3.5000, -1.0000,  3.7500],\n",
      "          [-0.2500,  4.2500,  0.5000]]]])\n",
      "\n",
      "\n",
      "\n",
      "      >Convolutional neural networks\n",
      "cnn resurgence in 2012\n",
      "alexnet\n",
      "\n",
      "\n",
      "\n",
      "convolution->max pooling->max ppoling ->hidden ->hidden->max pooling ->dense->dense->dense\n",
      "\n",
      "\n",
      "output of 1000 different classes\n",
      "\n",
      "class AlexNet(nn.Module):\n",
      "\n",
      "\tdef __init__(self, num_classes=1000):\n",
      "\t\tsuper(AlexNet, self).__init__()\n",
      "\t\t\n",
      "\t\tself.conv1=nnConv2d(3,64,kernel_size=11, stride=4, padding=2)\n",
      "\t\tself.relu=nn.ReLU(inplace=True)\n",
      "\t\tself.maxpool=nn.MaxPool2d(kernel_size=3,stride=2)\n",
      "\t\tself.conv2=nn.Conv2d(64,192,kernel_size=5,padding=2)\n",
      "\t\tself.conv3=nn.Conv2d(192,384,kernel_size=3,padding=1)\n",
      "\t\tself.conv4=nn.Conv2d(384,256,kernel_size=3,padding=1)\n",
      "\t\tself.conv5=nn.Conv2d(256,256,kernel_size=3,padding=1)\n",
      "self.avgpool=nn.AdaptiveAvgPool2d((6,6))\n",
      "\t\tself.fc1=nn.Linear(256*6*6,4096)\n",
      "\t\tself.fc2=nn.Linear(4096,4096)\n",
      "\t\tself.fc3=nn.Linear(4096,num_classes)\n",
      "\n",
      "\tdef forward(self, x):\n",
      "\t\tx=self.relu(self.conv1(x))\n",
      "\t\tx=self.maxpool(x)\n",
      "\t\tx=self.relu(self.conv2(x))\n",
      "\t\tx=self.maxpool(x)\n",
      "\t\tx=self.relu(self.conv3(x))\n",
      "\t\tx=self.relu(self.conv4(x))\n",
      "\t\tx=self.relu(self.conv5(x))\n",
      "\t\tx=self.maxpool(x)\n",
      "\t\tx=self.avgpool(x)\n",
      "\t\tx=x.view(x.size(0),256*6*6)\n",
      "\t\tx=self.relu(self.fc1(x))\n",
      "\t\tx=self.relu(self.fc2(x))\n",
      "\t\tx=self.relu(self.fc3(x))\n",
      "\t\treturn x\n",
      "\n",
      "net=AlexNet()\n",
      "\n",
      "\n",
      "   MNIST convolution network\n",
      "\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        \n",
      "        # Instantiate two convolutional layers\n",
      "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
      "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
      "        \n",
      "        # Instantiate the ReLU nonlinearity\n",
      "        self.relu = nn.ReLU()\n",
      "        \n",
      "        # Instantiate a max pooling layer\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        \n",
      "        # Instantiate a fully connected layer\n",
      "        self.fc = nn.Linear(7 * 7 * 10, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "\n",
      "        # Apply conv followd by relu, then in next line pool\n",
      "        x = self.relu(self.conv1(x))\n",
      "        x = self.pool(x)\n",
      "\n",
      "        # Apply conv followd by relu, then in next line pool\n",
      "        x = self.relu(self.conv2(x))\n",
      "        x = self.pool(x)\n",
      "\n",
      "        # Prepare the image for the fully connected layer\n",
      "        x = x.view(-1,7 * 7 * 10)\n",
      "\n",
      "        # Apply the fully connected layer and return the result\n",
      "        return self.fc(x)\n",
      "\n",
      "\n",
      "    32x32 convolution network\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self,num_classes=10):\n",
      "        super(Net, self).__init__()\n",
      "        \n",
      "        # Instantiate two convolutional layers\n",
      "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
      "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
      "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
      "        \n",
      "        # Instantiate the ReLU nonlinearity\n",
      "        self.relu = nn.ReLU()\n",
      "        \n",
      "        # Instantiate a max pooling layer\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        \n",
      "        # Instantiate a fully connected layer\n",
      "        self.fc = nn.Linear(128*4*4, num_classes)  #fully connect layer  = units of the last layer   4= 32/2*3 where 2 is the pool and 3 is the kernel\n",
      "\n",
      "\tdef forward(self, x):\n",
      "\t\tx=self.pool(F.relu(self.conv1(x)))\n",
      "\t\tx=self.pool(F.relu(self.conv2(x)))\n",
      "\t\tx=self.pool(F.relu(self.conv3(x)))\n",
      "\t\tx=x.view(-1,128*4*4)\n",
      "\t\treturn self.fc(x)\n",
      "\n",
      "net = Net()\n",
      "criterion=nn.CrossEntropyLoss()\n",
      "optimizer=optim.Adam(net.parameters(), lr=3e-4)\n",
      "\n",
      "\n",
      "for epoch in range(10)\n",
      "\tfor i, data in enumerate(trainloader,0):\n",
      "\t\tinputs,labels=data\n",
      "\n",
      "\t\toptimizer.zero_grad()\n",
      "\n",
      "\t\toutputs=net(inputs)\n",
      "\t\tloss=criterion(outputs,labels)\n",
      "\t\tloss.backward() #gradient\n",
      "\t\toptimizer.step() #update weights\n",
      "\n",
      "print('Finished Training')\n",
      "\n",
      "\n",
      "correct,total=0,0\n",
      "predictions=[]\n",
      "net.eval()\n",
      "for epoch in range(10):\n",
      "    for i, data in enumerate(testloader,0):\n",
      "        inputs,labels=data\n",
      "        outputs=net(inputs)\n",
      "        __,predicted=torch.max(outputs.data,1)\n",
      "        predictions.append(outputs)\n",
      "        total += labels.size(0)\n",
      "        correct+=(predicted==labels).sum().item()\n",
      "\n",
      "print(\"The testing set accuracy of the network is %d %%\" % (100*correct/total))\n",
      "\n",
      "     sample     see which images were correctly identified\n",
      "\n",
      "# Iterate over the data in the test_loader\n",
      "for i, data in enumerate(test_loader):\n",
      "  \n",
      "    # Get the image and label from data\n",
      "    image,label = data\n",
      "    \n",
      "    # Make a forward pass in the net with your image\n",
      "    output = net(image)\n",
      "    \n",
      "    # Argmax the results of the net\n",
      "    _, predicted = torch.max(output.data, 1)\n",
      "    if predicted == label:\n",
      "        print(\"Yipes, your net made the right prediction \" + str(predicted))\n",
      "    else:\n",
      "        print(\"Your net prediction was \" + str(predicted) + \", but the correct label is: \" + s\n",
      "\n",
      "\n",
      "       >The sequential module\n",
      "\n",
      "\n",
      "\n",
      "class AlexNet(nn.Module):\n",
      "\n",
      "\tdef __init__(self, num_classes=1000):\n",
      "\t\tsuper(AlexNet, self).__init__()\n",
      "\t\t\n",
      "self.features=nn.Sequential(\n",
      "\t\tConv2d(3,64,kernel_size=11, stride=4, padding=2),\n",
      "\t\tnn.MaxPool2d(kernel_size=3,stride=2),\n",
      "\t\tnn.Conv2d(64,192,kernel_size=5,padding=2),\n",
      "                nn.Conv2d(192,384,kernel_size=3,padding=1),\n",
      "\t\tnn.Conv2d(384,256,kernel_size=3,padding=1),\n",
      "                nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
      "                nn.MaxPool2d(kernel_size=3,stride=2),)\n",
      "\n",
      "\n",
      "self.avgpool=nn.AdaptiveAvgPool2d((6,6))\n",
      "self.classifer=nn.Sequential(\n",
      "\t\tnn.Dropout(),\n",
      "\t\tnn.Linear(256*6*6,4096),\n",
      "\t\tnn.Dropout(),\n",
      "\t\tnn.Linear(4096,4096),\n",
      "\t\tnn.RelU(inplace=True),\n",
      "\t\tself.fc3=nn.Linear(4096,num_classes)\n",
      ",)\n",
      "\n",
      "\n",
      "\tdef forward(self,x):\n",
      "\t\tx=self.features(x)\n",
      "\t\tx=self.avgpool(x)\n",
      "\t\tx=x.view(x.size(0),256*6*6)\n",
      "\t\tx=self.classifier(x)\n",
      "\t\treturn x\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   > sample    sequential\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        \n",
      "        # Declare all the layers for feature extraction\n",
      "               self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
      "                                      nn.ReLU(inplace=True),\n",
      "                                      nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1),\n",
      "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
      "                                      nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n",
      "                                      nn.ReLU(inplace=True),\n",
      "                                      nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1), \n",
      "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),)\n",
      "\n",
      "\n",
      "        self.classifier = nn.Sequential(nn.Linear(7 * 7 * 40, 1024), nn.ReLU(inplace=True),\n",
      "                                       \tnn.Linear(1024, 2048), nn.ReLU(inplace=True),\n",
      "                                        nn.Linear(2048, 10))\n",
      "\n",
      "\n",
      "    def forward(self, x):\n",
      "      \n",
      "        # Apply the feature extractor in the input\n",
      "        x = self.features(x)\n",
      "        \n",
      "        # Squeeze the three spatial dimensions in one\n",
      "        x = x.view(-1, 7 * 7 * 40)\n",
      "        \n",
      "        # Classify the images\n",
      "        x = self.classifier(x)\n",
      "        return x\n",
      "\n",
      "\n",
      "    > detecting overfitting\n",
      "1. high variance case\n",
      "2. low training error\n",
      "\n",
      "large gap between the variance an error this is called overfitting called high variance.\n",
      "\n",
      "\n",
      "create a validation set\n",
      "\n",
      "1. training set\n",
      "2. validation set\n",
      "3. testing test (used only once)\n",
      "\n",
      "training sets and validation set should not overlap each other\n",
      "\n",
      "indices=np.arange(50000)\n",
      "np.random.shuffle(indices)\n",
      "\n",
      "trainloader=torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=False, num_workers=2,\n",
      "sampler=torch.utils.data.SubsetRandomSampler(indices[:45000])\n",
      "\n",
      ")\n",
      "\n",
      "valloader=torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2,\n",
      ",sampler=torch.utils.data.SubsetRandomSampler(indices[45000:50000])\n",
      ")\n",
      "\n",
      "\n",
      "   > sampler  build the validation\n",
      "\n",
      "# Shuffle the indices\n",
      "indices = np.arange(60000)\n",
      "np.random.shuffle(indices)\n",
      "\n",
      "# Build the train loader\n",
      "train_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
      "                     transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
      "                     batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[:55000]))\n",
      "\n",
      "# Build the validation loader\n",
      "val_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
      "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
      "                   batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[55000:]))\n",
      "\n",
      "\n",
      "\n",
      "   > regularization techniques\n",
      "\n",
      "\n",
      "l2-regularization\n",
      "\n",
      "optimizer = optim.Adam(net.parameters(), lr=3e=4, weight_decay=0.0001)\n",
      "\n",
      "\n",
      "Dropout to reduce the complexity of the network\n",
      "\n",
      "self.classifier = nn.Sequential(\n",
      "\tnn.Dropout(p=0.5),\n",
      "\tnn.Linear(256*6*6, 4096),\n",
      "\tnn.ReLU(inplace=True),\n",
      "\tnn.Dropout(p=0.5)\n",
      "\tnn.Linear(4096,4096),\n",
      "\tnn.ReLU(inplace=True),\n",
      "\tnn.Linear(4096,num_classes),\n",
      ")\n",
      "\n",
      "\n",
      "    batch regularization\n",
      "\n",
      "1. computes the mean and variance for each feature\n",
      "\n",
      "2. normalizes the features\n",
      "\n",
      "3. scale and shift\n",
      "\n",
      "always train large neural networks using batch normalization.\n",
      "\n",
      "self.bn = nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.9)\n",
      "\n",
      "\n",
      "     early stopping\n",
      "\n",
      "1. stop training when no more improvement is occuring or loss has plateau\n",
      "\n",
      "hyperparameters\n",
      "1. train many networkss with different hyperparameters and test them in the validation set.  Then use the best performing net in the validation set to know the expected accuracy of the network in the new data.\n",
      "\n",
      "\n",
      "model.train()\n",
      "\n",
      "model.eval()\n",
      "\n",
      "  > sample\n",
      "\n",
      "# Instantiate the network\n",
      "model = Net()\n",
      "\n",
      "# Instantiate the cross-entropy loss\n",
      "criterion = CrossEntropyLoss()\n",
      "\n",
      "# Instantiate the Adam optimizer\n",
      "optimizer = optimum.Adam(model.parameters(), lr=3e-4, weight_decay=0.0001)\n",
      "\n",
      "\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        \n",
      "        # Define all the parameters of the net\n",
      "        self.classifier = nn.Sequential(\n",
      "            nn.Linear(28*28, 200),\n",
      "            nn.ReLU(inplace=True),\n",
      "            nn.Dropout(p=0.5),\n",
      "            nn.Linear(200,500),\n",
      "            nn.ReLU(inplace=True),\n",
      "            nn.Linear(500,10))\n",
      "\n",
      "    def forward(self, x):\n",
      "    \n",
      "    \t# Do the forward pass\n",
      "        return self.classifier(x)\n",
      "\n",
      "\n",
      "   batch normalization\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        \n",
      "        # Implement the sequential module for feature extraction\n",
      "        self.features = nn.Sequential(\n",
      "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
      "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(10),\n",
      "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
      "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(20))\n",
      "        \n",
      "        # Implement the fully connected layer for classification\n",
      "        self.fc = nn.Linear(in_features=7*7*20, out_features=10)\n",
      "\n",
      "\n",
      "\n",
      "      >Transfer Learning\n",
      "\n",
      "the deeper you progress in the cnn network the more abstract the features become.\n",
      "\n",
      "1. Decision layers\n",
      "2. parts of an object (wheel or eye)\n",
      "3. simple geometrical shapes (circle or squares)\n",
      "4. edges\n",
      "5. image\n",
      "\n",
      "from 5 to 1\n",
      "\n",
      "the low level features are very general and largely data dependant\n",
      "\n",
      "the decision layers are fully connected layer for applying rules to determine outcomes.\n",
      "\n",
      "transfer learning uses weights on very training weights of large datasets then training the network on small numbers of images.\n",
      "\n",
      "\n",
      "freeze most of the layer to prevent overfitting\n",
      "\n",
      "\n",
      "?           >finetuning\n",
      "\n",
      "model=Net()\n",
      "\n",
      "model.load_state_dict(torch.load('cifar10_net_path'))\n",
      "\n",
      "\n",
      "#freeze all the layers bar the final one\n",
      "\n",
      "for param in model.parameters():\n",
      "\tparam.requires_grad=False\n",
      "\n",
      "model.fc=nn.Linear(4*4*1024,100)\n",
      "\n",
      "model.train()\n",
      "\n",
      "       Torch vision\n",
      "\n",
      "import torchvision\n",
      "\n",
      "\n",
      "model=torchvision.models.resnet18(pretrained=True)\n",
      "\n",
      "model.fc=nn.Linear(512, num_classes)\n",
      "\n",
      "\n",
      "     sample\n",
      "\n",
      "# Create a new model\n",
      "model = Net()\n",
      "\n",
      "# Change the number of out channels\n",
      "model.fc = nn.Linear(7 * 7 * 512, 26)\n",
      "\n",
      "# Train and evaluate the model\n",
      "model.train()\n",
      "train_net(model, optimizer, criterion)\n",
      "print(\"Accuracy of the net is: \" + str(model.eval()))\n",
      "\n",
      "Accuracy of the net is: 0.57\n",
      "\n",
      "\n",
      "    sample load from a pretrained network\n",
      "\n",
      "# Create a model using\n",
      "model = Net()\n",
      "\n",
      "# Load the parameters from the old model\n",
      "model.load_state_dict(torch.load('my_net.pth'))\n",
      "\n",
      "# Change the number of out channels\n",
      "model.fc = nn.Linear(7 * 7 * 512, 26)\n",
      "\n",
      "# Train and evaluate the model\n",
      "model.train()\n",
      "train_net(model, optimizer, criterion)\n",
      "print(\"Accuracy of the net is: \" + str(model.eval()))\n",
      "\n",
      "Accuracy of the net is: 0.84\n",
      "\n",
      "\n",
      "    >resnet18\n",
      "\n",
      "# Import the module\n",
      "import torchvision\n",
      "\n",
      "# Download resnet18\n",
      "model = torchvision.models.resnet18(pretrained=True)\n",
      "\n",
      "# Freeze all the layers bar the last one\n",
      "for param in model.parameters():\n",
      "    param.requires_grad=False\n",
      "\n",
      "# Change the number of output units\n",
      "model.fc = nn.Linear(512, 7)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\spectrogram and fft.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\spectrogram and fft.txt\n",
      "Fourier transform views timeseries as a series of fast and slow waves\n",
      "\n",
      "1. timeseries can be described as a combination of quickly changing things and slowly changing things\n",
      "\n",
      "2. At each moment in time, we can describe the relative presence of fast and slow moving components\n",
      "\n",
      "steps\n",
      "\n",
      "1. choose a window size and shape\n",
      "2. at a timepoint, calculate the fft for that window\n",
      "3. slide the window over by one\n",
      "4. aggregate the results\n",
      "\n",
      "Called a short-time fourier transform\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\tensor basics.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\tensor basics.txt\n",
      "import tensorflow as tf\n",
      "\n",
      "d0 = tf.ones((1,))\n",
      "#1D\n",
      "d1 = tf.ones((2,))\n",
      "#2D\n",
      "d2 = tf.ones((2,2))\n",
      "#3d\n",
      "d3 = tf.ones((2,2,2))\n",
      "\n",
      "print(d3.numpy())\n",
      "\n",
      "#defines a 2x3 constant\n",
      "a= constant(3, shape=[2,3])\n",
      "\n",
      "#define a 2x2 constant\n",
      "b= constant([1,2,3,4], shape[2,2])\n",
      "\n",
      "operation\n",
      "tf.constant()  constant([1,2,3])\n",
      "tf.zeros() zeros([2,2])\n",
      "tf.zeros_like() zeros_like(input_tensor)\n",
      "tf.ones() ones([2,2])\n",
      "tf.ones_like() ones_like(input_tensor)\n",
      "tf.fill() fill([3,3],7)\n",
      "\n",
      "the variable shape is fixed but the values of the shape can change during run time.\n",
      "\n",
      "a0 = tf.Variable([1,2,3,4,5,6], dtype=tf.float32)\n",
      "a1 = tf.Variable([1,2,3,4,5,6], dtype=tf.int16)\n",
      "\n",
      "b=tf.constant(2,tf.float32)\n",
      "\n",
      "c0=tf.multiply(a0,b)\n",
      "c1=a0*b  #tf.multiply is overloaded allowing this expression to be used\n",
      "\n",
      "   sample   > convert a np array into a tensor flow constant\n",
      "\n",
      "# Import constant from TensorFlow\n",
      "from tensorflow import constant\n",
      "\n",
      "print(credit_numpy)\n",
      "# Convert the credit_numpy array into a tensorflow constant\n",
      "credit_constant = constant(credit_numpy)\n",
      "\n",
      "# Print constant datatype\n",
      "print('The datatype is:', credit_constant.dtype)\n",
      "\n",
      "# Print constant shape\n",
      "print('The shape is:', credit_constant.shape)\n",
      "\n",
      "   sample  > create a variable then convert it to a numpy array\n",
      "\n",
      "# Define the 1-dimensional variable A1\n",
      "A1 = Variable([1, 2, 3, 4])\n",
      "\n",
      "# Print the variable A1\n",
      "print(A1)\n",
      "\n",
      "# Convert A1 to a numpy array and assign it to B1\n",
      "B1 = A1.numpy()\n",
      "\n",
      "# Print B1\n",
      "print(B1)\n",
      "\n",
      "\n",
      "     >basic operations\n",
      "\n",
      "graphs contain edges and nodes\n",
      "\n",
      "where the edges are tensors and the nodes are operations\n",
      "\n",
      "MatMul\n",
      "Add\n",
      "\n",
      "Const & Const_1 are fed to Add_1\n",
      "Const_2 & const_3 are fed to Add resulting in Add_2\n",
      "\n",
      "Add_1 & Add_2 are fed to MatMul\n",
      "\n",
      "from tensorflow import constant, add\n",
      "\n",
      "A0=constant([1])\n",
      "B0=constant([2])\n",
      "\n",
      "1 dimensional tensors\n",
      "A1=constant([1,2])\n",
      "B1=constant([3,4])\n",
      "\n",
      "2 dimensional tensors\n",
      "A2 = constant([1,2],[3,4])\n",
      "B2 = constant([5,6],[7,8])\n",
      "\n",
      "C0=add(A0,B0)\n",
      "C1=add(A1,B1)\n",
      "C2=add(A2,B2)\n",
      "\n",
      "add requires that each tensor have the same shape\n",
      "\n",
      "from tensorflow import ones, matmul, multiply\n",
      "\n",
      "a0=ones(1)\n",
      "a31=ones([3,1])\n",
      "a34=ones([3,4])\n",
      "a43=ones([4,3])\n",
      "\n",
      "matmul(A43,A34) but not matmul(A43,A43)\n",
      "\n",
      "reduce_sum() sums over the dimension of a tensor\n",
      "\n",
      "A=ones([2,3,4])\n",
      "or 2*3*4 = 24 ones\n",
      "\n",
      "x=\n",
      "1 1 1\n",
      "1 1 1\n",
      "\n",
      "reduce_sum(x,0)\n",
      "[1,1,1]+[1,1,1]\n",
      "\n",
      "reduce_sum(x,1)\n",
      "[1,1]+[1,1]+[1,1]\n",
      "\n",
      "#sum over all dimensions\n",
      "B= reduce_sum(A) -> 24\n",
      "\n",
      "B0=reduce_sum(A,0) 3x4 of 2\n",
      "B1=reduce_sum(A,1) 2x4 of 3\n",
      "B3=reduce_sum(A,2) 2x3 of 4\n",
      "\n",
      "\n",
      "   sample  > tensor multiplication of two tensors\n",
      "\n",
      "# Define tensors A1 and A23 as constants\n",
      "A1 = constant([1, 2, 3, 4])\n",
      "A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
      "\n",
      "# Define B1 and B23 to have the correct shape\n",
      "B1 = ones_like(A1)\n",
      "B23 = ones_like(A23)\n",
      "\n",
      "# Perform element-wise multiplication\n",
      "C1 = A1*B1\n",
      "C23 = A23*B23\n",
      "\n",
      "# Print the tensors C1 and C23\n",
      "print('C1: {}'.format(C1.numpy()))\n",
      "print('C23: {}'.format(C23.numpy()))\n",
      "\n",
      "\n",
      "   sample    tensor matmul\n",
      "\n",
      "# Define features, params, and bill as constants\n",
      "features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
      "params = constant([[1000], [150]])\n",
      "bill = constant([[3913], [2682], [8617], [64400]])\n",
      "\n",
      "# Compute billpred using features and params\n",
      "billpred =matmul(features,params)\n",
      "\n",
      "# Compute and print the error\n",
      "error = bill-billpred \n",
      "print(error.numpy())\n",
      "\n",
      "output:\n",
      "[[-1687]\n",
      " [-3218]\n",
      " [-1933]\n",
      " [57850]]\n",
      "\n",
      "\n",
      "wealth=\n",
      "[11 50]\n",
      "[7  2 ]\n",
      "[4  60]\n",
      "[3  0 ]\n",
      "[25 10]\n",
      "\n",
      "    >advanced operations\n",
      "\n",
      "gradient() computes the slope of a function at a point\n",
      "reshape() reshapes a tensor\n",
      "random() populates tensor with entries drawn from a probability distribution\n",
      "\n",
      "need to find a optimum\n",
      "\n",
      "Minimum : lowest value of a loss function\n",
      "Maximum : highest value of a objective function\n",
      "\n",
      "gradient : find a point where the gradient is 0\n",
      "\n",
      "check if the gradient is increasing or decreasing\n",
      "minimum : change in gradient > 0\n",
      "maximum : change in gradient < 0\n",
      "\n",
      "x=tf.Variable(-1.0)\n",
      "\n",
      "with tf.GradientTape() as tape:\n",
      "\ttape.watch(x)\n",
      "\ty=tf.multiply(x,x)\n",
      "\n",
      "g=tape.gradient(y,x)\n",
      "print(g.numpy())\n",
      "\n",
      "     reshaping \n",
      "grayscale \n",
      "\n",
      "grays 0 - 255\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "#generate a 2x2 matrix\n",
      "gray = tf.random.uniform([2,2],maxval=255, dtype='int32')\n",
      "\n",
      "#reshape into a 4x1 vector\n",
      "gray = tf.reshape(gray,[2*2,1])\n",
      "\n",
      "color = tf.random.uniform([2,2,3],maxval=255,dtype='int32')\n",
      "\n",
      "color=tf.reshape(color,[2x2,3])\n",
      "\n",
      "\n",
      "   >sample  > sign language tensor\n",
      "\n",
      "# Reshape the grayscale image tensor into a vector\n",
      "gray_vector = reshape(gray_tensor, (28*28, 1))\n",
      "\n",
      "# Reshape the color image tensor into a vector\n",
      "color_vector = reshape(color_tensor, (28*28, 3))\n",
      "\n",
      "\n",
      "   sample  > compute the gradient for different x along the a parabolia\n",
      "\n",
      "def compute_gradient(x0):\n",
      "  \t# Define x as a variable with an initial value of x0\n",
      "\tx = Variable(x0)\n",
      "\twith GradientTape() as tape:\n",
      "\t\ttape.watch(x)\n",
      "        # Define y using the multiply operation\n",
      "\t\ty = multiply(x,x)\n",
      "    # Return the gradient of y with respect to x\n",
      "\treturn tape.gradient(y, x).numpy()\n",
      "\n",
      "# Compute and print gradients at x = -1, 1, and 0\n",
      "print(compute_gradient(-1.0))\n",
      "print(compute_gradient(1.0))\n",
      "print(compute_gradient(0.0))\n",
      "\n",
      "The slope is negative at x = -1, which means that we can lower the loss by increasing x. The slope at x = 0 is 0, which means that we cannot lower the loss by either increasing or decreasing x. This is because the loss is minimized at x = 0.\n",
      "\n",
      "\n",
      "   sample use matrix multiplication to predict a letter from an image\n",
      "\n",
      "You want to determine whether the letter is an X or a K. You don't have a trained neural network, but you do have a simple model, model, which can be used to classify letter.\n",
      "\n",
      "# Reshape model from a 1x3 to a 3x1 tensor\n",
      "model = reshape(model, (3, 1))\n",
      "\n",
      "# Multiply letter by model\n",
      "output = matmul(letter, model)\n",
      "\n",
      "# Sum over output and print prediction using the numpy method\n",
      "prediction = reduce_sum(output)\n",
      "print(prediction.numpy())\n",
      "\n",
      "model:\n",
      "[[ 1.]\n",
      " [ 0.]\n",
      " [-1.]]\n",
      "letter\n",
      "\n",
      "letter is\n",
      "[[1. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 0. 1.]]\n",
      "\n",
      "      >input data\n",
      "\n",
      "linear model\n",
      "\n",
      "convert data to a numpy array\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "housing=pd.read_csv('kc_housing.csv')\n",
      "housing=np.array(housing)\n",
      "\n",
      "\n",
      "read_csv\n",
      "1. filepath_or_buffer\n",
      "2.sep delimiter between columns\n",
      "3. delim_whitespace\n",
      "4. encoding\n",
      "\n",
      "print(housing[3][3],housing[3][17])\n",
      "\n",
      "'id', \n",
      "'date', \n",
      "'price', \n",
      "'bedrooms', \n",
      "'bathrooms',\n",
      "'sqft_living', \n",
      "'sqft_lot', \n",
      "'floors', \n",
      "'waterfront', (boolean)\n",
      "'view',   (boolean)\n",
      "'condition',\n",
      "'grade', \n",
      "'sqft_above', \n",
      "'sqft_basement', \n",
      "'yr_built', \n",
      "'yr_renovated',\n",
      "'zipcode', \n",
      "'lat', \n",
      "'long', \n",
      "'sqft_living15', \n",
      "'sqft_lot15'\n",
      "\n",
      "\n",
      "price float\n",
      "waterfront boolean\n",
      "\n",
      "price=np.array(housing['price'],np.float32)\n",
      "waterfront=np.array(housing['waterfront'],np.bool)\n",
      "\n",
      "or\n",
      "price=tf.cast(housing['price'],tf.float32)\n",
      "waterfront=tf.cast(housing['waterfront'],tf.bool)\n",
      "\n",
      "\n",
      "  sample   > load the dataframe\n",
      "\n",
      "# Import pandas under the alias pd\n",
      "import pandas as pd\n",
      "\n",
      "# Assign the path to a string variable named data_path\n",
      "data_path = 'kc_house_data.csv'\n",
      "\n",
      "# Load the dataset as a dataframe named housing\n",
      "housing = pd.read_csv(data_path)\n",
      "\n",
      "# Print the price column of housing\n",
      "print(housing['price'])\n",
      "\n",
      "\n",
      "# Import numpy and tensorflow with their standard aliases\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "# Use a numpy array to define price as a 32-bit float\n",
      "price = np.array(housing['price'], np.float32)\n",
      "\n",
      "# Define waterfront as a Boolean using cast\n",
      "waterfront = tf.cast(housing['waterfront'], tf.bool)\n",
      "\n",
      "# Print price and waterfront\n",
      "print(price)\n",
      "print(waterfront)\n",
      "\n",
      "\n",
      "        >Loss Functions\n",
      "\n",
      "how to train models\n",
      "\n",
      "loss functions tell us how well the model fits the data\n",
      "\n",
      "we want to minimize the loss function\n",
      "\n",
      "Mean squared Error (MSE)\n",
      "Mean absolute Error (MAE)\n",
      "Huber error\n",
      "\n",
      "loss functions are accessible from tf.keras.losses()\n",
      "tf.keras.losses.mse()\n",
      "tf.keras.losses.mae()\n",
      "tf.keras.losses.Huber()\n",
      "\n",
      "MSE\n",
      "1. strongly penalizes outliers\n",
      "2. high gradient sensitivity near minimum\n",
      "\n",
      "MAE\n",
      "1. Scales linearly with size of error\n",
      "2. low sensitivity near minimum\n",
      "\n",
      "Huber\n",
      "1. Similar to MSE near minimum\n",
      "2. Similar to MAE away from minimum\n",
      "\n",
      "\n",
      "import tensorflow as tf\n",
      "loss=tf.keras.losses.mse(targets, predictions)\n",
      "\n",
      "\n",
      "def linear_regression(intercept, slope=slope, features=features):\n",
      "\treturn intercept+features*slope\n",
      "\n",
      "def loss_function(intercept, slope, targets=targets, features=features):\n",
      "\tpredictions=linear_regression(intercept,slope)\n",
      "\n",
      "\treturn tf.keras.losses.mse(targets,predictions)\n",
      "\n",
      "\n",
      "loss_function(intercept, slope, test_targets, test_features)\n",
      "\n",
      "  >Sample    Price and prediction error\n",
      "\n",
      "# Import the keras module from tensorflow\n",
      "from tensorflow import keras\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Compute the mean squared error (mse)\n",
      "loss = keras.losses.mse(price, predictions)\n",
      "\n",
      "# Print the mean squared error (mse)\n",
      "print(loss.numpy())\n",
      "\n",
      "plt.clf()\n",
      "plt.plot(predictions)\n",
      "plt.plot(price,color='red',alpha=0.2)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "You may have noticed that the MAE was much smaller than the MSE, even though price and predictions were the same. This is because the different loss functions penalize deviations of predictions from price differently. MSE does not like large deviations and punishes them harshly.\n",
      "\n",
      "\n",
      "   sample  > loss function\n",
      "\n",
      "# Initialize a variable named scalar\n",
      "scalar = Variable(1.0, float32)\n",
      "\n",
      "# Define the model\n",
      "def model(scalar, features = features):\n",
      "  \treturn scalar * features\n",
      "\n",
      "# Define a loss function\n",
      "def loss_function(scaler, features = features, targets = targets):\n",
      "\t# Compute the predicted values\n",
      "\tpredictions = model(scalar, features)\n",
      "    \n",
      "\t# Return the mean absolute error loss\n",
      "\treturn keras.losses.mae(targets, predictions)\n",
      "\n",
      "# Evaluate the loss function and print the loss\n",
      "print(loss_function(scalar).numpy())\n",
      "\n",
      "        >Linear Regression\n",
      "\n",
      "assumes that the relationship between two variables can be described by a line\n",
      "\n",
      "price=intercept + size * slope + error\n",
      "\n",
      "the difference between the predicted price and the actual price is the error and it can be used to construct the loss function.\n",
      "\n",
      "\n",
      "def linear_regression(intercept, slope, features=size):\n",
      "\treturn intercept+features*slope\n",
      "\n",
      "def loss_function(intercept, slope, targets=price, features=size):\n",
      "\t# Compute the predicted values\n",
      "\tpredictions = linear_regression(intercept, slope)\n",
      "    \n",
      "\t# Return the mean absolute error loss\n",
      "\treturn keras.losses.mae(targets, predictions)\n",
      "\n",
      "\n",
      "#define an optimization operation\n",
      "\n",
      "opt=tf.keras.optimizers.Adam()\n",
      "\n",
      "\n",
      "opt=tf.keras.optimizers.Adam()\n",
      "for j in range(1000):\n",
      "    opt.minimize(lambda: loss_function(intercept,slope),\\\n",
      "    var_list=[intercept,slope])\n",
      "    print(loss_function(intercept,slope))\n",
      "\n",
      "\n",
      "print(intercept.numpy(),slope.numpy())\n",
      "\n",
      "\n",
      "    sample    mae to find loss\n",
      "\n",
      "# Define a linear regression model\n",
      "def linear_regression(intercept, slope, features = size_log):\n",
      "\treturn intercept+features*slope\n",
      "\n",
      "# Set loss_function() to take the variables as arguments\n",
      "def loss_function(intercept,slope, features = size_log, targets = price_log):\n",
      "\t# Set the predicted values\n",
      "\tpredictions = linear_regression(intercept, slope, features)\n",
      "    \n",
      "    # Return the mean squared error loss\n",
      "\treturn keras.losses.mae(targets,predictions)\n",
      "\n",
      "# Compute the loss for different slope and intercept values\n",
      "print(loss_function(0.1, 0.1).numpy())\n",
      "print(loss_function(0.1, 0.5).numpy())\n",
      "\n",
      "\n",
      "   >Sample  > plot a regression that has optimized to a solution\n",
      "\n",
      "# Initialize an adam optimizer\n",
      "opt = keras.optimizers.Adam(0.5)\n",
      "\n",
      "for j in range(100):\n",
      "\t# Apply minimize, pass the loss function, and supply the variables\n",
      "\topt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
      "\n",
      "\t# Print every 10th value of the loss\n",
      "\tif j % 10 == 0:\n",
      "\t\tprint(loss_function(intercept, slope).numpy())\n",
      "\n",
      "# Plot data and regression line\n",
      "plot_results(intercept, slope)\n",
      "\n",
      "    sample    optimize to find the regression line\n",
      "\n",
      "# Define the linear regression model\n",
      "def linear_regression(params, feature1 = size_log, feature2 = bedrooms):\n",
      "\treturn params[0] + feature1*params[1] + feature2*params[2]\n",
      "\n",
      "# Define the loss function\n",
      "def loss_function(params, targets = price_log, feature1 = size_log, feature2 = bedrooms):\n",
      "\t# Set the predicted values\n",
      "\tpredictions = linear_regression(params, feature1, feature2)\n",
      "  \n",
      "\t# Use the mean absolute error loss\n",
      "\treturn keras.losses.mae(targets, predictions)\n",
      "\n",
      "# Define the optimize operation\n",
      "opt = keras.optimizers.Adam()\n",
      "\n",
      "# Perform minimization and print trainable variables\n",
      "for j in range(10):\n",
      "\topt.minimize(lambda: loss_function(params), var_list=[params])\n",
      "\tprint_results(params)\n",
      "\n",
      "\n",
      "       >Batch training\n",
      "\n",
      "batch training\n",
      "\n",
      "the complete dataset can not fit into memory\n",
      "so pass in batches of data\n",
      "\n",
      "chunksize parameter provides batch size data blocks\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "for batch in pd.read_csv('kc_housing.csv', chunksize=100):\n",
      "\tprice=np.array(batch['price'],np.float32)\n",
      "\tsize=np.array(batch['size'],np.float32)\n",
      "\n",
      "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list[intercept, slope])\n",
      "\n",
      "\n",
      "Batch Training\n",
      "1. multiple updates per epoch\n",
      "2. requires division of dataset\n",
      "3. no limit on dataset size\n",
      "\n",
      "\n",
      "      sample  linear regression and loss function\n",
      "\n",
      "\n",
      "# Define the intercept and slope\n",
      "intercept = Variable(10.0,np.float32)\n",
      "slope = Variable(0.5, float32)\n",
      "\n",
      "# Define the model\n",
      "def linear_regression(intercept, slope, features):\n",
      "\t# Define the predicted values\n",
      "\treturn intercept+features*slope\n",
      "\n",
      "# Define the loss function\n",
      "def loss_function(intercept, slope, targets, features):\n",
      "\t# Define the predicted values\n",
      "\tpredictions = linear_regression(intercept, slope, features)\n",
      "    \n",
      " \t# Define the MSE loss\n",
      "\treturn keras.losses.mse(targets, predictions)\n",
      "\n",
      "\n",
      "\n",
      "  > sample optimize the slope and intercept using batches of price and size\n",
      "\n",
      "# Initialize adam optimizer\n",
      "opt = keras.optimizers.Adam()\n",
      "\n",
      "# Load data in batches\n",
      "for batch in pd.read_csv('kc_house_data.csv',chunksize=100):\n",
      "\tsize_batch = np.array(batch['sqft_lot'], np.float32)\n",
      "\n",
      "\t# Extract the price values for the current batch\n",
      "\tprice_batch = np.array(batch['price'], np.float32)\n",
      "\n",
      "\t# Complete the loss, fill in the variable list, and minimize\n",
      "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
      "\n",
      "# Print trained parameters\n",
      "print(intercept.numpy(), slope.numpy())\n",
      "\n",
      "\n",
      "        Dense Layers\n",
      "\n",
      "credit card default\n",
      "1. Bill Amount\n",
      "2. Married\n",
      "3. Default\n",
      "\n",
      "input,hidden, output is called forward propagation\n",
      "\n",
      "input is our features\n",
      "ouput is our prediction\n",
      "\n",
      "import tensorflow as tf\n",
      "inputs = tf.constant[[1,35]])\n",
      "weights=tf.Variable([[-0.05],[-0.01]])\n",
      "\n",
      "bias = tf.Variable([0.5])\n",
      "\n",
      "plays the role of an intercept in a simple regression model.\n",
      "\n",
      "product=tf.matmul(inputs, weights)\n",
      "\n",
      "dense=tf.keras.activations.sigmoid(product+bias)\n",
      "\n",
      "  > define the complete model\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "inputs=tf.constant(data, tf.float32)\n",
      "\n",
      "dense1 = tf.keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
      "\n",
      "dense2 = tf.keras.layers.Dense(5, activation='sigmoid')(dense1)\n",
      "\n",
      "output = tf.keras.layers.Dense(1,activation='sigmoid')(dense2)\n",
      "\n",
      "\n",
      "   sample    calculate a dense layer manually\n",
      "\n",
      "# Initialize bias1\n",
      "bias1 = Variable(1.0)\n",
      "\n",
      "# Initialize weights1 as 3x2 variable of ones\n",
      "weights1 = Variable(ones((3, 2)))\n",
      "\n",
      "# Perform matrix multiplication of borrower_features and weights1\n",
      "product1 = matmul(borrower_features,weights1)\n",
      "\n",
      "# Apply sigmoid activation function to product1 + bias1\n",
      "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
      "\n",
      "# Print shape of dense1\n",
      "print(\"\\n dense1's output shape: {}\".format(dense1.shape))\n",
      "\n",
      "\n",
      "   > sample calculate a second dense layer \n",
      "\n",
      "# From previous step\n",
      "bias1 = Variable(1.0)\n",
      "weights1 = Variable(ones((3, 2)))\n",
      "product1 = matmul(borrower_features, weights1)\n",
      "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
      "\n",
      "# Initialize bias2 and weights2\n",
      "bias2 = Variable(1.0)\n",
      "weights2 = Variable(ones((2, 1)))\n",
      "\n",
      "# Perform matrix multiplication of dense1 and weights2\n",
      "product2 = matmul(dense1,weights2)\n",
      "\n",
      "# Apply activation to product2 + bias2 and print the prediction\n",
      "prediction = keras.activations.sigmoid(product2 + bias2)\n",
      "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
      "print('\\n actual: 1')\n",
      "\n",
      "\n",
      " >sample  > shape\n",
      "\n",
      "# Compute the product of borrower_features and weights1\n",
      "products1 = matmul(borrower_features,weights1)\n",
      "\n",
      "# Apply a sigmoid activation function to products1 + bias1\n",
      "dense1 = keras.activations.sigmoid(products1+bias1)\n",
      "\n",
      "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
      "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
      "print('\\n shape of weights1: ', weights1.shape)\n",
      "print('\\n shape of bias1: ', bias1.shape)\n",
      "print('\\n shape of dense1: ', dense1.shape)\n",
      "\n",
      "shape of borrower_features:  (5, 3)\n",
      "\n",
      " shape of weights1:  (3, 2)\n",
      "\n",
      " shape of bias1:  (1,)\n",
      "\n",
      " shape of dense1:  (5, 2)\n",
      "\n",
      "borrower_features, is 5x3 because it consists of 5 examples for 3 features. The shape of weights1 is 3x2, as it was in the previous exercise, since it does not depend on the number of examples. Additionally, bias1 is a scalar. Finally, dense1 is 5x2, which means that we can multiply it by the following set of weights, weights2, which we defined to be 2x1 in the previous exercise.\n",
      "\n",
      "\n",
      "  >Sample constructing an keras model\n",
      "\n",
      "# Define the first dense layer\n",
      "dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
      "\n",
      "# Define a dense layer with 3 output nodes\n",
      "dense2 = keras.layers.Dense(3,activation='sigmoid')(dense1)\n",
      "\n",
      "# Define a dense layer with 1 output node\n",
      "predictions = keras.layers.Dense(1,activation='sigmoid')(dense2)\n",
      "\n",
      "# Print the shapes of dense1, dense2, and predictions\n",
      "print('\\n shape of dense1: ', dense1.shape)\n",
      "print('\\n shape of dense2: ', dense2.shape)\n",
      "print('\\n shape of predictions: ', predictions.shape)\n",
      "\n",
      "output:\n",
      "shape of dense1:  (100, 7)\n",
      "\n",
      " shape of dense2:  (100, 3)\n",
      "\n",
      " shape of predictions:  (100, 1)\n",
      "\n",
      "      >Activation Functions\n",
      "\n",
      "hidden layers performs matrix multiplication and applies an activation function \n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "young, old = 0.3, 0.6\n",
      "low_bill, high_bill = 0.1, 0.5\n",
      "\n",
      "young_high=1.0* young + 2*high_bill\n",
      "young_low=1.0*young + 2*low_bill\n",
      "\n",
      "old_high=1.0* old + 2.0 * high_bill\n",
      "old_low=1.0*old + 2.0 * low_bill\n",
      "\n",
      "print(young_high-young_low)\n",
      "print(old_high-old_low)\n",
      "\n",
      "output\n",
      "0.8\n",
      "0.8\n",
      "\n",
      "print(tf.keras.activations.sigmoid(young_high).numpy()-tf.keras.activations.sigmoid(young_low).numpy())\n",
      "\n",
      "print(tf.keras.activations.sigmoid(old_high).numpy()-tf.keras.activations.sigmoid(old_low).numpy())\n",
      "\n",
      "output\n",
      "0.16337568\n",
      "0.14204389\n",
      "\n",
      "activation functions\n",
      "sigmoid (binary classification problems)\n",
      "relu (all layers but the output layer (0 or max(value))\n",
      "softmax (output layer for multiple classification)\n",
      "\n",
      "relu varies between 0 and infinity\n",
      "\n",
      "\n",
      "inputs= tf.constant(borrower_features,tf.float32)\n",
      "\n",
      "dense1=tf.keras.layers.Dense(16,activation='relu')(inputs)\n",
      "\n",
      "dense2=tf.keras.layers.Dense(8,activation='sigmoid')(dense1)\n",
      "\n",
      "outputs = tf.keras.layers.Dense(4,activation='softmax')(dense2)\n",
      "\n",
      "\n",
      "   > sample  > input bill_amounts  \n",
      "\n",
      "\n",
      "# Construct input layer from features\n",
      "print(bill_amounts)\n",
      "inputs = constant(bill_amounts)\n",
      "\n",
      "# Define first dense layer\n",
      "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
      "\n",
      "# Define second dense layer\n",
      "dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
      "\n",
      "# Define output layer\n",
      "outputs = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
      "\n",
      "# Print error for first five examples\n",
      "error = default[:5] - outputs.numpy()[:5]\n",
      "print(error)\n",
      "\n",
      "input\n",
      "[77479 77057 78102]\n",
      " [  326   326   326]\n",
      " [13686  1992   604]\n",
      "\n",
      "output\n",
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]]\n",
      "\n",
      "  >Sample   > 10 inputs 8 hidden 6 output\n",
      "\n",
      "# Construct input layer from borrower features\n",
      "inputs = constant(borrower_features)\n",
      "\n",
      "#print(len(borrower_features[0]))\n",
      "\n",
      "# Define first dense layer\n",
      "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
      "\n",
      "# Define second dense layer\n",
      "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
      "\n",
      "# Define output layer\n",
      "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
      "\n",
      "# Print first five predictions\n",
      "print(outputs.numpy()[:5])\n",
      "\n",
      "\n",
      "         >Gradient Descent Optimizer\n",
      "\n",
      "tf.keras.optimizer.SGD()\n",
      "learning_rate\n",
      "\n",
      "\n",
      "RMS\n",
      "applies different learning rates to each feature\n",
      "\n",
      "tf.keras.optimizers.RMSprop()\n",
      "learning_rate\n",
      "momentum\n",
      "decay\n",
      "\n",
      "allows momentum to both build and decay\n",
      "\n",
      "adam optimizer\n",
      "\n",
      "adaptive moment (adam) optimizer\n",
      "\n",
      "tk.keras.optimizers.Adam()\n",
      "\n",
      "learning_rate\n",
      "beta1 (decay)\n",
      "\n",
      "performs well with default parameter values\n",
      "\n",
      "\n",
      "def model(bias, weights, features=borrower_features):\n",
      "\tproduct=tf.matmul(features,weights)\n",
      "\treturn tf.keras.activations.sigmoid(product+bias)\n",
      "\n",
      "def loss_function(bias, weights, targets=default, features=borrower_features):\n",
      "\tpredictions=model(bias,weights)\n",
      "\treturn tf.keras.loss.binary_crossentropy(targets,predictions)\n",
      "\n",
      "opt=tk.keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.9)\n",
      "opt.minimize(lambda: loss_function(bias,weights), var_list=[bias, weights])\n",
      "\n",
      "\n",
      "    >sample  > minimize loss over 100 iterations\n",
      "\n",
      "# Initialize x_1 and x_2\n",
      "x_1 = Variable(6.0,float32)\n",
      "x_2 = Variable(0.3,float32)\n",
      "\n",
      "# Define the optimization operation\n",
      "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
      "\n",
      "for j in range(100):\n",
      "\t# Perform minimization using the loss function and x_1\n",
      "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
      "\t# Perform minimization using the loss function and x_2\n",
      "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
      "\n",
      "# Print x_1 and x_2 as numpy arrays\n",
      "print(x_1.numpy(), x_2.numpy())\n",
      "\n",
      "output\n",
      "4.3801394 0.42052683\n",
      "\n",
      "\n",
      "The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as loss_function()\n",
      "\n",
      "\n",
      " > sample rms with momentum\n",
      "\n",
      "# Initialize x_1 and x_2\n",
      "x_1 = Variable(0.05,float32)\n",
      "x_2 = Variable(0.05,float32)\n",
      "\n",
      "# Define the optimization operation for opt_1 and opt_2\n",
      "opt_1 = keras.optimizers.RMSprop(learning_rate=.01, momentum=0.99)\n",
      "opt_2 = keras.optimizers.RMSprop(learning_rate=.01, momentum=0.0)\n",
      "\n",
      "for j in range(100):\n",
      "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
      "    # Define the minimization operation for opt_2\n",
      "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
      "\n",
      "# Print x_1 and x_2 as numpy arrays\n",
      "print(x_1.numpy(), x_2.numpy())\n",
      "\n",
      "\n",
      "      Training a network in tensorflow\n",
      "\n",
      "the eggholder function\n",
      "\n",
      "there exists a global minimum but it is difficult to identify by inspection.\n",
      "\n",
      "use random and algorithmic selection of the initial values.\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "weights= tf.Variable(tf.random.normal([500],[500])\n",
      "\n",
      "\n",
      "weights= tf.Variable(tf.random.truncated_normal([500],[500])\n",
      "\n",
      "\n",
      "\n",
      "#define a dense layer with the default initializer\n",
      "dense=tf.keras.layers.Dense(32, activation='relu')\n",
      "\n",
      "dense=tf.keras.layers.Dense(32, activation='relu', \\\n",
      "\tkernel_initializer='zeros')\n",
      "\n",
      "\n",
      "dropout \n",
      "this will random drop weights on the network\n",
      "\n",
      "inputs=np.array(borrow_features, np.float32)\n",
      "dense1=tf.keras.layers.Dense(32, activation='relu')(inputs)\n",
      "\n",
      "dense2 = tf.keras.layers.Dense(16,activation='relu')(dense1)\n",
      "\n",
      "#drop 25 percent of the weights randomly\n",
      "dropout1=tf.keras.layers.Dropout(0.25)(dense2)\n",
      "\n",
      "outputs=tf.keras.layers.Dense(1, activation='sigmoid') (dropout1)\n",
      "\n",
      "\n",
      "   >sample initializing the weights of the neural network\n",
      "\n",
      "# Define the layer 1 weights\n",
      "w1 = Variable(random.normal([23, 7]))\n",
      "\n",
      "# Initialize the layer 1 bias\n",
      "b1 = Variable(ones([7]))\n",
      "\n",
      "# Define the layer 2 weights\n",
      "w2 = Variable(random.normal([7,1]))\n",
      "\n",
      "# Define the layer 2 bias\n",
      "b2 = Variable(0)\n",
      "\n",
      "   sample\n",
      "\n",
      "# Define the model\n",
      "def model(w1, b1, w2, b2, features = borrower_features):\n",
      "\t# Apply relu activation functions to layer 1\n",
      "\tlayer1 = keras.activations.sigmoid(matmul(features, w1) + b1)\n",
      "    # Apply dropout\n",
      "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
      "\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
      "\n",
      "# Define the loss function\n",
      "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
      "\tpredictions = model(w1, b1, w2, b2)\n",
      "\t# Pass targets and predictions to the cross entropy loss\n",
      "\treturn keras.losses.binary_crossentropy(targets, predictions)\n",
      "\n",
      "\n",
      "    sample\n",
      "\n",
      "# Train the model\n",
      "for j in range(100):\n",
      "    # Complete the optimizer\n",
      "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
      "                 var_list=[w1, b1, w2, b2])\n",
      "\n",
      "# Make predictions with model\n",
      "model_predictions = model(w1, b1, w2, b2, test_features)\n",
      "\n",
      "# Construct the confusion matrix\n",
      "confusion_matrix(test_targets, model_predictions)\n",
      "\n",
      "        Sequential\n",
      "\n",
      "28x28 image matrix\n",
      "\n",
      "16 input\n",
      "8 hidden\n",
      "4 output\n",
      "\n",
      "from tensorflow import keras\n",
      "\n",
      "model = keras.Sequential()\n",
      "\n",
      "model.add(keras.layers.Dense(16,activation='relu', input_shape=(28*28,)))\n",
      "\n",
      "model.add(Dense(8,activation='relu'))\n",
      "\n",
      "model.add(Dense(4,activation='softmax'))\n",
      "\n",
      "\n",
      "        Functional api\n",
      "\n",
      "model1_inputs=tf.keras.Input(shape=(28*28,))\n",
      "model2_inputs=tf.keras.Input(shape=(10,))\n",
      "\n",
      "model1_layer1=Dense(12,activation='relu')(model1_inputs)\n",
      "model1_layer2=Dense(4,activation='softmax')(model1_layer1)\n",
      "\n",
      "\n",
      "model2_layer1=Dense(12,activation='relu')(model2_inputs)\n",
      "model2_layer2=Dense(4,activation='softmax')(model2_layer1)\n",
      "\n",
      "merged=tf.keras.layers.add([model1_layer2,model2_layer2])\n",
      "\n",
      "\n",
      "model.compile('adam',loss='categorical_crossentropy')\n",
      "\n",
      "   Sign language    >  4 images\n",
      "\n",
      "# Define a Keras sequential model\n",
      "model=keras.Sequential()\n",
      "\n",
      "# Define the first dense layer 28x28\n",
      "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
      "\n",
      "# Define the second dense layer\n",
      "model.add(keras.layers.Dense(8,activation='relu'))\n",
      "\n",
      "# Define the output layer\n",
      "model.add(keras.layers.Dense(4,activation='softmax'))\n",
      "\n",
      "# Print the model architecture\n",
      "print(model.summary())\n",
      "\n",
      "\n",
      "\n",
      "   sign language with dropout rate of .25\n",
      "\n",
      "# Define the first dense layer\n",
      "model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))\n",
      "\n",
      "# Apply dropout to the first layer's output\n",
      "model.add(keras.layers.Dropout(0.25))\n",
      "\n",
      "# Define the output layer\n",
      "model.add(keras.layers.Dense(4, activation='softmax'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile('adam', loss='categorical_crossentropy')\n",
      "\n",
      "# Print a model summary\n",
      "print(model.summary())\n",
      "\n",
      "\n",
      "   > creating two inputs and Merge the outputs\n",
      "\n",
      "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
      "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
      "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
      "\n",
      "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
      "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
      "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
      "\n",
      "# Merge model outputs and define a functional model\n",
      "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
      "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
      "\n",
      "# Print a model summary\n",
      "print(model.summary())\n",
      "\n",
      "\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 12)           9420        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 12)           9420        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 4)            52          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 4)            52          dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 4)            0           dense_9[0][0]                    \n",
      "                                                                 dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 18,944\n",
      "Trainable params: 18,944\n",
      "Non-trainable params: 0\n",
      "______________________________\n",
      "\n",
      "    Training with Keras\n",
      "\n",
      "train and evaluate\n",
      "\n",
      "1. load and clean the data\n",
      "2. define the model\n",
      "3. train and validate model\n",
      "4. evaluate model\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "model=tf.keras.Sequential()\n",
      "\n",
      "model.add(tf.keras.layers.Dense(16,activation='relu', input_shape(784,)))\n",
      "\n",
      "model.add(tf.keras.layers.Dense(4,activation='softmax'))\n",
      "\n",
      "model.compile('adam',loss='categorical_crossentropy')\n",
      "\n",
      "model.fit(image_features, image_labels)\n",
      "\n",
      "batch_size (example 32 by default)\n",
      "epochs (times trained)\n",
      "validation_split\n",
      "1) training\n",
      "2. validation\n",
      "\n",
      "validation_split=0.20\n",
      "\n",
      "\n",
      "performing validation\n",
      "1. loss\n",
      "2. val_loss\n",
      "\n",
      "if training loss becames substantially less than validation loss than overfitting is occurring. \n",
      "\n",
      "1. Add dropout\n",
      "2. or regularize the data\n",
      "\n",
      "metrics['accuracy']\n",
      "\n",
      "model.fit(features,labels, epochs=10, validation_split=0.20)\n",
      "\n",
      "model.evaluate(test)\n",
      "\n",
      "\n",
      "\n",
      "    sample     compile the model\n",
      "\n",
      "# Define a sequential model\n",
      "model=keras.Sequential()\n",
      "\n",
      "# Define a hidden layer\n",
      "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
      "\n",
      "# Define the output layer\n",
      "model.add(keras.layers.Dense(4,activation='softmax'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile('SGD', loss='categorical_crossentropy',metrics=['accuracy'])\n",
      "\n",
      "# Complete the fitting operation\n",
      "model.fit(sign_language_features, sign_language_labels, epochs=5)\n",
      "\n",
      "\n",
      "    sample    RMSprop optimizer\n",
      "\n",
      "# Define sequential model\n",
      "model = keras.Sequential()\n",
      "\n",
      "# Define the first layer\n",
      "model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))\n",
      "\n",
      "# Add activation function to classifier\n",
      "model.add(keras.layers.Dense(4, activation='softmax'))\n",
      "\n",
      "# Set the optimizer, loss function, and metrics\n",
      "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "# Add the number of epochs and the validation split\n",
      "model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=.10)\n",
      "\n",
      "\n",
      "   > sample  > large number of neurons\n",
      "\n",
      "# Define sequential model\n",
      "model=keras.Sequential()\n",
      "\n",
      "# Define the first layer\n",
      "model.add(keras.layers.Dense(1024, activation='relu', input_shape=(784,)))\n",
      "# Add activation function to classifier\n",
      "model.add(keras.layers.Dense(4, activation='softmax'))\n",
      "\n",
      "# Finish the model compilation\n",
      "model.compile(optimizer=keras.optimizers.Adam(lr=0.001), \n",
      "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "# Complete the model fit operation\n",
      "model.fit(sign_language_features, sign_language_labels, epochs=50, validation_split=.5)\n",
      "\n",
      "if val_loss started to increase before the training process was terminated, then we may have overfitted. when this happens decrease the number of epochs.\n",
      "\n",
      "\n",
      "    sample  comparing a small model to a large model\n",
      "\n",
      "# Evaluate the small model using the train data\n",
      "small_train = small_model.evaluate(train_features, train_labels)\n",
      "\n",
      "# Evaluate the small model using the test data\n",
      "small_test = small_model.evaluate(test_features, test_labels)\n",
      "\n",
      "# Evaluate the large model using the train data\n",
      "large_train = large_model.evaluate(train_features, train_labels)\n",
      "\n",
      "# Evaluate the large model using the test data\n",
      "large_test = large_model.evaluate(test_features, test_labels)\n",
      "\n",
      "# Print losses\n",
      "print('\\n Small - Train: {}, Test: {}'.format(small_train, small_test))\n",
      "print('Large - Train: {}, Test: {}'.format(large_train, large_test))\n",
      "\n",
      "\n",
      "      Estimators api\n",
      "\n",
      "\n",
      "high level submodule:estimators\n",
      "mid level: layers, datasets, metrics\n",
      "low level: python\n",
      "\n",
      "\n",
      "estimators enforce best practices\n",
      "\n",
      "1. define feature columns\n",
      "2. load and transform data\n",
      "3. define an estimator\n",
      "4. apply train operation\n",
      "\n",
      "Defining feature columns:\n",
      "\n",
      "for an image\n",
      "features_list[tf.feature_column.numeric_column('image',shape(784,))]\n",
      "\n",
      "\n",
      "     Loading and transforming data\n",
      "\n",
      "def input_fn():\n",
      "    features={\"size\":[1340,1690,2720],'rooms':[1,3,4]}\n",
      "    layers=[221900,538000,180000]\n",
      "\n",
      "    return features, labels\n",
      "\n",
      "size=tf.feature_column.numeric_column(\"size\")\n",
      "rooms=tf.feature_column.categorical_column_with_vocabulary_list(\"rooms\",[\"1\",\"2\",\"3\",\"4\",\"5\"],default_value=0)\n",
      "feature_list=[size,\n",
      "              tf.feature_column.indicator_column(rooms)]\n",
      "\n",
      "model0=tf.estimator.DNNRegressor(feature_columns=feature_list,hidden_units=[10,6,6,1])\n",
      "model0.train(input_fn,steps=20)\n",
      "\n",
      "\n",
      "model1=tf.estimator.DNNClassifier(feature_columns=feature_list, hidden_units=[32,16,8],n_classes=4)\n",
      "\n",
      "model0.train(input_fn,steps=20)\n",
      "\n",
      "    >sample   > define feature columns and build an input_fn\n",
      "\n",
      "# Define feature columns for bedrooms and bathrooms\n",
      "bedrooms = feature_column.numeric_column(\"bedrooms\")\n",
      "bathrooms = feature_column.numeric_column(\"bathrooms\")\n",
      "\n",
      "# Define the list of feature columns\n",
      "feature_list = [bedrooms, bathrooms]\n",
      "\n",
      "def input_fn():\n",
      "\t# Define the labels\n",
      "\tlabels = np.array(housing['price'])\n",
      "\t# Define the features\n",
      "\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \n",
      "                'bathrooms':np.array(housing['bathrooms'])}\n",
      "\treturn features, labels\n",
      "\n",
      "  > use a linear regressor\n",
      "\n",
      "# Define the model and set the number of steps\n",
      "model = estimator.LinearRegressor(feature_columns=feature_list) \n",
      "model.train(input_fn, steps=2)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['transform']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
