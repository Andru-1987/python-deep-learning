{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\building a linear model - optimize.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\building a linear model - optimize.txt\n",
      "        >Least Squares Optimization\n",
      "\n",
      "a1=covariance(x,y)/variance(x)\n",
      "a0=mean(y) - a1*mean(x)\n",
      "\n",
      "\n",
      "steps\n",
      "\n",
      "x_mean=np.mean(x)\n",
      "y_mean=np.mean(y)\n",
      "\n",
      "x_dev=x-x_mean\n",
      "y_dev=y-y_mean\n",
      "\n",
      "covariance= np.sum(x_dev*y_dev)\n",
      "variance= np.sum(x_dev**2)\n",
      "\n",
      "a1 = covariance/variance\n",
      "a0 = y_mean - (a1*x_mean)\n",
      "\n",
      "    >Optimize\n",
      "\n",
      "from scipy import optimize\n",
      "\n",
      "x_data,y_data = load_data()\n",
      "\n",
      "def model_func(x,a0,a1):\n",
      "\treturn a0 + (a1*x)\n",
      "\n",
      "\n",
      "param_opt,param_cov= optimize.curve_fit(model_func, x_data, y_data)\n",
      "\n",
      "a0= param_opt[0] #a0 is the intercept\n",
      "a1= param_opt[1] #a1 is the slope\n",
      "\n",
      "print(\"optimize library a0={} and a1={}\".format(a0,a1))\n",
      "\n",
      "\n",
      "   > Least squares OLS\n",
      "\n",
      "df=pd.DataFrame(dict(x_name=x_data,y_name=y_data))\n",
      "model_fit = ols(formula=\"y_name ~ x_name\", data=df).fit()\n",
      "\n",
      "a0 = model_fit.params['Intercept']\n",
      "a1 = model_fit.params['x_name']\n",
      "\n",
      "print( \"ols a0 = {:0.4f}\".format(a0) )\n",
      "print( \"ols a1 = {:0.4f}\".format(a1) )\n",
      "\n",
      "\n",
      "  >Sample    > manually calculate a linear model\n",
      "\n",
      "# prepare the means and deviations of the two variables\n",
      "x_mean = np.mean(x)\n",
      "y_mean = np.mean(y)\n",
      "x_dev = x - x_mean\n",
      "y_dev = y - y_mean\n",
      "\n",
      "# Complete least-squares formulae to find the optimal a0, a1\n",
      "a1 = np.sum(x_dev * y_dev) / np.sum( np.square(x_dev) )\n",
      "a0 =y_mean - (a1 * x_mean)\n",
      "\n",
      "# Use the those optimal model parameters a0, a1 to build a model\n",
      "y_model = model(x, a0, a1)\n",
      "\n",
      "# plot to verify that the resulting y_model best fits the data y\n",
      "fig, rss = compute_rss_and_plot_fit(a0, a1)\n",
      "\n",
      "\n",
      "  >Sample    using optimize to create a linear model\n",
      "\n",
      "# Define a model function needed as input to scipy\n",
      "def model_func(x, a0, a1):\n",
      "    return a0 + (a1*x)\n",
      "\n",
      "# Load the measured data you want to model\n",
      "x_data, y_data  = load_data()\n",
      "\n",
      "# call curve_fit, passing in the model function and data; then unpack the results\n",
      "param_opt, param_cov = optimize.curve_fit(model_func, x_data, y_data)\n",
      "a0 = param_opt[0]  # a0 is the intercept in y = a0 + a1*x\n",
      "a1 = param_opt[1]  # a1 is the slope     in y = a0 + a1*x\n",
      "\n",
      "# test that these parameters result in a model that fits the data\n",
      "fig, rss = compute_rss_and_plot_fit(a0, a1)\n",
      "\n",
      "  >Sample  > using ols to model a linear model\n",
      "\n",
      "# Pass data and `formula` into ols(), use and `.fit()` the model to the data\n",
      "model_fit = ols(formula=\"y_column ~ x_column\", data=df).fit()\n",
      "\n",
      "# Use .predict(df) to get y_model values, then over-plot y_data with y_model\n",
      "y_model = model_fit.predict(df)\n",
      "fig = plot_data_with_model(x_data, y_data, y_model)\n",
      "\n",
      "# Extract the a0, a1 values from model_fit.params\n",
      "a0 = model_fit.params['Intercept']\n",
      "a1 = model_fit.params['x_column']\n",
      "\n",
      "# Visually verify that these parameters a0, a1 give the minimum RSS\n",
      "fig, rss = compute_rss_and_plot_fit(a0, a1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\deep learning tensors and layers and encoders.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\deep learning tensors and layers and encoders.txt\n",
      "first_layers = model.layers[0]\n",
      "\n",
      "print(first_layer.input)\n",
      "print(first_layer.ouput)\n",
      "print(first_layer.weights)\n",
      "\n",
      "\n",
      "tensors are 2 dimensions\n",
      "\n",
      "T2=[[1,2,3],\n",
      "[4,5,6],\n",
      "[7,8,9]]\n",
      "\n",
      "A three dimension tensors is an array of matrices.\n",
      "\n",
      "\n",
      "import keras.backend as K\n",
      "\n",
      "inp= model.layers[0].input\n",
      "out= model.layers[0].output\n",
      "\n",
      "inp_to_out = K.function([inp],[out])\n",
      "\n",
      "print(inp_to_out([X_train]))\n",
      "\n",
      "autoencoders are models have the same inputs as outputs\n",
      "1) dimensionality reduction\n",
      "a. smaller dimensional space representation of our inputs\n",
      "2) De-noising data\n",
      "a. if trained with clean data, irrelevant noise will be filtered out during reconstruction.\n",
      "3. Anomaly detection:\n",
      "a. a poor reconstruction will result when the model is fed with unseen inputs\n",
      "\n",
      "\n",
      "autoencoder= Sequential()\n",
      "\n",
      "autoencoder.add(Dense(4, input_shape(100,), activation='relu'))\n",
      "\n",
      "autoencoder.add(Dense(100,activation='sigmoid')\n",
      "\n",
      "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
      "\n",
      "\n",
      "encoder=Sequential()\n",
      "encoder.add(autoencoder.layers[0])\n",
      "\n",
      "encoder.predict(X_test)\n",
      "\n",
      "\n",
      "  \n",
      "# Import keras backend\n",
      "import keras.backend as K\n",
      "\n",
      "# Input tensor from the 1st layer of the model\n",
      "inp =  model.layers[0].input\n",
      "\n",
      "# Output tensor from the 1st layer of the model\n",
      "out = model.layers[0].output\n",
      "\n",
      "# Define a function from inputs to outputs\n",
      "inp_to_out = K.function([inp], [out])\n",
      "\n",
      "# Print the results of passing X_test through the 1st layer\n",
      "print(inp_to_out([X_test]))\n",
      "\n",
      "for i in range(0, 21):\n",
      "  \t# Train model for 1 epoch\n",
      "    h = model.fit(X_train, y_train, batch_size=16, epochs=1, verbose=0)\n",
      "    if i%4==0: \n",
      "      # Get the output of the first layer\n",
      "      layer_output = inp_to_out([X_test])[0]\n",
      "      \n",
      "      # Evaluate model accuracy for this epoch\n",
      "      test_accuracy = model.evaluate(X_test, y_test)[1] \n",
      "      \n",
      "      # Plot 1st vs 2nd neuron output\n",
      "      plot()\n",
      "\n",
      "# Start with a sequential model\n",
      "autoencoder = Sequential()\n",
      "\n",
      "# Add a dense layer with the original image as input\n",
      "autoencoder.add(Dense(32, input_shape=(784, ), activation=\"relu\"))\n",
      "\n",
      "# Add an output layer with as many nodes as the image\n",
      "autoencoder.add(Dense(784, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile your model\n",
      "autoencoder.compile(optimizer=\"adadelta\", loss=\"binary_crossentropy\")\n",
      "\n",
      "# Take a look at your model structure\n",
      "autoencoder.summary()\n",
      "\n",
      "\n",
      "# Build your encoder\n",
      "encoder = Sequential()\n",
      "encoder.add(autoencoder.layers[0])\n",
      "\n",
      "# Encode the images and show the encodings\n",
      "preds = encoder.predict(X_test_noise)\n",
      "show_encodings(preds)\n",
      "\n",
      "# Build your encoder\n",
      "encoder = Sequential()\n",
      "encoder.add(autoencoder.layers[0])\n",
      "\n",
      "# Encode the images and show the encodings\n",
      "preds = encoder.predict(X_test_noise)\n",
      "show_encodings(preds)\n",
      "\n",
      "# Predict on the noisy images with your autoencoder\n",
      "decoded_imgs = autoencoder.predict(X_test_noise)\n",
      "\n",
      "# Plot noisy vs decoded images\n",
      "compare_plot(X_test_noise, decoded_imgs)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\deep learning with keras - binary classification.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\deep learning with keras - binary classification.txt\n",
      "x and y coordinates and labels 0 or 1 representing the colors\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "sns.pairplot(circles, hue=\"target\")\n",
      "\n",
      "topology \n",
      "\n",
      "1. two input layer one for x and y\n",
      "2. four hidden layer\n",
      "3. one output layer\n",
      "\n",
      "sigmoid function = 1/(1+e**-Z)\n",
      " >\n",
      "\n",
      "model= Sequential()\n",
      "\n",
      "model.add(Dense(4, input_shape=(2,),\n",
      "activation='sigmoid'\n",
      "))\n",
      "\n",
      "#model.add(Dense(50, activation='relu'))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(optimizer=Adam(0.01),loss='binary_crossentropy')\n",
      "#model.compile(optimizer=Adam(0.01),loss='mae')\n",
      "\n",
      "model.summary()\n",
      "\n",
      "plot_model(model, to_file='model.png')\n",
      "img=plt.imread('model.png')\n",
      "plt.imshow(img)\n",
      "plt.show()\n",
      "\n",
      "model.compile(optimizer='sgd',loss='binary_crossentropy')\n",
      "\n",
      "model.train(coordinates, labels, epochs=20)\n",
      "\n",
      "preds= model.predict(coordinates)\n",
      "\n",
      "\n",
      " >\n",
      "\n",
      "variance, skewness, kurtosis, entropy, class\n",
      "\n",
      "# Import seaborn\n",
      "import seaborn as sns\n",
      "\n",
      "print(banknotes.keys)\n",
      "\n",
      "# Use pairplot and set the hue to be our class\n",
      "sns.pairplot(banknotes, hue='class') \n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "# Describe the data\n",
      "print('Dataset stats: \\n', banknotes.describe)\n",
      "\n",
      "# Count the number of observations of each class\n",
      "print('Observations per class: \\n', banknotes['class'].value_counts)\n",
      "\n",
      "  multi class classification\n",
      "\n",
      "xCoord, yCoord competitor\n",
      "\n",
      "1. 2 input, 128 dense, 64 dense, 32 dense, 4 outputs\n",
      "\n",
      "softmax\n",
      ".6 Michael\n",
      ".1 Susan\n",
      ".2 Kate\n",
      ".1 Steve\n",
      "\n",
      "\n",
      "model.add(Dense(4, activation='softmax'))\n",
      "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
      "\n",
      "The log loss decreases as the model becomes more accurate in predicting.\n",
      "\n",
      "\n",
      "  >To Categorical\n",
      " 2 input, [128,64,32] hidden layer, 4 output\n",
      "\n",
      "import pandas as pd\n",
      "from keras.utils import to_categorical\n",
      "\n",
      "df=pd.read_csv('data.csv')\n",
      "\n",
      "df.response=pd.Categorical(df.response)\n",
      "df.response=df.response.cat.codes\n",
      "\n",
      "#turn response variable into one-hot encode response vector\n",
      "y=to_categorical(df.response)\n",
      "\n",
      " >\n",
      "# Import to_categorical from keras utils module\n",
      "from keras.utils import to_categorical\n",
      "\n",
      "# Instantiate a sequential model\n",
      "model = Sequential()\n",
      "  \n",
      "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
      "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
      "model.add(Dense(64, activation='relu'))\n",
      "model.add(Dense(32, activation='relu'))\n",
      "  \n",
      "# Add a dense layer with as many neurons as competitors\n",
      "model.add(Dense(4, activation=\"softmax\"))\n",
      "  \n",
      "# Compile your model using categorical_crossentropy loss\n",
      "model.compile(loss=\"categorical_crossentropy\",\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "              \n",
      "model.summary()\n",
      "\n",
      "\n",
      "\n",
      "# Transform into a categorical variable\n",
      "darts.competitor = pd.Categorical(darts.competitor)\n",
      "\n",
      "# Assign a number to each category (label encoding)\n",
      "darts.competitor = darts.competitor.cat.codes \n",
      "\n",
      "# Print the label encoded competitors\n",
      "print('Label encoded competitors: \\n',darts.competitor.head())\n",
      "\n",
      "# Transform into a categorical variable\n",
      "darts.competitor = pd.Categorical(darts.competitor)\n",
      "\n",
      "\n",
      "# Use to_categorical on your labels\n",
      "coordinates = darts.drop(['competitor'], axis=1)\n",
      "competitors = to_categorical(darts.competitor)\n",
      "\n",
      "# Now print the to_categorical() result\n",
      "print('One-hot encoded competitors: \\n',competitors)\n",
      "\n",
      "# Train your model on the training data for 200 epochs\n",
      "model.fit(coord_train,competitors_train,epochs=200)\n",
      "\n",
      "# , your model accuracy on the test data\n",
      "accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
      "\n",
      "# Print accuracy\n",
      "print('Accuracy:', accuracy)\n",
      "\n",
      "# Predict on coords_small_test\n",
      "preds = model.predict(coords_small_test)\n",
      "\n",
      "# Print preds vs true values\n",
      "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
      "for i,pred in enumerate(preds):\n",
      "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
      "\n",
      "# Predict on coords_small_test\n",
      "preds = model.predict(coords_small_test)\n",
      "\n",
      "# Print preds vs true values\n",
      "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
      "for i,pred in enumerate(preds):\n",
      "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
      "\n",
      "# Extract the indexes of the highest probable predictions\n",
      "preds = [np.argmax(pred) for pred in preds]\n",
      "\n",
      "# Print preds vs true values\n",
      "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
      "for i,pred in enumerate(preds):\n",
      "  print(\"{:25} | {}\".format(pred,competitors_small_test[i]))\n",
      "\n",
      "\n",
      " > multi-label\n",
      "\n",
      "model=Sequential()\n",
      "\n",
      "model.add(Dense(2,input_shape=(1,)))\n",
      "\n",
      "model.add(Dense(3,activation='sigmoid'))\n",
      "\n",
      "#each output will be between 0 and 1\n",
      "\n",
      "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
      "model.fit(X_train, y_train, epochs=100, validation_split=0.2)\n",
      "\n",
      "#one versus rest classification\n",
      "\n",
      "#sensor measurements result in parcels to water\n",
      "\n",
      "Multi-label classification problems differ from multi-class problems in that each observation can be labeled with zero or more classes. So classes are not mutually exclusive. \n",
      "\n",
      "To account for this behavior what we do is have an output layer with as many neurons as classes but this time, unlike in multi-class problems, each output neuron has a sigmoid activation function. This makes the output layer able to output a number between 0 and 1 in any of its neurons.\n",
      "\n",
      " \n",
      "\n",
      "# Instantiate a Sequential model\n",
      "\n",
      "model=Sequential()\n",
      "\n",
      "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
      "\n",
      "model.add(Dense(64,input_shape=(20,),activiation='relu'))\n",
      "\n",
      "# Add an output layer of 3 neurons with sigmoid activation\n",
      "model.add(Dense(3,activation='sigmoid'))\n",
      "\n",
      "# Compile your model with adam and binary crossentropy loss\n",
      "model.compile(optimizer=\"adam\",\n",
      "           loss='binary_crossentropy',\n",
      "           metrics=['accuracy'])\n",
      "\n",
      "model.summary()\n",
      "\n",
      "# Train for 100 epochs using a validation split of 0.2\n",
      "model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2)\n",
      "\n",
      "# Predict on sensors_test and round up the predictions\n",
      "preds = model.predict(sensors_test)\n",
      "preds_rounded = np.round(preds)\n",
      "\n",
      "# Print rounded preds\n",
      "print('Rounded Predictions: \\n', preds_rounded)\n",
      "\n",
      "# Evaluate your model's accuracy on the test data\n",
      "accuracy = model.evaluate(sensors_test, parcels_test)[1]\n",
      "\n",
      "# Print accuracy\n",
      "print('Accuracy:', accuracy)\n",
      "\n",
      "\n",
      " callbacks\n",
      "1) EarlyStopping\n",
      "2) ModelCheckpoint\n",
      "3) History\n",
      "\n",
      "print(history.history['loss'])\n",
      "print(history.history['acc'])\n",
      "print(history.history['val_loss'])\n",
      "print(history.history['val_acc'])\n",
      "\n",
      "history, modelcheckpoint, earlystopping\n",
      "\n",
      "fit does callbacks\n",
      "\n",
      "#print(history.history['loss'])\n",
      "plt.figure()\n",
      "plt.plot(history.history['loss'])\n",
      "plt.xlabel('loss')\n",
      "plt.show()\n",
      "\n",
      "  >Early stopping\n",
      "#useful because we don't know how many epochs will be required to complete training\n",
      "\n",
      "from keras.callbacks import EarlyStopping\n",
      "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
      "\n",
      "#patience is the number of epochs to improve before the model is stopped\n",
      "\n",
      "\n",
      "model.fit(X_train, y_train, epochs=100,\n",
      "validation_data=(X_test,y_test),\n",
      "callbacks=[early_stopping])\n",
      "\n",
      "\n",
      " >model checkpoint\n",
      "\n",
      "from keras.callbacks import ModelCheckpoint\n",
      "\n",
      "#allows up to save the model as strings\n",
      "\n",
      "model_save=ModelCheckpoint('best_model.hdf5', save_best_only=True)\n",
      "\n",
      "model.fit(X_train, y_train, epochs=100,\n",
      "validation_data=(X_test,y_test),\n",
      "callbacks=[model_save])\n",
      "\n",
      " >\n",
      "\n",
      "# Train your model and save its history\n",
      "history = model.fit(X_train, y_train, epochs = 50,\n",
      "               validation_data=(X_test, y_test))\n",
      "\n",
      "# Plot train vs test loss during training\n",
      "plot_loss(history.history['loss'], history.history['val_loss'])\n",
      "\n",
      "# Plot train vs test accuracy during training\n",
      "plot_accuracy(history.history['acc'], history.history['val_acc'])\n",
      "\n",
      "# Import the early stopping callback\n",
      "from keras.callbacks import EarlyStopping\n",
      "\n",
      "# Define a callback to monitor val_acc\n",
      "monitor_val_acc = EarlyStopping(monitor='val_acc', \n",
      "                       patience=5)\n",
      "\n",
      "# Train your model using the early stopping callback\n",
      "model.fit(X_train, y_train, \n",
      "           epochs=1000, validation_data=(X_test,y_test),\n",
      "           callbacks=[monitor_val_acc])\n",
      "\n",
      "\n",
      " >\n",
      "\n",
      "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
      "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
      "\n",
      "# Early stop on validation accuracy\n",
      "monitor_val_acc = EarlyStopping(monitor ='val_acc', patience=3)\n",
      "\n",
      "# Save the best model as best_banknote_model.hdf5\n",
      "modelCheckpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only = True)\n",
      "\n",
      "# Fit your model for a stupid amount of epochs\n",
      "history = model.fit(X_train, y_train,\n",
      "                    epochs = 10000000,\n",
      "                    callbacks = [monitor_val_acc, modelCheckpoint],\n",
      "                    validation_data = (X_test, y_test))\n",
      "\n",
      " >Learning Curves\n",
      "\n",
      "1) loss learning curves decrease as epochs go by\n",
      "\n",
      "accuracy learning curves\n",
      "1) increase as epochs go by\n",
      "\n",
      "****model overfitting can be identified if the training curves and the validation curves diverge\n",
      "\n",
      "\n",
      "init_weights=model.get_weights()\n",
      "\n",
      "train_accs[]\n",
      "tests_accs[]\n",
      "\n",
      "for train_size in train_sizes:\n",
      "\tX_train_frac, -, y_train_frac, = train_test_split(X_train,y_train,train_size=train_size)\n",
      "\tmodel.set_weights(initial_weights)\n",
      "\tmodel.fit(X_train_frac, y_train_frac, epochs=100, verbose=0,\n",
      "\tcallbacks[EarlyStopping(monitor='loss', patience=1)]\n",
      "\t\n",
      "\ttrain_acc=model.evalute(X_train_frac, y_train_frac, verbose=0)[1]\n",
      "\ttrain_accs.append(train_acc)\n",
      "\n",
      "\n",
      "\ttest_acc=model.evalute(X_test_frac, y_test_frac, verbose=0)[1]\n",
      "\ttest_accs.append(train_acc)\n",
      "\n",
      "\n",
      " >\n",
      "\n",
      "# Instantiate a Sequential model\n",
      "model = Sequential()\n",
      "\n",
      "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
      "model.add(Dense(16, input_shape = (64,), activation = 'relu'))\n",
      "\n",
      "# Output layer with 10 neurons (one per digit) and softmax\n",
      "model.add(Dense(10, activation='softmax'))\n",
      "\n",
      "# Compile your model\n",
      "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
      "\n",
      "# Test if your model works and can process input data\n",
      "print(model.predict(X_train))\n",
      "\n",
      "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
      "history = model.fit(X_train, y_train, epochs=60, validation_data=(X_test, y_test), verbose=0)\n",
      "\n",
      "# Extract from the history object loss and val_loss to plot the learning curve\n",
      "plot_loss(history.history['loss'],history.history['val_loss'])\n",
      "\n",
      "for size in training_sizes:\n",
      "  \t# Get a fraction of training data (we only care about the training data)\n",
      "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
      "\n",
      "    # Reset the model to the initial weights and train it on the new data fraction\n",
      "    model.set_weights(initial_weights)\n",
      "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
      "\n",
      "    # Evaluate and store the train fraction and the complete test set results\n",
      "    train_accs.append(model.evaluate(X_train_frac, y_train_frac)[1])\n",
      "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
      "    \n",
      "# Plot train vs test accuracies\n",
      "plot_results(train_accs, test_accs)\n",
      "\n",
      "  Activation Functions\n",
      "\n",
      "a=sum of inputs * weights + bias\n",
      "\n",
      "a is passed into an activation function producing y\n",
      "\n",
      "1. sigmoid varies between 0 and 1\n",
      "2. tanh varies between -1 and 1\n",
      "3. relu varies between 0 and infinity\n",
      "4. Leaky relu between negative value and infinity\n",
      "\n",
      "np.random.seed(1)\n",
      "\n",
      "def get_model(act_function):\n",
      "\tmodel=Sequential()\n",
      "\tmodel.add(Dense(4, input_shape=(2,), activation=act_function))\n",
      "\tmodel.add(Dense(1, activation='sigmoid'))\n",
      "\treturn model\n",
      "\n",
      "\n",
      "activations=['relu','sigmoid','tanh','leaky_relu']\n",
      "\n",
      "for funct in activations:\n",
      "\tmodel= get_model(act_function=funct)\n",
      "\thistory=model.fit(X_train, y_train,\n",
      "\t\tvalidation_data=(X_test,y_test),\n",
      "\t\tepochs=100,verbose=False)\n",
      "\tactivation_results[funct]=history\n",
      "\n",
      "val_loss_per_funct = {k:v.history['val_loss] for k,v in activation_results.items()}\n",
      "\n",
      "val_loss_curves.pd.DataFrame(val_loss_per_funct)\n",
      "\n",
      "val_loss_curves.plot(title='Loss per Activation function')\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "# Activation functions to try\n",
      "activations = ['relu','leaky_relu','sigmoid','tanh']\n",
      "\n",
      "# Loop over the activation functions\n",
      "activation_results = {}\n",
      "\n",
      "for act in activations:\n",
      "  # Get a new model with the current activation\n",
      "  model = get_model(act)\n",
      "  # Fit the model\n",
      "  history = model.fit(X_train, y_train,\n",
      "\t\tvalidation_data=(X_test,y_test),\n",
      "\t\tepochs=20,verbose=0)\n",
      "  activation_results[act] = history\n",
      "\n",
      "# Create a dataframe from val_loss_per_function\n",
      "val_loss=  {k:v.history['val_loss'] for k,v in activation_results.items()}\n",
      "\n",
      "# Call plot on the dataframe\n",
      "val_loss_per_function=pd.DataFrame(val_loss)\n",
      "val_loss_per_function.plot()\n",
      "plt.show()\n",
      "\n",
      "# Create a dataframe from val_acc_per_function\n",
      "val_acc =  {k:v.history['val_acc'] for k,v in activation_results.items()}\n",
      "\n",
      "# Call plot on the dataframe\n",
      "val_acc_per_function=pd.DataFrame(val_acc)\n",
      "val_acc_per_function.plot()\n",
      "plt.show()\n",
      "\n",
      "  batch size and batch normalization\n",
      "\n",
      "1. mini-batches advantages\n",
      "a. networks train faster (more weight updates in same amount of time)\n",
      "b. less RAM memory required, can train on huge datasets\n",
      "c. noise can help networks reach a lower error, escaping local minima\n",
      "\n",
      "2. mini-batches disadvantages\n",
      "a. more iterations need to be run\n",
      "b. need to be adjusted, we need to find a good batch size\n",
      "\n",
      "keras uses a batch size of 32\n",
      "\n",
      "The smaller a batch size, the more weight updates per epoch, but at a cost of a more unstable gradient descent. Specially if the batch size is too small and it's not \n",
      "\n",
      "standardization \n",
      "data-mean/standard deviation\n",
      "\n",
      "batch normalization makes sure that independently of the changes, the inputs to the next layer are normalized\n",
      "\n",
      "batch normalization advantages\n",
      "1. improves gradient flow\n",
      "2. allows higher learning rates\n",
      "3. reduces dependence on weight initializations\n",
      "4. acts as an unintended form of regularization\n",
      "5. limits internal covariate shift\n",
      "\n",
      "from keras.layers import BatchNormalization\n",
      "\n",
      "model=Sequential()\n",
      "model.add(Dense(3, input_shape=(2,), activation='relu'))\n",
      "model.add(BatchNormalization())\n",
      "model.add(Dense(1,activation='sigmoid')\n",
      "\n",
      " \n",
      "\n",
      "model = get_model()\n",
      "\n",
      "# Fit your model for 5 epochs with a batch of size the training set\n",
      "model.fit(X_train, y_train, epochs=5, batch_size=len(X_train))\n",
      "print(\"\\n The accuracy when using the whole training set as a batch was: \",\n",
      "      model.evaluate(X_test, y_test)[1])\n",
      "\n",
      " >\n",
      "\n",
      "# Import batch normalization from keras layers\n",
      "from keras.layers import BatchNormalization\n",
      "\n",
      "# Build your deep network\n",
      "batchnorm_model = Sequential()\n",
      "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
      "batchnorm_model.add(BatchNormalization())\n",
      "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
      "batchnorm_model.add(BatchNormalization())\n",
      "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
      "batchnorm_model.add(BatchNormalization())\n",
      "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
      "\n",
      "# Compile your model with sgd\n",
      "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "\n",
      "# Train your standard model, storing its history\n",
      "history1 = standard_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
      "\n",
      "# Train the batch normalized model you recently built, store its history\n",
      "history2 = batchnorm_model.fit(X_test, y_test, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
      "\n",
      "# Call compare_acc_histories passing in both model histories\n",
      "compare_histories_acc(history1, history2)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\deep learning with keras - ltsm.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\deep learning with keras - ltsm.txt\n",
      "lstms are a type of reocurring neural network\n",
      "\n",
      "the output of an lstm can be fed back into the hidden layer of a neural network.\n",
      "\n",
      "lstm can use past predictions to make new predictions\n",
      "\n",
      "1. internal state of previous unit\n",
      "2. output of previous unit\n",
      "3. input at time f\n",
      "4. input to next LSTM unit\n",
      "5. updated state for next unit\n",
      "6. output of LSTM unit\n",
      "\n",
      "lstm learn what to ignore, what to keep, and to select the most important pieces of past information.\n",
      "\n",
      "lstm have been used for image captioning, speech to text, text translation, and document summarization and text generation.\n",
      "\n",
      "this is a sentence\n",
      "\n",
      "42 11 23 1\n",
      "\n",
      "text='Hi this is a small sentence'\n",
      "\n",
      "seq_len=3\n",
      "\n",
      "words=text.split()\n",
      "\n",
      "lines=[]\n",
      "\n",
      "for i in range(seq_len, len(words)+1):\n",
      "\tline= ' '.join(words[i-seq_len:i])\n",
      "\tlines.append(line)\n",
      "\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "\n",
      "tokenizer=Tokenizer()\n",
      "\n",
      "tokenizer.fit_on_texts(lines)\n",
      "\n",
      "seqences=tokenizer.texts_to_sequences(lines)\n",
      "\n",
      "print(tokenizer.index_word)\n",
      "\n",
      "from keras.layers import Dense, LSTM, Embedding\n",
      "\n",
      "model=Sequential()\n",
      "\n",
      "#length of the tokenizer dictionary plus 1\n",
      "\n",
      "vocab_size= len(tokenizer.index_word)+1\n",
      "\n",
      "model.add(Embedding(input_dim=vocab_size, output_dim=8, input_length=2))\n",
      "\n",
      "model.add(LSTM(8))\n",
      "\n",
      "model.add(Dense(8, activation='relu'))\n",
      "\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "\n",
      "  \n",
      "\n",
      "# Import the Embedding, LSTM and Dense layer\n",
      "from keras.layers import LSTM, Embedding, Dense\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "# Add an Embedding layer with the right parameters\n",
      "model.add(Embedding(input_dim=vocab_size, output_dim=8, input_length=3))\n",
      "\n",
      "\n",
      "\n",
      "# Add a 32 unit LSTM layer\n",
      "model.add(LSTM(32))\n",
      "\n",
      "# Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax\n",
      "model.add(Dense(32, activation='relu'))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "model.summary()\n",
      "\n",
      "def predict_text(test_text):\n",
      "  if len(test_text.split())!=3:\n",
      "    print('Text input should be 3 words!')\n",
      "    return False\n",
      "  \n",
      "  # Turn the test_text into a sequence of numbers\n",
      "  test_seq = tokenizer.texts_to_sequences([test_text])\n",
      "  test_seq = np.array(test_seq)\n",
      "  \n",
      "  # Get the model's next word prediction by passing in test_seq\n",
      "  pred = model.predict(test_seq).argmax(axis = 1)[0]\n",
      "  \n",
      "  # Return the word associated to the predicted index\n",
      "  return tokenizer.index_word[pred]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\deep learning with keras hyperparameter.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\deep learning with keras hyperparameter.txt\n",
      "  Neural network hyperparameters\n",
      "\n",
      "1. number of layers\n",
      "2. number of neurons per layer\n",
      "3. layer order\n",
      "4. layer activations\n",
      "5. batch sizes\n",
      "6. learning rates\n",
      "7. optimizers\n",
      "\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "tree=DecisionTreeClassifier()\n",
      "\n",
      "params={'max_depth':[3,None],'max_features':range(1,4),'min_samples_leaf':range(1,4)}\n",
      "\n",
      "tree_cv=RandomizedSearchCV(tree,params,cv=5)\n",
      "\n",
      "print(tree_cv.best_params_)\n",
      "\n",
      "\n",
      "  Turn a Keras model into a sklearn Estimate\n",
      "\n",
      "def create_model(optimizer='adam', activation='relu'):\n",
      "\tmodel=Sequential()\n",
      "\tmodel.add(Dense(16,input_shape=(2,), activation=activation))\n",
      "\tmodel.add(Dense(1, activation='sigmoid'))\n",
      "\tmodel.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
      "\treturn model\n",
      "\n",
      "\n",
      "from keras.wrappers.scikit_learn import KerasClassifier\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "model= KerasClassifier(build_fn=create_model, epochs=6, batch_size=16)\n",
      "\n",
      "kfold=cross_val_score(model, X, y, cv=5)\n",
      "\n",
      "kfold.mean()\n",
      "kfold.std()\n",
      "\n",
      "#grid search looks over all combinations of parameters\n",
      "\n",
      "1. Random search is preferred over grid search\n",
      "2. Don't use many epochs\n",
      "3. Use a smaller sample of your dataset\n",
      "4. play with batch sizes, activations, optimizers and learning rates\n",
      "\n",
      "params = dict(optimizer=['sgd','adam'],epochs=3,\n",
      "batch_size=[5,10,20], activation=['relu','tanh'])\n",
      "\n",
      "random_search=RandomizedSearchCV(model,params_dist=params,cv=3)\n",
      "\n",
      "random_search_results= random_search.fit(X,y)\n",
      "\n",
      "print(\"Best: %f using %s\".format(random_search_results.best_score_,\n",
      "random_search_results.best_params_))\n",
      "\n",
      "def create_model(n1=1, nn=256):\n",
      "\tmodel=Sequential()\n",
      "\tmodel.add(Dense(16,input_shape(2,), activation='relu'))\n",
      "\n",
      "for i in range(n1):\n",
      "\tmodel.add(Dense(nn,activation='relu'))\n",
      "\n",
      "\n",
      "params=dict(n1=[1,2,9], nn=[128,256,1000])\n",
      "\n",
      "random_search=RandomizedSearchCV(model,params_dist=params,cv=3)\n",
      "\n",
      "random_search_results= random_search.fit(X,y)\n",
      "\n",
      "print(\"Best: %f using %s\".format(random_search_results.best_score_,\n",
      "random_search_results.best_params_))\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "# Creates a model given an activation and learning rate\n",
      "def create_model(learning_rate=0.01, activation='relu'):\n",
      "  \n",
      "  \t# Create an Adam optimizer with the given learning rate\n",
      "  \topt = Adam(lr=learning_rate)\n",
      "  \t\n",
      "  \t# Create your binary classification model  \n",
      "  \tmodel = Sequential()\n",
      "  \tmodel.add(Dense(128, input_shape=(30,), activation=activation))\n",
      "  \tmodel.add(Dense(256, activation=activation))\n",
      "  \tmodel.add(Dense(1, activation='sigmoid'))\n",
      "  \t\n",
      "  \t# Compile your model with your optimizer, loss, and metrics\n",
      "  \tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
      "  \treturn model\n",
      "\n",
      "\n",
      "# Creates a model given an activation and learning rate\n",
      "def create_model(learning_rate=0.01, activation='relu'):\n",
      "  \n",
      "  \t# Create an Adam optimizer with the given learning rate\n",
      "  \topt = Adam(lr=learning_rate)\n",
      "  \t\n",
      "  \t# Create your binary classification model  \n",
      "  \tmodel = Sequential()\n",
      "  \tmodel.add(Dense(128, input_shape=(30,), activation=activation))\n",
      "  \tmodel.add(Dense(256, activation=activation))\n",
      "  \tmodel.add(Dense(1, activation='sigmoid'))\n",
      "  \t\n",
      "  \t# Compile your model with your optimizer, loss, and metrics\n",
      "  \tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
      "  \treturn model\n",
      "\n",
      "\n",
      "# Import KerasClassifier from keras wrappers\n",
      "from keras.wrappers.scikit_learn import KerasClassifier\n",
      "\n",
      "# Create a KerasClassifier\n",
      "model = KerasClassifier(build_fn = create_model)\n",
      "\n",
      "# Define the parameters to try out\n",
      "params = {'activation':['relu', 'tanh'], 'batch_size':[32, 128, 256], \n",
      "          'epochs':[50, 100, 200], 'learning_rate':[0.1, 0.01, .001]}\n",
      "\n",
      "# Create a randomize search cv object passing in the parameters to try\n",
      "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
      "\n",
      "\n",
      "# Import KerasClassifier from keras wrappers\n",
      "from keras.wrappers.scikit_learn import KerasClassifier\n",
      "\n",
      "# Create a KerasClassifier\n",
      "model = KerasClassifier(build_fn = create_model, epochs = 50, \n",
      "             batch_size = 128, verbose = 0)\n",
      "\n",
      "# Calculate the accuracy score for each fold\n",
      "kfolds = cross_val_score(model, X, y, cv = 5)\n",
      "\n",
      "# Print the mean accuracy\n",
      "print('The mean accuracy was:', kfolds.mean())\n",
      "\n",
      "# Print the accuracy standard deviation\n",
      "print('With a standard deviation of:', kfolds.std())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\ingesting data.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\ingesting data.txt\n",
      "pandas ingest data into data frames\n",
      "structure is rows and columns\n",
      "\n",
      "import pandas as pd\n",
      "df=pd.read_csv('us_tax_data_2016.csv')\n",
      "\n",
      "df=pd.read_csv('us_tax_data_2016.tsv',sep='\\t')\n",
      "\n",
      "sample\n",
      "\n",
      "# Import pandas with the alias pd\n",
      "import pandas as pd\n",
      "\n",
      "# Load TSV using the sep keyword argument to set delimiter\n",
      "data = pd.read_csv('vt_tax_data_2016.tsv', '\\t')\n",
      "\n",
      "# Plot the total number of tax returns by income group\n",
      "counts = data.groupby(\"agi_stub\").N1.sum()\n",
      "counts.plot.bar()\n",
      "plt.show()\n",
      "\n",
      "    modifying flat file imports\n",
      "\n",
      "df.shape\n",
      "\n",
      "usecols: choose columns to load\n",
      "\n",
      "col_names=['STATEFIPS','STATE','zipcode','agi_stub','N1']\n",
      "col_nums=[0,1,2,3,4]\n",
      "\n",
      "tax_data_v1=pd.read_csv('us_tax_data_2016.csv',usecols=col_names)\n",
      "\n",
      "\n",
      "limiting rows with nrows\n",
      "\n",
      "tax_data_v1=pd.read_csv('us_tax_data_2016.csv',nrows=1000)\n",
      "\n",
      "nrows and skiprows are useful to process a file in chunks\n",
      "\n",
      "if you skip the row with the headers then specify header=None\n",
      "\n",
      "tax_data_next500 = pd.read_csv('us_tax_data_2016.csv',\n",
      "\tnrows=500, skiprows=1000, header=None,names=col_names)\n",
      "\n",
      "names become the column header names\n",
      "\n",
      "\n",
      "sample  > column names are case sensitive\n",
      "\n",
      "# Create list of columns to use\n",
      "cols = ['zipcode','agi_stub','mars1','MARS2','NUMDEP']\n",
      "\n",
      "# Create data frame from csv using only selected columns\n",
      "data = pd.read_csv(\"vt_tax_data_2016.csv\", usecols=cols)\n",
      "\n",
      "# View counts of dependents and tax returns by income level\n",
      "print(data.groupby(\"agi_stub\").sum())\n",
      "\n",
      "\n",
      "sample  > nrows and skiprows and names\n",
      "\n",
      "# Create data frame of next 500 rows with labeled columns\n",
      "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
      "                       \t\t  nrows=500,\n",
      "                       \t\t  skiprows=500,\n",
      "                       \t\t  header=None,\n",
      "                       \t\t  names=list(vt_data_first500))\n",
      "\n",
      "# View the Vermont data frames to confirm they're different\n",
      "print(vt_data_first500.head())\n",
      "print(vt_data_next500.head())\n",
      "\n",
      "\n",
      "     handling errors and missing data\n",
      "\n",
      " common flat file import issues\n",
      "1. column data types are wrong\n",
      "2. values are missing\n",
      "3. record that cannot be ready by pandas\n",
      "\n",
      "print(df.dtypes)\n",
      "\n",
      "read_csv dtype argument takes a dictionary of column names and data types.\n",
      "\n",
      "dtype={'zipcode':str}\n",
      "\n",
      "pandas automatically intrepets some values as missing (null) or na\n",
      "\n",
      "\n",
      "   customizing missing data values\n",
      "\n",
      "na_values to setup custom missing value.  you can pass a single value, list, or dictionary of columns and values.\n",
      "\n",
      "tax_data=pd.read_csv('us_tax_data_2016.csv', na_values={'zipcode':0})\n",
      "\n",
      "print(tax_data[tax_data.zipcode.isna()])\n",
      "\n",
      "  Lines with errors\n",
      "\n",
      "error_bad_lines=False to skip unparseable records\n",
      "\n",
      "warn_bad_lines=True to see messages when records are skipped\n",
      "\n",
      "tax_data=pd.read_csv('us_tax_data_2016.csv', \n",
      "\terror_bad_lines=False,\n",
      "\twarn_bad_lines=True)\n",
      "\n",
      "sample  > dtype using a dictionary\n",
      "\n",
      "# Create dict specifying data types for agi_stub and zipcode\n",
      "data_types = {'agi_stub':'category',\n",
      "\t\t\t  'zipcode':str}\n",
      "\n",
      "# Load csv using dtype to set correct data types\n",
      "data = pd.read_csv(\"vt_tax_data_2016.csv\", dtype=data_types)\n",
      "\n",
      "# Print data types of resulting frame\n",
      "print(data.dtypes.head())\n",
      "\n",
      "sample  > na_values\n",
      "\n",
      "# Create dict specifying that 0s in zipcode are NA values\n",
      "null_values = {'zipcode':0}\n",
      "\n",
      "# Load csv using na_values keyword argument\n",
      "data = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
      "                   na_values=null_values)\n",
      "\n",
      "# View rows with NA ZIP codes\n",
      "print(data[data.zipcode.isna()])\n",
      "\n",
      "\n",
      "sample  > error_bad_lines and warn_bad_lines\n",
      "\n",
      "try:\n",
      "  # Set warn_bad_lines to issue warnings about bad records\n",
      "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
      "                     error_bad_lines=False, \n",
      "                     warn_bad_lines=True)\n",
      "  \n",
      "  # View first 5 records\n",
      "  print(data.head())\n",
      "  \n",
      "except pd.io.common.CParserError:\n",
      "    print(\"Your data contained rows that could not be parsed.\")\n",
      "\n",
      "\n",
      "   > introduction to spreadsheets\n",
      "\n",
      "survey_data=pd.read_excel('fcc_survey.xlsx')\n",
      "\n",
      "print(survey_data.head())\n",
      "\n",
      "nrows: limit number of rows to load\n",
      "skiprows: specify number of rows to skip\n",
      "\n",
      "usecols: choose columns by name, positional number or letter(\"A:P\")\n",
      "\n",
      "\n",
      "survey_data=pd.read_excel('fcc_survey.xlsx',\n",
      "skiprows=2,\n",
      "usecols=\"W:AB, AR\"\n",
      ")\n",
      "\n",
      "sample\n",
      "\n",
      "# Load pandas as pd\n",
      "import pandas as pd\n",
      "\n",
      "# Read spreadsheet and assign it to survey_responses\n",
      "survey_responses = pd.read_excel('fcc_survey.xlsx')\n",
      "\n",
      "survey_responses = pd.read_excel('fcc_survey.xlsx',sheet_name=None)\n",
      "\n",
      "reads all sheets in a workbook\n",
      "\n",
      "for key, value in survey_responses.items():\n",
      "\tprint(key, type(value))\n",
      "\n",
      "the value is a dataframe\n",
      "\n",
      "\n",
      "# View the head of the data frame\n",
      "print(survey_responses.head())\n",
      "\n",
      "sample    skiprows and column ranges\n",
      "\n",
      "# Create string of lettered columns to load\n",
      "col_string = 'AD,AW:BA'\n",
      "\n",
      "# Load data with skiprows and usecols set\n",
      "survey_responses = pd.read_excel(\"fcc_survey_headers.xlsx\", \n",
      "                        skiprows=2, \n",
      "                        usecols=col_string)\n",
      "\n",
      "# View the names of the columns selected\n",
      "print(survey_responses.columns)\n",
      "\n",
      "    fixing decimals\n",
      "\n",
      "def fix_decimal(num):\n",
      "### convert numeric value with comma as decimal separator to float\n",
      "  print(num)\n",
      "  return float(num.replace(',', '.')) if num else 0\n",
      "  \n",
      "tmp = pd.read_excel(\"test.xlsx\", converters={0: fix_decimal} )\n",
      "\n",
      "\n",
      "   > getting data from multiple worksheets\n",
      "\n",
      "specifying sheets to get data\n",
      "\n",
      "read_excel loads the first sheet in an excel file by default\n",
      "\n",
      "sheet_name keyword argument to load other sheets or a list of name/numbers to load more than one sheet at a time\n",
      "\n",
      "sheets are 0 indexed\n",
      "\n",
      "survey_data_sheet2 = pd.read_excel('fcc_survey.xlsx',sheet_name=1)\n",
      "\n",
      "survey_data_responses = pd.read_excel('fcc_survey.xlsx',sheet_name=None)\n",
      "\n",
      "for key, value in survey_data_responses.items():\n",
      "\tprint(key, type(value))\n",
      "\n",
      "all_responses=pd.DataFrame()\n",
      "\n",
      "for sheet_name, frame in survey_data_responses.items():\n",
      "\tframe['Year']=sheet_name\n",
      "\n",
      "\tall_responses=all_responses.append(frame)\n",
      "\n",
      "print(all_response.Year.unique())\n",
      "\n",
      "\n",
      "sample   read and plot\n",
      "\n",
      "# Create df from second worksheet by referencing its position\n",
      "responses_2017 = pd.read_excel(\"fcc_survey.xlsx\",\n",
      "                               sheet_name=1)\n",
      "\n",
      "# Graph where people would like to get a developer job\n",
      "job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
      "job_prefs.plot.barh()\n",
      "plt.show()\n",
      "\n",
      " > sample\n",
      "\n",
      "# Load all sheets in the Excel file\n",
      "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
      "                                sheet_name=[0,'2017'])\n",
      "\n",
      "# View the sheet names in all_survey_data\n",
      "print(all_survey_data.keys())c\n",
      "\n",
      "\n",
      "   sample load all dataframes\n",
      "\n",
      "# Create an empty data frame\n",
      "all_responses = pd.DataFrame()\n",
      "\n",
      "# Set up for loop to iterate through values in responses\n",
      "for df in responses.values():\n",
      "  # Print the number of rows being added\n",
      "  print(\"Adding {} rows\".format(df.shape[0]))\n",
      "  # Append df to all_responses, assign result\n",
      "  all_responses = all_responses.append(df)\n",
      "\n",
      "# Graph employment statuses in sample\n",
      "counts = all_responses.groupby(\"EmploymentStatus\").EmploymentStatus.count()\n",
      "counts.plot.barh()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "          >importing boolean data\n",
      "\n",
      "print(bootcamp_data.sum())\n",
      "print(bootcamp_data.isna().sum())\n",
      "\n",
      "\n",
      "NA and missing values in boolan columns are changed to True\n",
      "\n",
      "true_values=True\n",
      "false_values=False\n",
      "\n",
      "each takes a list of values to treat as True/False, respectively.\n",
      "\n",
      "bool_data=pd.read_excel(\"fcc_survey.xlsx\",\n",
      "\tdtype={\n",
      "\t\"AttendedBootcamp\":bool,\n",
      "\t\"LoanYesNo\":bool,\n",
      "\t\"LoanTF\":bool\n",
      "\t},\n",
      "       true_values=['Yes'],\n",
      "       false_values=['No'])\n",
      "\n",
      "\n",
      "na as coded as true\n",
      "\n",
      "are there missing values, or could there be in the future\n",
      "how will this column be used in analysis\n",
      "what could happen if a value wee incorrectly coded as true\n",
      "\n",
      "sample\n",
      "\n",
      "# Set dtype to load appropriate column(s) as Boolean data\n",
      "survey_data = pd.read_excel(\"fcc_survey_subset.xlsx\",\n",
      "                            dtype={'HasDebt':bool})\n",
      "\n",
      "# View financial burdens by Boolean group\n",
      "print(survey_data.groupby('HasDebt').sum())\n",
      "\n",
      "\n",
      "sample  true_values false_values\n",
      "\n",
      "# Load file with Yes as a True value and No as a False value\n",
      "survey_subset = pd.read_excel(\"fcc_survey_yn_data.xlsx\",\n",
      "                              dtype={\"HasDebt\": bool,\n",
      "                              \"AttendedBootCampYesNo\": bool},\n",
      "                              true_values=['Yes'],\n",
      "                              false_values=['No'])\n",
      "\n",
      "# View the data\n",
      "print(survey_subset.head())\n",
      "\n",
      "\n",
      "   > parse dates\n",
      "\n",
      "dates and times have their own data type and internal representation\n",
      "\n",
      "by default pandas loads datetime columns as strings\n",
      "\n",
      "use parse_dates parameter to parse datetime columns\n",
      "\n",
      "parse_dates can accept:\n",
      "1. a list of column names or numbers to parse\n",
      "2. a list containing lists of columns to combine and parse\n",
      "\n",
      "\n",
      "date_cols=['Part1StartTime','Part1EndTime']\n",
      "\n",
      "survey_df= pd.read_excel('fcc_survey.xlsx',\n",
      "\tparse_dates=date_cols)\n",
      "\n",
      "\n",
      "date_cols=['Part1StartTime','Part1EndTime',\n",
      "[['Part2StartDate','Part2StartTime']]\n",
      "]\n",
      "\n",
      "survey_df= pd.read_excel('fcc_survey.xlsx',\n",
      "\tparse_dates=date_cols)\n",
      "\n",
      "\n",
      "date_cols={\n",
      "\"Part1Start\":\"Part1StartTime\",\n",
      "\"Part1End\":\"Part1EndTime\",\n",
      "\"Part2Start\":[\"Part2StartDate\",\"Part2StartTime\"]\n",
      "}\n",
      "\n",
      "survey_df= pd.read_excel('fcc_survey.xlsx',\n",
      "\tparse_dates=date_cols)\n",
      "\n",
      "\n",
      "use pd.to_datetime() after loading data if parse_dates does not work\n",
      "\n",
      "\n",
      "see strftime.org for all for the format \n",
      "\n",
      "%Y Year(4-digit)\n",
      "%m Month padded\n",
      "%d Day padded\n",
      "%H Hour 24 hour clock\n",
      "%M Minute padded\n",
      "\n",
      "format_string=\"%m%d%Y %H:%M:%S\"\n",
      "\n",
      "survey_df[\"Part2EndTime\"]=pd.to_datetime(survey_df['Part2EndTime'],format=format_string)\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Load file, with Part1StartTime parsed as datetime data\n",
      "survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
      "                            parse_dates=['Part1StartTime'])\n",
      "\n",
      "# Print first few values of Part1StartTime\n",
      "print(survey_data.Part1StartTime.head())\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "\n",
      "# Create dict of columns to combine into new datetime column\n",
      "datetime_cols = {\"Part2Start\": ['Part2StartDate', 'Part2StartTime']}\n",
      "\n",
      "\n",
      "# Load file, supplying the dict to parse_dates\n",
      "survey_data = pd.read_excel(\"fcc_survey_dts.xlsx\",\n",
      "                            parse_dates=datetime_cols)\n",
      "\n",
      "# View summary statistics about Part2Start\n",
      "print(survey_data.Part2Start.describe())\n",
      "\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Parse datetimes and assign result back to Part2EndTime\n",
      "survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"], \n",
      "                                             format=\"%m%d%Y %H:%M:%S\")\n",
      "\n",
      "# Print first few values of Part2EndTime\n",
      "print(survey_data[\"Part2EndTime\"])\n",
      "\n",
      "\n",
      "       >Connecting to a database\n",
      "\n",
      "SQL alchemy\n",
      "create_engine() makes an engine to handle database connections\n",
      "1. need string url of database to connect to\n",
      "SQLite url format: sqlite:///filename.db\n",
      "\n",
      "pd.read_sql(query, engine)\n",
      "\n",
      "engine is a way to connect to the database\n",
      "\n",
      "\n",
      "from sqlachemy import create_engine\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Import sqlalchemy's create_engine() function\n",
      "from sqlalchemy import create_engine\n",
      "\n",
      "# Create the database engine\n",
      "engine = create_engine(\"sqlite:///data.db\")\n",
      "\n",
      "# View the tables in the database\n",
      "print(engine.table_names())\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Load libraries\n",
      "import pandas as pd\n",
      "from sqlalchemy import create_engine\n",
      "\n",
      "# Create the database engine\n",
      "engine = create_engine('sqlite:///data.db')\n",
      "\n",
      "# Load hpd311calls without any SQL\n",
      "hpd_calls = pd.read_sql(\"hpd311calls\", engine)\n",
      "\n",
      "# View the first few rows of data\n",
      "print(hpd_calls.head())\n",
      "\n",
      "sample\n",
      "\n",
      "# Create the database engine\n",
      "engine = create_engine(\"sqlite:///data.db\")\n",
      "\n",
      "# Create a SQL query to load the entire weather table\n",
      "query = \"\"\"\n",
      "SELECT * \n",
      "  FROM weather;\n",
      "\"\"\"\n",
      "\n",
      "# Load weather with the SQL query\n",
      "weather = pd.read_sql(query, engine)\n",
      "\n",
      "# View the first few rows of data\n",
      "print(weather.head())\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Create database engine for data.db\n",
      "engine = create_engine(\"sqlite:///data.db\")\n",
      "\n",
      "# Write query to get date, tmax, and tmin from weather\n",
      "query = \"\"\"\n",
      "SELECT date, \n",
      "       tmax, \n",
      "       tmin\n",
      "  FROM weather;\n",
      "\"\"\"\n",
      "\n",
      "# Make a data frame by passing query and engine to read_sql()\n",
      "temperatures = pd.read_sql(query,engine)\n",
      "\n",
      "# View the resulting data frame\n",
      "print(temperatures)\n",
      "\n",
      "sample\n",
      "\n",
      "# Create query to get hpd311calls records about safety\n",
      "query = \"\"\"\n",
      "select *\n",
      "from hpd311calls\n",
      "where complaint_type='SAFETY';\n",
      "\"\"\"\n",
      "\n",
      "# Query the database and assign result to safety_calls\n",
      "safety_calls = pd.read_sql(query,engine)\n",
      "\n",
      "# Graph the number of safety calls by borough\n",
      "call_counts = safety_calls.groupby('borough').unique_key.count()\n",
      "call_counts.plot.barh()\n",
      "plt.show()\n",
      "\n",
      "sample\n",
      "\n",
      "# Create query for records with max temps <= 32 or snow >= 1\n",
      "query = \"\"\"\n",
      "SELECT *\n",
      "  FROM weather\n",
      "  where tmax<32 or snow>=1;\n",
      "\"\"\"\n",
      "\n",
      "# Query database and assign result to wintry_days\n",
      "wintry_days = pd.read_sql(query,engine)\n",
      "\n",
      "# View summary stats about the temperatures\n",
      "print(wintry_days.describe())\n",
      "\n",
      "   querie\n",
      "\n",
      "select distinct column_name from table\n",
      "select distinct * from table\n",
      "\n",
      "select avg(tmax) from weather\n",
      "select count(*) from table\n",
      "select count(distinct column_name) from table\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Create query for unique combinations of borough and complaint_type\n",
      "query = \"\"\"\n",
      "SELECT distinct borough, \n",
      "      complaint_type\n",
      "  from hpd311calls;\n",
      "\"\"\"\n",
      "\n",
      "# Load results of query to a data frame\n",
      "issues_and_boros = pd.read_sql(query,engine)\n",
      "\n",
      "# Check assumption about issues and boroughs\n",
      "print(issues_and_boros)\n",
      "\n",
      "sample\n",
      "\n",
      "# Create query to get call counts by complaint_type\n",
      "query = \"\"\"\n",
      "select complaint_type, \n",
      "     count(*)\n",
      "  FROM hpd311calls\n",
      "  group by complaint_type;\n",
      "\"\"\"\n",
      "\n",
      "# Create data frame of call counts by issue\n",
      "calls_by_issue = pd.read_sql(query, engine)\n",
      "\n",
      "# Graph the number of calls for each housing issue\n",
      "calls_by_issue.plot.barh(x=\"complaint_type\")\n",
      "plt.show()\n",
      "\n",
      "# Create a query to get month and max tmax by month\n",
      "query = \"\"\"\n",
      "SELECT max(tmax), \n",
      "       month\n",
      "  FROM weather\n",
      "  group by month;\"\"\"\n",
      "\n",
      "# Get data frame of monthly weather stats\n",
      "weather_by_month = pd.read_sql(query, engine)\n",
      "\n",
      "# View weather stats by month\n",
      "print(weather_by_month)\n",
      "\n",
      "SELECT month, \n",
      "       MAX(tmax), \n",
      "       MIN(tmin)\n",
      "  FROM weather \n",
      " GROUP BY month;\n",
      "\n",
      "\n",
      "  > join\n",
      "select * from hpd311calls join weather on hpd311calls.created_date = weather. date\n",
      "\n",
      "select hpd311calls.borough,\n",
      "\tcount(*),\n",
      "boro_census.total_population,\n",
      "boro_census.housing_units\n",
      "from hpd311calls\n",
      "join boro_census\n",
      "\ton hpd311calls.borough=boro_census.borough\n",
      "group by hpd311calls.borough\n",
      "\n",
      "\n",
      "# Query to get water leak calls and daily precipitation\n",
      "query = \"\"\"\n",
      "SELECT hpd311calls.*, weather.prcp\n",
      "  FROM hpd311calls\n",
      "  JOIN weather\n",
      "    ON hpd311calls.created_date = weather.date\n",
      "  where hpd311calls.complaint_type = 'WATER LEAK';\"\"\"\n",
      "\n",
      "# Load query results into the leak_calls data frame\n",
      "leak_calls = pd.read_sql(query, engine)\n",
      "\n",
      "# View the data frame\n",
      "print(leak_calls.head())\n",
      "\n",
      "# Modify query to join tmax and tmin from weather by date\n",
      "query = \"\"\"\n",
      "SELECT hpd311calls.created_date, \n",
      "\t   COUNT(*), \n",
      "       weather.tmax,\n",
      "       weather.tmin\n",
      "  FROM hpd311calls \n",
      "       join weather\n",
      "       on hpd311calls.created_date = weather.date\n",
      " WHERE hpd311calls.complaint_type = 'HEAT/HOT WATER' \n",
      " GROUP BY hpd311calls.created_date;\n",
      " \"\"\"\n",
      "\n",
      "# Query database and save results as df\n",
      "df = pd.read_sql(query, engine)\n",
      "\n",
      "# View first 5 records\n",
      "print(df.head())\n",
      "\n",
      "\n",
      "   > introduction to json\n",
      "\n",
      "based on javascript\n",
      "data is organized into collections of objects\n",
      "attribute value pairs\n",
      "\n",
      "\n",
      "read_json()\n",
      "\n",
      "oriented-record\n",
      "1. list of records\n",
      "\n",
      "oriented-column\n",
      "1. array of values for a column name\n",
      "\n",
      "{ 'age_adjusted_death_rate\":{\n",
      "\t'0': '7.6',\n",
      "\t'1': '8.1'\n",
      "     }\n",
      "}\n",
      "\n",
      "oriented - split\n",
      "\n",
      "{\n",
      "columns:['age_adjusted_death_rate',\n",
      "\t'death_rate',\n",
      "\t'deaths'\n",
      "\t'leading_cause',\n",
      "\t'race_ethnicity',\n",
      "\t'sex',\n",
      "\t'year'\n",
      "],\n",
      "\n",
      "'index':[]\n",
      "'data':[]\n",
      "}\n",
      "\n",
      "sample\n",
      "\n",
      "# Load pandas as pd\n",
      "import pandas as pd\n",
      "\n",
      "# Load the daily report to a data frame\n",
      "pop_in_shelters = pd.read_json('dhs_daily_report.json')\n",
      "\n",
      "# View summary stats about pop_in_shelters\n",
      "print(pop_in_shelters.describe())\n",
      "\n",
      "sample  > orient split\n",
      "\n",
      "try:\n",
      "    # Load the JSON with orient specified\n",
      "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
      "                      orient='split')    \n",
      "                      \n",
      "    # Plot total population in shelters over time\n",
      "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
      "    df.plot(x=\"date_of_census\", \n",
      "            y=\"total_individuals_in_shelter\")\n",
      "    plt.show()\n",
      "    \n",
      "except ValueError:\n",
      "    print(\"pandas could not parse the JSON.\")\n",
      "\n",
      "\n",
      "   > api\n",
      "\n",
      "requests\n",
      "send and get data an url\n",
      "requests.get(url_string)\n",
      "params: dictionary\n",
      "headers: users authentication key\n",
      "\n",
      "response object containing data and meta data\n",
      "response.json()\n",
      "\n",
      "import requests\n",
      "https://api.yelp.com/v3/businesses/search\n",
      "\n",
      "params={\n",
      "\"term\":\"bookstore\",\n",
      "\"location\":\"San Francisco\"\n",
      "}\n",
      "\n",
      "headers={'Authorization':'Bear {}'.format(token) }\n",
      "\n",
      "response = requests.get(url,params=params,headers=headers)\n",
      "data=response.json()\n",
      "print(data)\n",
      "\n",
      "\n",
      "sample  > query yelp\n",
      "\n",
      "# Create dictionary to query API for cafes in NYC\n",
      "parameters = {\"term\":\"cafe\",\n",
      "          \t  \"location\":\"NYC\"}\n",
      "\n",
      "# Query the Yelp API with headers and params set\n",
      "response = requests.get(api_url,\n",
      "                params=parameters,\n",
      "                headers=headers)\n",
      "\n",
      "# Extract JSON data from response\n",
      "data = response.json()\n",
      "\n",
      "# Load \"businesses\" values to a data frame and print head\n",
      "cafes = pd.DataFrame(data['businesses'])\n",
      "print(cafes.head())\n",
      "\n",
      "\n",
      "sample   authorization token\n",
      "\n",
      "# Create dictionary that passes Authorization and key string\n",
      "headers = {'Authorization': \"Bearer {}\".format(api_key)}\n",
      "\n",
      "# Query the Yelp API with headers and params set\n",
      "response = requests.get(api_url,params=params,headers=headers)\n",
      "\n",
      "\n",
      "\n",
      "# Extract JSON data from response\n",
      "data = response.json()\n",
      "\n",
      "# Load \"businesses\" values to a data frame and print names\n",
      "cafes = pd.DataFrame(data['businesses'])\n",
      "print(cafes.name)\n",
      "\n",
      "\n",
      "\n",
      "    nested json data\n",
      "\n",
      "json has attribute-value\n",
      "\n",
      "\n",
      "pandas.io.json  submodule for reading and writing json\n",
      "\n",
      "json_normalize to flatten nested data\n",
      "returns a flatten dataframe\n",
      "\n",
      "import pandas as pd\n",
      "import requests\n",
      "from pandas.io.json import json_normalize\n",
      "\n",
      "\n",
      "# Create dictionary that passes Authorization and key string\n",
      "headers = {'Authorization': \"Bearer {}\".format(api_key)}\n",
      "\n",
      "parameters = {\"term\":\"cafe\",\n",
      "          \t  \"location\":\"NYC\"}\n",
      "\n",
      "# Query the Yelp API with headers and params set\n",
      "response = requests.get(api_url,params=params,headers=headers)\n",
      "\n",
      "# Extract JSON data from response\n",
      "data = response.json()\n",
      "\n",
      "bookstores= json_normalize(data['businesses'],sep='_')\n",
      "print(list(bookstores))\n",
      "\n",
      "json_normalize()\n",
      "1.record_path: string/list of string attributes to nested data\n",
      "2. meta: list of other attributes to load to data frame\n",
      "3. meta_prefix: string to prefix to meta column names\n",
      "\n",
      "\n",
      "df=json_normalize(data['businesses'],\n",
      "\tsep=\"_\",\n",
      "\trecord_path=\"categories\",\n",
      "\tmeta=['name','alias','rating',['coordinates','lattitude'],\n",
      "['coordinates','lattitude']],\n",
      "meta_prefix='biz_')\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Load json_normalize()\n",
      "from pandas.io.json import json_normalize\n",
      "\n",
      "# Isolate the JSON data from the API response\n",
      "data = response.json()\n",
      "\n",
      "# Flatten business data into a data frame, replace separator\n",
      "cafes = json_normalize(data[\"businesses\"],\n",
      "             sep=\"_\")\n",
      "\n",
      "# View data\n",
      "print(cafes.head())\n",
      "\n",
      "sample\n",
      "\n",
      "# Specify record path to get categories data\n",
      "flat_cafes = json_normalize(data[\"businesses\"],\n",
      "                            sep=\"_\",\n",
      "                    \t\trecord_path=\"categories\")\n",
      "\n",
      "# View the data\n",
      "print(flat_cafes.head())\n",
      "\n",
      "sample\n",
      "\n",
      "# Load other business attributes and set meta prefix\n",
      "flat_cafes = json_normalize(data[\"businesses\"],\n",
      "                            sep=\"_\",\n",
      "                    \t\trecord_path=\"categories\",\n",
      "                    \t\tmeta=[\"name\", \n",
      "                                  \"alias\",  \n",
      "                                  \"rating\",\n",
      "                          \t\t  [\"coordinates\", \"latitude\"], \n",
      "                          \t\t  [\"coordinates\", \"longitude\"]],\n",
      "                    \t\tmeta_prefix=\"biz_\")\n",
      "\n",
      "# View the data\n",
      "print(flat_cafes.head())\n",
      "\n",
      "   > combining multiple datasets\n",
      "\n",
      "df1.append(d2)\n",
      "\n",
      "\n",
      "bookstores=first_20_bookstores.append(next_20_bookstores,ignore_index=True)\n",
      "\n",
      "set ignore_index=True to renumber rows\n",
      "\n",
      "\n",
      "  > merging\n",
      "\n",
      "merge: is both a pandas function and a data frame method\n",
      "1. second data frame to merge\n",
      "2. column to merge on, left_on, right_on\n",
      "\n",
      "merged= call_counts.merge(weather, left_on='created_date',right_on='date')\n",
      "print(merged.head())\n",
      "\n",
      "\n",
      "   sample\n",
      "\n",
      "# Add an offset parameter to get cafes 51-100\n",
      "params = {\"term\": \"cafe\", \n",
      "          \"location\": \"NYC\",\n",
      "          \"sort_by\": \"rating\", \n",
      "          \"limit\": 50,\n",
      "          \"offset\": 50}\n",
      "\n",
      "result = requests.get(api_url, headers=headers, params=params)\n",
      "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
      "\n",
      "# Append the results, setting ignore_index to renumber rows\n",
      "cafes = top_50_cafes.append(next_50_cafes, ignore_index=True)\n",
      "\n",
      "# Print shape of cafes\n",
      "print(cafes.shape)\n",
      "\n",
      "\n",
      "sample\n",
      "\n",
      "# Merge crosswalk into cafes on their zip code fields\n",
      "cafes_with_pumas = cafes.merge(crosswalk,left_on='location_zip_code',\n",
      "right_on='zipcode')\n",
      "\n",
      "print(cafes.info())\n",
      "print(crosswalk.info())\n",
      "\n",
      "# Merge pop_data into cafes_with_pumas on puma field\n",
      "cafes_with_pop = cafes_with_pumas.merge(pop_data,on='puma')\n",
      "\n",
      "# View the data\n",
      "print(cafes_with_pop.head())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\nlp and rnn and machine translation and generative summaries.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\nlp and rnn and machine translation and generative summaries.txt\n",
      "nlp\n",
      "1. generation of text\n",
      "2. machine translation\n",
      "3. sentence or word auto-completion\n",
      "4. generation of textual summaries\n",
      "5. automated chatbots\n",
      "\n",
      "introduction to sequential data\n",
      "\n",
      "1. sequential data is any data where the order matters\n",
      "a. text data\n",
      "b. time series data\n",
      "c. dna sequences\n",
      "\n",
      "names are an order sequence of characters\n",
      "a. the order distribution is some probability of what character comes next in the sequence\n",
      "\n",
      "\\t start token\n",
      "\\n end token\n",
      "\n",
      "\n",
      "data['name']=data['name'].apply(lamdba x: '\\t'+x)\n",
      "\n",
      "data['target']=data['name'].apply(lambda x: x[1:len(x)]+'\\n')\n",
      "\n",
      "vocabulary - set of all unique characters used in the dataset\n",
      "\n",
      "\n",
      "def get_vocabulary(names):\n",
      "\tvocabulary=set(['\\t','\\n'])\n",
      "\tfor name in names:\n",
      "\t\tfor c in name:\n",
      "\t\t\tif not in all_chars:\n",
      "\t\t\t\tvocabulary.add(c)\n",
      "\treturn vocabulary\n",
      "\n",
      "ctoi = { char: idx for idx, char in enumerate(sorted(vocabulary))}\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "\n",
      "# Insert a tab in front of all the names\n",
      "names_df['input'] = names_df['input'].apply(lambda x : '\\t' + x)\n",
      "\n",
      "# Append a newline at the end of every name\n",
      "# We already appended a tab in front, so the target word should start at index 1\n",
      "names_df['target'] = names_df['input'].apply(lambda x: x[1:len(x)]+'\\n')\n",
      "\n",
      "\n",
      "# Get the vocabulary\n",
      "vocabulary = get_vocabulary(names_df['input'])\n",
      "\n",
      "# Sort the vocabulary\n",
      "vocabulary_sorted = sorted(vocabulary)\n",
      "\n",
      "# Create the mapping of the vocabulary chars to integers\n",
      "char_to_idx = { char : idx for idx, char in enumerate(vocabulary_sorted) }\n",
      "\n",
      "# Create the mapping of the integers to vocabulary chars\n",
      "idx_to_char = { idx : char for idx, char in enumerate(vocabulary_sorted) }\n",
      "\n",
      "# Print the dictionaries\n",
      "print(char_to_idx)\n",
      "print(idx_to_char)\n",
      "\n",
      "{'\\t': 0, '\\n': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
      "{0: '\\t', 1: '\\n', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n",
      "\n",
      "\n",
      "        >Recurrent neural network\n",
      "\n",
      "1. feed-forward neural networks\n",
      "a. input layer\n",
      "b. hidden layer\n",
      "c. output layer\n",
      "\n",
      "they perform the same computations for every element in the sequence.  the output depends on what came before.  the hidden state passes information to the next network.  the state can be thought of as the memory of the network.\n",
      "\n",
      "\n",
      "\\t->j\n",
      "j->o\n",
      "o->h\n",
      "h->n\n",
      "n->\\n\n",
      "\n",
      "the input vector is translated to the output vector for each state\n",
      "\n",
      "character to integer mapping\n",
      "one-hot encoding of the characters\n",
      "\n",
      "time-step: length of the longest name\n",
      "\n",
      "def get_max_len(names):\n",
      "\tlength_list=[]\n",
      "\tfor l in names:\n",
      "\t\tlength_list.append(len(l))\n",
      "\tmax_len = np.max(length_list)\n",
      "\treturn max_len\n",
      "\n",
      "\n",
      "max_len - get_max_len(names)\n",
      "\n",
      "input_data = np.zeros((len(names.name),max_len+1, len(vocabulary)), dtype='float32')\n",
      "\n",
      "\n",
      "for n_idx, name iun enumerate(names.name):\n",
      "\tfor c_idx, char in enumerate(name):\n",
      "\t\tinput_data[n_idx, c_idx, char_to_idx[char]]=1   #one hot encoding\n",
      "\n",
      "\n",
      "\n",
      "target_data = np.zeros((len(names.name),max_len+1, len(vocabulary)), dtype='float32')\n",
      "\n",
      "\n",
      "for n_idx, name iun enumerate(names.target):\n",
      "\tfor c_idx, char in enumerate(name):\n",
      "\t\ttarget_data[n_idx, c_idx, char_to_idx[char]]=1   #one hot encoding\n",
      "\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "\n",
      "model.add(SimpleRNN(50, input_shape=(max_len+1, len(vocabulary)),\n",
      "\treturn_sequences=True))\n",
      "\n",
      "\n",
      "model.add(TimeDistributed(Dense(len(vocabulary), activation='softmax')))\n",
      "\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
      "\n",
      "model.summary()\n",
      "\n",
      "\n",
      "     \n",
      "\n",
      "\n",
      "# Find the length of longest name\n",
      "max_len = get_max_len(names_df['input'])\n",
      "\n",
      "# Initialize the input vector\n",
      "input_data = np.zeros((len(names_df['input']), max_len+1, len(vocabulary)), dtype='float32')\n",
      "\n",
      "# Initialize the target vector\n",
      "target_data = np.zeros((len(names_df['input']), max_len+1, len(vocabulary)), dtype='float32')\n",
      "\n",
      "\n",
      "# Iterate for each name in the dataset\n",
      "for n_idx, name in enumerate(names_df['input']):\n",
      "  # Iterate over each character and convert it to a one-hot encoded vector\n",
      "  for c_idx, char in enumerate(name):\n",
      "    input_data[n_idx, c_idx, char_to_idx[char]] = 1\n",
      "\n",
      "# Iterate for each name in the dataset\n",
      "for n_idx, name in enumerate(names_df['target']):\n",
      "  # Iterate over each character and convert it to a one-hot encoded vector\n",
      "  for c_idx, char in enumerate(name):\n",
      "    target_data[n_idx,c_idx,char_to_idx[char]] = 1\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "\n",
      "model.add(SimpleRNN(50, input_shape=(max_len+1, len(vocabulary)),\n",
      "\treturn_sequences=True))\n",
      "\n",
      "\n",
      "model.add(TimeDistributed(Dense(len(vocabulary), activation='softmax')))\n",
      "\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
      "\n",
      "model.summary()\n",
      "\n",
      "   \n",
      "\n",
      "1st dimension is the number of samples (names)\n",
      "2nd dimension is the number of time steps\n",
      "3rd dimension is the vocabulary size\n",
      "\n",
      "model.fit(input_data, target_data, batch_size=128, epochs=15)\n",
      "\n",
      "\n",
      "output_seq = np.zeros((1,max_len+1,len(vocabulary)))\n",
      "output_seq[0,0, char_to_idx['\\t\\]]=1\n",
      "\n",
      "probs= model.predict_proba(output_seq,verbose=0)[:,1,:]\n",
      "first_char=np.random.choice(sorted(list(vocabulary)),replace=False, p=probs.reshape(28))\n",
      "\n",
      "output_seq=[0,1,char_to_idex[first_char]]=1\n",
      "probs=model.predict_proba(output_seq, verbose=0)[:,2,:]\n",
      "second_char=np.random.choice(sorted(list(vocabulary)),replace=False, p=probs.reshape(28))\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "# Fit the model for 5 epochs using a batch size of 128 \n",
      "model.fit(input_data,target_data, batch_size=128, epochs=5)\n",
      "\n",
      "# Create a 3-D zero vector and initialize it with the start token\n",
      "output_seq = np.zeros((1, max_len+1, len(vocabulary)))\n",
      "output_seq[0, 0, char_to_idx['\\t']] = 1\n",
      "\n",
      "# Get the probabilities for the first character\n",
      "probs = model.predict_proba(output_seq, verbose=0)[:,1,:]\n",
      "\n",
      "# Sample vocabulary to get first character\n",
      "first_char = np.random.choice(sorted(list(vocabulary)), replace=False, p=probs.reshape(len(vocabulary)))\n",
      "\n",
      "# Print the character generated\n",
      "print(first_char)\n",
      "\n",
      "u\n",
      "\n",
      "# Update the vector to contain first the character\n",
      "output_seq[0, 1, char_to_idx[first_char]] = 1\n",
      "\n",
      "# Get the probabilities for the second character\n",
      "probs = model.predict_proba(output_seq, verbose=0)[:,2,:]\n",
      "\n",
      "# Sample vocabulary to get second character\n",
      "second_char = np.random.choice(sorted(list(vocabulary)), replace=False, p=probs.reshape(len(vocabulary)))\n",
      "\n",
      "# Print the second character\n",
      "print(second_char)\n",
      "\n",
      "e\n",
      "\n",
      "      >Long Short Term Memory\n",
      "\n",
      "The combination of linear and non-linear transformation can approxiate any function.\n",
      "\n",
      "error is the squared difference between the actual output and the predicted output.\n",
      "\n",
      "gradient is the rate of change of error with respect to weights.\n",
      "\n",
      "# Create a sequential model\n",
      "model = Sequential()\n",
      "\n",
      "# Create a dense layer of 12 units\n",
      "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
      "\n",
      "# Create a dense layer of 8 units\n",
      "model.add(Dense(8, init='uniform', activation='relu'))\n",
      "\n",
      "# Create a dense layer of 1 unit\n",
      "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
      "\n",
      "# Compile the model and get gradients\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
      "gradients = backend.gradients(model.output, model.trainable_weights)\n",
      "\n",
      "\n",
      ".gradients() that can be used to get the gradient values of the weights.\n",
      "\n",
      "\n",
      "   > vanishing gradients\n",
      "\n",
      "# Create a dummy input vector\n",
      "input_vector = np.random.random((1,8))\n",
      "\n",
      "# Create a tensorflow session to run the network\n",
      "sess = tf.InteractiveSession()\n",
      "\n",
      "# Initialize all the variables\n",
      "sess.run(tf.global_variables_initializer())\n",
      "\n",
      "# Evaluate the gradients using the training examples\n",
      "evaluated_gradients = sess.run(gradients,feed_dict={model.input:input_vector})\n",
      "\n",
      "# Print gradient values from third layer and two nodes of the second layer\n",
      "print(evaluated_gradients[4])\n",
      "print(evaluated_gradients[2][4])\n",
      "\n",
      " You will also check some of the gradient values from different layers. Note that tensorflow has been imported as tf.\n",
      "\n",
      "\n",
      "[[0.00010391]\n",
      " [0.        ]\n",
      " [0.00119292]\n",
      " [0.00073658]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.00102488]]\n",
      "[-9.6207787e-04  0.0000000e+00 -1.1812783e-03 -5.3114894e-07\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  9.8195439e-04]\n",
      "\n",
      "\n",
      "         >Long Short Term Memory\n",
      "\n",
      "\"The birds are flying in the sky\"\n",
      "\n",
      "predict the last word in the sentence\n",
      "\n",
      "\n",
      "rnn can not handle long term dependencies.\n",
      "\n",
      "long-short term memory were designed to handle long term dependencies\n",
      "\n",
      "1. capture the short term history\n",
      "2. capture the long term history\n",
      "3. the hidden layer can be used as an output\n",
      "4. the cell state and the hidden state can be used to pass information on to the next input\n",
      "\n",
      "\n",
      "vocabulary = sorted(set(text))\n",
      "\n",
      "char_to_idx= dict((char,idx) for idx, char in enumerate(vocabulary))\n",
      "idx_to_char=dict((idx,char) for idx, char in enumerate(vocabulary))\n",
      "\n",
      "\n",
      "input_data=[]\n",
      "\n",
      "target_data=[]\n",
      "\n",
      "maxlen=40\n",
      "\n",
      "for i in range(0, len(text)-maxlen):\n",
      "\tinput_data.append(text[i:i+maxlen])\n",
      "\n",
      "\ttarget_data.append(text[i+maxlen])\n",
      "\n",
      "\n",
      "\n",
      "x=np.zeros((len(input_data), maxlen, len(vocabulary)), dtype='float32')\n",
      "\n",
      "y=np.zeros((len(target_data), len(vocabulary)), dtype='float32')\n",
      "\n",
      "\n",
      "for s_idx, sequence in enumerate(input_data):\n",
      "\t#3d\n",
      "\tfor idx, char in enumerate(sequence):\n",
      "\t\tx[s_idx, idx,char_to_idx[char]]=1\n",
      "\t#2d\n",
      "\ty[s_idx,char_to_idx[target_data[i]]]=1\n",
      "\n",
      "\n",
      "\n",
      "model=Sequential()\n",
      "\n",
      "model.add(LSTM(128, input_shape=(maxlen, len(vocabulary))))\n",
      "\n",
      "model.add(Dense(len(vocabulary), activation='softmax'))\n",
      "\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
      "\n",
      "model.summary()\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "# Find the vocabulary\n",
      "vocabulary = sorted(set(text))\n",
      "print(vocabulary)\n",
      "\n",
      "# Print the vocabulary size\n",
      "print('Vocabulary size:', len(vocabulary))\n",
      "\n",
      "\n",
      "# Dictionary to save the mapping from char to integer\n",
      "char_to_idx= dict((char,idx) for idx, char in enumerate(vocabulary))\n",
      "\n",
      "# Dictionary to save the mapping from integer to char\n",
      "idx_to_char=dict((idx,char) for idx, char in enumerate(vocabulary))\n",
      "\n",
      "# Print char_to_idx and idx_to_char\n",
      "print(char_to_idx)\n",
      "print(idx_to_char)\n",
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, ',': 4, '-': 5, '.': 6, ':': 7, ';': 8, '?': 9, 'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15, 'g': 16, 'h': 17, 'i': 18, 'j': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'o': 24, 'p': 25, 'q': 26, 'r': 27, 's': 28, 't': 29, 'u': 30, 'v': 31, 'w': 32, 'x': 33, 'y': 34, 'z': 35}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: ',', 5: '-', 6: '.', 7: ':', 8: ';', 9: '?', 10: 'a', 11: 'b', 12: 'c', 13: 'd', 14: 'e', 15: 'f', 16: 'g', 17: 'h', 18: 'i', 19: 'j', 20: 'k', 21: 'l', 22: 'm', 23: 'n', 24: 'o', 25: 'p', 26: 'q', 27: 'r', 28: 's', 29: 't', 30: 'u', 31: 'v', 32: 'w', 33: 'x', 34: 'y', 35: 'z'}\n",
      "\n",
      "\n",
      "input_data = []\n",
      "target_data = []\n",
      "\n",
      "# Iterate to get all substrings of length maxlen\n",
      "for i in range(0, len(text) - maxlen):\n",
      "    # Find the sequence of length maxlen starting at i\n",
      "    input_data.append(text[i:i+maxlen])\n",
      "\n",
      "    \n",
      "    # Find the next char after this sequence \n",
      "    target_data.append(text[i+maxlen])\n",
      "\n",
      "# Print number of sequences in input data\n",
      "print('No of Sequences:', len(input_data))\n",
      "\n",
      "\n",
      "No of Sequences: 9960\n",
      "\n",
      "# Create a 3-D zero vector to contain the encoded input sequences\n",
      "x=np.zeros((len(input_data), maxlen, len(vocabulary)), dtype='float32')\n",
      "\n",
      "# Create a 2-D zero vector to contain the encoded target characters\n",
      "y=np.zeros((len(target_data), len(vocabulary)), dtype='float32')\n",
      "\n",
      "for s_idx, sequence in enumerate(input_data):\n",
      "\tfor idx, char in enumerate(sequence):\n",
      "\t\tx[s_idx, idx,char_to_idx[char]]=1\n",
      "\n",
      "\ty[s_idx,char_to_idx[target_data[s_idx]]]=1\n",
      "\n",
      "# Print number of sequences in input data\n",
      "print('No of Sequences:', len(input_data))\n",
      "\n",
      "# Create Sequential model \n",
      "model = Sequential()\n",
      "\n",
      "# Add an LSTM layer of 128 units\n",
      "model.add(LSTM(128, input_shape=(maxlen, len(vocabulary))))\n",
      "\n",
      "# Add a Dense output layer\n",
      "model.add(Dense(len(vocabulary), activation='softmax'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
      "\n",
      "# Print model summary\n",
      "model.summary()\n",
      "\n",
      "model.fit(x,y, batch_size=64, epochs=1, validation_split=0.2)\n",
      "\n",
      "\n",
      "\n",
      "    test\n",
      "\n",
      "sentence= \"that, poor contempt, or claim'd thou sle\"\n",
      "\n",
      "X_test=np.zeros((1,maxlen, len(vocabulary)))\n",
      "for t,char in enumerate(sentence):\n",
      "\tX_test[0,t,char_to_idx[char]]=1\n",
      "\n",
      "preds=model.predict(X_test, verbose=0)\n",
      "\n",
      "prob_next_char = preds[0]\n",
      "\n",
      "next_index=np.argmax(prob_next_char)\n",
      "\n",
      "next_char= idx_to_char[next_index]\n",
      "\n",
      "\n",
      "\n",
      "def generate_text(sentence, n):\n",
      "\tgenerated = sentence\n",
      "\tfor i in range(n):\n",
      "\t\tx_pred= np.zeros((1, maxlen, len(vocabulary)))\n",
      "\t\tfor t, char in enumerate(sentence):\n",
      "\t\t\tx_pred[0,t,char_to_idx[char]]=1\n",
      "\n",
      "\t\tpreds=model.predict(x_pred,verbose=0)[0]\n",
      "\n",
      "\t\tnext_index= np.argmax(preds)\n",
      "\t\tnext_char= idx_to_char[next_index]\n",
      "\n",
      "\t        sentence = sentence[1:]+next_char\n",
      "\n",
      "\t\tgenerated += next_char\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "# Create Sequential model \n",
      "model = Sequential()\n",
      "\n",
      "# Add an LSTM layer of 128 units\n",
      "model.add(LSTM(128, input_shape=(maxlen, len(vocabulary))))\n",
      "\n",
      "# Add a Dense output layer\n",
      "model.add(Dense(len(vocabulary), activation='softmax'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
      "\n",
      "# Fit the model\n",
      "model.fit(x, y, batch_size=64, epochs=1, validation_split=0.2)\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "# Input sequence\n",
      "sentence = \"that, poor contempt, or claim'd thou sle\"\n",
      "\n",
      "# Create a 3-D zero vector to contain the encoding of sentence.\n",
      "X_test = np.zeros((1, maxlen, len(vocabulary)))\n",
      "\n",
      "# Iterate over each character and convert them to one-hot encoded vector.\n",
      "for s_idx, char in enumerate(sentence):\n",
      "    X_test[0, s_idx, char_to_idx[char]] = 1\n",
      "\n",
      "\n",
      "preds = model.predict(X_test, verbose=0)\n",
      "\n",
      "# Get the probability distribution for the first character after the sequence\n",
      "preds_next_char = preds[0]\n",
      "\n",
      "\n",
      "<<<<<<<<\n",
      "\n",
      "\n",
      "# Input sequence\n",
      "sentence = \"that, poor contempt, or claim'd thou sle\"\n",
      "\n",
      "# Create a 3-D zero vector to contain the encoding of sentence.\n",
      "X_test = np.zeros((1, maxlen, len(vocabulary)))\n",
      "\n",
      "# Iterate over each character and convert them to one-hot encoded vector.\n",
      "for s_idx, char in enumerate(sentence):\n",
      "    X_test[0, s_idx, char_to_idx[char]] = 1\n",
      "    \n",
      "# Get the probability distribution using model predict\n",
      "preds = model.predict(X_test, verbose=0)\n",
      "\n",
      "# Get the probability distribution for the first character after the sequence\n",
      "preds_next_char = preds[0]\n",
      "\n",
      "\n",
      "# Get the index of the most probable next character\n",
      "next_index = np.argmax(preds)\n",
      "\n",
      "# Map the index to the actual character and print it\n",
      "next_char = idx_to_char[next_index]\n",
      "\n",
      "# Print the next character\n",
      "print(next_char)\n",
      "\n",
      "# Input sequence and generate text\n",
      "sentence = \"that, poor contempt, or claim'd thou sle\"\n",
      "generate_text(sentence,500)\n",
      "\n",
      "            Sequence to Sequence Models\n",
      "\n",
      "\n",
      "sequence to sequence generation generate an new sequence based on the input sequence\n",
      "\n",
      "\n",
      "fixed length input\n",
      "fixed length output\n",
      "\n",
      "input output length different in general\n",
      "\n",
      "\n",
      "examples\n",
      "1. machine translation\n",
      "2. question answering\n",
      "3. NER/POS tagging\n",
      "4. Text summarization\n",
      "5. Grammer summarization\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "# Consider only the first 50 lines of the dataset\n",
      "for i in range(50):\n",
      "\t# Split each line into two at the tab character\n",
      "    eng_fra_line = str(lines[i]).split('\\t')\n",
      "    \n",
      "    # Separate out the English sentence \n",
      "    eng_line = eng_fra_line[0]\n",
      "    \n",
      "    # Append the start and end token to each French sentence\n",
      "    fra_line = '\\t' + eng_fra_line[1] + '\\n'\n",
      "    \n",
      "    # Append the English and French sentence to the list of sentences\n",
      "    english_sentences.append(eng_line)\n",
      "    french_sentences.append(fra_line)\n",
      "\n",
      "\n",
      "# Create an empty set to contain the English vocabulary \n",
      "english_vocab = set()\n",
      "\n",
      "# Iterate over each English sentence\n",
      "for eng_line in english_sentences:\n",
      "  \n",
      "    # Convert the English line to a set\n",
      "    eng_line_set = set(eng_line)\n",
      "    \n",
      "    # Update English vocabulary with new characters from this line.\n",
      "    english_vocab = english_vocab.union(eng_line_set)\n",
      "\n",
      "# Sort the vocabulary\n",
      "english_vocab = sorted(list(english_vocab))\n",
      "\n",
      "print(english_vocab)\n",
      "\n",
      "# Create an empty set to contain the French vocabulary \n",
      "french_vocab = set()\n",
      "\n",
      "# Iterate over each French sentence\n",
      "for fra_line in french_sentences:\n",
      "  \n",
      "    # Convert the French line to a set\n",
      "    fra_line_set = set(fra_line)\n",
      "    \n",
      "    # Update French vocabulary with new characters from this line.\n",
      "    french_vocab = french_vocab.union(fra_line_set)\n",
      "\n",
      "# Sort the vocabulary\n",
      "french_vocab = sorted(list(french_vocab))\n",
      "\n",
      "\n",
      "# Dictionary to contain the character to integer mapping for English\n",
      "eng_char_to_idx = dict((char,idx) for idx, char in enumerate(english_vocab))\n",
      "\n",
      "# Dictionary to contain the integer to character mapping for English\n",
      "eng_idx_to_char = dict((idx,char) for idx, char in enumerate(english_vocab))\n",
      "\n",
      "# Dictionary to contain the character to integer mapping for English\n",
      "eng_char_to_idx = dict((char,idx) for idx, char in enumerate(english_vocab))\n",
      "\n",
      "# Dictionary to contain the integer to character mapping for English\n",
      "eng_idx_to_char = dict((idx,char) for idx, char in enumerate(english_vocab))\n",
      "\n",
      "\n",
      "  > neural machine translation\n",
      "\n",
      "input -> encoder -> decoder -> output\n",
      "\n",
      "encoder accepts input sequence\n",
      "summarizes information in state vectors\n",
      "state vectors passed to decoder\n",
      "\n",
      "the decoder is implemented using lstms\n",
      "\n",
      "1. input at each step is the output from the previous time step\n",
      "\n",
      "training\n",
      "1. input is actual output for the current step\n",
      "\n",
      "the technique is known as teacher forcing\n",
      "\n",
      "decoder for translation\n",
      "1. initial states: final states fo the encoder\n",
      "2. inputs: french sentences\n",
      "3. output: translated sentences\n",
      "4. final state ignored\n",
      "5. no of time-steps: length of the french sentence\n",
      "\n",
      "three dimensional input\n",
      "1. number of sentences\n",
      "2. number of time steps (length in characters of the longest sentence)\n",
      "3. the length of the one hot encode grid for the characters\n",
      "\n",
      "\n",
      "\n",
      "max_len_eng_sent=max([len(sentence) for sentence in english_sentences])\n",
      "\n",
      "max_len_fra_sent=max([len(sentence) for sentence in french_sentences])\n",
      "\n",
      "\n",
      "eng_input_data=np.zeros((len(english_sentences), max_len_eng_sent,len(english_vocab)),dtype='float32')\n",
      "\n",
      "fra_input_data=np.zeros((len(french_sentences), max_len_fra_sent,len(french_vocab)),dtype='float32')\n",
      "\n",
      "target_data=np.zeros((len(french_sentences), max_len_fra_sent,len(french_vocab)),dtype='float32')\n",
      "\n",
      " > one hot encode\n",
      "\n",
      "for i in range(no_of_sentences):\n",
      "\n",
      "\tfor k, ch in enumerate(english_sentences[i]):\n",
      "\t\teng_input_data[i,k, eng_char_to_idx[ch]]=1.\n",
      "\n",
      "\n",
      "\tfor k, ch in enumerate(french_sentences[i]):\n",
      "\t\tfra_input_data[i,k, fra_char_to_idx[ch]]=1.\n",
      "\n",
      "\t#target data will be one timestep ahead\t\n",
      "\tif k>0:\n",
      "\t        target_data[i,k-1, fra_char_to_idx[ch]]=1.\n",
      "\n",
      "\n",
      "inputs = Input(shape=(784,))\n",
      "\n",
      "predictions = Dense(64, activation='relu')(inputs)\n",
      "\n",
      "model = Model(inputs=inputs, outputs=predictions)\n",
      "\n",
      "#None it can take vary number of input sequences at random\n",
      "\n",
      "encoder_input = Input(shape=(None, len(english_vocab)))\n",
      "\n",
      "encoder_LSTM = LSTM(256, return_state=True)\n",
      "\n",
      "\n",
      "encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\n",
      "\n",
      "#ignore the output and save the states\n",
      "encoder_states=[encoder_h, encoder_c]\n",
      "\n",
      "decoder_input = Input(Shape=(None, len(french_vocab)))\n",
      "\n",
      "decoder_LSTM = LSTM(256, return_sequences=True, return_state=True)\n",
      "\n",
      "decoder_out,_ , _ = decoder_LSTM(decoder_input,\n",
      "initial_state=encoder_states)\n",
      "\n",
      "decoder_dense = Dense(len(french_vocab), activation='softmax')\n",
      "decoder_out=decoder_dense(decoder_out)\n",
      "\n",
      "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_out])\n",
      "\n",
      "model.summary()\n",
      "\n",
      "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
      "\n",
      "model.fit(x=[input_data_prefix, input_data_suffix],y=target_data,\n",
      "batch_size=64, epochs=1, validation_split=0.2)\n",
      "\n",
      "\n",
      "     >\n",
      "\n",
      "# Find the length of the longest English sentence\n",
      "max_len_eng_sent = max([len(sentence) for sentence in english_sentences])\n",
      "\n",
      "# Find the length of the longest French sentence\n",
      "max_len_fra_sent = max([len(sentence) for sentence in french_sentences])\n",
      "\n",
      "# Create a 3-D zero vector for the input English data\n",
      "eng_input_data=np.zeros((len(english_sentences), max_len_eng_sent,len(english_vocab)),dtype='float32')\n",
      "\n",
      "# Create a 3-D zero vector for the input French data\n",
      "fra_input_data=np.zeros((len(french_sentences), max_len_fra_sent,len(french_vocab)),dtype='float32')\n",
      "\n",
      "# Create the target vector\n",
      "target_data=np.zeros((len(french_sentences), max_len_fra_sent,len(french_vocab)),dtype='float32')\n",
      "\n",
      "# Create a 3-D zero vector for the input English data\n",
      "eng_input_data=np.zeros((len(english_sentences), max_len_eng_sent,len(english_vocab)),dtype='float32')\n",
      "\n",
      "# Create a 3-D zero vector for the input French data\n",
      "fra_input_data=np.zeros((len(french_sentences), max_len_fra_sent,len(french_vocab)),dtype='float32')\n",
      "\n",
      "# Create the target vector\n",
      "target_data=np.zeros((len(french_sentences), max_len_fra_sent,len(french_vocab)),dtype='float32')\n",
      "\n",
      "\n",
      "# Iterate over the 50 sentences\n",
      "for i in range(50):\n",
      "    # Iterate over each English character of each sentence\n",
      "    for k, ch in enumerate(english_sentences[i]):\n",
      "        # Convert the character to one-hot encoded vector\n",
      "        eng_input_data[i, k, eng_char_to_idx[ch]] = 1.\n",
      "    \n",
      "    # Iterate over each French character of each sentence\n",
      "    for k, ch in enumerate(french_sentences[i]):\n",
      "        # Convert the character to one-hot encoded vector\n",
      "        fra_input_data[i, k, fra_char_to_idx[ch]] = 1.\n",
      "\n",
      "        # Target data will be one timestep ahead and excludes start character\n",
      "        if k > 0:\n",
      "            target_data[i, k-1, fra_char_to_idx[ch]] = 1.\n",
      "\n",
      "# Create input layer\n",
      "encoder_input = Input(shape=(None, len(english_vocab)))\n",
      "\n",
      "# Create LSTM Layer of size 256\n",
      "encoder_LSTM = LSTM(256, return_state = True)\n",
      "\n",
      "# Save encoder output, hidden and cell state\n",
      "encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\n",
      "\n",
      "# Save encoder states\n",
      "encoder_states = [encoder_h, encoder_c]\n",
      "\n",
      "# Create decoder input layer\n",
      "decoder_input = Input(shape=(None, len(french_vocab)))\n",
      "\n",
      "# Create LSTM layer of size 256\n",
      "decoder_LSTM = LSTM(256, return_sequences=True, return_state = True)\n",
      "\n",
      "# Save decoder output\n",
      "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
      "\n",
      "# Create a Dense layer with softmax activation\n",
      "decoder_dense = Dense(len(french_vocab), activation='softmax')\n",
      "\n",
      "# Save the decoder output\n",
      "decoder_out = decoder_dense(decoder_out)\n",
      "\n",
      "# Build model\n",
      "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_out])\n",
      "\n",
      "# Compile the model\n",
      "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
      "\n",
      "# Print model summary\n",
      "model.summary()\n",
      "\n",
      "# Fit the model\n",
      "model.fit(x=[eng_input_data,fra_input_data], y=target_data,\n",
      "          \t\tbatch_size=64, epochs=1, validation_split=0.2)\n",
      "\n",
      "\n",
      "     >using the trained model\n",
      "\n",
      "  > decoder outputs\n",
      "\n",
      "decoder_out, decoder_hidden, decoder_cell = decoder_LSTM(decoder_input, initial_state=decoder_input_states)\n",
      "\n",
      "decoder_states = [decoder_hidden, decoder_cell]\n",
      "\n",
      "decoder_out = decoder_dense(decoder_out)\n",
      "\n",
      "<<<<<<<< decoder inference model\n",
      "\n",
      "decoder_model_inf= Model(inputs=[decoder_input] + decoder_input_states, output=[decoder_out]+decoder_states)\n",
      "\n",
      "\n",
      "inp_seq = tokenized_eng_sentences[10:11]\n",
      "\n",
      "states_val = encoder_model.predict(inp_seq)\n",
      "\n",
      "target_seq = np.zeros((1,1, len(french_vocab)))\n",
      "target_seq[0,0,fra_char_to_index_dict['\\t']]=1\n",
      "\n",
      "#generate the first character\n",
      "\n",
      "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(\n",
      "x=[target_seq]+states_val)\n",
      "\n",
      "#find index of the most probable next character\n",
      "max_val_index=np.argmax(decoder_out[0,-1:])\n",
      "\n",
      "#get actual character using index to character map\n",
      "sampled_fra_char = idx_to_char[max_val_index]\n",
      "\n",
      "target_seq = np.zeros((1,1, len(french_vocab)))\n",
      "target_seq[0,0,max_val_index]=1\n",
      "states_val=[decoder_h,decoder_c]\n",
      "\n",
      "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq]+states_val)\n",
      "\n",
      "max_val_index=np.argmax(decoder_out[0,-1:])\n",
      "sampled_fra_char = idx_to_char[max_val_index]\n",
      "\n",
      "\n",
      "translated_sent=''\n",
      "stop_condition=False\n",
      "\n",
      "while not stop_condition:\n",
      "      decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq]+states_val)\n",
      "\n",
      "      max_val_index=np.argmax(decoder_out[0,-1:])\n",
      "\n",
      "      sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
      "      translated_sent += fra_index_to_char_dict[max_val_index]\n",
      "\n",
      "      if((sampled_fra_char=='\\n') or (len(translated_sent)> max_len_fra_sent)):\n",
      "\tstop_condition=True\n",
      "\n",
      "      target_seq = np.zeros((1,1,len(french_vocab)))\n",
      "      target_seq[0,0,max_val_index]=1\n",
      "      states_val=[decoder_h,decoder_c]\n",
      "\n",
      "print(translated_sent)\n",
      "\n",
      "\n",
      "   >\n",
      "\n",
      "# Create encoder inference model\n",
      "encoder_model_inf = Model(encoder_input, encoder_states)\n",
      "\n",
      "# Create decoder input states for inference\n",
      "decoder_state_input_h = Input(shape=(256,))\n",
      "decoder_state_input_c = Input(shape=(256,))\n",
      "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
      "\n",
      "# Create decoder output states for inference\n",
      "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, initial_state=decoder_input_states)\n",
      "decoder_states = [decoder_h , decoder_c]\n",
      "\n",
      "# Create decoder dense layer\n",
      "decoder_out = decoder_dense(decoder_out)\n",
      "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states, outputs=[decoder_out] + decoder_states )\n",
      "\n",
      "# Get encoder internal state by passing a sentence as input\n",
      "inp_seq = eng_input_data[0:1]\n",
      "states_val = encoder_model_inf.predict(inp_seq)\n",
      "\n",
      "# Seed the first character and get output from the decoder \n",
      "target_seq = np.zeros((1, 1, len(french_vocab)))\n",
      "target_seq[0, 0, fra_char_to_idx['\\t']] = 1  \n",
      "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
      "\n",
      "# Find out the next character from the Decoder output\n",
      "max_val_index = np.argmax(decoder_out[0,-1,:])\n",
      "sampled_fra_char = fra_idx_to_char[max_val_index]\n",
      "\n",
      "# Print the first character predicted by the decoder\n",
      "print(sampled_fra_char)\n",
      "\n",
      "\n",
      "\n",
      "    >\n",
      "\n",
      "# Fill up target seq with the new char generated \n",
      "target_seq = np.zeros((1, 1, len(french_vocab)))\n",
      "target_seq[0, 0, max_val_index] = 1\n",
      "\n",
      "# Get decoder final states from last time\n",
      "states_val = [decoder_h, decoder_c]\n",
      "\n",
      "# Generate the next character\n",
      "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
      "\n",
      "# Map the prediction to char and print it\n",
      "max_val_index = np.argmax(decoder_out[0,-1,:])\n",
      "sampled_fra_char = fra_idx_to_char[max_val_index]\n",
      "\n",
      "print(sampled_fra_char)\n",
      "\n",
      "\n",
      "# Generate 10 French sentences from inp_seq\n",
      "for seq_index in range(10):\n",
      "  \n",
      "    # Get next encoded english sentence\n",
      "    inp_seq = eng_input_data[seq_index:seq_index+1]\n",
      "    \n",
      "    # Get the translated sentence\n",
      "    translated_sent = translate_eng_sentence(inp_seq)\n",
      "    \n",
      "    # Print the original English sentence\n",
      "    print('English sentence:', english_sentences[seq_index])\n",
      "    \n",
      "    # Print the translated French sentence\n",
      "    print('French sentence:', translated_sent)\n",
      "\n",
      "\n",
      "            >Convert email data to seq2seq\n",
      "\n",
      "\n",
      "autocomplete emails\n",
      "\n",
      "train the encoder and decoder model with the sentences\n",
      "\n",
      "sentences are inspected by prefix and suffix\n",
      "\n",
      "\n",
      "prefix_sentences=[]\n",
      "\n",
      "suffix_sentences=[]\n",
      "\n",
      "for email in corpus:\n",
      "\tfor index in range(len(email)):\n",
      "\t\tprefix=email[:index+1]\n",
      "\t\tsuffix='\\t' + email[index+1:]+'\\n'\n",
      "\n",
      "\t\tprefix_sentences.append(prefix)\n",
      "\t\tsuffix_sentences.append(suffix)\n",
      "\n",
      "vocabulary=set(['\\t','\\n'])\n",
      "\n",
      "for email in corpus:\n",
      "\tfor char in email:\n",
      "\t\tif (char not in vocabulary):\n",
      "\t\t\tvocabulary.add(char)\n",
      "\n",
      "\n",
      "vocabulary = sorted(list(vocabulary))\n",
      "\n",
      "char_to_idx = dict((char,idx) for idx, char in enumerate(vocabulary))\n",
      "idx_to_char = dict((idx,char) for idx, char in enumerate(vocabulary))\n",
      "\n",
      "#3d: number sentences, number of steps, and size of vocabulary\n",
      "\n",
      "max_len_prefix_sent= max([len(prefix) for prefix in prefix_sentences])\n",
      "max_len_suffix_sent= max([len(suffix) for suffix in suffix_sentences])\n",
      "\n",
      "\n",
      "# Initialize the input vector\n",
      "input_data_prefix = np.zeros((len(prefix_sentences), max_len_prefix_sent, len(vocabulary)), dtype='float32')\n",
      "\n",
      "input_data_suffix = np.zeros((len(suffix_sentences), max_len_suffix_sent, len(vocabulary)), dtype='float32')\n",
      "\n",
      "# Initialize the target vector\n",
      "target_data = np.zeros((len(suffix_sentences), max_len_suffix_sent, len(vocabulary)), dtype='float32')\n",
      "\n",
      "\n",
      "\n",
      "for i in range(len(prefix_sentences)):\n",
      "\tfor k, ch in enumerate(prefix_sentences[i]):\n",
      "\t\tinput_data_prefix[i,k,char_to_idx[ch]]=1\n",
      "\n",
      "\tfor k, ch in enumerate(suffix_sentences[i]):\n",
      "\t\tinput_data_suffix[i,k,char_to_idx[ch]]=1\n",
      "\n",
      "\tif k>0:\n",
      "\t\ttarget_data[i,k-1,char_to_idx[ch]]=1\n",
      "\n",
      "\n",
      "    >\n",
      "\n",
      "# Empty lists to store the prefixes and the suffixes\n",
      "prefix_sentences = []\n",
      "suffix_sentences = []\n",
      "\n",
      "# Create one prefix and one suffix at each character of each email\n",
      "for email in corpus:\n",
      "    for index in range(len(email)):\n",
      "        # Find the prefix and suffix\n",
      "        prefix = email[: index+1]\n",
      "        suffix = '\\t' + email[index+1 :] + '\\n'\n",
      "        \n",
      "        # Add the prefix and suffix to the list of prefix and suffix sentences\n",
      "        prefix_sentences.append(prefix)\n",
      "        suffix_sentences.append(suffix)\n",
      "\n",
      "\n",
      "\n",
      "# Initialize vocabulary with the start and end token\n",
      "vocabulary = set(['\\t', '\\n'])\n",
      "\n",
      "# Iterate for each char in each email\n",
      "for email in corpus:\n",
      "\tfor char in email:\n",
      "\t\tif (char not in vocabulary):\n",
      "\t\t\tvocabulary.add(char)        \n",
      "# Sort the vocabulary\n",
      "vocabulary = sorted(vocabulary)\n",
      "\n",
      "# Create char to int and int to char mapping\n",
      "char_to_idx = dict((char,idx) for idx, char in enumerate(vocabulary))\n",
      "idx_to_char = dict((idx,char) for idx, char in enumerate(vocabulary))\n",
      "\n",
      "max_len_prefix_sent= max([len(prefix) for prefix in prefix_sentences])\n",
      "max_len_suffix_sent= max([len(suffix) for suffix in suffix_sentences])\n",
      "\n",
      "input_data_prefix = np.zeros((len(prefix_sentences), max_len_prefix_sent, len(vocabulary)), dtype='float32')\n",
      "\n",
      "input_data_suffix = np.zeros((len(suffix_sentences), max_len_suffix_sent, len(vocabulary)), dtype='float32')\n",
      "\n",
      "# Initialize the target vector\n",
      "target_data = np.zeros((len(suffix_sentences), max_len_suffix_sent, len(vocabulary)), dtype='float32')\n",
      "\n",
      "for i in range(len(prefix_sentences)):\n",
      "    # Iterate over each character in each prefix\n",
      "    for k, ch in enumerate(prefix_sentences[i]):\n",
      "        # Convert the character to a one-hot encoded vector\n",
      "        input_data_prefix[i, k, char_to_idx[ch]] = 1\n",
      "        \n",
      "    # Iterate over each character in each suffix\n",
      "    for k, ch in enumerate(suffix_sentences[i]):\n",
      "        # Convert the character to a one-hot encoded vector\n",
      "        input_data_suffix[i, k, char_to_idx[ch]] = 1\n",
      "\n",
      "        # Target data is one timestep ahead and excludes start character\n",
      "        if k > 0:\n",
      "            target_data[i, k-1, char_to_idx[ch]] = 1\n",
      "\n",
      "\n",
      "encoder summarizes input information states\n",
      "the encoder is implemented using lstm\n",
      "\n",
      "decoder initial sate is the final state of the encoder\n",
      "final states ignored.\n",
      "\n",
      "input during training - original target\n",
      "input during inference - predicted target\n",
      "\n",
      "\n",
      "      auto - completion encoder\n",
      "\n",
      "#None the input sequences can be varied length\n",
      "\n",
      "encoder_input = Input(shape=(None, len(vocabulary)))\n",
      "\n",
      "encoder_LSTM = LSTM(256, return_state=True)\n",
      "\n",
      "encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\n",
      "\n",
      "encoder_states=[encoder_h,encoder_c]\n",
      "\n",
      "decoder_input = Input(shape=(None, len(vocabulary)))\n",
      "\n",
      "decoder_LSTM = LSTM(256, return_sequences=True, return_state=True)\n",
      "\n",
      "decoder_out, _, _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
      "\n",
      "decoder_dense  = Dense(len(vocabulary), activation='softmax')\n",
      "\n",
      "decoder_out = decoder_dense(decoder_out)\n",
      "\n",
      "\n",
      "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_out])\n",
      "\n",
      "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
      "\n",
      "model.fit(x=[input_data_prefix, input_data_suffix], y=target_data, batch_size=64, epochs=1, validation_split=0.2)\n",
      "\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "# Create decoder input layer\n",
      "decoder_input = Input(shape=(None, len(vocabulary)))\n",
      "\n",
      "# Create LSTM layer of size 256\n",
      "decoder_LSTM = LSTM(256, return_sequences=True, return_state=True)\n",
      "\n",
      "# Save decoder output\n",
      "decoder_out, _, _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
      "\n",
      "# Create a `Dense` layer with softmax activation\n",
      "decoder_dense  = Dense(len(vocabulary), activation='softmax')\n",
      "\n",
      "# Save the decoder output\n",
      "decoder_out = decoder_dense(decoder_out)\n",
      "\n",
      "# Create decoder input layer\n",
      "decoder_input = Input(shape=(None, len(vocabulary)))\n",
      "\n",
      "# Create LSTM layer of size 256\n",
      "decoder_LSTM = LSTM(256, return_sequences=True, return_state=True)\n",
      "\n",
      "# Save decoder output\n",
      "decoder_out, _, _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
      "\n",
      "# Create a `Dense` layer with softmax activation\n",
      "decoder_dense  = Dense(len(vocabulary), activation='softmax')\n",
      "\n",
      "# Save the decoder output\n",
      "decoder_out = decoder_dense(decoder_out)\n",
      "\n",
      "# Build model\n",
      "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_out])\n",
      "\n",
      "# Compile the model\n",
      "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
      "\n",
      "# Print model summary\n",
      "model.summary()\n",
      "\n",
      "\n",
      "model.fit(x=[input_data_prefix, input_data_suffix], y=target_data, batch_size=64, epochs=1, validation_split=0.2)\n",
      "\n",
      "#build the inference model for prediction\n",
      "\n",
      "# Create encoder inference model\n",
      "encoder_model_inf = Model(encoder_input, encoder_states)\n",
      "\n",
      "# Create decoder input states for inference\n",
      "decoder_state_input_h = Input(shape=(256,))\n",
      "decoder_state_input_c = Input(shape=(256,))\n",
      "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
      "\n",
      "\n",
      "# Get decoder output and feed it to the dense layer for final output prediction\n",
      "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input,\n",
      "\tinitial_state=decoder_input_states)\n",
      "decoder_states = [decoder_h , decoder_c]\n",
      "\n",
      "decoder_dense  = Dense(len(vocabulary), activation='softmax')\n",
      "\n",
      "decoder_out = decoder_dense(decoder_out)\n",
      "\n",
      "# Create decoder inference model\n",
      "decoder_model_inf = Model(inputs=[decoder_input]+decoder_input_states, outputs=[decoder_out]+decoder_states)\n",
      "\n",
      "\n",
      "      predictions\n",
      "\n",
      "\n",
      "\n",
      "the decoder outputs the suffix sentences\n",
      "\n",
      "encoder_model_inf = Model(encoder_input, encoder_states)\n",
      "decoder_state_input_h = Input(shape=(256,))\n",
      "decoder_state_input_c = Input(shape=(256,))\n",
      "\n",
      "decoder_input_states=[decoder_state_input_h,decoder_state_input_c]\n",
      "\n",
      "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input,\n",
      "\tinitial_state=decoder_input_states)\n",
      "\n",
      "decoder_states=[decoder_h,decoder_c]\n",
      "\n",
      "decoder_out = decoder_dense(decoder_out)\n",
      "\n",
      "decoder_model_inf = Model(inputs=[decoder_input]+decoder_input_states, outputs=[decoder_out]+decoder_states)\n",
      "\n",
      "\n",
      "#input a prefix\n",
      "inp_seq=input_data_prefix[4:5]\n",
      "states_val = encoder_model_inf.predict(inp_seq)\n",
      "\n",
      "target_seq = np.zeros((1,1,len(vocabulary)))\n",
      "\n",
      "target_seq [0,0,char_to_idx['\\t']]=1\n",
      "\n",
      "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(\n",
      "\tx=[target_seq] + state_val)\n",
      "\n",
      "max_val_index = np.argmax(decoder_out[0,-1,:])\n",
      "\n",
      "sampled_suffix_char = idx_to_char[max_val_index]\n",
      "\n",
      "target_seq = np.zeros((1,1,len(vocabulary)))\n",
      "\n",
      "target_seq[0,0, max_val_index]=1\n",
      "\n",
      "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(\n",
      "\tx=[target_seq] + state_val)\n",
      "\n",
      "states_val=[decoder_h,decoder_c]\n",
      "\n",
      "\n",
      "suffix_sent=''\n",
      "stop_condition=False\n",
      "\n",
      "while not stop_condition:\n",
      "      decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq]+states_val)\n",
      "\n",
      "      max_val_index=np.argmax(decoder_out[0,-1:])\n",
      "\n",
      "      sampled_output_char = idx_to_char[max_val_index]\n",
      "      suffix_sent += sampled_output_char\n",
      "\n",
      "      if((sampled_output_char=='\\n') or (len(suffix_sent)> max_len_suffix_sent)):\n",
      "\t        stop_condition=True\n",
      "\n",
      "      target_seq = np.zeros((1,1,len(vocabulary)))\n",
      "      target_seq[0,0,max_val_index]=1\n",
      "      states_val=[decoder_h,decoder_c]\n",
      "\n",
      "print(suffix_sent)  >\n",
      "\n",
      "# Pass input prefix to the Encoder inference model and get the states\n",
      "inp_seq = input_data_prefix[4:5]\n",
      "states_val = encoder_model_inf.predict(inp_seq)\n",
      "\n",
      "# Seed the first character and get output from the decoder \n",
      "target_seq = np.zeros((1, 1, len(vocabulary)))\n",
      "target_seq[0, 0, char_to_idx['\\t']] = 1  \n",
      "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
      "\n",
      "# Find out the next character from the Decoder output\n",
      "max_val_index = np.argmax(decoder_out[0,-1,:])\n",
      "sampled_suffix_char = idx_to_char[max_val_index]\n",
      "\n",
      "# Print the first character\n",
      "print(sampled_suffix_char)\n",
      "\n",
      " > second character\n",
      "\n",
      "# Insert the generated character from last time to the target sequence \n",
      "target_seq = np.zeros((1, 1, len(vocabulary)))\n",
      "target_seq[0, 0, max_val_index] = 1\n",
      "\n",
      "# Initialize the decoder state to the states from last iteration\n",
      "states_val = [decoder_h, decoder_c]\n",
      "\n",
      "# Get decoder output\n",
      "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
      "\n",
      "# Get most probable next character and print it.\n",
      "max_val_index = np.argmax(decoder_out[0,-1,:])\n",
      "sampled_suffix_char = idx_to_char[max_val_index]\n",
      "print(sampled_suffix_char)\n",
      "\n",
      "\n",
      "# Generate 10 suffixes\n",
      "for seq_index in range(10):\n",
      "  \n",
      "    # Get the next tokenized sentence\n",
      "    inp_seq = input_data_prefix[seq_index:seq_index+1]\n",
      "    \n",
      "    # Generate the suffix sentence\n",
      "    suffix_sent = generate_suffix_sentence(inp_seq)\n",
      "    \n",
      "    # Print the prefix sentence\n",
      "    print('Prefix Sentence:', prefix_sentences[seq_index])\n",
      "    \n",
      "    # Print the suffix sentence\n",
      "    print('Suffix Sentence:', suffix_sent)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\pmf and cdf  general social survey.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\pmf and cdf  general social survey.txt\n",
      "   >PMF  \n",
      "\n",
      "unique value in the set and how often it is used\n",
      "\n",
      "\n",
      "   Cumulative distribution functions\n",
      "\n",
      "cdf is the probability that your get a value <=x\n",
      "for a given value of x\n",
      "\n",
      "pmf is the probability that you get exactly x\n",
      "\n",
      "pmf of 1,2,2,3,5\n",
      "\n",
      "pmf(1)=1/5\n",
      "pmf(2)=2/5\n",
      "pmf(3)=1/5\n",
      "pmf(5)=1/5\n",
      "\n",
      "cdf is the cumulative sum of the probabilities of pmf\n",
      "\n",
      "cdf(1)=1/5\n",
      "cdf(2)=3/5\n",
      "cdf(3)=4/5\n",
      "cdf(5)=1\n",
      "\n",
      "cdf accumulates to 1\n",
      "\n",
      "\n",
      "cdf=Cdf(gss['age'])\n",
      "cdf.plot()\n",
      "plt.xlabel('age')\n",
      "plt.ylabel('cdf')\n",
      "plt.show()\n",
      "\n",
      "  get the probability for q or get the q for a probability\n",
      "\n",
      "q=51\n",
      "p=cdf(q)\n",
      "print(p)\n",
      "\n",
      "p=.25\n",
      "q=cdf.inverse(p)\n",
      "print(q)\n",
      "\n",
      "\n",
      "   sample  > get the probability for age for 30\n",
      "\n",
      "# Select the age column\n",
      "age = gss['age']\n",
      "\n",
      "# Compute the CDF of age\n",
      "cdf_age = Cdf(age)\n",
      "\n",
      "# Calculate the CDF of 30\n",
      "print(cdf_age[30])\n",
      "\n",
      "\n",
      "    sample  > plotting the probabilitie of real income\n",
      "\n",
      "# Select realinc\n",
      "income = gss[\"realinc\"]\n",
      "\n",
      "# Make the CDF\n",
      "cdf_income = Cdf(income)\n",
      "\n",
      "# Plot it\n",
      "cdf_income.plot()\n",
      "\n",
      "# Label the axes\n",
      "plt.xlabel('Income (1986 USD)')\n",
      "plt.ylabel('CDF')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "Recall from the video that the interquartile range (IQR) is the difference between the 75th and 25th percentiles. It is a measure of variability that is robust in the presence of errors or extreme values\n",
      "\n",
      "\n",
      " > sample  > what age represents the 75th percentile\n",
      "\n",
      "\n",
      "age = gss['age']\n",
      "\n",
      "p=.75\n",
      "cdf_income=cdf(age)\n",
      "# Compute the CDF of age\n",
      "percentile_75th = cdf_income.inverse(p)\n",
      "\n",
      "# Calculate the 75th percentile \n",
      "print(percentile_75th)\n",
      "\n",
      "output:57 years old\n",
      "\n",
      "\n",
      "  > sample  > iqr\n",
      "\n",
      "# Calculate the 75th percentile \n",
      "percentile_75th = cdf_income.inverse(0.75)\n",
      "\n",
      "# Calculate the 25th percentile\n",
      "percentile_25th = cdf_income.inverse(0.25)\n",
      "\n",
      "# Calculate the interquartile range\n",
      "iqr = percentile_75th - percentile_25th\n",
      "\n",
      "# Print the interquartile range\n",
      "print(iqr)\n",
      "\n",
      "29676.0\n",
      "\n",
      "  > sample  > plotting multiple pmfs\n",
      "\n",
      "\n",
      "def cdf(data,label):\n",
      "    # sort the data:\n",
      "    data_sorted = np.sort(data)\n",
      "\n",
      "    # calculate the proportional values of samples\n",
      "    p = 1. * np.arange(len(data)) / (len(data) - 1)\n",
      "\n",
      "    # plot the sorted data:\n",
      "    fig = plt.figure()\n",
      "    #ax1 = fig.add_subplot(121)\n",
      "    #ax1.plot(p, data_sorted)\n",
      "    #ax1.set_xlabel('$p$')\n",
      "    #ax1.set_ylabel('$x$')\n",
      "\n",
      "    #ax2 = fig.add_subplot(122)\n",
      "    #ax2.plot(data_sorted, p)\n",
      "    #ax2.set_xlabel('${}$'.format(label))\n",
      "    #ax2.set_ylabel('$p$')\n",
      "    return (pd.DataFrame({label:data_sorted,\"probability\":p}))\n",
      "\n",
      "\n",
      "def Pmf(data,label):\n",
      "    total_count=data.count()\n",
      "    return_df=data.value_counts().rename_axis(label).reset_index(name='Counts')\n",
      "    return_df[\"probability\"]=return_df[\"Counts\"]/total_count\n",
      "    return_df=return_df.sort_values(by=label)\n",
      "    return return_df\n",
      "\n",
      "\n",
      "#print(df[\"GENDER1\"])\n",
      "male=df[\"GENDER1\"]==1\n",
      "\n",
      "male_age=age[male]\n",
      "female_age=age[~male]\n",
      "\n",
      "male_age_proba=Pmf(male_age,\"AGE\")\n",
      "female_age_proba=Pmf(female_age,\"AGE\")\n",
      "\n",
      "plt.plot(male_age_proba[\"AGE\"],male_age_proba[\"probability\"])\n",
      "plt.plot(female_age_proba[\"AGE\"],female_age_proba[\"probability\"])\n",
      "plt.legend([\"Male\",\"Female\"])\n",
      "plt.xlabel(\"AGE\")\n",
      "plt.ylabel(\"PMF\")\n",
      "plt.show()\n",
      "   \n",
      "\n",
      "statistics resource\n",
      "\n",
      "https://github.com/AllenDowney/ThinkStats2/blob/master/code/regression.py\n",
      "\n",
      "\n",
      "\n",
      "  sample  > cdf\n",
      "\n",
      "print(df[df[\"EDUC\"]<=12][\"EDUC\"].count()/df[\"EDUC\"].count())\n",
      "\n",
      "\n",
      "# Select educ\n",
      "educ = gss['educ']\n",
      "\n",
      "# Bachelor's degree\n",
      "bach = (educ >= 16)\n",
      "\n",
      "# Associate degree\n",
      "assc = (educ >= 14) & (educ < 16)\n",
      "\n",
      "# High school\n",
      "high = (educ <= 12)\n",
      "print(high.mean())\n",
      "\n",
      "\n",
      "  >Sample  > use the cdf to predict if incomes will be higher with more education\n",
      "\n",
      "income = gss['realinc']\n",
      "\n",
      "# Plot the CDFs\n",
      "Cdf(income[high]).plot(label='High school')\n",
      "Cdf(income[assc]).plot(label='Associate')\n",
      "Cdf(income[bach]).plot(label='Bachelor')\n",
      "\n",
      "# Label the axes\n",
      "plt.xlabel('Income (1986 USD)')\n",
      "plt.ylabel('CDF')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "     >Model distribution\n",
      "many things in the real world have a normal distribution\n",
      "\n",
      "\n",
      "sample= np.random.normal(size=1000)\n",
      "\n",
      "Cdf(sample).plot()\n",
      "\n",
      "\n",
      "from scipy.stats import norm\n",
      "\n",
      "xs=np.linspace(-3,3)\n",
      "ys=norm(0,1).cdf(xs)\n",
      "\n",
      "norm has a mean of 0 and a standard deviation of 1\n",
      "\n",
      "plt.plot(xs, ys, color='gray')\n",
      "\n",
      "\n",
      "   probability density function\n",
      "\n",
      "xs=np.linspace(-3,3)\n",
      "ys=norm(0,1).pdf(xs)\n",
      "\n",
      "plt.plot(xs, ys, color='gray')\n",
      "\n",
      " >kernal density estimation (KDE)\n",
      "\n",
      "KDE is way to go from a Probability mass function PMF to a Probability density function PDF.\n",
      "\n",
      "sns.kdeplot(education)\n",
      "\n",
      "   Modeling distributions\n",
      "1. CDFs for exploration\n",
      "2. PMFs if there are a small number of unique values\n",
      "3. KDE if there are a lot of values.\n",
      "\n",
      "\n",
      "  >Sample  > passing the mean and std to the norm with cdf\n",
      "\n",
      "# Extract realinc and compute its log\n",
      "income = gss['realinc']\n",
      "log_income = np.log10(income)\n",
      "\n",
      "# Compute mean and standard deviation\n",
      "mean = log_income.mean()\n",
      "std = log_income.std()\n",
      "print(mean, std)\n",
      "\n",
      "# Make a norm object\n",
      "from scipy.stats import norm\n",
      "dist = norm(mean,std)\n",
      "\n",
      "# Evaluate the model CDF\n",
      "xs = np.linspace(2, 5.5)\n",
      "#ys = norm(mean,std).cdf(xs)\n",
      "ys=dist.cdf(xs)\n",
      "\n",
      "# Plot the model CDF\n",
      "plt.clf()\n",
      "plt.plot(xs, ys, color='gray')\n",
      "\n",
      "# Create and plot the Cdf of log_income\n",
      "#log_income.plot()\n",
      "Cdf(log_income).plot()\n",
      "    \n",
      "# Label the axes\n",
      "plt.xlabel('log10 of realinc')\n",
      "plt.ylabel('CDF')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "   Sample  > compare pdf with kde\n",
      "\n",
      "# Evaluate the normal PDF\n",
      "xs = np.linspace(2, 5.5)\n",
      "ys = dist.pdf(xs)\n",
      "\n",
      "# Plot the model PDF\n",
      "plt.clf()\n",
      "plt.plot(xs, ys, color='gray')\n",
      "\n",
      "# Plot the data KDE\n",
      "sns.kdeplot(log_income)\n",
      "\n",
      "# Label the axes\n",
      "plt.xlabel('log10 of realinc')\n",
      "plt.ylabel('PDF')\n",
      "plt.show()\n",
      "\n",
      "           BRFSS   \n",
      "\n",
      "Behavioral Risk Factor Surveillance System\n",
      "\n",
      "heath resource csv\n",
      "https://github.com/pgsmith2000/BRFSS/tree/master/data\n",
      "\n",
      "https://healthdata.gov/dataset/behavioral-risk-factor-surveillance-system-brfss-national-cardiovascular-disease-0\n",
      "\n",
      "Scatter plot\n",
      "\n",
      "height=brfss[\"HTM4\"] #centimeters\n",
      "weight=brfss[\"WTKG3\"] #millimeters\n",
      "\n",
      "plt.plot(height,weight,\"o\")\n",
      "\n",
      "alpha is the Transparency\n",
      "\n",
      "plt.plot(height,weight,\"o\",alpha=0.02,markersize=1)\n",
      "\n",
      "Jittering is adding random noise\n",
      "\n",
      "height_jitter=height + np.random.normal(0,2,size=len(brfss))\n",
      "\n",
      "weight_jitter=weight + np.random.normal(0,2,size=len(brfss))\n",
      "\n",
      "plt.plot(height_jitter, weight_jitter,\"o\", markersize=1, alpha=0.02)\n",
      "plt.show()\n",
      "\n",
      "axis allows zoom\n",
      "\n",
      "axis is (lower and upper bounds for the x and y axis)\n",
      "\n",
      "plt.axis(140,200,0,160)\n",
      "\n",
      "it takes some effort to make an effective scatter plot\n",
      "\n",
      "\n",
      "  >   Sample    > Age distribution from brfss\n",
      "\n",
      "# Extract age\n",
      "age = brfss[\"AGE\"]\n",
      "\n",
      "# Plot the PMF\n",
      "Pmf(age).plot()\n",
      "\n",
      "# Label the axes\n",
      "plt.xlabel('Age in years')\n",
      "plt.ylabel('PMF')\n",
      "plt.show()\n",
      "\n",
      "   Sample  > Scatter plot age by weight\n",
      "\n",
      "# Select the first 1000 respondents\n",
      "brfss = brfss[:1000]\n",
      "\n",
      "# Extract age and weight\n",
      "age = brfss['AGE']\n",
      "weight = brfss['WTKG3']\n",
      "\n",
      "# Make a scatter plot\n",
      "\n",
      "plt.plot(age,weight,'o', alpha=0.1)\n",
      "\n",
      "plt.xlabel('Age in years')\n",
      "plt.ylabel('Weight in kg')\n",
      "\n",
      "plt.show()\n",
      "\n",
      "  >Sample    Add jitter to age\n",
      "\n",
      "# Select the first 1000 respondents\n",
      "brfss = brfss[:1000]\n",
      "\n",
      "# Add jittering to age\n",
      "age = brfss['AGE'] + np.random.normal(0,2,size=len(brfss))\n",
      "# Extract weight\n",
      "weight = brfss['WTKG3']\n",
      "\n",
      "# Make a scatter plot\n",
      "plt.plot(age,weight,'o', alpha=0.2,markersize=5)\n",
      "\n",
      "plt.xlabel('Age in years')\n",
      "plt.ylabel('Weight in kg')\n",
      "plt.show()\n",
      "\n",
      "       Box plots and violin plots\n",
      "\n",
      "violin plot shows the density of each category\n",
      "\n",
      "data= brfss.dropna(subset=['AGE','WTKG3'])\n",
      "\n",
      "\n",
      "sns.violinplot(x='AGE', y='WTKGS', data=data, inner=None)\n",
      "plt.show()\n",
      "\n",
      "the width is the category density with an upper and lower range on the y value\n",
      "\n",
      "   box plot\n",
      "\n",
      "sns.boxplot(x='AGE', y='WTKG3', data=data, whis=10)\n",
      "plt.show()\n",
      "\n",
      "each box represents the interquartile range\n",
      "or iqr from the 25th to 75th percentile.  the line in the middle is the median.  The spines show the minimum and maximum values.\n",
      "\n",
      "the heaviest people are the furthest away from the median\n",
      "\n",
      "switch to a logrithmic scale for simplification\n",
      "\n",
      "sns.boxplot(x='AGE', y='WTKG3', data=data, whis=10)\n",
      "plt.yscale('log')\n",
      "plt.show()\n",
      "\n",
      "   >Sample    Box plot\n",
      "\n",
      "# Drop rows with missing data\n",
      "data = brfss.dropna(subset=['_HTMG10', 'WTKG3'])\n",
      "\n",
      "# Make a box plot\n",
      "sns.boxplot(x='_HTMG10', y='WTKG3', data=data, whis=10)\n",
      "\n",
      "# Plot the y-axis on a log scale\n",
      "plt.yscale('log')\n",
      "\n",
      "# Remove unneeded lines and label axes\n",
      "sns.despine(left=True, bottom=True)\n",
      "plt.xlabel('Height in cm')\n",
      "plt.ylabel('Weight in kg')\n",
      "plt.show()\n",
      "\n",
      "  >sample  > plot the income distribution of eight income categories\n",
      "\n",
      "# Extract income\n",
      "income = brfss['INCOME2']\n",
      "\n",
      "print(income)\n",
      "# Plot the PMF\n",
      "Pmf(income).plot()\n",
      "\n",
      "\n",
      "# Label the axes\n",
      "plt.xlabel('Income level')\n",
      "plt.ylabel('PMF')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  >Sample violin plot by income category\n",
      "\n",
      "\n",
      "# Drop rows with missing data\n",
      "data = brfss.dropna(subset=['INCOME2', 'HTM4'])\n",
      "\n",
      "# Make a violin plot\n",
      "\n",
      "sns.violinplot(x=\"INCOME2\",y=\"HTM4\",data=data,inner=None)\n",
      "# Remove unneeded lines and label axes\n",
      "sns.despine(left=True, bottom=True)\n",
      "plt.xlabel('Income level')\n",
      "plt.ylabel('Height in cm')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "         Correlation\n",
      "\n",
      "\n",
      "the coefficient of correlation highlights the strength of the relationships in data.\n",
      "\n",
      "in statistics correlation means pearson correlation coefficient.\n",
      "\n",
      "columns=['HTM4','WTKG3','AGE']\n",
      "\n",
      "subset=brfss[columns]\n",
      "subset.corr()\n",
      "\n",
      "correlation only works with linear relationships\n",
      "\n",
      "so age to weight is low but there is a non linear correlation.  older and younger people are lighter.\n",
      "\n",
      "\n",
      "xs=np.linspace(-1,1)\n",
      "ys=xs**2\n",
      "\n",
      "ys += normal(0,0.05, len(xs))\n",
      "\n",
      "if correlation is close to 1 or -1 then there is a strong linear correlation.\n",
      "\n",
      "\n",
      "correlation says nothing about slope.\n",
      "\n",
      "\n",
      "  >Sample    Correlation between columns\n",
      "look for linear correlation\n",
      "\n",
      "# Select columns\n",
      "columns = ['AGE','INCOME2','_VEGESU1']\n",
      "subset = brfss[columns]\n",
      "\n",
      "# Compute the correlation matrix\n",
      "print(subset.corr())\n",
      "\n",
      "\n",
      "   >Simple Regression \n",
      "\n",
      "correlation between weight and age\n",
      "\n",
      "plt.clf()\n",
      "plt.scatter(age[weight_filter],weight)\n",
      "plt.show()\n",
      "\n",
      "from scipy.stats import linregress\n",
      "\n",
      "\n",
      "\n",
      "rvalue is correlation\n",
      "\n",
      "using linregress to plot a line\n",
      "\n",
      "get the min and max of the observed x\n",
      "\n",
      "fx=np.array([xs.min(),xs.max()])\n",
      "fy=res.intercept + res.slope * fx\n",
      "plt.plot(fx,fy,'-')\n",
      "\n",
      "regression line can not handle nans\n",
      "\n",
      "\n",
      "  >Sample   > Linear Regressor\n",
      "\n",
      "from scipy.stats import linregress\n",
      "\n",
      "# Extract the variables\n",
      "subset = brfss.dropna(subset=['INCOME2', '_VEGESU1'])\n",
      "xs = subset['INCOME2']\n",
      "ys = subset['_VEGESU1']\n",
      "\n",
      "# Compute the linear regression\n",
      "res = linregress(xs,ys)\n",
      "print(res)\n",
      "\n",
      "\n",
      "  >Sample  > plotting the line of best fit\n",
      "\n",
      "# Plot the scatter plot\n",
      "plt.clf()\n",
      "x_jitter = xs + np.random.normal(0, 0.15, len(xs))\n",
      "plt.plot(x_jitter, ys, 'o', alpha=0.2)\n",
      "\n",
      "# Plot the line of best fit\n",
      "res=linregress(xs,ys)\n",
      "fx = np.array([xs.min(),xs.max()])\n",
      "fy = res.intercept + res.slope * fx\n",
      "plt.plot(fx, fy, '-', alpha=0.7)\n",
      "\n",
      "plt.xlabel('Income code')\n",
      "plt.ylabel('Vegetable servings per day')\n",
      "plt.ylim([0, 6])\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    Limits of simple regression\n",
      "\n",
      "\n",
      "regression is not symmetric\n",
      "\n",
      "the slope for vegetable consumption to income differs from income unit to vegetable consumption.\n",
      "\n",
      "the slopes are different because they are based on different assumptions.  one variable is known and the other is random.\n",
      "\n",
      "regression does not tell you much about causation.\n",
      "\n",
      "   >Sample    multiple regression\n",
      "\n",
      "#ordinary least squares\n",
      "\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "results =smf.ols('INCOME2 ~ _VEGESU1\", data=brfss).fit()\n",
      "results.params\n",
      "\n",
      "_VEGESU1 (slope) 0.232515\n",
      "Intrecept: 5.39999\n",
      "\n",
      "  Sample linregress vs statsmodels\n",
      "\n",
      "\n",
      "from scipy.stats import linregress\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "# Run regression with linregress\n",
      "subset = brfss.dropna(subset=['INCOME2', '_VEGESU1'])\n",
      "xs = subset['INCOME2']\n",
      "ys = subset['_VEGESU1']\n",
      "res = linregress(xs,ys)\n",
      "print(res)\n",
      "\n",
      "# Run regression with StatsModels\n",
      "results = smf.ols('INCOME2 ~ _VEGESU1', data = brfss).fit()\n",
      "print(results.params)\n",
      "\n",
      "LinregressResult(slope=0.06988048092105019, intercept=1.5287786243363106, rvalue=0.11967005884864107, pvalue=1.378503916247615e-238, stderr=0.002110976356332332)\n",
      "Intercept    1.528779\n",
      "INCOME2      0.069880\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "      Multiple Regression\n",
      "\n",
      "realinc ~ educ\n",
      "\n",
      "\n",
      "results=smf.ols('realinc ~ educ',data=gss).fit()\n",
      "\n",
      "realinc is the variable we are trying to predict\n",
      "\n",
      "results=smf.ols('realinc ~ educ+age',data=gss).fit()\n",
      "\n",
      "age is additive to the prediction with educ\n",
      "\n",
      "grouped= gss.groupby('age')\n",
      "mean_income_by_age=grouped['realinc'].mean()\n",
      "\n",
      "plt.plot(mean_income_by_age,'o', alpha=0.5)\n",
      "plt.xlabel('age (years)')\n",
      "plt.ylable('income')\n",
      "\n",
      "the correlation between age and income is non-linear.  correlation can not measure non linear\n",
      "\n",
      "   > adding a quaratic termin\n",
      "\n",
      "print(\"adding a quadratic variable for age\")\n",
      "df[\"AGE2\"]=df[\"AGE\"]**2\n",
      "\n",
      "results= smf.ols(\"REALINC ~ EDUC+AGE+AGE2\", data=df).fit()\n",
      "\n",
      "print(results.params)\n",
      "print(\"The additive of age is small\")\n",
      "\n",
      "fx=df[\"REALINC\"]\n",
      "fy=results.params.Intercept + results.params.EDUC * fx + results.params.AGE*fx + results.params.AGE2*fx\n",
      "\n",
      "plt.plot(fx,fy,'-',color='blue')\n",
      "plt.xlabel(\"Income\")\n",
      "plt.ylabel(\"Education\")\n",
      "plt.show()\n",
      "\n",
      "\n",
      "  >Sample  > plot real income by education using groupby\n",
      "\n",
      "# Group by educ\n",
      "grouped = gss.groupby('educ')\n",
      "\n",
      "# Compute mean income in each group\n",
      "mean_income_by_educ = grouped['realinc'].mean()\n",
      "\n",
      "# Plot mean income as a scatter plot\n",
      "plt.plot(mean_income_by_educ)\n",
      "\n",
      "# Label the axes\n",
      "plt.xlabel('Education (years)')\n",
      "plt.ylabel('Income (1986 $)')\n",
      "plt.show()\n",
      "\n",
      "  Sample  > adding a education quadratic educ2\n",
      "\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "# Add a new column with educ squared\n",
      "gss['educ2'] = gss['educ']**2\n",
      "\n",
      "# Run a regression model with educ, educ2, age, and age2\n",
      "results =smf.ols('realinc ~ educ + educ2 + age + age2 ',data=gss).fit()\n",
      "\n",
      "# Print the estimated parameters\n",
      "print(results.params)\n",
      "\n",
      "Intercept   -23241.884034\n",
      "educ          -528.309369\n",
      "educ2          159.966740\n",
      "age           1696.717149\n",
      "age2           -17.196984\n",
      "dtype: float64\n",
      "\n",
      "    using Predict to visualize\n",
      "\n",
      "\n",
      "df2=pd.DataFrame()\n",
      "df2['age']=np.linspace(18,85)\n",
      "df2['age2']=df2['age']**2\n",
      "df2['educ']=12\n",
      "df2['educ2']=df2['educ']**2\n",
      "\n",
      "pred12=results.predict(df)\n",
      "\n",
      "plt.plot(df2['age'],pred12,label='High school')\n",
      "\n",
      "plt.plot(mean_income_by_age,'o',alpha=0.5)\n",
      "plt.xlabel('age')\n",
      "plt.ylabel('income')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      " > sample > predict\n",
      "\n",
      "results = smf.ols('realinc ~ educ + educ2 + age + age2', data=gss).fit()\n",
      "\n",
      "# Make the DataFrame\n",
      "df = pd.DataFrame()\n",
      "df['educ'] = np.linspace(0,20)\n",
      "df['age'] = 30\n",
      "df['educ2'] = df['educ']**2\n",
      "df['age2'] = df['age']**2\n",
      "\n",
      "# Generate and plot the predictions\n",
      "pred =results.predict(df)\n",
      "print(pred.head())\n",
      "\n",
      "  >sample  > predict income by age 30 for education 0 to 20\n",
      "\n",
      "# Plot mean income in each age group\n",
      "plt.clf()\n",
      "grouped = gss.groupby('educ')\n",
      "mean_income_by_educ = grouped['realinc'].mean()\n",
      "plt.plot(mean_income_by_educ,'o',alpha=0.5)\n",
      "\n",
      "# Plot the predictions\n",
      "pred = results.predict(df)\n",
      "plt.plot(df['educ'], pred, label='Age 30')\n",
      "\n",
      "# Label axes\n",
      "plt.xlabel('Education (years)')\n",
      "plt.ylabel('Income (1986 $)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "    >Logistic Regression\n",
      "\n",
      "categorical variables: sex and race\n",
      "\n",
      "C(sex) indicates a categorical variable\n",
      "\n",
      "\n",
      "print(\"should a permit be required to own a gun\")\n",
      "print(df[\"GUNLAW\"].value_counts())\n",
      "formula=\"GUNLAW ~ AGE+AGE2+EDUC+EDUC2+C(SEX)\"\n",
      "results = smf.logit(formula,data=df).fit()\n",
      "\n",
      "logit must be a 0 or 1 value\n",
      "\n",
      "   Sample  > Gun law prediction\n",
      "\n",
      "print(\"should a permit be required to own a gun\")\n",
      "formula=\"GUNLAW ~ AGE+AGE2+EDUC+EDUC2+C(SEX)\"\n",
      "print(\"logit must be a variable of 0 or 1\")\n",
      "gun_filter=(df[\"GUNLAW\"]==0) | (df[\"GUNLAW\"]==1)\n",
      "print(df[gun_filter][\"GUNLAW\"].value_counts())\n",
      "results = smf.logit(formula,data=df[gun_filter]).fit()\n",
      "print(results.params)\n",
      "\n",
      "print(\"women are more likely to support gun permit control\")\n",
      "\n",
      "df2=pd.DataFrame()\n",
      "df2['AGE']=np.linspace(18,89)\n",
      "df2['EDUC']=12\n",
      "df2['AGE2']=df2['AGE']**2\n",
      "df2['EDUC2']=df2['EDUC']**2\n",
      "df2['SEX']=1\n",
      "pred1=results.predict(df2)\n",
      "df2['SEX']=2\n",
      "pred2=results.predict(df2)\n",
      "\n",
      "grouped=df[gun_filter].groupby('AGE')\n",
      "favor_by_age=grouped['GUNLAW'].mean()\n",
      "\n",
      "plt.plot(favor_by_age,'o',alpha=0.5)\n",
      "plt.plot(df2['AGE'],pred1,label=\"Male\")\n",
      "plt.plot(df2['AGE'],pred2,label=\"FeMale\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "   Sample  > grass prediction\n",
      "# Recode grass\n",
      "gss['grass'].replace(2, 0, inplace=True)\n",
      "\n",
      "print\n",
      "# Run logistic regression\n",
      "results = smf.logit(\"grass ~ age+age2+educ+educ2+C(sex)\",data=gss).fit()\n",
      "results.params\n",
      "\n",
      "\n",
      "# Make a DataFrame with a range of ages\n",
      "df = pd.DataFrame()\n",
      "df['age'] = np.linspace(18, 89)\n",
      "df['age2'] = df['age']**2\n",
      "\n",
      "# Set the education level to 12\n",
      "df['educ'] = 12\n",
      "df['educ2'] = df['educ']**2\n",
      "\n",
      "\n",
      "df['sex'] = 1\n",
      "pred1 = results.predict(df)\n",
      "\n",
      "df['sex'] = 2\n",
      "pred2 = results.predict(df)\n",
      "\n",
      "plt.clf()\n",
      "grouped = gss.groupby('age')\n",
      "favor_by_age = grouped['grass'].mean()\n",
      "plt.plot(favor_by_age, 'o', alpha=0.5)\n",
      "\n",
      "plt.plot(df['age'], pred1, label='Male')\n",
      "plt.plot(df['age'],pred2,label='FeMale')\n",
      "\n",
      "plt.xlabel('Age')\n",
      "plt.ylabel('Probability of favoring legalization')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\pyspark.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\pyspark.txt\n",
      "Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n",
      "\n",
      "As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\n",
      "\n",
      "\n",
      "# Verify SparkContext\n",
      "sc=SparkContext\n",
      "print(sc)\n",
      "\n",
      "# Print Spark version\n",
      "print(sc.version)\n",
      "\n",
      "spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you'll be using the Spark DataFrame abstraction built on top of RDDs.\n",
      "\n",
      "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\n",
      "\n",
      "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\n",
      "\n",
      "\n",
      "# Import SparkSession from pyspark.sql\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "# Create my_spark\n",
      "my_spark = SparkSession.builder.getOrCreate()\n",
      "\n",
      "# Print my_spark\n",
      "print(my_spark)\n",
      "\n",
      "\n",
      "# Print the tables in the catalog\n",
      "print(spark.catalog.listTables())\n",
      "\n",
      "  > example  > sql like extraction\n",
      "\n",
      "# Don't change this query\n",
      "query = \"FROM flights SELECT * LIMIT 10\"\n",
      "\n",
      "# Get the first 10 rows of flights\n",
      "flights10 = spark.sql(query)\n",
      "\n",
      "# Show the results\n",
      "flights10.show()\n",
      "\n",
      "\n",
      "   > sample   > group by in sql\n",
      "\n",
      "# Don't change this query\n",
      "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
      "\n",
      "# Run the query\n",
      "flight_counts = spark.sql(query)\n",
      "\n",
      "# Convert the results to a pandas DataFrame\n",
      "pd_counts = flight_counts.toPandas()\n",
      "\n",
      "# Print the head of pd_counts\n",
      "print(pd_counts.head())\n",
      "\n",
      "\n",
      "In the last exercise, you saw how to move data from Spark to pandas. However, maybe you want to go the other direction, and put a pandas DataFrame into a Spark cluster! The SparkSession class has a method for this as well.\n",
      "\n",
      "The .createDataFrame() method takes a pandas DataFrame and returns a Spark DataFrame.\n",
      "\n",
      "The output of this method is stored locally, not in the SparkSession catalog. This means that you can use all the Spark DataFrame methods on it, but you can't access the data in other contexts.\n",
      "\n",
      "For example, a SQL query (using the .sql() method) that references your DataFrame will throw an error. To access the data in this way, you have to save it as a temporary table.\n",
      "\n",
      "You can do this using the .createTempView() Spark DataFrame method, which takes as its only argument the name of the temporary table you'd like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame.\n",
      "\n",
      "There is also the method .createOrReplaceTempView(). This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You'll use this method to avoid running into problems with duplicate tables.\n",
      "\n",
      "Check out the diagram to see all the different ways your Spark data structures interact with each other.\n",
      "\n",
      "\n",
      "# Create pd_temp\n",
      "pd_temp = pd.DataFrame(np.random.random(10))\n",
      "\n",
      "# Create spark_temp from pd_temp\n",
      "spark_temp = spark.createDataFrame(pd_temp)\n",
      "# Examine the tables in the catalog\n",
      "print(spark.catalog.listTables())\n",
      "\n",
      "# Add spark_temp to the catalog\n",
      "spark_temp.createOrReplaceTempView('temp')\n",
      "\n",
      "# Examine the tables in the catalog again\n",
      "print(spark.catalog.listTables())\n",
      "\n",
      "query='select * from temp'\n",
      "\n",
      "result=spark.sql(query).toPandas()\n",
      "\n",
      "print(result.head())\n",
      "\n",
      "\n",
      "\n",
      "   >sample   > read csv\n",
      "\n",
      "SparkSession has a .read attribute which has several methods for reading different data sources into Spark DataFrames. Using these you can create a DataFrame from a .csv file just like with regular pandas DataFrames!\n",
      "\n",
      "# Don't change this file path\n",
      "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
      "\n",
      "# Read in the airports data\n",
      "airports = spark.read.csv(file_path,header=True)\n",
      "\n",
      "# Show the data\n",
      "\n",
      "airports.show()\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "\n",
      "    >creating columns\n",
      "\n",
      "\n",
      "The new column must be an object of class Column. Creating one of these is as easy as extracting a column from your DataFrame using df.colName.\n",
      "\n",
      "Updating a Spark DataFrame is somewhat different than working in pandas because the Spark DataFrame is immutable. This means that it can't be changed, and so columns can't be updated in place.\n",
      "\n",
      "  sample  withColumn\n",
      "\n",
      "# Create the DataFrame flights\n",
      "flights = spark.table(\"flights\")\n",
      "\n",
      "# Show the head\n",
      "flights.show()\n",
      "\n",
      "# Add duration_hrs\n",
      "flights = flights.withColumn(\"duration_hrs\", flights.air_time/60)\n",
      "\n",
      "\n",
      "       .filter()\n",
      "\n",
      "Let's take a look at the .filter() method. As you might suspect, this is the Spark counterpart of SQL's WHERE clause. The .filter() method takes either an expression that would follow the WHERE clause of a SQL expression as a string, or a Spark Column of boolean (True/False) values.\n",
      "\n",
      "flights.filter(\"air_time > 120\").show()\n",
      "flights.filter(flights.air_time > 120).show()\n",
      "\n",
      "columns\n",
      "\n",
      "['year',\n",
      " 'month',\n",
      " 'day',\n",
      " 'dep_time',\n",
      " 'dep_delay',\n",
      " 'arr_time',\n",
      " 'arr_delay',\n",
      " 'carrier',\n",
      " 'tailnum',\n",
      " 'flight',\n",
      " 'origin',\n",
      " 'dest',\n",
      " 'air_time',\n",
      " 'distance',\n",
      " 'hour',\n",
      " 'minute']\n",
      "\n",
      "# Filter flights by passing a string\n",
      "long_flights1 = flights.filter(\"distance>1000\")\n",
      "\n",
      "# Filter flights by passing a column of boolean values\n",
      "long_flights2 = flights.filter(flights.distance > 1000)\n",
      "\n",
      "# Print the data to check they're equal\n",
      "long_flights1.show()\n",
      "long_flights2.show()\n",
      "\n",
      "output: same results\n",
      "\n",
      "         .select()\n",
      "\n",
      "# Select the first set of columns\n",
      "selected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n",
      "\n",
      "# Select the second set of columns\n",
      "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
      "\n",
      "# Define first filter\n",
      "filterA = flights.origin == \"SEA\"\n",
      "\n",
      "# Define second filter\n",
      "filterB = flights.dest == \"PDX\"\n",
      "\n",
      "# Filter the data, first by filterA then by filterB\n",
      "selected2 = temp.filter(filterA).filter(filterB)\n",
      "\n",
      "     select () .alias()\n",
      "\n",
      "# Define avg_speed\n",
      "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
      "\n",
      "# Select the correct columns\n",
      "speed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
      "\n",
      "# Create the same table using a SQL expression\n",
      "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")\n",
      "\n",
      "speed1.show()\n",
      "speed2.show()\n",
      "\n",
      "     >Aggregating \n",
      "\n",
      "All of the common aggregation methods, like .min(), .max(), and .count() are GroupedData methods. These are created by calling the .groupBy() DataFrame method\n",
      "\n",
      "   Group by\n",
      "\n",
      "# Find the shortest flight from PDX in terms of distance\n",
      "flights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()\n",
      "\n",
      "# Find the longest flight from SEA in terms of air time\n",
      "flights.filter(flights.origin == \"SEA\").groupBy().max(\"air_time\").show()\n",
      "\n",
      "\n",
      "    groupby withcolumns\n",
      "\n",
      "# Average duration of Delta flights\n",
      "flights.filter(flights.carrier==\"DL\").filter(flights.origin==\"SEA\").groupBy().avg(\"air_time\").show()\n",
      "\n",
      "# Total hours in the air\n",
      "flights.withColumn(\"duration_hours\", flights.air_time/60).groupBy().sum(\"duration_hours\").show()\n",
      "\n",
      "\n",
      "    groupby  count avg\n",
      "\n",
      "# Group by tailnum\n",
      "by_plane = flights.groupBy(\"tailnum\")\n",
      "\n",
      "# Number of flights each plane made\n",
      "by_plane.count().show()\n",
      "\n",
      "# Group by origin\n",
      "by_origin = flights.groupBy(\"origin\")\n",
      "\n",
      "# Average duration of flights from PDX and SEA\n",
      "by_origin.avg(\"air_time\").show()\n",
      "\n",
      "  > pyspark.sql.functions\n",
      "\n",
      "In addition to the GroupedData methods you've already seen, there is also the .agg() method. This method lets you pass an aggregate column expression that uses any of the aggregate functions from the pyspark.sql.functions submodule.\n",
      "\n",
      "# Import pyspark.sql.functions as F\n",
      "import pyspark.sql.functions as F\n",
      "\n",
      "# Group by month and dest\n",
      "by_month_dest = flights.groupBy(\"month\", \"dest\")\n",
      "\n",
      "# Average departure delay by month and destination\n",
      "by_month_dest.avg(\"dep_delay\").show()\n",
      "\n",
      "# Standard deviation of departure delay\n",
      "by_month_dest.agg(F.stddev(\"dep_delay\")).show()\n",
      "\n",
      "\n",
      "        Joining\n",
      "\n",
      "This column is called the key. Examples of keys here include the tailnum and carrier columns from the flights table.\n",
      "\n",
      "flights and planes\n",
      "\n",
      "\n",
      "In PySpark, joins are performed using the DataFrame method .join(). This method takes three arguments. The first is the second DataFrame that you want to join with the first one. The second argument, on, is the name of the key column(s) as a string. The names of the key column(s) must be the same in each table.\n",
      "\n",
      "\n",
      "# Examine the data\n",
      "airports.show()\n",
      "\n",
      "# Rename the faa column\n",
      "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
      "\n",
      "# Join the DataFrames\n",
      "flights_with_airports = flights.join(airports, on=\"dest\", how=\"leftouter\")\n",
      "\n",
      "# Examine the new DataFrame\n",
      "flights_with_airports.show()\n",
      "\n",
      "\n",
      "     >Machine Learning pipelines\n",
      "\n",
      "At the core of the pyspark.ml module are the Transformer and Estimator classes. Almost every other class in the module behaves similarly to these two basic classes.\n",
      "\n",
      "\n",
      "\n",
      "# Rename year column\n",
      "planes = planes.withColumnRenamed(\"year\",\"plane_year\")\n",
      "\n",
      "# Join the DataFrames\n",
      "model_data = flights.join(flights, on=[\"tailnum\"], how=\"leftouter\")\n",
      "\n",
      "model_data.show()\n",
      "\n",
      "['tailnum', 'year', 'month', 'day', 'dep_time', 'dep_delay', 'arr_time', 'arr_delay', 'carrier', 'flight', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute', 'year', 'month', 'day', 'dep_time', 'dep_delay', 'arr_time', 'arr_delay', 'carrier', 'flight', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute']\n",
      "\n",
      "\n",
      "# Rename year column\n",
      "planes = planes.withColumnRenamed(\"year\",\"plane_year\")\n",
      "\n",
      "# Join the DataFrames\n",
      "model_data = flights.join(planes, on=[\"tailnum\"], how=\"leftouter\")\n",
      "\n",
      "model_data.show()\n",
      "\n",
      "print(model_data.columns)\n",
      "\n",
      "     TRANSFORMING\n",
      "\n",
      "it's important to know that Spark only handles numeric data.\n",
      "\n",
      "DataFrame must be either integers or decimals\n",
      "\n",
      "To remedy this, you can use the .cast() method in combination with the .withColumn() method. It's important to note that .cast() works on columns, while .withColumn() works on DataFrames\n",
      "\n",
      "integer\n",
      "double\n",
      "\n",
      "\n",
      "# Cast the columns to integers\n",
      "model_data = model_data.withColumn(\"arr_delay\",  model_data.arr_delay.cast(\"integer\"))\n",
      "model_data = model_data.withColumn(\"air_time\", model_data.arr_time.cast(\"integer\"))\n",
      "model_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\n",
      "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))\n",
      "\n",
      "# Create the column plane_age\n",
      "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)\n",
      "\n",
      "\n",
      "    create new features  > check if is_late\n",
      "\n",
      "# Create is_late\n",
      "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay>0)\n",
      "\n",
      "\n",
      "# Convert to an integer\n",
      "model_data = model_data.withColumn(\"label\",model_data.is_late.cast('integer'))\n",
      "\n",
      "print(model_data.label)\n",
      "\n",
      "# Remove missing values\n",
      "model_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")\n",
      "\n",
      "model_data.show()\n",
      "\n",
      "    Strings and factors\n",
      " So far this hasn't been an issue; even boolean columns can easily be converted to integers without any trouble. But you'll also be using the airline and the plane's destination as features in your model.\n",
      "\n",
      "\n",
      "Fortunately, PySpark has functions for handling this built into the pyspark.ml.features submodule. You can create what are called 'one-hot vectors' to represent the carrier and the destination of each flight. A one-hot vector is a way of representing a categorical feature where every observation has a vector in which all elements are zero except for at most one element, which has a value of one\n",
      "\n",
      "The first step to encoding your categorical feature is to create a StringIndexer. Members of this class are Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column.\n",
      "\n",
      "The second step is to encode this numeric column as a one-hot vector using a OneHotEncoder. This works exactly the same way as the StringIndexer by creating an Estimator and then a Transformer. The end result is a column that encodes your categorical feature as a vector that's suitable for machine learning routines!\n",
      "\n",
      "\n",
      "All you have to remember is that you need to create a StringIndexer and a OneHotEncoder, and the Pipeline will take care of the rest\n",
      "\n",
      "\n",
      "n this exercise you'll create a StringIndexer and a OneHotEncoder to code the carrier column. To do this, you'll call the class constructors with the arguments inputCol and outputCol.\n",
      "\n",
      "The inputCol is the name of the column you want to index or encode, and the outputCol is the name of the new column that the Transformer should create.\n",
      "\n",
      "\n",
      "# Create a StringIndexer\n",
      "carr_indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n",
      "\n",
      "# Create a OneHotEncoder\n",
      "carr_encoder = OneHotEncoder(inputCol=\"carrier_index\",outputCol=\"carrier_fact\")\n",
      "\n",
      "\n",
      "# Create a StringIndexer\n",
      "dest_indexer = StringIndexer(inputCol=\"dest\", outputCol=\"dest_index\")\n",
      "\n",
      "# Create a OneHotEncoder\n",
      "dest_encoder = OneHotEncoder(inputCol=\"dest_index\",outputCol=\"dest_fact\")\n",
      "\n",
      "The last step in the Pipeline is to combine all of the columns containing our features into a single column. This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form. You can do this by storing each of the values from a column as an entry in a vector. Then, from the model's point of view, every observation is a vector that contains all of the information about it and a label that tells the modeler what value that observation corresponds to.\n",
      "\n",
      "Because of this, the pyspark.ml.feature submodule contains a class called VectorAssembler. This Transformer takes all of the columns you specify and combines them into a new vector column.\n",
      "\n",
      "\n",
      "# Make a VectorAssembler\n",
      "vec_assembler = VectorAssembler(inputCols=[\"month\",\"air_time\",\"carrier_fact\",\"dest_fact\",\"plane_age\"], outputCol=\"features\")\n",
      "\n",
      "\n",
      "Pipeline is a class in the pyspark.ml module that combines all the Estimators and Transformers that you've already created. This lets you reuse the same modeling process over and over again by wrapping it up in one simple object. Neat, right?\n",
      "\n",
      "# Import Pipeline\n",
      "from pyspark.ml import Pipeline\n",
      "\n",
      "# Make the pipeline\n",
      "flights_pipe = Pipeline(stages=[dest_indexer,dest_encoder,carr_indexer,carr_encoder,vec_assembler])\n",
      "\n",
      "# Fit and transform the data\n",
      "piped_data = flights_pipe.fit(model_data).transform(model_data)\n",
      "\n",
      "training, test = piped_data.randomSplit([.6,.4])\n",
      "\n",
      "        Logistic Regression\n",
      "\n",
      "# Import LogisticRegression\n",
      "from pyspark.ml.classification import LogisticRegression\n",
      "\n",
      "# Create a LogisticRegression Estimator\n",
      "lr = LogisticRegression()\n",
      "\n",
      "\n",
      "  > cross validation\n",
      "\n",
      "You'll be using cross validation to choose the hyperparameters by creating a grid of the possible pairs of values for the two hyperparameters, elasticNetParam and regParam, and using the cross validation error to compare all the different models so you can choose the best one!\n",
      "\n",
      "\n",
      "# Import the evaluation submodule\n",
      "import pyspark.ml.evaluation as evals\n",
      "\n",
      "# Create a BinaryClassificationEvaluator\n",
      "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
      "\n",
      "The submodule pyspark.ml.tuning includes a class called ParamGridBuilder that does just that (maybe you're starting to notice a pattern here; PySpark has a submodule for just about everything!).\n",
      "\n",
      "You'll need to use the .addGrid() and .build() methods to create a grid that you can use for cross validation. The .addGrid() method takes a model parameter (an attribute of the model Estimator, lr, that you created a few exercises ago) and a list of values that you want to try. The .build() method takes no arguments, it just returns the grid that you'll use later.\n",
      "\n",
      "# Import the tuning submodule\n",
      "import pyspark.ml.tuning as tune\n",
      "\n",
      "# Create the parameter grid\n",
      "grid = tune.ParamGridBuilder()\n",
      "\n",
      "# Add the hyperparameter\n",
      "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
      "grid = grid.addGrid(lr.elasticNetParam, [0,1])\n",
      "\n",
      "# Build the grid\n",
      "grid = grid.build()\n",
      "\n",
      "# Create the CrossValidator\n",
      "cv = tune.CrossValidator(estimator=lr,\n",
      "               estimatorParamMaps=grid,\n",
      "               evaluator=evaluator\n",
      "               )\n",
      "\n",
      "# Call lr.fit()\n",
      "best_lr = lr.fit(training)\n",
      "\n",
      "# Print best_lr\n",
      "print(best_lr)\n",
      "\n",
      "# Use the model to predict the test set\n",
      "test_results = best_lr.transform(test)\n",
      "\n",
      "# Evaluate the predictions\n",
      "print(evaluator.evaluate(test_results))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\python functions with arguments.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\python functions with arguments.txt\n",
      "adding arguments to the decorator\n",
      "\n",
      "def run_three_times(func):\n",
      "\tdef wrapper(*args, **kwargs):\n",
      "\tfor i in range(3):\n",
      "\t\tfunc(*args,**kwargs)\n",
      "\treturn wrapper\n",
      "\n",
      "  >decorator factory\n",
      "\n",
      "write a function that returns a decorator object rather than runs a decorator.   A decorator can only have one parameter being func for function.\n",
      "\n",
      "def run_n_times(n):\n",
      "\tdef decorator(func):\n",
      "\t\tdef wrapper(**args,**kwargs):\n",
      "\t\t\tfor i in range(n)\n",
      "\t\t\t\tfunc(*args,**kwargs)\n",
      "\t\treturn wrapper\n",
      "\treturn decorator\n",
      "\n",
      "\n",
      "@run_n_times(3)\n",
      "def print_sum(a,b)\n",
      "\tprint(a+b)\n",
      "\n",
      "  Sample  > passing parameters to the decorator\n",
      "\n",
      "# Make print_sum() run 10 times with the run_n_times() decorator\n",
      "@run_n_times(10)\n",
      "def print_sum(a, b):\n",
      "  print(a + b)\n",
      "  \n",
      "print_sum(15, 20)\n",
      "\n",
      "   sample  > decorator factor to a variable\n",
      "\n",
      "# Use run_n_times() to create the run_five_times() decorator\n",
      "run_five_times = run_n_times(5)\n",
      "\n",
      "@run_five_times\n",
      "def print_sum(a, b):\n",
      "  print(a + b)\n",
      "  \n",
      "print_sum(4, 100)\n",
      "\n",
      "  > sample  > access the function directly   decorator then function.\n",
      "\n",
      "# Modify the print() function to always run 20 times\n",
      "print = run_n_times(20)(print)\n",
      "\n",
      "print('What is happening?!?!')\n",
      "\n",
      "\n",
      "  > sample  > creating the decorator factory\n",
      "\n",
      "ef html(open_tag, close_tag):\n",
      "  def decorator(func):\n",
      "    @wraps(func)\n",
      "    def wrapper(*args, **kwargs):\n",
      "      msg = func(*args, **kwargs)\n",
      "      return '{}{}{}'.format(open_tag, msg, close_tag)\n",
      "    # Return the decorated function\n",
      "    return wrapper\n",
      "  # Return the decorator\n",
      "  return decorator\n",
      "\n",
      "# Make hello() return bolded text\n",
      "@html(\"<b>\",\"</b>\")\n",
      "def hello(name):\n",
      "  return 'Hello {}!'.format(name)\n",
      "  \n",
      "print(hello('Alice'))\n",
      "\n",
      "\n",
      "# Make goodbye() return italicized text\n",
      "@html(\"<i>\",\"</i>\")\n",
      "def goodbye(name):\n",
      "  return 'Goodbye {}.'.format(name)\n",
      "  \n",
      "print(goodbye('Alice'))\n",
      "\n",
      "\n",
      " > sample  > decorated functions can be called within another decorated function. for example goodbye\n",
      "\n",
      "# Wrap the result of hello_goodbye() in <div> and </div>\n",
      "html(\"<div>\",\"</div>\")\n",
      "def hello_goodbye(name):\n",
      "  return '\\n{}\\n{}\\n'.format(hello(name), goodbye(name))\n",
      "  \n",
      "print(hello_goodbye('Alice'))\n",
      "\n",
      "    Time out\n",
      "\n",
      "def function1():\n",
      "\t\"\"\"this functions sometimes runs for a long time\n",
      "\n",
      "def function2():\n",
      "\t\"\"\"This function does not return control\n",
      "\n",
      "create a time function that raises an error if the function run longer than expected\n",
      "\n",
      "\n",
      "import signal\n",
      "\n",
      "def raise_timeout(*args, **kwargs):\n",
      "\traise TimeoutError()\n",
      "signal.signal(signalnum=signal.SIGALRM, handler=raise_timeout)\n",
      "\n",
      "\n",
      "#set off an alerm in 5 seconds\n",
      "signal.alarm(5)\n",
      "\n",
      "signal.alarm(0)\n",
      "\n",
      "def timeout_in_5s(func):\n",
      "\t@wraps(func)\n",
      "\tdef wrapper(*args,**kwargs):\n",
      "\t\tsignal.alarm(5)\n",
      "\t\t\ttry:\n",
      "\t\t\t\treturn func(*args,**kwargs)\n",
      "\t\t\tfinally:\n",
      "\t\t\t\tsignal.alarm(0)\n",
      "\n",
      "\treturn wrapper()\n",
      "\n",
      "\n",
      "@timeout_in_5s\n",
      "def foo():\n",
      "\ttime.sleep(10)\n",
      "\tprint('foo!')\n",
      "\n",
      "  >timeout\n",
      "\n",
      "def timeout(n_seconds):\n",
      "\tdef decorator(func):\n",
      "\t\t@wraps(func)\n",
      "\t\tdef wrapper(*args,**kwargs):\n",
      "\t\t\tsignal.alarm(n_seconds)\n",
      "\t\t\ttry:\n",
      "\t\t\t\treturn func(*args,**kwargs)\n",
      "\t\t\tfinally:\n",
      "\t\t\t\tsignal.alarm(0)\n",
      "\t\treturn wrapper\n",
      "\treturn decorator\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@timeout(20)\n",
      "def bar():\n",
      "\ttime.sleep(10)\n",
      "\tprint('bar!')\n",
      "\n",
      "\n",
      "  sample   > decorator factory\n",
      "\n",
      "def tag(*tags):\n",
      "  # Define a new decorator, named \"decorator\", to return\n",
      "  def decorator(func):\n",
      "    # Ensure the decorated function keeps its metadata\n",
      "    @wraps(func)\n",
      "    def wrapper(*args, **kwargs):\n",
      "      # Call the function being decorated and return the result\n",
      "      return func(*args, **kwargs)\n",
      "    wrapper.tags = tags\n",
      "    return wrapper\n",
      "  # Return the new decorator\n",
      "  return decorator\n",
      "\n",
      "@tag('test', 'this is a tag')\n",
      "def foo():\n",
      "  pass\n",
      "\n",
      "print(foo.tags)\n",
      "\n",
      "\n",
      " > sample  > decorator checks if the parameter passed to wrapper is a dictionary\n",
      "\n",
      "def returns_dict(func):\n",
      "  # Complete the returns_dict() decorator\n",
      "  def wrapper(*args, **kwargs):\n",
      "    result = func(*args,**kwargs)\n",
      "    assert(type(result) == dict)\n",
      "    return result\n",
      "  return wrapper\n",
      "  \n",
      "@returns_dict\n",
      "def foo(value):\n",
      "  return value\n",
      "\n",
      "try:\n",
      "  print(foo([1,2,3]))\n",
      "except AssertionError:\n",
      "  print('foo() did not return a dict!')\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\python toolbox p1.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\python toolbox p1.txt\n",
      "functions \n",
      "1. with parameters\n",
      "2. one paramter\n",
      "3. return a value\n",
      "\n",
      "x=str(5)\n",
      "print(x)\n",
      "output:5\n",
      "\n",
      "def square():\n",
      "\tnew_value=4**2\n",
      "\tprint(new_value)\n",
      "\n",
      "square\n",
      "output:16\n",
      "\n",
      "def square(value):\n",
      "\tnew_value=value**2\n",
      "\tprint(new_value)\n",
      "\treturn (new_value)\n",
      "\n",
      "DocStrings\n",
      "\n",
      "''' returns the square of value '''\n",
      "\n",
      "\n",
      "  >multiple arguments and returning multiple values\n",
      "\n",
      "\n",
      "def raise_to_power(value1, power):\n",
      "\t\"\"\"raise value1 to the power value2\"\"\"\n",
      "\tnew_value=value1**power\n",
      "\treturn new_value\n",
      "\n",
      "\n",
      "result = raise_to_power(2,3)\n",
      "\n",
      "   >tuples\n",
      "1. like a list\n",
      "2. immutable\n",
      "3. constructed using parantheses\n",
      "\n",
      "\n",
      "even_nums=(2,4,6)\n",
      "a,b,c= even_nums\n",
      "\n",
      "b=even_num[1]\n",
      "\n",
      "\n",
      "def raise_to_power(value1, power):\n",
      "\t\"\"\"raise value1 to the power value2\"\"\"\n",
      "\tnew_value1=value1**power\n",
      "\tnew_value2=power**value1\n",
      "\tnew_tuple= (new_value1, new_value2)\n",
      "\treturn(new_tuple)\n",
      "\n",
      "\n",
      "  Sample\n",
      "\n",
      "# Define shout with parameters word1 and word2\n",
      "def shout(word1, word2):\n",
      "    \"\"\"Concatenate strings with three exclamation marks\"\"\"\n",
      "    # Concatenate word1 with '!!!': shout1\n",
      "    \n",
      "    shout1=word1+\"!!!\"\n",
      "    # Concatenate word2 with '!!!': shout2\n",
      "    shout2=word2+\"!!!\"\n",
      "    \n",
      "    # Concatenate shout1 with shout2: new_shout\n",
      "    new_shout=shout1+shout2\n",
      "\n",
      "    # Return new_shout\n",
      "    return new_shout\n",
      "\n",
      "# Pass 'congratulations' and 'you' to shout(): yell\n",
      "yell=shout('congratulations','you')\n",
      "\n",
      "# Print yell\n",
      "print(yell)\n",
      "\n",
      "  >Sample  > return a tuple\n",
      "\n",
      "# Define shout_all with parameters word1 and word2\n",
      "def shout(word1, word2):\n",
      "    \"\"\"Concatenate strings with three exclamation marks\"\"\"\n",
      "    # Concatenate word1 with '!!!': shout1\n",
      "    \n",
      "    shout1=word1+\"!!!\"\n",
      "    # Concatenate word2 with '!!!': shout2\n",
      "    shout2=word2+\"!!!\"\n",
      "    \n",
      "    \n",
      "    # Construct a tuple with shout1 and shout2: shout_words\n",
      "    shout_words=(shout1,shout2)\n",
      "    \n",
      "    # Return shout_words\n",
      "    return shout_words\n",
      "\n",
      "# Pass 'congratulations' and 'you' to shout_all(): yell1, yell2\n",
      "yell1,yell2=shout('congratulations','you')\n",
      "\n",
      "# Print yell1 and yell2\n",
      "print(yell1)\n",
      "print(yell2)\n",
      "\n",
      "\n",
      "    >dictionary of data of number of tweets\n",
      "\n",
      " >sample\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Import Twitter data as DataFrame: df\n",
      "df = pd.read_csv('tweets.csv')\n",
      "\n",
      "# Initialize an empty dictionary: langs_count\n",
      "langs_count = {}\n",
      "\n",
      "# Extract column from DataFrame: col\n",
      "col = df['lang']\n",
      "\n",
      "# Iterate over lang column in DataFrame\n",
      "for entry in col:\n",
      "\n",
      "    # If the language is in langs_count, add 1 \n",
      "    if entry in langs_count.keys():\n",
      "        langs_count[entry]+=1\n",
      "    # Else add the language to langs_count, set the value to 1\n",
      "    else:\n",
      "        langs_count[entry]=1\n",
      "\n",
      "# Print the populated dictionary\n",
      "print(langs_count)\n",
      "\n",
      "  sample  > building a function for the dictionary count\n",
      "\n",
      "# Define count_entries()\n",
      "def count_entries(df,col_name):\n",
      "    \"\"\"Return a dictionary with counts of \n",
      "    occurrences as value for each key.\"\"\"\n",
      "\n",
      "    # Initialize an empty dictionary: langs_count\n",
      "    langs_count = {}\n",
      "    \n",
      "    # Extract column from DataFrame: col\n",
      "    col = df[col_name]\n",
      "    \n",
      "    # Iterate over lang column in DataFrame\n",
      "    for entry in col:\n",
      "\n",
      "        # If the language is in langs_count, add 1\n",
      "        if entry in langs_count.keys():\n",
      "            langs_count[entry] += 1\n",
      "        # Else add the language to langs_count, set the value to 1\n",
      "        else:\n",
      "            langs_count[entry] = 1\n",
      "\n",
      "    # Return the langs_count dictionary\n",
      "    return langs_count\n",
      "\n",
      "# Call count_entries(): result\n",
      "result=count_entries(tweets_df,\"lang\")\n",
      "\n",
      "# Print the result\n",
      "print(result)\n",
      "\n",
      "  >Scope\n",
      "1. defining variables\n",
      "2. not all variables are accessible\n",
      "a. global scope - defined within a module\n",
      "b. local scope - defined within a function\n",
      "c. built-in-score names in the pre-defined built-ins module\n",
      "\n",
      "\n",
      "def square(value):\n",
      "\tnew_value=value**2\n",
      "\tprint(new_value)\n",
      "\treturn (new_value)\n",
      "\n",
      "square(3)\n",
      "\n",
      "suppose we define the name globally\n",
      "\n",
      "new_val=10\n",
      "def square(value):\n",
      "\tnew_value=value**2\n",
      "\tprint(new_value)\n",
      "\treturn (new_value)\n",
      "\n",
      "\n",
      "The function will accessible the global scope for the variable new_val\n",
      "\n",
      "If the function can not find the variable in the local scope only then will it look for the variable in the global scope.\n",
      "if neither than the built-in-scope is searched.\n",
      "\n",
      "new_val=10\n",
      "def square(value):\n",
      "\tglobal new_val\n",
      "\tnew_val=value**2\n",
      "\treturn (new_val)\n",
      "\n",
      "global calls the global variable\n",
      "\n",
      "  Sample   builtins\n",
      "\n",
      "import builtins\n",
      "\n",
      "dir(builtins)\n",
      "\n",
      "   >Nested Functions\n",
      "\n",
      "def outer(....):\n",
      "\tx=...\n",
      "\tdef inner():\n",
      "\t\ty=x**2\n",
      "\treturn\n",
      "\n",
      "why nest functions?\n",
      "\n",
      "\n",
      "def mod2plus5(x1,x2,x3)\n",
      "\tdef inner(x):\n",
      "\t\treturn x%2+5\n",
      "\treturn(inner(x1),inner(x2),inner(x3))\n",
      "\n",
      "\n",
      "def raise_val(n):\n",
      "\tdef inner(x):\n",
      "\t\traised=x**n\n",
      "\t\treturn raised\n",
      "\treturn inner\n",
      "\n",
      "building closures\n",
      "\n",
      "square=raised_val(2)\n",
      "cube=raised_val(3)\n",
      "\n",
      "print(square(2), cube(4))\n",
      "\n",
      "output: 4 and 64\n",
      "\n",
      "\n",
      "def raise_val(n):\n",
      "\tdef inner():\n",
      "\t\tnonlocal n\n",
      "\t\tn=2\n",
      "\t\tprint(n)\n",
      "\treturn inner\n",
      "\tinner()\n",
      "\tprint(n)\n",
      "\n",
      "output:2\n",
      "\n",
      "LEGB rule\n",
      "1.Local scope\n",
      "2. enclosing functions\n",
      "3. global\n",
      "4. built-in\n",
      "\n",
      "\n",
      "  Sample  > calling an nested function\n",
      "\n",
      "# Define three_shouts\n",
      "def three_shouts(word1, word2, word3):\n",
      "    \"\"\"Returns a tuple of strings\n",
      "    concatenated with '!!!'.\"\"\"\n",
      "\n",
      "    # Define inner\n",
      "    def inner(word):\n",
      "        \"\"\"Returns a string concatenated with '!!!'.\"\"\"\n",
      "        return word + '!!!'\n",
      "\n",
      "    # Return a tuple of strings\n",
      "    return (inner(word1), inner(word2), inner(word3))\n",
      "\n",
      "# Call three_shouts() and print\n",
      "print(three_shouts('a', 'b', 'c'))\n",
      "\n",
      "\n",
      "  Sample  > calling a outer then return a pointer to the inner function\n",
      "\n",
      "# Define echo\n",
      "def echo(n):\n",
      "    \"\"\"Return the inner_echo function.\"\"\"\n",
      "\n",
      "    # Define inner_echo\n",
      "    def inner_echo(word1):\n",
      "        \"\"\"Concatenate n copies of word1.\"\"\"\n",
      "        echo_word = word1 * n\n",
      "        return echo_word\n",
      "\n",
      "    # Return inner_echo\n",
      "    return(inner_echo)\n",
      "\n",
      "# Call echo: twice\n",
      "twice = echo(2)\n",
      "\n",
      "# Call echo: thrice\n",
      "thrice =echo(3)\n",
      "\n",
      "# Call twice() and thrice() then print\n",
      "print(twice('hello'), thrice('hello'))\n",
      "\n",
      "  Sample  >  The inner function pointer references the outer function variable echo_word\n",
      "\n",
      "# Define echo_shout()\n",
      "def echo_shout(word):\n",
      "    \"\"\"Change the value of a nonlocal variable\"\"\"\n",
      "    \n",
      "    # Concatenate word with itself: echo_word\n",
      "    echo_word=word*2\n",
      "    \n",
      "    # Print echo_word\n",
      "    print(echo_word)\n",
      "    \n",
      "    # Define inner function shout()\n",
      "    def shout():\n",
      "        \"\"\"Alter a variable in the enclosing scope\"\"\"    \n",
      "        # Use echo_word in nonlocal scope\n",
      "        nonlocal echo_word\n",
      "        \n",
      "        # Change echo_word to echo_word concatenated with '!!!'\n",
      "        echo_word = echo_word+\"!!!\"\n",
      "    \n",
      "    # Call function shout()\n",
      "    shout()\n",
      "    \n",
      "    # Print echo_word\n",
      "    print(echo_word)\n",
      "\n",
      "# Call function echo_shout() with argument 'hello'\n",
      "echo_shout(\"hello\")\n",
      "\n",
      "      setting up default arguments and flexible list of arguments\n",
      "\n",
      "\n",
      "def power(number, pow=1)\n",
      "\tnew_value=number **pow\n",
      "\treturn new_value\n",
      "\n",
      "power(3)\n",
      "\n",
      "  Flexible arguments\n",
      "\n",
      "def add_all(*args):\n",
      "\tsum_all=0\n",
      "\tfor num in args:\n",
      "\t\tsum_all+=num\n",
      "\treturn sum_all\n",
      "\n",
      "\n",
      "converts all the arguments to a tuple called args\n",
      "\n",
      "add_all(5,10,15,20)\n",
      "\n",
      "       >flexible arguments : **kwargs\n",
      "\n",
      "print_all(name=\"david nishimoto\", employer=\"esi\")\n",
      "\n",
      "def print_all(**kwargs):\n",
      "\tfor key, value in kwargs.items():\n",
      "\tprint(key+\\\":\\\"+value)\n",
      "\n",
      "**kwargs becomes a key/value pair in a dictionary\n",
      "\n",
      "\n",
      "  Sample  > using default parameters\n",
      "\n",
      "# Define shout_echo\n",
      "def shout_echo(word1, echo=1):\n",
      "    \"\"\"Concatenate echo copies of word1 and three\n",
      "     exclamation marks at the end of the string.\"\"\"\n",
      "\n",
      "    # Concatenate echo copies of word1 using *: echo_word\n",
      "    echo_word = word1*echo\n",
      "\n",
      "    # Concatenate '!!!' to echo_word: shout_word\n",
      "    shout_word = echo_word + '!!!'\n",
      "\n",
      "    # Return shout_word\n",
      "    return shout_word\n",
      "\n",
      "# Call shout_echo() with \"Hey\": no_echo\n",
      "no_echo = shout_echo(\"Hey\")\n",
      "\n",
      "# Call shout_echo() with \"Hey\" and echo=5: with_echo\n",
      "with_echo = shout_echo(\"Hey\",5)\n",
      "\n",
      "# Print no_echo and with_echo\n",
      "print(no_echo)\n",
      "print(with_echo)\n",
      "\n",
      "  Sample   default parameters and parameters by name\n",
      "\n",
      "# Define shout_echo\n",
      "def shout_echo(word1, echo=1, intense=False):\n",
      "    \"\"\"Concatenate echo copies of word1 and three\n",
      "    exclamation marks at the end of the string.\"\"\"\n",
      "\n",
      "    # Concatenate echo copies of word1 using *: echo_word\n",
      "    echo_word = word1 * echo\n",
      "\n",
      "    # Make echo_word uppercase if intense is True\n",
      "    if intense is True:\n",
      "        # Make uppercase and concatenate '!!!': echo_word_new\n",
      "        echo_word_new = echo_word.upper() + '!!!'\n",
      "    else:\n",
      "        # Concatenate '!!!' to echo_word: echo_word_new\n",
      "        echo_word_new = echo_word + '!!!'\n",
      "\n",
      "    # Return echo_word_new\n",
      "    return echo_word_new\n",
      "\n",
      "# Call shout_echo() with \"Hey\", echo=5 and intense=True: with_big_echo\n",
      "with_big_echo =shout_echo(word1=\"Hey\",echo=5,intense=True) \n",
      "\n",
      "# Call shout_echo() with \"Hey\" and intense=True: big_no_echo\n",
      "big_no_echo = shout_echo(word1=\"Hey\",intense=True)\n",
      "\n",
      "# Print values\n",
      "print(with_big_echo)\n",
      "print(big_no_echo)\n",
      "\n",
      "  Sample  > passing a tuple *args\n",
      "\n",
      "# Define gibberish\n",
      "def gibberish(*args):\n",
      "    \"\"\"Concatenate strings in *args together.\"\"\"\n",
      "\n",
      "    # Initialize an empty string: hodgepodge\n",
      "    hodgepodge=\"\"\n",
      "\n",
      "    # Concatenate the strings in args\n",
      "    for word in args:\n",
      "        hodgepodge += word\n",
      "\n",
      "    # Return hodgepodge\n",
      "    return hodgepodge\n",
      "\n",
      "# Call gibberish() with one string: one_word\n",
      "one_word = gibberish(\"luke\")\n",
      "\n",
      "# Call gibberish() with five strings: many_words\n",
      "many_words = gibberish(\"luke\", \"leia\", \"han\", \"obi\", \"darth\")\n",
      "\n",
      "# Print one_word and many_words\n",
      "print(one_word)\n",
      "print(many_words)\n",
      "\n",
      "  >Sample  > passing key value pairs\n",
      "\n",
      "# Define report_status\n",
      "def report_status(**kwargs):\n",
      "    \"\"\"Print out the status of a movie character.\"\"\"\n",
      "\n",
      "    print(\"\\nBEGIN: REPORT\\n\")\n",
      "\n",
      "    # Iterate over the key-value pairs of kwargs\n",
      "    for key, value in kwargs.items():\n",
      "        # Print out the keys and values, separated by a colon ':'\n",
      "        print(key + \": \" + value)\n",
      "\n",
      "    print(\"\\nEND REPORT\")\n",
      "\n",
      "# First call to report_status()\n",
      "report_status(name=\"luke\", affiliation=\"jedi\", status=\"missing\")\n",
      "\n",
      "# Second call to report_status()\n",
      "report_status(name=\"anakin\", affiliation=\"sith lord\", status=\"deceased\")\n",
      "\n",
      "\n",
      "   Bringing it together\n",
      "\n",
      "\n",
      " >Sample   > count tweets by language\n",
      "\n",
      "# Define count_entries()\n",
      "def count_entries(df, col_name):\n",
      "    \"\"\"Return a dictionary with counts of\n",
      "    occurrences as value for each key.\"\"\"\n",
      "\n",
      "    # Initialize an empty dictionary: cols_count\n",
      "    cols_count = {}\n",
      "\n",
      "    # Extract column from DataFrame: col\n",
      "    col = df[col_name]\n",
      "    \n",
      "    # Iterate over the column in DataFrame\n",
      "    for entry in col:\n",
      "\n",
      "        # If entry is in cols_count, add 1\n",
      "        if entry in cols_count.keys():\n",
      "            cols_count[entry] += 1\n",
      "\n",
      "        # Else add the entry to cols_count, set the value to 1\n",
      "        else:\n",
      "            cols_count[entry] = 1\n",
      "\n",
      "    # Return the cols_count dictionary\n",
      "    return cols_count\n",
      "\n",
      "# Call count_entries(): result1\n",
      "result1 = count_entries(tweets_df,'lang')\n",
      "\n",
      "# Call count_entries(): result2\n",
      "result2 = count_entries(tweets_df,'source')\n",
      "\n",
      "# Print result1 and result2\n",
      "print(result1)\n",
      "print(result2)\n",
      "\n",
      "\n",
      "   sample  > passing multiple parameters\n",
      "\n",
      "# Define count_entries()\n",
      "def count_entries(df, *args):\n",
      "    \"\"\"Return a dictionary with counts of\n",
      "    occurrences as value for each key.\"\"\"\n",
      "    \n",
      "    #Initialize an empty dictionary: cols_count\n",
      "    cols_count = {}\n",
      "    \n",
      "    # Iterate over column names in args\n",
      "    for col_name in args:\n",
      "    \n",
      "        # Extract column from DataFrame: col\n",
      "        col = df[col_name]\n",
      "    \n",
      "        # Iterate over the column in DataFrame\n",
      "        for entry in col:\n",
      "    \n",
      "            # If entry is in cols_count, add 1\n",
      "            if entry in cols_count.keys():\n",
      "                cols_count[entry] += 1\n",
      "    \n",
      "            # Else add the entry to cols_count, set the value to 1\n",
      "            else:\n",
      "                cols_count[entry] = 1\n",
      "\n",
      "    # Return the cols_count dictionary\n",
      "    return cols_count\n",
      "\n",
      "# Call count_entries(): result1\n",
      "result1 = count_entries(tweets_df, 'lang')\n",
      "\n",
      "# Call count_entries(): result2\n",
      "result2 = count_entries(tweets_df, 'lang', 'source')\n",
      "\n",
      "\n",
      "          Lambda function\n",
      "\n",
      "raise_to_power = lambda x, y: x**y\n",
      "\n",
      "raise_to_power(2,3)\n",
      "output 8\n",
      "\n",
      "anonymous functions\n",
      "function map takes two arguments map(func, seq)\n",
      "map() applies the function all elements in the sequence\n",
      "\n",
      "nums=[48,6,9,21,1]\n",
      "square_all = map(lambda num: num**2, nums)\n",
      "\n",
      "print(square_all) #map object\n",
      "print(list(square_all))\n",
      "\n",
      " >Sample  \n",
      "add_bangs=(lambda a: a+'!!!')\n",
      "add_bangs('hello')\n",
      "\n",
      " >Sample  > lambda function with parameters\n",
      "\n",
      "# Define echo_word as a lambda function: echo_word\n",
      "echo_word = (lambda word1,echo:word1*echo)\n",
      "\n",
      "# Call echo_word: result\n",
      "result = echo_word('hey',5)\n",
      "\n",
      "# Print result\n",
      "print(result)\n",
      "\n",
      "   >sample   > map function\n",
      "\n",
      "# Create a list of strings: spells\n",
      "spells = [\"protego\", \"accio\", \"expecto patronum\", \"legilimens\"]\n",
      "\n",
      "# Use map() to apply a lambda function over spells: shout_spells\n",
      "result = map(lambda item: item+'!!!', spells)\n",
      "\n",
      "# Convert shout_spells to a list: shout_spells_list\n",
      "shout_spells_list=list(result)\n",
      "\n",
      "# Print the result\n",
      "print(shout_spells_list)\n",
      "\n",
      "   Sample  > filter function\n",
      "\n",
      "# Create a list of strings: fellowship\n",
      "fellowship = ['frodo', 'samwise', 'merry', 'pippin', 'aragorn', 'boromir', 'legolas', 'gimli', 'gandalf']\n",
      "\n",
      "# Use filter() to apply a lambda function over fellowship: result\n",
      "result = filter(lambda member: len(member)>6, fellowship)\n",
      "\n",
      "# Convert result to a list: result_list\n",
      "result_list=list(result)\n",
      "\n",
      "# Print result_list\n",
      "print(result_list)\n",
      "\n",
      "  Sample  > reduce function\n",
      "\n",
      "The reduce() function is useful for performing some computation on a list and, unlike map() and filter(), returns a single value as a result. \n",
      "\n",
      "# Import reduce from functools\n",
      "from functools import reduce\n",
      "\n",
      "# Create a list of strings: stark\n",
      "stark = ['robb', 'sansa', 'arya', 'brandon', 'rickon']\n",
      "\n",
      "# Use reduce() to apply a lambda function over stark: result\n",
      "result = reduce(lambda item1, item2: item1 + item2, stark)\n",
      "\n",
      "# Print the result\n",
      "print(result)\n",
      "\n",
      "\n",
      "  Sample  > filter\n",
      "\n",
      "list1=['600809','600141','600329','1','2','3']\n",
      "\n",
      "df=pd.DataFrame({'STK_ID':list1})\n",
      "print(df.head())\n",
      "\n",
      "stk_list = ['600809','600141','600329']\n",
      "\n",
      "result=filter(lambda item: item in stk_list,df['STK_ID'])\n",
      "\n",
      "print(list(result))\n",
      "\n",
      "\n",
      "       Error Handling\n",
      "\n",
      "try-except clause\n",
      "1. runs the code following try\n",
      "2. if it can't run the code then it runs the code after except\n",
      "\n",
      "\n",
      "def sqrt(x):\n",
      "\ttry:\n",
      "\t\treturn x**0.5\n",
      "\texcept:\n",
      "\t\tprint(\"x must be an int or float\")\n",
      "\n",
      "\n",
      "Exception types:\n",
      "TypeError\n",
      "UnboundLocalError\n",
      "UnicodeError\n",
      "\n",
      "Raising an error\n",
      "\n",
      "def sqrt(x):\n",
      "\tif x<0:\n",
      "\t\traise ValueError(\"x must be non-negative\")\n",
      "\n",
      "\ttry:\n",
      "\t\treturn x**0.5\n",
      "\texcept:\n",
      "\t\tprint(\"x must be an int or float\")\n",
      "\n",
      "\n",
      "   Sample  > error handling\n",
      "\n",
      "# Define shout_echo\n",
      "def shout_echo(word1, echo=1):\n",
      "    \"\"\"Concatenate echo copies of word1 and three\n",
      "    exclamation marks at the end of the string.\"\"\"\n",
      "\n",
      "    # Initialize empty strings: echo_word, shout_words\n",
      "    \n",
      "    echo_word=''\n",
      "    shout_words=''\n",
      "\n",
      "    # Add exception handling with try-except\n",
      "    try:\n",
      "        # Concatenate echo copies of word1 using *: echo_word\n",
      "        echo_word = word1*echo\n",
      "\n",
      "        # Concatenate '!!!' to echo_word: shout_words\n",
      "        shout_words = echo_word+\"!!!\"\n",
      "    except:\n",
      "        # Print error message\n",
      "        print(\"word1 must be a string and echo must be an integer.\")\n",
      "\n",
      "    # Return shout_words\n",
      "    return shout_words\n",
      "\n",
      "# Call shout_echo\n",
      "shout_echo(\"particle\", echo=\"accelerator\")\n",
      "\n",
      "  >Sample  > raise error\n",
      "\n",
      "# Define shout_echo\n",
      "def shout_echo(word1, echo=1):\n",
      "    \"\"\"Concatenate echo copies of word1 and three\n",
      "    exclamation marks at the end of the string.\"\"\"\n",
      "\n",
      "    # Raise an error with raise\n",
      "    if echo<0:\n",
      "        raise ValueError(\"echo must be greater than or equal to 0\")\n",
      "\n",
      "    # Concatenate echo copies of word1 using *: echo_word\n",
      "    echo_word = word1 * echo\n",
      "\n",
      "    # Concatenate '!!!' to echo_word: shout_word\n",
      "    shout_word = echo_word + '!!!'\n",
      "\n",
      "    # Return shout_word\n",
      "    return shout_word\n",
      "\n",
      "# Call shout_echo\n",
      "shout_echo(\"particle\", echo=5)\n",
      "\n",
      "    Bringing it together\n",
      "\n",
      " Sample   use the filter function to search the first 2 character for RT\n",
      "\n",
      "\n",
      "# Select retweets from the Twitter DataFrame: result\n",
      "result = filter(lambda x: x[0:2]=='RT', tweets_df['text'])\n",
      "\n",
      "# Create list from filter object result: res_list\n",
      "res_list=list(result)\n",
      "\n",
      "# Print all retweets in res_list\n",
      "for tweet in res_list:\n",
      "    print(tweet)\n",
      "\n",
      "  >Sample  > exception handling\n",
      "\n",
      "# Define count_entries()\n",
      "def count_entries(df, col_name='lang'):\n",
      "    \"\"\"Return a dictionary with counts of\n",
      "    occurrences as value for each key.\"\"\"\n",
      "\n",
      "    # Initialize an empty dictionary: cols_count\n",
      "    cols_count = {}\n",
      "\n",
      "    # Add try block\n",
      "    try:\n",
      "        # Extract column from DataFrame: col\n",
      "        col = df[col_name]\n",
      "        \n",
      "        # Iterate over the column in dataframe\n",
      "        for entry in col:\n",
      "    \n",
      "            # If entry is in cols_count, add 1\n",
      "            if entry in cols_count.keys():\n",
      "                cols_count[entry] += 1\n",
      "            # Else add the entry to cols_count, set the value to 1\n",
      "            else:\n",
      "                cols_count[entry] = 1\n",
      "    \n",
      "        # Return the cols_count dictionary\n",
      "        return cols_count\n",
      "\n",
      "    # Add except block\n",
      "    except:\n",
      "       print('The DataFrame does not have a ' + col_name + ' column.')\n",
      "\n",
      "# Call count_entries(): result1\n",
      "result1 = count_entries(tweets_df, 'lang')\n",
      "\n",
      "# Print result1\n",
      "print(result1)\n",
      "\n",
      "  Sample  > raise error\n",
      "\n",
      "# Define count_entries()\n",
      "def count_entries(df, col_name='lang'):\n",
      "    \"\"\"Return a dictionary with counts of\n",
      "    occurrences as value for each key.\"\"\"\n",
      "    \n",
      "    # Raise a ValueError if col_name is NOT in DataFrame\n",
      "    if col_name not in df.columns:\n",
      "        raise ValueError('The DataFrame does not have a ' + col_name + ' column.')\n",
      "\n",
      "    # Initialize an empty dictionary: cols_count\n",
      "    cols_count = {}\n",
      "    \n",
      "    # Extract column from DataFrame: col\n",
      "    col = df[col_name]\n",
      "    \n",
      "    # Iterate over the column in DataFrame\n",
      "    for entry in col:\n",
      "\n",
      "        # If entry is in cols_count, add 1\n",
      "        if entry in cols_count.keys():\n",
      "            cols_count[entry] += 1\n",
      "            # Else add the entry to cols_count, set the value to 1\n",
      "        else:\n",
      "            cols_count[entry] = 1\n",
      "        \n",
      "        # Return the cols_count dictionary\n",
      "    return cols_count\n",
      "\n",
      "# Call count_entries(): result1\n",
      "result1=count_entries(tweets_df,'lang')\n",
      "\n",
      "# Print result1\n",
      "print(result1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\sql optimization.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\sql optimization.txt\n",
      "nations, cities, earthquakes\n",
      "\n",
      "players\n",
      "playerstats\n",
      "teams\n",
      "\n",
      "customers->Orders\n",
      "\n",
      "select ps.Team, count(p.PlayerName) as NonNthAmerPlayers from\n",
      "PlayerStats ps inner join(select PlayerName from Players where Country<>'USA' or Country <>'Canada')\n",
      "p on p.PlayerName=ps.PlayerName\n",
      "group by ps.Team\n",
      "having Count(p.PlayerName)\n",
      ">=24 Order by NonNthAmerPlayers desc\n",
      "\n",
      "\n",
      "   Suggestions:\n",
      "\n",
      "1. use upper case for all sql syntax\n",
      "2. create a new line for each major processing syntax: select, from where\n",
      "3. indent code: sub-queries, on statements, and/or conditions\n",
      "to avoid long single lines of code, for example, several column names\n",
      "4. complete the query with a semi-colon (;)\n",
      "5. alias where required, using as\n",
      "\n",
      "comment blocks of the query\n",
      "\n",
      " >\n",
      "\n",
      "SELECT PlayerName, Country,\n",
      "    ROUND(Weight_kg/SQUARE(Height_cm/100),2) BMI\n",
      "FROM Players \n",
      "WHERE Country = 'USA'\n",
      "    OR Country = 'Canada'\n",
      "ORDER BY BMI;\n",
      "\n",
      "\n",
      "      > ALIASING\n",
      "\n",
      "short name to entity\n",
      "1. temporary and exist during the running of query\n",
      "2. columns\n",
      "3. sub-queries\n",
      "\n",
      "makes the query easier to read\n",
      "avoid repetitive use of long table or column names\n",
      "easily identify join tables and associated columns\n",
      "identify new columns\n",
      "identify sub-queries\n",
      "avoid ambiguity when columns from joined tables share the same name\n",
      "rename columns\n",
      "\n",
      "\n",
      "SELECT\n",
      "\tCountryName,\n",
      "\tCode2,\n",
      "\tCapital,\n",
      "\tPop2017\n",
      "FROM Natons\n",
      "INNER JOIN Cities\n",
      "\tON Capital=CityName\n",
      "\n",
      "\n",
      "SELECT Country,\n",
      "\tNearestPop as City\n",
      "\tMAX(Magnitude) as MaxMagnitude\n",
      "FROM Earthquakes\n",
      "GROUP BY Country, NearestPop;\n",
      "\n",
      "\n",
      "SELECT n.CountryName as Country\n",
      "\tn.Capital,\n",
      "\te.MaxMagnitude\n",
      "FROM Nations n\n",
      "INNER JOIN\n",
      "\t(SELECT Country, NearestPop as City,\n",
      "\tMAX(Magnitude) as MaxMagnitude\n",
      "\tFROM Earthquakes\n",
      "\tGROUP BY Country, NearestPop) e\n",
      "\tON n.Code2 = e.Country AND n.Capital=e.City\n",
      "\n",
      "\n",
      "SELECT p.PlayerName, p.Country,\n",
      "         SUM(ps.TotalPoints) AS TotalPoints  \n",
      "FROM PlayerStats ps\n",
      "INNER JOIN Players p\n",
      "   ON ps.PlayerName = p.PlayerName\n",
      "WHERE p.Country = 'Australia'\n",
      "GROUP BY p.PlayerName, p.Country\n",
      "\n",
      "or\n",
      "\n",
      "SELECT Team, \n",
      "   ROUND(AVG(BMI),2) AS AvgTeamBMI -- Alias the new column\n",
      "FROM PlayerStats as ps -- Alias PlayerStats table\n",
      "INNER JOIN\n",
      "\t\t(SELECT PlayerName, Country,\n",
      "\t\t\tWeight_kg/SQUARE(Height_cm/100) BMI\n",
      "\t\t FROM Players) as p -- Alias the sub-query\n",
      "             -- Alias the joining columns\n",
      "\tON ps.PlayerName = p.PlayerName \n",
      "GROUP BY Team\n",
      "HAVING AVG(BMI) >= 25;\n",
      "\n",
      "\n",
      "      Query order\n",
      "\n",
      "SELECT Country, Place, Magnitude\n",
      "FROM Earthquakes\n",
      "WHERE Magnitude >=9\n",
      "ORDER BY Magnitude DESC\n",
      "\n",
      "ORDER OF THE DATABASE\n",
      "1. FROM\n",
      "2. ON\n",
      "3. JOIN\n",
      "4. WHERE\n",
      "5. GROUP BY\n",
      "6. HAVING\n",
      "7. SELECT\n",
      "8. DISTINCT\n",
      "9. ORDER BY\n",
      "10. TOP\n",
      "\n",
      "find, merge, then select\n",
      "\n",
      "\n",
      "\n",
      "SELECT Date, Place, NearestPop, Magnitude\n",
      "FROM Earthquakes\n",
      "WHERE Country = 'NZ'\n",
      "\tAND Magnitude >= 7.5\n",
      "ORDER BY Magnitude DESC;\n",
      "\n",
      "\n",
      " SELECT Date, Place ,NearestPop, Magnitude \n",
      "FROM Earthquakes WHERE Country = 'JP' AND Magnitude >= 8\n",
      "ORDER BY Magnitude DESC;\n",
      "\n",
      "SELECT n.CountryName AS Country\n",
      "\t,e.NearestPop AS ClosestCity\n",
      "    ,e.Date\n",
      "    ,e.Magnitude\n",
      "FROM Nations AS n\n",
      "INNER JOIN Earthquakes AS e\n",
      "\tON n.Code2 = e.Country\n",
      "WHERE e.Magnitude >= 9\n",
      "ORDER BY e.Magnitude DESC;\n",
      "\n",
      "SELECT n.CountryName AS Country\n",
      "\t,e.Place AS ClosestCity\n",
      "    ,e.Date\n",
      "    ,e.Magnitude\n",
      "FROM Nations AS n\n",
      "INNER JOIN Earthquakes AS e\n",
      "\tON n.Code2 = e.Country\n",
      "WHERE e.Magnitude >= 9\n",
      "ORDER BY e.Magnitude DESC;\n",
      "\n",
      "\n",
      "   > filtering with where\n",
      "\n",
      "select * from playerStats\n",
      "where position='SG'\n",
      "\n",
      "where is processed after from\n",
      "\n",
      "\n",
      "select PlayerName,\n",
      "Team,\n",
      "TotalRebounds\n",
      "from\n",
      "(select PlayerName, Team,\n",
      "(DRebound+ORebound) as TotalRebounds\n",
      "from PlayerStats)tr\n",
      "where TotalRebounds>=1000\n",
      "order by TotalRebounds desc;\n",
      "\n",
      "use a subquery to get totals\n",
      "not using a subquery will increase the time it takes for the query to run\n",
      "applying functions to columns in the where filter condition could increase query times\n",
      "avoid using calculations or functions in the where conditions\n",
      "\n",
      "  \n",
      "\n",
      "-- First query\n",
      "\n",
      "SELECT PlayerName, \n",
      "    Team, \n",
      "    Position,\n",
      "    AvgRebounds\n",
      "    FROM\n",
      "    (\n",
      "        select\n",
      "        PlayerName,\n",
      "        Team,\n",
      "        Position,\n",
      "        (DRebound+ORebound)/CAST(GamesPlayed AS numeric) AS AvgRebounds\n",
      "        FROM PlayerStats\n",
      "    )tr\n",
      "WHERE AvgRebounds >= 12;\n",
      "\n",
      "\n",
      "-- Second query\n",
      "\n",
      "-- Add the new column to the select statement\n",
      "SELECT PlayerName, \n",
      "       Team, \n",
      "       Position, \n",
      "       AvgRebounds -- Add the new column\n",
      "FROM\n",
      "     -- Sub-query starts here                             \n",
      "\t(SELECT \n",
      "      PlayerName, \n",
      "      Team, \n",
      "      Position,\n",
      "      -- Calculate average total rebounds\n",
      "     (DRebound+ORebound)/CAST(GamesPlayed AS numeric) AS AvgRebounds\n",
      "\t FROM PlayerStats) tr\n",
      "where AvgRebounds >= 12; -- Filter rows\n",
      "\n",
      "SELECT PlayerName, \n",
      "      Country,\n",
      "      College, \n",
      "      DraftYear, \n",
      "      DraftNumber \n",
      "FROM Players \n",
      "--WHERE UPPER(LEFT(College,5)) LIKE 'LOU%';\n",
      "WHERE College LIKE 'Louisiana%'; -- Add the wildcard filterv\n",
      "                   -- Add the new wildcard filter\n",
      "\n",
      "\n",
      "    filtering with Having\n",
      "\n",
      "precedence\n",
      "\n",
      "from -> where -> having -> select\n",
      "\n",
      "do not use having to filter individual or ungrouped rows\n",
      "\n",
      "applying having filter to a numeric column must use an aggregate function\n",
      "\n",
      "select\n",
      "team,\n",
      "sum(DRebound+ORebound) as TotRebounds,\n",
      "sum(DRebound) TotalDef,\n",
      "sum(ORebound) TotalOff,\n",
      "from PlayerStats\n",
      "group by Team\n",
      "having sum(ORebound)>=1000\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "SELECT Country, COUNT(*) CountOfPlayers \n",
      "FROM Players\n",
      "where Country \n",
      "    IN ('Argentina','Brazil','Dominican Republic'\n",
      "        ,'Puerto Rico')\n",
      "GROUP BY Country\n",
      "\n",
      "\n",
      "SELECT Team, \n",
      "\tSUM(TotalPoints) AS TotalPFPoints\n",
      "FROM PlayerStats\n",
      "-- Filter for only rows with power forwards\n",
      "where Position='PF'\n",
      "GROUP BY Team\n",
      "-- Filter for total points greater than 3000\n",
      "having sum(TotalPoints)>3000;\n",
      "\n",
      "\n",
      "   > interrogation after select\n",
      "\n",
      "processing order\n",
      "1. from\n",
      "2. on\n",
      "3. join\n",
      "4. where\n",
      "5. group by\n",
      "6. having\n",
      "7. select\n",
      "8. distinct\n",
      "9. order by\n",
      "10. top 5 or top 1 percent\n",
      "\n",
      "\n",
      "select only the columns you need if the table has millions of rows\n",
      "\n",
      "select * in joins returns duplicates of joining columns\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "SELECT \n",
      "latitude,longitude,magnitude, depth, NearestPop\n",
      "\n",
      "FROM Earthquakes\n",
      "WHERE Country = 'PG' -- Papua New Guinea country code\n",
      "\tOR Country = 'ID'; -- Indonesia country code\n",
      "\n",
      "\n",
      "SELECT top 10 -- Limit the number of rows to ten\n",
      "      Latitude,\n",
      "      Longitude,\n",
      "\t  Magnitude,\n",
      "\t  Depth,\n",
      "\t  NearestPop\n",
      "FROM Earthquakes\n",
      "WHERE Country = 'PG'\n",
      "\tOR Country = 'ID'\n",
      "order by Depth; -- Order results from shallowest to deepest\n",
      "\n",
      "\n",
      "SELECT top 25 percent -- Limit rows to the upper quartile\n",
      "       Latitude,\n",
      "       Longitude,\n",
      "\t   Magnitude,\n",
      "\t   Depth,\n",
      "\t   NearestPop\n",
      "FROM Earthquakes\n",
      "WHERE Country = 'PG'\n",
      "\tOR Country = 'ID'\n",
      "order by Magnitude desc; -- Order the results\n",
      "\n",
      "\n",
      "    Managing duplicates\n",
      "\n",
      "\n",
      "select PlayerName, Team from PlayerStats;\n",
      "\n",
      "player could have played for multiple teams in a season\n",
      "\n",
      "remove duplicates with distinct\n",
      "\n",
      "select distinct(PlayerName)\n",
      "from PlayerStats;\n",
      "\n",
      "we can get duplicates when we append queries to get one dataset\n",
      "\n",
      "\n",
      "remove duplicates with Union\n",
      "\n",
      "select FruitName, FruitType\n",
      "from Fruits1\n",
      "\n",
      "union\n",
      "\n",
      "select FruitName, FruitType\n",
      "from Fruits2\n",
      "\n",
      "\n",
      "show all rows with union all\n",
      "\n",
      "\n",
      "consider using union all if duplicate rows are ok, it does not make use of the internal sort\n",
      "\n",
      "distinct and union will make use of the internal sort.\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "\n",
      "SELECT NearestPop, -- Add the closest city\n",
      "\t\tCountry \n",
      "FROM Earthquakes\n",
      "WHERE Magnitude >= 8\n",
      "\tAND NearestPop IS NOT NULL\n",
      "ORDER BY NearestPop;\n",
      "\n",
      "--You want to know the closest city to earthquakes with a magnitude of 8 or higher.\n",
      "\n",
      "SELECT distinct(NearestPop),-- Remove duplicate city\n",
      "\t\tCountry\n",
      "FROM Earthquakes\n",
      "WHERE magnitude >= 8 -- Add filter condition \n",
      "\tAND NearestPop IS NOT NULL\n",
      "ORDER BY NearestPop;\n",
      "\n",
      "--Get the number of cities near earthquakes of magnitude 8 or more\n",
      "SELECT NearestPop, \n",
      "       Country, \n",
      "       count(NearestPop) NumEarthquakes -- Number of cities\n",
      "FROM Earthquakes\n",
      "WHERE Magnitude >= 8\n",
      "\tAND Country IS NOT NULL\n",
      "group by NearestPop, Country -- Group columns\n",
      "ORDER BY NumEarthquakes DESC;\n",
      "\n",
      "--You want a query that returns all cities listed in the Earthquakes\n",
      "\n",
      "SELECT CityName AS NearCityName, -- City name column\n",
      "\t   CountryCode\n",
      "FROM Cities\n",
      "\n",
      "union -- Append queries\n",
      "\n",
      "SELECT Capital AS NearCityName, -- Nation capital column\n",
      "       Code2 AS CountryCode\n",
      "FROM Nations;\n",
      "\n",
      "\n",
      "SELECT CityName AS NearCityName,\n",
      "\t   CountryCode\n",
      "FROM Cities\n",
      "\n",
      "union all -- Append queries\n",
      "\n",
      "SELECT Capital AS NearCityName,\n",
      "       Code2 AS CountryCode  -- Country code column\n",
      "FROM Nations;\n",
      "\n",
      "\n",
      "UNION ALL returns more rows than UNION because it does not remove duplicates\n",
      "\n",
      "\n",
      "            subqueries\n",
      "\n",
      "running a query within a query\n",
      "\n",
      "a subquery is processed first before an outer query is processed\n",
      "\n",
      "\n",
      "select OrderID,\n",
      "\tCustomerID,\n",
      "\tNumDays\n",
      "from\n",
      "\t(select *,\n",
      "\t\tDateDiff(Day,OrderDate,ShippedDate) as NumDays\n",
      "\tfrom Orders) as o\n",
      "where NumDays >=35\n",
      "\n",
      "\n",
      "sub-query with Where\n",
      "\n",
      "     subquery where in\n",
      "\n",
      "select CustomerID\n",
      "\t,CompanyName\n",
      "from Customers\n",
      "Where CustomerID\n",
      "\tin (\tselect CustomerID\n",
      "\t\tfrom Orders\n",
      "\t\twhere Freight > 800);\n",
      "\n",
      "\n",
      "  > subquery inline\n",
      "\n",
      "select CustomerID,\n",
      "\tCompanyName,\n",
      "\t(select avg(Freight) from Orders o where c.CustomerID=o.CustomerID) as AvgFreight\n",
      "from Customers c;\n",
      "\n",
      "subquery contains a reference to the outer query.  subquery cannot run independently of the outer query.\n",
      "\n",
      "\n",
      "select CustomerID\n",
      "\t,CompanyName\n",
      "from Custoemrs\n",
      "where CustomerID in\n",
      "(\n",
      "select CustomerID\n",
      "from Orders\n",
      "where Freight>800);\n",
      "\n",
      "uncorrelated subquery do not contain a reference to the outer query\n",
      "\n",
      "\n",
      "correlated sub query executes for each row in the outer query.  can be inefficient because the correlated subquery executes for every row in the outer query.\n",
      "\n",
      "\n",
      "uncorrelated\n",
      "subquery executes only once and retures the results to the outer query\n",
      "\n",
      "\n",
      "select c.CustomerID\n",
      "\t,c.CompanyName\n",
      "\t,avg(o.Freight)\n",
      "from Customers c\n",
      "inner join Orders o\n",
      "\ton c.CustomerID=o.CustomerID\n",
      "group by c.CustomerID,\n",
      "\tc.CompanyName;\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "SELECT UNStatisticalRegion,\n",
      "       CountryName \n",
      "FROM Nations\n",
      "WHERE Code2 -- Country code for outer query \n",
      "         IN (SELECT Country -- Country code for sub-query\n",
      "             FROM Earthquakes\n",
      "             WHERE depth >= 400 ) -- Depth filter\n",
      "ORDER BY UNStatisticalRegion;\n",
      "\n",
      "\n",
      "SELECT UNContinentRegion,\n",
      "       CountryName, \n",
      "        (SELECT avg(magnitude) -- Add average magnitude\n",
      "        FROM Earthquakes e \n",
      "         \t  -- Add country code reference\n",
      "        WHERE n.Code2 = e.Country) AS AverageMagnitude \n",
      "FROM Nations n\n",
      "ORDER BY UNContinentRegion DESC, \n",
      "         AverageMagnitude DESC;\n",
      "\n",
      "\n",
      "SELECT\n",
      "\tn.CountryName,\n",
      "\t (SELECT MAX(c.Pop2017) -- Add 2017 population column\n",
      "\t FROM Cities AS c \n",
      "                       -- Outer query country code column\n",
      "\t WHERE c.CountryCode = n.Code2) AS BiggestCity\n",
      "FROM Nations AS n; -- Outer query table\n",
      "\n",
      "or\n",
      "\n",
      "SELECT n.CountryName, \n",
      "       c.BiggestCity \n",
      "FROM Nations AS n\n",
      "inner join -- Join the Nations table and sub-query\n",
      "    (SELECT CountryCode, \n",
      "     MAX(Pop2017) AS BiggestCity \n",
      "     FROM Cities\n",
      "     GROUP BY CountryCode) AS c\n",
      "ON n.Code2 = c.CountryCode; -- Add the joining columns\n",
      "\n",
      "\n",
      "     presence and absence\n",
      "\n",
      "is data presence or absence in another table\n",
      "\n",
      "intersect or EXCEPT\n",
      "\n",
      "customers and orders\n",
      "\n",
      "\n",
      "select CustomerID\n",
      "from Customers\n",
      "intersect\n",
      "select CustomerID\n",
      "from Orders;\n",
      "\n",
      "\n",
      "select CustomerID\n",
      "from Customers\n",
      "accept\n",
      "select CustomerID\n",
      "from Orders;\n",
      "\n",
      "\n",
      "great for data interrogation\n",
      "remove duplicates from the returned results\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "SELECT Capital\n",
      "FROM Nations -- Table with capital cities\n",
      "\n",
      "INTERSECT -- Add the operator to compare the two queries\n",
      "\n",
      "SELECT NearestPop -- Add the city name column\n",
      "FROM Earthquakes;\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "SELECT Code2 -- Add the country code column\n",
      "FROM Nations\n",
      "\n",
      "EXCEPT -- Add the operator to compare the two queries\n",
      "\n",
      "SELECT Country\n",
      "FROM Earthquakes; -- Table with country codes\n",
      "\n",
      "You want to know which countries have no recorded earthquakes.\n",
      "\n",
      "  >\n",
      "\n",
      "SELECT CountryName\n",
      "FROM Nations -- Table from Earthquakes database\n",
      "\n",
      "INTERSECT -- Operator for the intersect between tables\n",
      "\n",
      "SELECT Country\n",
      "FROM Players; -- Table from NBA Season 2017-2018 database\n",
      "\n",
      "\n",
      "     alternative methods\n",
      "\n",
      "not in\n",
      "not exists\n",
      "exists\n",
      "\n",
      "\n",
      "\n",
      "select CustomerID,\n",
      "\tCompanyName,\n",
      "\tContactName\n",
      "From Customers c\n",
      "where exists\n",
      "\t(select 1\n",
      "\tfrom Orders o\n",
      "\twhere c.CustomerID=o.CustomerID);\n",
      "\n",
      "health at scale\n",
      "\n",
      "\n",
      "select CustomerID,\n",
      "\tCompanyName,\n",
      "\tContactName\n",
      "From Customers c\n",
      "where CustomerID in\n",
      "\t(select CustomerID\n",
      "\tfrom Orders o\n",
      "\twhere c.CustomerID=o.CustomerID);\n",
      "\n",
      "exists will stop searning the subquery when the condition is true\n",
      "\n",
      "in collects all the results from a subquery before passing to the outer query.\n",
      "\n",
      "consider using exists instead of in with a sub-query\n",
      "\n",
      "\n",
      "\n",
      "select UNStatisticalRegion as UN_Region\n",
      "\t,CountryName\n",
      "\t,Capital\n",
      "from Nations\n",
      "where Capital not in\n",
      "\t(select NearestPop\n",
      "\tfrom Earthquakes\n",
      "\twhere NearestPop is not null)\n",
      "\n",
      "NearestPop with a null will cause the subquery to return no results\n",
      "\n",
      "\n",
      "for Exists results can contain any column from the outer query and in any order\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "\n",
      "-- First attempt\n",
      "SELECT CountryName,\n",
      "       Pop2017, -- 2017 country population\n",
      "\t  Capital, -- Capital city\t   \n",
      "       WorldBankRegion\n",
      "FROM Nations\n",
      "WHERE Capital in -- Add the operator to compare queries\n",
      "        (SELECT NearestPop \n",
      "\t     FROM Earthquakes);\n",
      "\n",
      "or\n",
      "\n",
      "-- Second attempt\n",
      "SELECT CountryName,   \n",
      "\t   Capital,\n",
      "       Pop2016, -- 2016 country population\n",
      "       WorldBankRegion\n",
      "FROM Nations AS n\n",
      "WHERE exists -- Add the operator to compare queries\n",
      "\t  (SELECT 1\n",
      "\t   FROM Earthquakes AS e\n",
      "\t   WHERE n.Capital = e.NearestPop); -- Columns being compared\n",
      "\n",
      "\n",
      "SELECT WorldBankRegion,\n",
      "       CountryName\n",
      "FROM Nations\n",
      "WHERE Code2 not in -- Add the operator to compare queries\n",
      "\t(SELECT CountryCode -- Country code column\n",
      "\t FROM Cities);\n",
      "\n",
      "\n",
      "\n",
      "SELECT WorldBankRegion,\n",
      "       CountryName,\n",
      "\t   Code2,\n",
      "       Capital, -- Country capital column\n",
      "\t   Pop2017\n",
      "FROM Nations AS n\n",
      "WHERE not exists -- Add the operator to compare queries\n",
      "\t(SELECT 1\n",
      "\t FROM Cities AS c\n",
      "\t WHERE n.Code2 = c.CountryCode); \n",
      "\n",
      "\n",
      "SELECT WorldBankRegion,\n",
      "       CountryName,\n",
      "       Capital -- Capital city name column\n",
      "FROM Nations\n",
      "WHERE Capital NOT IN\n",
      "\t(SELECT NearestPop -- City name column\n",
      "     FROM Earthquakes);\n",
      "\n",
      "\n",
      "      ALTERNATIVE METHODS\n",
      "\n",
      "joins can show if data is missing\n",
      "\n",
      "inner join matches in both tables and the check for presence of data\n",
      "\n",
      "left join matches left query and matches in the right query\n",
      "\n",
      "exclusive left join returns only left query not presence in the right query\n",
      "\n",
      "intersect presence\n",
      "except absence\n",
      "\n",
      "great for data interrogation\n",
      "number of columns must be the same\n",
      "\n",
      "exists and not exists\n",
      "\n",
      "subquery will stop searching as soon as it evaluates to true\n",
      "\n",
      "in and not in\n",
      "1. results can only contain columns from the outer query\n",
      "2. no results returned because of the way not in handles nulls in the sub-query\n",
      "\n",
      "left join with is null creates exclusive join\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "-- Initial query\n",
      "SELECT TeamName,\n",
      "       TeamCode,\n",
      "\t   City\n",
      "FROM Teams AS t -- Add table\n",
      "WHERE  Exists-- Operator to compare queries\n",
      "      (SELECT 1 \n",
      "\t  FROM Earthquakes AS e -- Add table\n",
      "\t  WHERE t.City = e.NearestPop)\n",
      "\n",
      "-- Second query\n",
      "SELECT t.TeamName,\n",
      "       t.TeamCode,\n",
      "\t   t.City,\n",
      "\t   e.Date,\n",
      "\t   e.Place, -- Place description\n",
      "\t   e.Country -- Country code\n",
      "FROM Teams AS t\n",
      "INNER JOIN  Earthquakes AS e -- Operator to compare tables\n",
      "\t  ON t.City = e.NearestPop\n",
      "\n",
      "\n",
      "-- First attempt\n",
      "SELECT c.CustomerID,\n",
      "       c.CompanyName,\n",
      "\t   c.ContactName,\n",
      "\t   c.ContactTitle,\n",
      "\t   c.Phone \n",
      "FROM Customers c\n",
      "LEFT OUTER JOIN Orders o -- Joining operator\n",
      "\tON c.CustomerID = o.CustomerID -- Joining columns\n",
      "WHERE c.Country = 'France';\n",
      "\n",
      "\n",
      "-- Second attempt\n",
      "SELECT c.CustomerID,\n",
      "       c.CompanyName,\n",
      "\t   c.ContactName,\n",
      "\t   c.ContactTitle,\n",
      "\t   c.Phone \n",
      "FROM Customers c\n",
      "LEFT OUTER JOIN Orders o\n",
      "\tON c.CustomerID = o.CustomerID\n",
      "WHERE c.Country = 'France'\n",
      "\tAND o.OrderID is null; -- Filter condition\n",
      "\n",
      "\n",
      "An inclusive LEFT OUTER JOIN returns all the rows in the left query, whereas an exclusive LEFT OUTER JOIN returns only rows in the left query that are absent in the right query.\n",
      "\n",
      "\n",
      "     Time Statistics\n",
      "\n",
      "\n",
      "STATISTICS TIME\n",
      "\n",
      "computes the milliseconds to parse, compile and execute the query\n",
      "1. cpu time: time taken by server processors to process the query\n",
      "2. elapsed time: total duration of the  query\n",
      "a. affected by the network time\n",
      "b. load on the server\n",
      "c. best measure to use\n",
      "\n",
      "set statistics time on\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "-- Query 1\n",
      "SELECT * \n",
      "FROM Teams\n",
      "-- Sub-query 1\n",
      "WHERE City in -- Sub-query filter operator\n",
      "      (SELECT CityName \n",
      "       FROM Cities) -- Table from Earthquakes database\n",
      "-- Sub-query 2\n",
      "   AND City in -- Sub-query filter operator\n",
      "\t   (SELECT CityName \n",
      "\t    FROM Cities\n",
      "\t\tWHERE CountryCode IN ('US','CA'))\n",
      "-- Sub-query 3\n",
      "    AND City in -- Sub-query filter operator\n",
      "        (SELECT CityName \n",
      "         FROM Cities\n",
      "\t     WHERE Pop2017 >2000000);\n",
      "\n",
      "\n",
      "-- Query 2\n",
      "SELECT * \n",
      "FROM Teams AS t\n",
      "WHERE EXISTS -- Sub-query filter operator\n",
      "\t(SELECT 1 \n",
      "     FROM Cities AS c\n",
      "     WHERE t.City = c.CityName -- Columns being compared\n",
      "        AND c.CountryCode IN ('US','CA')\n",
      "          AND c.Pop2017 > 2000000);\n",
      "\n",
      "\n",
      "     Page read statistics\n",
      "\n",
      "analyzing disks io\n",
      "\n",
      "all data in either memory or on the disk is stored in 8 kilobyte size pages.\n",
      "\n",
      "one page can store many rows or one value could span multiple pages\n",
      "\n",
      "A page can only belong to one table\n",
      "\n",
      "sql server works with pages cached in memory\n",
      "\n",
      "if a page is not cached in memory it is read from disk and cached in memory\n",
      "\n",
      "       set statistics io on\n",
      "\n",
      "1. logical reads are imported for each table\n",
      "2. number of 8 kilobyte pages read per table\n",
      "\n",
      "the more pages that need read the slower a query will run.\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "-- Query 2\n",
      "SELECT * \n",
      "FROM Teams AS t\n",
      "WHERE EXISTS -- Sub-query filter operator\n",
      "\t(SELECT 1 \n",
      "     FROM Cities AS c\n",
      "     WHERE t.City = c.CityName -- Columns being compared\n",
      "        AND c.CountryCode IN ('US','CA')\n",
      "          AND c.Pop2017 > 2000000);\n",
      "\n",
      "\n",
      "-- Example 2\n",
      "SELECT c.CustomerID,\n",
      "       c.CompanyName,\n",
      "       COUNT(o.CustomerID)\n",
      "FROM Customers AS c\n",
      "INNER JOIN Orders AS o -- Join operator\n",
      "    ON c.CustomerID = o.CustomerID\n",
      "WHERE o.ShipCity IN -- Shipping destination column\n",
      "     ('Berlin','Bern','Bruxelles','Helsinki',\n",
      "\t 'Lisboa','Madrid','Paris','London')\n",
      "GROUP BY c.CustomerID,\n",
      "         c.CompanyName;\n",
      "\n",
      "  > indexes\n",
      "\n",
      "clustered\n",
      "1. table data pages are stored in order by the columns with the index\n",
      "2. only one allower per table\n",
      "3. speeds up search operations\n",
      "4. b-tree structure\n",
      "\n",
      "non-clustered\n",
      "1. text book with an index at the back\n",
      "2. contains ordered pointers to the data\n",
      "3. structure contains an ordered layer of index pointers to unordered table data pages\n",
      "4. a table can have more than one clustered index\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "-- Query 1\n",
      "SELECT *\n",
      "FROM Cities\n",
      "WHERE CountryCode = 'RU' -- Country code\n",
      "\t\tOR CountryCode = 'CN' -- Country code\n",
      "\n",
      "\n",
      "-- Query 2\n",
      "SELECT *\n",
      "FROM Cities\n",
      "WHERE CountryCode IN ('JM','NZ') -- Country codes\n",
      "\n",
      "\n",
      "  > execution plan\n",
      "\n",
      "optimized for the lowest code\n",
      "1. processor usage\n",
      "2. memory usage\n",
      "3. data page reads\n",
      "\n",
      "plan tells us:\n",
      "index used\n",
      "type of joins used\n",
      "location and relative cost of: filter conditions, sorting, aggregations\n",
      "\n",
      "\n",
      "execution plans are read right to left\n",
      "\n",
      "table scan (entire table is scanned)\n",
      "cluster index ( index used)\n",
      "\n",
      "\n",
      "  >\n",
      "\n",
      "SELECT CityName AS NearCityName,\n",
      "\t   CountryCode\n",
      "FROM Cities\n",
      "\n",
      "union  -- Append queries\n",
      "\n",
      "SELECT Capital AS NearCityName,\n",
      "       Code2 AS CountryCode\n",
      "FROM Nations;\n",
      "\n",
      " An execution plan does not provide information about the total duration a query takes to complete. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto\\python_files\\python_notes\\XGboost for linear regression problems.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto\\python_files\\python_notes\\XGboost for linear regression problems.txt\n",
      "     >XG boost for regression\n",
      "1. predicting contineous or real values\n",
      "\n",
      "Root mean squared error (RMSE)\n",
      "Mean absolute error (MAE)\n",
      "\n",
      "RMSE is \n",
      "error=actual - predicted\n",
      "squaredError=error**2\n",
      "Totaling the squared error\n",
      "Mean of the squared error\n",
      "square root the mean of the squared error\n",
      "\n",
      "punishes larger errors more\n",
      "\n",
      "MAE\n",
      "\n",
      "error=actual - predicted\n",
      "total the absolute error\n",
      "Mean of the total absolute error\n",
      "\n",
      "less frequently used\n",
      "\n",
      "\n",
      "  Objective (loss) functions and base learners\n",
      "\n",
      "objective function (loss) quantifies how far off a prediction is from the actual results\n",
      "\n",
      "Measures the difference between the prediction and the target for some collection of data\n",
      "\n",
      "Goal: Find the model that yields the minimum value of the loss function.\n",
      "\n",
      "Loss Function names\n",
      "\treg:linear - use for regression problems\n",
      "\n",
      "\treg:logistic - use for classification problems when you want decision not probability.\n",
      "\t\n",
      "\treg:logistic - when you want the probability rather than just the decision\n",
      "\n",
      "\n",
      "XGBoost model is composed of many individual models that combine to give a final prediction\n",
      "\n",
      "each of the individual models is combined the base learners that is slight better than random guessing.  The base learners that are better than 50% are combined into a single prediction.\n",
      "\n",
      "the base learners are non-linear\n",
      "\n",
      "there are two kinds of base learners: tree and linear\n",
      "\n",
      "import xgboost as xgb\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "https://github.com/ageron/handson-ml/blob/master/datasets/housing/housing.csv\n",
      "\n",
      "https://github.com/girishkuniyal/Predict-housing-prices-in-Portland\n",
      "\n",
      "https://www.datasethub.com/datasets/idaho/filetypes/csv\n",
      "\n",
      "https://people.ischool.berkeley.edu/~chandangope/project/\n",
      "\n",
      "\n",
      "X_train, X_test, y_train, x_test= train_test_split(X,y , test_size=0.2, random_state=42)\n",
      "\n",
      "\n",
      "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
      "\n",
      "xg_reg.fit(X_train, y_train)\n",
      " \n",
      "\n",
      "preds= xg_reg.predict(X_test)\n",
      "\n",
      "rmse=np.sqrt(mean_squared_error(y_test,y_preds))\n",
      "\n",
      "print(RMSE: %f\" %(rmse))\n",
      "\n",
      "  >Linear base learners\n",
      "- a linear learner\n",
      "- allows you to create a regularized linear regression using XGBoost's powerful learning API\n",
      "\n",
      "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
      "DM_test = xgb.DMatrix(data=X_test,label=y_test)\n",
      "\n",
      "#params={\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
      "params={\"booster\":\"gblinear\",\"objective\":\"reg:squarederror\"}\n",
      "\n",
      "xg_reg=xgb.train(params=params, dtrain=DM_train, num_boost_round=10)\n",
      "\n",
      "pred=xg_reg.predict(DM_test)\n",
      "\n",
      "rmse=np.sqrt(mean_squared_error(y_test,y_pred))\n",
      "\n",
      "print(\"RMSE: %f\" %(rmse))\n",
      "\n",
      "\n",
      "  Sample XGboose (reg:linear)\n",
      "\n",
      "# Create the training and test sets\n",
      "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
      "\n",
      "# Instantiate the XGBRegressor: xg_reg\n",
      "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
      "\n",
      "# Fit the regressor to the training set\n",
      "\n",
      "xg_reg.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set: preds\n",
      "preds= xg_reg.predict(X_test)\n",
      "\n",
      "# Compute the rmse: rmse\n",
      "rmse=np.sqrt(mean_squared_error(y_test,preds))\n",
      "print(\"RMSE: %f\" % (rmse))\n",
      "\n",
      " >Sample XGBoost (trees)\n",
      "\n",
      "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
      "\n",
      "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
      "DM_test = xgb.DMatrix(data=X_test,label=y_test)\n",
      "\n",
      "# Create the parameter dictionary: params\n",
      "params={\"booster\":\"gblinear\",\"objective\":\"reg:squarederror\"}\n",
      "\n",
      "# Train the model: xg_reg\n",
      "xg_reg=xgb.train(params=params, dtrain=DM_train, num_boost_round=5)\n",
      "\n",
      "# Predict the labels of the test set: preds\n",
      "preds=xg_reg.predict(DM_test)\n",
      "\n",
      "# Compute and print the RMSE\n",
      "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
      "print(\"RMSE: %f\" % (rmse))\n",
      "\n",
      " >Cross validation (rmse)\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary: params\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
      "\n",
      "# Perform cross-validation: cv_results\n",
      "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "\n",
      "# Print cv_results\n",
      "print(cv_results)\n",
      "\n",
      "# Extract and print final boosting round metric\n",
      "print(cv_results[\"test-rmse-mean\"].tail(1))\n",
      "\n",
      "\n",
      " >Cross validation (mae)\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary: params\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
      "\n",
      "# Perform cross-validation: cv_results\n",
      "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
      "\n",
      "# Print cv_results\n",
      "print(cv_results)\n",
      "\n",
      "# Extract and print final boosting round metric\n",
      "print(cv_results[\"test-mae-mean\"].tail(1))\n",
      "\n",
      "\n",
      "               >regularization\n",
      "\n",
      "regularization penalizes a model as it get more complex\n",
      "want models that are accurate and as simple as possible\n",
      "\n",
      "regularization parameters in xgboost\n",
      "1. gamma - minimum loss reduction allowed for split to occur\n",
      "2. alpha - l1 regularization on leaf weights, large value means more regularization\n",
      "a. causes many leafs in the base learners\n",
      "3. lambda is another name of l2 regularization on leaf weights\n",
      "a. smoother penalty on large numbers and decrease gradually\n",
      "\n",
      "boston_dmatrix = xgb.DMatrix(data=X,label=y)\n",
      "params={\"objective\":\"reg:linear\",\"max_depth\":4}\n",
      "\n",
      "l1_params=[1,10,100]\n",
      "rmses_l1=[]\n",
      "\n",
      "for reg in l1_params:\n",
      "\tparams['alpha']=reg\n",
      "\tcv_results= xgb.cv(dtrain=boston_dmatrix, params=params, nfold=4,\n",
      "\tnum_boost_round=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "\trmse_l1.append(cv_results[\"test-rmse-mean\"].tail(1).values[0])\n",
      "\n",
      "num_boost_round is the number of trees\n",
      "\n",
      "print(\"best rmse as a function of l1:\")\n",
      "\n",
      "print(pd.DataFrame(list(zip(l1_params,rmse_l1)), columns(\"l1\",\"rmse\"]))\n",
      "\n",
      "\n",
      "       >Base learners in XGBoost\n",
      "\n",
      "1. Linear base learner\n",
      "a. Sum of linear terms\n",
      "b. The boosted model is a weighted sum of linear models which is itself linear\n",
      "\n",
      "2. The tree based learner\n",
      "a. decision trees as base models\n",
      "b. The boosted model is the weighted sum of decision trees (non linear)\n",
      "c. almost exclusively used in XGBoost\n",
      "\n",
      "\n",
      "zip creates a generator of parallel values\n",
      "\n",
      "zip([1,2,3],[\"a\",\"b\",\"c\"])\n",
      "output: [1,\"a\"],[2,\"b\"],[3,\"c\"]\n",
      "\n",
      "list() instantiates the full generator and passing that into the dataframe which converts the whole expression.\n",
      "\n",
      " >Sample  to_dict\n",
      "\n",
      "keys = ['name', 'age', 'food']\n",
      "values = ['Monty', 42, 'spam']\n",
      "index=np.arange(0,len(keys)-1)\n",
      "\n",
      "df=pd.DataFrame(list(zip(keys,values)), columns=['keys','values'])\n",
      "df.set_index('keys')\n",
      "print(df.head())\n",
      "\n",
      "data_dict = df.iloc[index].set_index('keys')['values'].to_dict() \n",
      "print(data_dict)\n",
      "\n",
      " >Sample L2 regularization\n",
      "l2 regularization penalty - also known as \"lambda\" - and see its effect on overall model performance on the Ames housing dataset.\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "reg_params = [1, 10, 100]\n",
      "\n",
      "# Create the initial parameter dictionary for varying l2 strength: params\n",
      "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
      "\n",
      "# Create an empty list for storing rmses as a function of l2 complexity\n",
      "rmses_l2 = []\n",
      "\n",
      "# Iterate over reg_params\n",
      "for reg in reg_params:\n",
      "\n",
      "    # Update l2 strength\n",
      "    params[\"lambda\"] = reg\n",
      "    \n",
      "    # Pass this updated param dictionary into cv\n",
      "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "    \n",
      "    # Append best rmse (final round) to rmses_l2\n",
      "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
      "\n",
      "# Look at best rmse per l2 param\n",
      "print(\"Best rmse as a function of l2:\")\n",
      "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\",\"rmse\"]))\n",
      "\n",
      " >Sample  Plot the Trees\n",
      "Have a look at each of the plots. They provide insight into how the model arrived at its final decisions and what splits it made to arrive at those decisions.\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary: params\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
      "\n",
      "# Train the model: xg_reg\n",
      "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
      "\n",
      "# Plot the first tree\n",
      "xgb.plot_tree(xg_reg,num_trees=0)\n",
      "plt.show()\n",
      "\n",
      "# Plot the fifth tree\n",
      "xgb.plot_tree(xg_reg,num_trees=4)\n",
      "plt.show()\n",
      "\n",
      "# Plot the last tree sideways\n",
      "xgb.plot_tree(xg_reg,num_trees=9, rankdir='LR')\n",
      "plt.show()\n",
      "\n",
      "  Sample Plot the importance features\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary: params\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
      "\n",
      "# Train the model: xg_reg\n",
      "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
      "\n",
      "# Plot the feature importances\n",
      "xgb.plot_importance(xg_reg)\n",
      "plt.show()\n",
      "\n",
      "  >Why tuning\n",
      "\n",
      "\n",
      "\n",
      "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
      "untunedParams={\"objective\":\"reg:linear\"}\n",
      "\n",
      "untuned_cv_results=xgb.cv(dtrain=housing_dmatrix, params=untuned_params, nfold=4, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "\n",
      "print(\"Untuned rmse: %f\" %((untuned_cv_results[\"test-mae-mean\"]).tail(1)))\n",
      "\n",
      "    \n",
      "  A more turn parameter resultset\n",
      "\n",
      "tunedParams={\"objective\":\"reg:linear\",\"colsample_bytree\":0.3, \"learning_rate\":0.1, \"max_depth\":5}\n",
      "\n",
      "tuned_cv_results=xgb.cv(dtrain=housing_dmatrix, params=tuned_params, nfold=4, num_boost_round=200, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "\n",
      "print(\"tuned rmse: %f\" %((tuned_cv_results[\"test-mae-mean\"]).tail(1)))\n",
      "\n",
      "\n",
      "  Sample tuning with number of trees\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
      "\n",
      "# Create the parameter dictionary for each tree: params \n",
      "params = {\"objective\":\"reg:linear\",'max_depth':3}\n",
      "\n",
      "\n",
      "# Create list of number of boosting rounds\n",
      "num_rounds = [5, 10, 15]\n",
      "\n",
      "# Empty list to store final round rmse per XGBoost model\n",
      "final_rmse_per_round = []\n",
      "\n",
      "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
      "for curr_num_rounds in num_rounds:\n",
      "\n",
      "    # Perform cross-validation: cv_results\n",
      "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "    \n",
      "    # Append final round RMSE\n",
      "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
      "\n",
      "# Print the resultant DataFrame\n",
      "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
      "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))\n",
      "\n",
      "  Sample  early stop rounds\n",
      "\n",
      "\n",
      "# Create your housing DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary for each tree: params\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
      "\n",
      "# Perform cross-validation with early stopping: cv_results\n",
      "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,early_stopping_rounds=10, num_boost_round=50, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "\n",
      "# Print cv_results\n",
      "print(\"rmse: %f\" %((cv_results[\"test-rmse-mean\"]).tail(1)))\n",
      "\n",
      "           >Tree Tunable parameters\n",
      "\n",
      "*learning rate: low learning rate will take more trees to reduce residual error (boosting rounds)\n",
      "* gamma: min loss reduction to create new tree split (regularized)\n",
      "* lambda: l2 reg on leaf weights (regularized)\n",
      "* max_depth : max depth per tree\n",
      "* subsample: % samples used per tree (low value - underfitting and high value - overfitting)\n",
      "* colsample_bytree: %features used per tree (features 0 to 1) a small colsample_bytree value means that additional regularization is being added to the model.\n",
      "\n",
      "           >Linear base learner parameters\n",
      "* lambda: L2 reg on weights\n",
      "* alpha: L1 reg on weights\n",
      "* lambda_bias: L2 reg term on bias\n",
      "\n",
      "You can also tune the number of estimators used for both base model types!\n",
      "\n",
      "\n",
      "  Sample Tuning the learning rate (eta)\n",
      "\n",
      "# Create your housing DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary for each tree (boosting round)\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
      "\n",
      "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
      "eta_vals = [0.001,0.01,0.1]\n",
      "best_rmse = []\n",
      "\n",
      "# Systematically vary the eta \n",
      "for curr_val in eta_vals:\n",
      "\n",
      "    params[\"eta\"] = curr_val\n",
      "    \n",
      "    # Perform cross-validation: cv_results\n",
      "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,early_stopping_rounds=5, num_boost_round=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "    \n",
      "    \n",
      "    \n",
      "    # Append the final round rmse to best_rmse\n",
      "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
      "\n",
      "# Print the resultant DataFrame\n",
      "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))\n",
      "\n",
      "   Sample tuning (max depths)\n",
      "\n",
      "# Create your housing DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
      "\n",
      "# Create the parameter dictionary\n",
      "params = {\"objective\":\"reg:linear\"}\n",
      "\n",
      "# Create list of max_depth values\n",
      "max_depths = [2, 5, 10, 20]\n",
      "best_rmse = []\n",
      "\n",
      "# Systematically vary the max_depth\n",
      "for curr_val in max_depths:\n",
      "\n",
      "    params[\"max_depth\"] = curr_val\n",
      "    \n",
      "    # Perform cross-validation\n",
      "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
      "                 num_boost_round=10, early_stopping_rounds=5,\n",
      "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "    \n",
      "    # Append the final round rmse to best_rmse\n",
      "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
      "\n",
      "# Print the resultant DataFrame\n",
      "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))\n",
      "\n",
      "  Sample colsample_bytree_vals\n",
      "\n",
      "# Create your housing DMatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
      "\n",
      "# Create the parameter dictionary\n",
      "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
      "\n",
      "# Create list of hyperparameter values: colsample_bytree_vals\n",
      "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
      "best_rmse = []\n",
      "\n",
      "# Systematically vary the hyperparameter value \n",
      "for curr_val in colsample_bytree_vals:\n",
      "\n",
      "    params[\"colsample_bytree\"] = curr_val\n",
      "    \n",
      "    # Perform cross-validation\n",
      "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
      "                 num_boost_round=10, early_stopping_rounds=5,\n",
      "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "    \n",
      "    # Append the final round rmse to best_rmse\n",
      "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
      "\n",
      "# Print the resultant DataFrame\n",
      "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))\n",
      "\n",
      "\n",
      "  Review of grid search and random search\n",
      "\n",
      "grid search : exhaustive searches of a given hyperparameter\n",
      "\n",
      "\n",
      "   GridSearchCV\n",
      "\n",
      "# Import GridSearchCV\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "gbm_param_grid = {'learning_rate':[0.01,0.1,0.5,0.9]\n",
      "\t'n_estimators':[200],\n",
      "\t'subsample':[0.3,0.5,0.9]}\n",
      "\n",
      "gbm=xgb.XGBRegressor()\n",
      "\n",
      "grid_mse=GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
      "scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
      "\n",
      "\n",
      "grid_mse.fit(X,y)\n",
      "\n",
      "\n",
      "print(\"Best parameters found:\", grid_mse.best_params_)\n",
      "print(\"Lowest RMSE found:, np.sqrt(grid_mse.best_score_)))\n",
      "\n",
      "\n",
      "  Random search \n",
      "* you set the number of iterations you would like to random search to continue\n",
      "* you create the range of hyperparameters values per hyper parameter - randomly draw value from the range of hyperparameters values\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "import random\n",
      "\n",
      "param_dist = {'learning_rate':np.arange(0.05,1.05,.05), 'n_estimators':[200], 'subsample':np.arange(0.05,1.05,.05),}\n",
      "\n",
      "gbm=xgb.XGBRegressor()\n",
      "\n",
      "random_search = RandomizedSearchCV(estimator=gbm,param_distributions=param_dist, n_iter=25,\n",
      "scoring='neg_mean_squared_error',cv=4,verbose=1)\n",
      "random_search.fit(X, y)\n",
      "\n",
      "print(\"Best Parameters\",random_search.best_params_)\n",
      "print(\"Lowest RMSE found:\",np.sqrt(np.abs(random_search.best_score_)))\n",
      "\n",
      "\n",
      "  Sample search GridSearchCV\n",
      "\n",
      "gbm_param_grid = {'colsample_bytree':[0.3,0.7],\n",
      "\t'n_estimators':[50],\n",
      "\t'max_depth':[2,5]}\n",
      "\n",
      "gbm=xgb.XGBRegressor()\n",
      "\n",
      "# Perform grid search: grid_mse\n",
      "grid_mse=GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
      "scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
      "grid_mse.fit(X,y)\n",
      "\n",
      "\n",
      "# Print the best parameters and lowest RMSE\n",
      "print(\"Best parameters found: \", grid_mse.best_params_)\n",
      "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n",
      "\n",
      "\n",
      "  >Sample RandomSearchCV\n",
      "\n",
      "# Create the parameter grid: gbm_param_grid \n",
      "gbm_param_grid = {\n",
      "    'n_estimators': [25],\n",
      "    'max_depth': np.arange(2, 12)\n",
      "}\n",
      "\n",
      "# Instantiate the regressor: gbm\n",
      "gbm = xgb.XGBRegressor(n_estimators=10)\n",
      "\n",
      "# Perform random search: grid_mse\n",
      "randomized_mse = RandomizedSearchCV(estimator=gbm,param_distributions=gbm_param_grid, n_iter=5, scoring='neg_mean_squared_error',cv=4,verbose=1)\n",
      "randomized_mse.fit(X, y)\n",
      "\n",
      "      Review of pipelines using sklearn\n",
      "\n",
      "takes a list of named 2-tuples (name, pipeline_step) as input\n",
      "\n",
      "the pipeline can contain estimator or transformer objects\n",
      "\n",
      "pipeline implement fit/predict models\n",
      "\n",
      "pipelines can be used as input estimators into grid and randomized search and cross_val_score methods\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "https://github.com/eric-bunch/boston_housing\n",
      "\n",
      "\n",
      "rf_pipeline = Pipeline[(\"st_scaler\",StandardScaler()),\n",
      "                       (\"rf_model\",RandomForestRegressor())]\n",
      "\n",
      "\n",
      "\n",
      "scores=cross_val_score(rf_pipeline,X,y,\n",
      "\tscoring=\"neg_mean_squared_error\", cv=10)\n",
      "\n",
      "final_avg_rmse=np.mean(np.sqrt(np.abs(scores)))\n",
      "\n",
      "print(\"Final RMSE:\", final_avg_rmse)\n",
      "\n",
      "  >label encoder and onehotencoder\n",
      "\n",
      "1. LabelEncoder converts a categorical column of strings into integers\n",
      "\n",
      "2. OneHotEncoder: takes a column of integers and encodes them as dummy variables where each variable is a column\n",
      "\n",
      "***Can not be done within a pipeline\n",
      "\n",
      "\n",
      "      Preprocessing with DictVectorizer\n",
      "\n",
      "*Traditionally used in text processing\n",
      "* converts lists of feature mappings into vectors\n",
      "* we need to convert a dataframe into a list of dictionary entries\n",
      "\n",
      "  Sample (LabelEncoder) find the objects\n",
      "\n",
      "# Import LabelEncoder\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "# Fill missing values with 0\n",
      "df.LotFrontage = df.LotFrontage.fillna(0)\n",
      "\n",
      "# Create a boolean mask for categorical columns\n",
      "categorical_mask = (df.dtypes == object)\n",
      "\n",
      "# Get list of categorical column names\n",
      "categorical_columns = df.columns[categorical_mask].tolist()\n",
      "\n",
      "# Print the head of the categorical columns\n",
      "print(df[categorical_columns].head())\n",
      "\n",
      "# Create LabelEncoder object: le\n",
      "le = LabelEncoder()\n",
      "\n",
      "\n",
      "  >XGboost pipeline\n",
      "\n",
      "# Import necessary modules\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "# Fill LotFrontage missing values with 0\n",
      "X.LotFrontage = X.LotFrontage.fillna(0)\n",
      "\n",
      "\n",
      "# Setup the pipeline steps: steps\n",
      "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
      "         (\"xgb_model\", xgb.XGBRegressor())]\n",
      "\n",
      "# Create the pipeline: xgb_pipeline\n",
      "xgb_pipeline = Pipeline(steps)\n",
      "\n",
      "# Fit the pipeline\n",
      "xgb_pipeline.fit(X.to_dict(\"records\"), y)\n",
      "\n",
      "# Apply LabelEncoder to categorical columns\n",
      "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
      "\n",
      "# Print the head of the LabelEncoded categorical columns\n",
      "print(df[categorical_columns].head())\n",
      "\n",
      "  >Sample (OneHotEncoding)\n",
      "\n",
      "# Import OneHotEncoder\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "# Create OneHotEncoder: ohe\n",
      "ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)\n",
      "\n",
      "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
      "df_encoded = ohe.fit_transform(df)\n",
      "\n",
      "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
      "print(df_encoded[:5, :])\n",
      "\n",
      "# Print the shape of the original DataFrame\n",
      "print(df.shape)\n",
      "\n",
      "# Print the shape of the transformed array\n",
      "print(df_encoded.shape)\n",
      "\n",
      "\n",
      "  Sample (DictVectorizer) label and encode at the same time\n",
      "\n",
      "# Import DictVectorizer\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "# Convert df into a dictionary: df_dict\n",
      "df_dict = df.to_dict(\"records\")\n",
      "\n",
      "# Create the DictVectorizer object: dv\n",
      "dv = DictVectorizer(sparse=False)\n",
      "\n",
      "# Apply dv on df: df_encoded\n",
      "df_encoded = dv.fit_transform(df_dict)\n",
      "\n",
      " >Sample (xgboost in the pipeline\n",
      "# Import necessary modules\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "# Fill LotFrontage missing values with 0\n",
      "X.LotFrontage = X.LotFrontage.fillna(0)\n",
      "\n",
      "\n",
      "# Setup the pipeline steps: steps\n",
      "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
      "         (\"xgb_model\", xgb.XGBRegressor())]\n",
      "\n",
      "# Create the pipeline: xgb_pipeline\n",
      "xgb_pipeline = Pipeline(steps)\n",
      "\n",
      "# Fit the pipeline\n",
      "xgb_pipeline.fit(X.to_dict(\"records\"), y)\n",
      "\n",
      "     Incorporating xgboost into pipelines\n",
      "\n",
      "# Import necessary modules\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "# Fill LotFrontage missing values with 0\n",
      "X.LotFrontage = X.LotFrontage.fillna(0)\n",
      "\n",
      "\n",
      "# Setup the pipeline steps: steps\n",
      "steps = [ (\"st_scaler\", StandardScaler()),\n",
      "\t (\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
      "         (\"xgb_model\", xgb.XGBRegressor())]\n",
      "\n",
      "# Create the pipeline: xgb_pipeline\n",
      "xgb_pipeline = Pipeline(steps)\n",
      "\n",
      "# Fit the pipeline\n",
      "xgb_pipeline.fit(X.to_dict(\"records\"), y)\n",
      "\n",
      "\n",
      "scores=cross_val_score(xg_pipeline,X,y,\n",
      "\tscoring=\"neg_mean_squared_error\", cv=10)\n",
      "\n",
      "final_avg_rmse=np.mean(np.sqrt(np.abs(scores)))\n",
      "print(\"Final XGB RMSE:\",final_avg_rmse)\n",
      "\n",
      "   sklearn_pandas\n",
      "1. DataFrameMapper - Interoperability between pandas and scikit-learn\n",
      "\n",
      "2. CategoricalImputer - Allow for imputation of categorical variables before conversion to integers\n",
      "\n",
      "3. sklearn.preprocessing - Imputer - Native imputation of numerical columns in scikit-learn\n",
      "\n",
      "4. sklearn.pipeline:\n",
      "\tfeatureUnion - combine multiple pipelines of features into a single pipeline of features\n",
      "\n",
      "\n",
      "  >Sample Creating the xgboost pipeline\n",
      "\n",
      "# Import necessary modules\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "# Fill LotFrontage missing values with 0\n",
      "X.LotFrontage = X.LotFrontage.fillna(0)\n",
      "\n",
      "# Setup the pipeline steps: steps\n",
      "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
      "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:squarederror\"))]\n",
      "\n",
      "# Create the pipeline: xgb_pipeline\n",
      "xgb_pipeline = Pipeline(steps)\n",
      "\n",
      "xgb_pipeline.fit(X.to_dict(\"records\"), y)\n",
      "\n",
      "# Cross-validate the model\n",
      "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict(\"records\"), y, cv=10, scoring=\"neg_mean_squared_error\")\n",
      "\n",
      "\n",
      "# Print the 10-fold RMSE\n",
      "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))\n",
      "\n",
      "\n",
      "    Sample using categorical imputer\n",
      "\n",
      "Specifically, you'll be able to impute missing categorical values directly using the Categorical_Imputer() class in sklearn_pandas, and the DataFrameMapper() class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.\n",
      "\n",
      "We've also created a transformer called a Dictifier that encapsulates converting a DataFrame using .to_dict(\"records\") without you having to do it explicitly (and so that it works in a pipeline). Finally, we've also provided the list of feature names in kidney_feature_names, the target name in kidney_target_name, the features in X, and the target in y\n",
      "\n",
      "# Import necessary modules\n",
      "from sklearn_pandas import DataFrameMapper\n",
      "from sklearn_pandas import CategoricalImputer\n",
      "\n",
      "# Check number of nulls in each feature column\n",
      "nulls_per_column = X.isnull().sum()\n",
      "print(nulls_per_column)\n",
      "\n",
      "# Create a boolean mask for categorical columns\n",
      "categorical_feature_mask = X.dtypes == object\n",
      "\n",
      "# Get list of categorical column names\n",
      "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
      "\n",
      "# Get list of non-categorical column names\n",
      "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
      "\n",
      "# Apply numeric imputer\n",
      "numeric_imputation_mapper = DataFrameMapper(\n",
      "                                            [([numeric_feature], Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
      "                                            input_df=True,\n",
      "                                            df_out=True\n",
      "                                           )\n",
      "\n",
      "# Apply categorical imputer\n",
      "categorical_imputation_mapper = DataFrameMapper(\n",
      "                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],\n",
      "                                                input_df=True,\n",
      "                                                df_out=True\n",
      "                                               )\n",
      "\n",
      "\n",
      "   Sample FeatureUnion\n",
      "\n",
      "# Import FeatureUnion\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "\n",
      "# Combine the numeric and categorical transformations\n",
      "numeric_categorical_union = FeatureUnion([\n",
      "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
      "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
      "                                         ])\n",
      "\n",
      "   Sample Full Pipeline\n",
      "\n",
      "# Create full pipeline\n",
      "pipeline = Pipeline([\n",
      "                     (\"featureunion\", numeric_categorical_union),\n",
      "                     (\"dictifier\", Dictifier()),\n",
      "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
      "                     (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
      "                    ])\n",
      "\n",
      "# Perform cross-validation\n",
      "cross_val_scores = cross_val_score(pipeline, kidney_data, y, cv=3, scoring=\"roc_auc\")\n",
      "\n",
      "# Print avg. AUC\n",
      "print(\"3-fold AUC: \", np.mean(cross_val_scores))\n",
      "\n",
      "\n",
      "   >Tuning hyperparameter in a pipeline\n",
      "\n",
      "xbg_pipeline = Pipeline[('st_scaler',StandardScaler()),\n",
      "\t\t('xgb_model',xgb.XGBRegressor())]\n",
      "\n",
      "\n",
      "gbm_param_grid={\n",
      "\t'xgb_model__subsample': np.arange(.05,1,.05),\n",
      "\t'xgb_model__max_depth': np.arange(3,20,1),\n",
      "\t'xgb_model__colsample_bytree': np.arange(.1,1.05,.05)}\n",
      "\n",
      "\n",
      "randomized_neg_mse= RandomizedSearchCV(estimator=xgb_pipeline,\n",
      "\tparam_distributions=gbm_param_grid, n_iter=10,\n",
      "\tscoring='neg_mean_squared_error', cv=4)\n",
      "\n",
      "\n",
      "\n",
      "randomized_neg_mse.fit(X, y)\n",
      "\n",
      "print(\"Best rmse: \", np.sqrt(np.abs(randomized_neg_mse.best_score_)))\n",
      "\n",
      "  Sample RandomSearchCV the pipeline\n",
      "\n",
      "# Create the parameter grid\n",
      "gbm_param_grid = {\n",
      "    'clf__learning_rate': np.arange(.05, 1, .05),\n",
      "    'clf__max_depth': np.arange(3,10, 1),\n",
      "    'clf__n_estimators': np.arange(50, 200, 50)\n",
      "}\n",
      "\n",
      "# Perform RandomizedSearchCV\n",
      "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,\n",
      "                                        param_distributions=gbm_param_grid,\n",
      "                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n",
      "\n",
      "# Fit the estimator\n",
      "randomized_roc_auc.fit(X, y)\n",
      "\n",
      "# Compute metrics\n",
      "print(\"Best rmse: \", randomized_roc_auc.best_score_)\n",
      "print(randomized_roc_auc.best_estimator_)\n",
      "\n",
      "\n",
      "example of a pipeline\n",
      "https://stackoverflow.com/questions/52055658/sklearn-pandas-in-a-pipeline-returns-typeerror-builtin-function-or-method-obj\n",
      "\n",
      "https://dunyaoguz.github.io/my-blog/dataframemapper.html\n",
      "\n",
      "\n",
      "Installation\n",
      "pip install https://github.com/scikit-learn/scikit-learn/archive/master.zip\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "import codecs\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['passing']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python_files\\\\python_notes')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pattern=\"(\\s{1}airlines.csv\\s{1})\"\n",
    "pattern=\"football\"\n",
    "path= 'C:\\\\Users\\\\dnishimoto\\\\python_files'  \n",
    "for filename in [item for item in os.listdir(path) if item.endswith(\".txt\")  ]:\n",
    "    if os.access(path + \"\\\\\" + filename, os.R_OK):\n",
    "        with open(path + \"\\\\\" + filename,\"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if re.search(pattern,line):\n",
    "                    print(filename)\n",
    "                    print(\"\\t{}\".format(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJupyterNotesFnc(path,phrase):\n",
    "    pySource=\"\"\n",
    "    count=0\n",
    "    path=os.path.expanduser(path)\n",
    "    for filename in [item for item in os.listdir(path) if item.endswith(\".ipynb\")  ]:\n",
    "        if os.access(path + \"\\\\\" + filename, os.R_OK):\n",
    "            with open(path + \"\\\\\" + filename,\"r\", encoding=\"utf8\") as f:\n",
    "                source = f.read()\n",
    "                y = json.loads(source)\n",
    "                #print(y)\n",
    "                doc=[]\n",
    "                found=False\n",
    "                for x in y['cells']:\n",
    "                    for line in x['source']:\n",
    "                    #print(line)\n",
    "                        if phrase in line:\n",
    "                            doc.append(line)\n",
    "                            found=True\n",
    "                if found==True:\n",
    "                    print(\"{}\\n\".format(filename))\n",
    "                    for item in doc:\n",
    "                        print(\"\\t{}\".format(item))\n",
    "                count+=1\n",
    "\n",
    "path= 'C:\\\\Users\\\\dnishimoto\\\\python_files\\\\python-deep-learning-master'               \n",
    "readJupyterNotesFnc(path,\"passing\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
