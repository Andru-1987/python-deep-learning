{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\pre-processing techniques.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\pre-processing techniques.txt\n",
      "df.describe()\n",
      "\n",
      "removing missing data\n",
      "\n",
      "df.dropna()\n",
      "\n",
      "df.drop(['col1'],axis=1)\n",
      "\n",
      "print(df[df['b']==7])\n",
      "\n",
      "print(df['B'].isnull().sum())\n",
      "\n",
      "print(df[df['B'].notnull()])\n",
      "\n",
      "\n",
      "  >sample  > find the number nulls in the category_desc column\n",
      "\n",
      "# Check how many values are missing in the category_desc column\n",
      "print(volunteer['category_desc'].isnull().sum())\n",
      "\n",
      "# Subset the volunteer dataset\n",
      "volunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n",
      "\n",
      "# Print out the shape of the subset\n",
      "print(volunteer_subset.shape)\n",
      "\n",
      "\n",
      "      Working with data types\n",
      "\n",
      "how to convert data types in the dataset\n",
      "\n",
      "print(volunteer.dtypes)\n",
      "\n",
      "\n",
      "object\n",
      "int64\n",
      "float64\n",
      "datetime64 or timedelta\n",
      "\n",
      "df['C']=df['C'].,astype('float')\n",
      "\n",
      "  >sample    converting to int\n",
      "\n",
      " # Print the head of the hits column\n",
      "print(volunteer[\"hits\"].dtypes)\n",
      "\n",
      "# Convert the hits column to type int\n",
      "volunteer[\"hits\"] = volunteer['hits'].astype(int)\n",
      "\n",
      "# Look at the dtypes of the dataset\n",
      "print(volunteer.dtypes)\n",
      "\n",
      "\n",
      "       >Training and test sets\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, x_train, y_test=train_test_split(X,y)\n",
      "\n",
      "75% and 25% by default\n",
      "\n",
      "Stratifed sampling\n",
      "100 samples, 80% class 1 and 20% class 2\n",
      "\n",
      "Training set: 75 samples, 60 class 1 and 15 class 2\n",
      "Test set: 25 samples, 20 class 1 and 5 class 2\n",
      "\n",
      "y['labels'].value_counts()\n",
      "\n",
      "\n",
      "    Sample  value_counts()\n",
      "\n",
      "volunteer['category_desc'].value_counts()\n",
      "\n",
      "Strengthening Communities    307\n",
      "Helping Neighbors in Need    119\n",
      "Education                     92\n",
      "Health                        52\n",
      "Environment                   32\n",
      "Emergency Preparedness        15\n",
      "Name: category_desc, dtype: int64\n",
      "\n",
      "    sample  > train_test_split using stratify\n",
      "\n",
      "\n",
      "# Create a data with all columns except category_desc\n",
      "volunteer_X = volunteer.drop(['category_desc'], axis=1)\n",
      "\n",
      "# Create a category_desc labels dataset\n",
      "volunteer_y = volunteer[['category_desc']]\n",
      "\n",
      "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
      "X_train, X_test,y_train,y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n",
      "\n",
      "# Print out the category_desc counts on the training y labels\n",
      "print(y_train['category_desc'].value_counts())\n",
      "\n",
      "\n",
      "y_train output\n",
      "\n",
      "Strengthening Communities    230\n",
      "Helping Neighbors in Need     89\n",
      "Education                     69\n",
      "Health                        39\n",
      "Environment                   24\n",
      "Emergency Preparedness        11\n",
      "Name: category_desc, dtype: int64\n",
      "\n",
      "\n",
      "    Standardizing data\n",
      "\n",
      "standardization is technique for taking contineously distributed data and make it look normally distributed\n",
      "\n",
      "Log normalization and feature scaling\n",
      "\n",
      "applied to contineous numerical data\n",
      "\n",
      "model in linear space\n",
      "\n",
      "dataset features have high variance\n",
      "\n",
      "dataset features are contineous and on different scales\n",
      "\n",
      "   > sample  > knn k nearest neighbor\n",
      "\n",
      "# Split the dataset and labels into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "\n",
      "# Fit the k-nearest neighbors model to the training data\n",
      "knn.fit(X_train,y_train)\n",
      "\n",
      "# Score the model on the test data\n",
      "print(knn.score(X_test,y_test))\n",
      "\n",
      "\n",
      "       >Log normalization\n",
      "\n",
      "log normalization can be helpful if you have a column with high variance\n",
      "\n",
      "applies log transformation\n",
      "\n",
      "natural log using the contant _e_ = 2.718\n",
      "\n",
      "log 30 is 3.4\n",
      "\n",
      "because 3.4 ** 2.718 = 30 or 3.4**_e_=30\n",
      "\n",
      "captures relative changes and captures the magnitude of change and keeps everything in positive space\n",
      "\n",
      "Log normalization reduces the variance in the model\n",
      "\n",
      "print(df.var())\n",
      "\n",
      "col1: 0.12858\n",
      "col2  1691.72167\n",
      "\n",
      "import numpy as np\n",
      "df['log_column2']= np.log(df['col2'])\n",
      "print(df)\n",
      "\n",
      "log_2 has scaled down the values\n",
      "\n",
      "\n",
      "  o sample  > log normalization\n",
      "\n",
      "# Print out the variance of the Proline column\n",
      "print(wine['Proline'].var())\n",
      "\n",
      "# Apply the log normalization function to the Proline column\n",
      "wine['Proline_log'] = np.log(wine['Proline'])\n",
      "\n",
      "\n",
      "# Check the variance of the normalized Proline column\n",
      "print(wine['Proline_log'])\n",
      "\n",
      "\n",
      "    scaling data for feature comparison\n",
      "\n",
      "1. features on different scales\n",
      "2. model with linear characteristics\n",
      "3. center features around 0 and transform to unit variance\n",
      "4. transforms to approximately normal distribution\n",
      "\n",
      "variance is low in the column but differs across columns\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler=StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(df),\n",
      "columns=df.columns)\n",
      "\n",
      "\n",
      "print(df_scaled)\n",
      "\n",
      "print(df.var())\n",
      "\n",
      "The variance on each column is the same\n",
      "\n",
      "\n",
      "    sample standardscaler normalization\n",
      "\n",
      "# Import StandardScaler from scikit-learn\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "\n",
      "# Create the scaler\n",
      "ss = StandardScaler()\n",
      "\n",
      "# Take a subset of the DataFrame you want to scale \n",
      "wine_subset = wine[['Ash','Alcalinity of ash','Magnesium']]\n",
      "\n",
      "# Apply the scaler to the DataFrame subset\n",
      "wine_subset_scaled = ss.fit_transform(wine_subset)\n",
      "\n",
      "\n",
      "  o sample  > log normalization\n",
      "\n",
      "# Print out the variance of the Proline column\n",
      "print(wine['Proline'].var())\n",
      "\n",
      "# Apply the log normalization function to the Proline column\n",
      "wine['Proline_log'] = np.log(wine['Proline'])\n",
      "\n",
      "\n",
      "# Check the variance of the normalized Proline column\n",
      "print(wine['Proline_log'])\n",
      "\n",
      "\n",
      "    scaling data for feature comparison\n",
      "\n",
      "1. features on different scales\n",
      "2. model with linear characteristics\n",
      "3. center features around 0 and transform to unit variance\n",
      "4. transforms to approximately normal distribution\n",
      "\n",
      "variance is low in the column but differs across columns\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler=StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(df),\n",
      "columns=df.columns)\n",
      "\n",
      "\n",
      "print(df_scaled)\n",
      "\n",
      "print(df.var())\n",
      "\n",
      "The variance on each column is the same\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "X_train,y_train, X_test,y_test = train_test_split(X,y)\n",
      "\n",
      "knn=KNeighborsClassifier()\n",
      "knn.fit(X_train, y_train)\n",
      "knn.score(X_test,y_test)\n",
      "\n",
      "\n",
      "  >sample  > knn\n",
      "\n",
      "# Split the dataset and labels into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
      "\n",
      "# Fit the k-nearest neighbors model to the training data\n",
      "knn.fit(X_train,y_train)\n",
      "\n",
      "# Score the model on the test data\n",
      "print(knn.score(X_test, y_test))\n",
      "\n",
      "   >sample  > apply standard scaler fit_transform\n",
      "\n",
      "# Create the scaling method.\n",
      "ss = StandardScaler()\n",
      "\n",
      "# Apply the scaling method to the dataset used for modeling.\n",
      "X_scaled = ss.fit_transform(X)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
      "\n",
      "# Fit the k-nearest neighbors model to the training data.\n",
      "knn.fit(X_train,y_train)\n",
      "\n",
      "# Score the model on the test data.\n",
      "print(knn.score(X_test,y_test))\n",
      "\n",
      "\n",
      "     >Feature Engineering\n",
      "\n",
      "creation of new features based on existing features\n",
      "insight into relationships between features\n",
      "extract and expand data\n",
      "dataset-dependent\n",
      "\n",
      "   >Encoding categorical variables\n",
      "\n",
      "the classifiers require numeric input\n",
      "\n",
      "encoding is required\n",
      "\n",
      "fav_color=blue,green,orange, green\n",
      "\n",
      "  encode binary as 1 and 0\n",
      "\n",
      "users['sub_enc']=users['subscribed'].apply(lambda val: 1 if val=='y' else 0)\n",
      "\n",
      "   >Label encoding\n",
      "\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "le=LabelEncoder()\n",
      "users['sub_enc_le']=le.fit_tranform(user['subscribed'])\n",
      "\n",
      "   one hot-encoding\n",
      "\n",
      "pd.get_dummies(users['fav_colors'])\n",
      "\n",
      "        >Sample   > label encoder\n",
      "\n",
      "# Set up the LabelEncoder object\n",
      "enc = LabelEncoder()\n",
      "\n",
      "# Apply the encoding to the \"Accessible\" column\n",
      "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
      "\n",
      "# Compare the two columns\n",
      "print(hiking[['Accessible', 'Accessible_enc']].head())\n",
      "\n",
      "    Sample  > get_dummies\n",
      "\n",
      "# Transform the category_desc column\n",
      "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
      "\n",
      "# Take a look at the encoded columns\n",
      "print(category_enc.head())\n",
      "\n",
      "\n",
      "  >Engineering numerical features\n",
      "\n",
      "\n",
      "columns=['day1','day2','day3']\n",
      "df['mean']=df.apply(lambda row: row[columns].mean(), axis=1)\n",
      "\n",
      "print(df)\n",
      "\n",
      "\n",
      "  > Dates\n",
      "\n",
      "df['date_converted']=pd.to_datetime(df['date'])\n",
      "\n",
      "df['month']=df['date_converted'].apply(lambda row: row.month)\n",
      "\n",
      "  > sample   > creating an average column\n",
      "\n",
      "# Create a list of the columns to average\n",
      "run_columns = ['run1', 'run2', 'run3', 'run4', 'run5']\n",
      "\n",
      "#running_times_5k.columns\n",
      "\n",
      "\n",
      "# Use apply to create a mean column\n",
      "running_times_5k[\"mean\"] = running_times_5k.apply(lambda x: x[run_columns].mean(), axis=1)\n",
      "\n",
      "# Take a look at the results\n",
      "print(running_times_5k[\"mean\"])\n",
      "\n",
      "   sample  > extract the month\n",
      "\n",
      "# First, convert string column to date column\n",
      "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
      "\n",
      "# Extract just the month from the converted column\n",
      "volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].apply(lambda row: row.month)\n",
      "\n",
      "# Take a look at the converted and new month columns\n",
      "print(volunteer[[\"start_date_converted\", \"start_date_month\"]].head())\n",
      "\n",
      "\n",
      "   >Feature engineering from text\n",
      "\n",
      "\n",
      "import re\n",
      "\n",
      "my_string=\"temperature: 75.6 F\"\n",
      "\n",
      "pattern=re.compile(\"\\d+\\.\\d+\")\n",
      "\n",
      "look for the float value in the string\n",
      "\\d digits\n",
      "\\. decimal period\n",
      "\n",
      "temp = re.match(pattern, my_string)\n",
      "\n",
      "print(float(temp.group(0))\n",
      "\n",
      "Vectorize the text\n",
      "\n",
      "tfidf vector term frequency inverse document frequency\n",
      "\n",
      "put weights on words that are more significant\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "print(documents.head())\n",
      "\n",
      "\n",
      "tfidf_vec = TfidfVectorizer()\n",
      "text_tfidf = tfidf_vec.fit_transform(documents)\n",
      "\n",
      "naives bayes classifier\n",
      "each feature is independent of others\n",
      "\n",
      "\n",
      "  > sample  > extract miles from a string\n",
      "\n",
      "# Write a pattern to extract numbers and decimals\n",
      "def return_mileage(length):\n",
      "    pattern = re.compile(\"\\d+\\.\\d+\")\n",
      "    \n",
      "    # Search the text for matches\n",
      "    mile = re.match(pattern,length)\n",
      "    \n",
      "    # If a value is returned, use group(0) to return the found value\n",
      "    if mile is not None:\n",
      "        return float(mile.group(0))\n",
      "        \n",
      "# Apply the function to the Length column and take a look at both columns\n",
      "hiking[\"Length_num\"] = hiking['Length'].apply(lambda row: return_mileage(row))\n",
      "print(hiking[[\"Length\", \"Length_num\"]].head())\n",
      "\n",
      "  > sample  > vectorize text\n",
      "\n",
      "# Take the title text\n",
      "title_text = volunteer['title']\n",
      "\n",
      "# Create the vectorizer method\n",
      "tfidf_vec = TfidfVectorizer()\n",
      "\n",
      "# Transform the text into tf-idf vectors\n",
      "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
      "print(text_tfidf)\n",
      "\n",
      "   sample    naive bayes\n",
      "\n",
      "# Split the dataset according to the class distribution of category_desc\n",
      "print(text_tfidf)\n",
      "y = volunteer[\"category_desc\"]\n",
      "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)\n",
      "\n",
      "# Fit the model to the training data\n",
      "nb.fit(X_train,y_train)\n",
      "\n",
      "# Print out the model's accuracy\n",
      "print(nb.score(X_test,y_test))\n",
      "\n",
      "\n",
      "       Feature selection\n",
      "\n",
      "removing unnecessary features that might create noise\n",
      "remove correlated features\n",
      "remove duplicate features\n",
      "\n",
      "it is an iterative process\n",
      "\n",
      "depends on the end goal\n",
      "\n",
      "the feature move to together \n",
      "use pearson correlation coefient (-1 and 1)\n",
      "\n",
      "df.corr()\n",
      "\n",
      "\n",
      "   sample  > df.corr   > drop columns\n",
      "\n",
      "# Create a list of redundant column names to drop\n",
      "to_drop = [\"category_desc\", \"locality\", \"region\", \"postalcode\", \"vol_requests\"]\n",
      "\n",
      "# Drop those columns from the dataset\n",
      "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
      "\n",
      "# Print out the head of the new dataset\n",
      "print(volunteer_subset.head())\n",
      "\n",
      "volunteer.corr()\n",
      "\n",
      "\n",
      "  >sample  > drop columns with corr > .75\n",
      "\n",
      "import numpy as np\n",
      "# Print out the column correlations of the wine dataset\n",
      "print(wine.corr())\n",
      "\n",
      "corr=wine.corr()\n",
      "\n",
      "m = ~(corr.mask(np.eye(len(corr), dtype=bool)).abs() > 0.75).any()\n",
      "\n",
      "raw = corr.loc[m, m]\n",
      "print(raw)\n",
      "\n",
      "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\n",
      "to_drop = \"Flavanoids\"\n",
      "\n",
      "# Drop that column from the DataFrame\n",
      "wine = wine.drop(to_drop, axis=1)\n",
      "\n",
      "       >Selecting features using text vectors\n",
      "\n",
      "looking at word weights\n",
      "print(tfidf_vec.vocabulary_)\n",
      "\n",
      "word weight and index of te word\n",
      "\n",
      "3 is the third row\n",
      "get the data and the indices\n",
      "text_tfidf[3].data\n",
      "text_tfidf[3].indices\n",
      "\n",
      "vocab= {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
      "\n",
      "reverses the indice and the word\n",
      "\n",
      "combine the vocabulary with the word vectorized weights\n",
      "\n",
      "zipped_row=dict(zip(text_tfidf[3].indices,\n",
      "text_tfidf[3].data))\n",
      "\n",
      "for index,item in zipped_row.items():\n",
      "    print(vocab.get(index),item)\n",
      "\n",
      "now you can see word importance\n",
      "\n",
      "\n",
      "# Add in the rest of the parameters\n",
      "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
      "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
      "    \n",
      "    # Let's transform that zipped dict into a series\n",
      "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
      "    \n",
      "    # Let's sort the series to pull out the top n weighted words\n",
      "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
      "    return [original_vocab[i] for i in zipped_index]\n",
      "\n",
      "original_vocab=tfidf_vec.vocabulary_\n",
      "\n",
      "# Print out the weighted words\n",
      "print(return_weights(vocab,original_vocab, text_tfidf, 8, 3))\n",
      "\n",
      "\n",
      "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
      "    filter_list = []\n",
      "    for i in range(0, vector.shape[0]):\n",
      "    \n",
      "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
      "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
      "        filter_list.extend(filtered)\n",
      "    # Return the list in a set, so we don't get duplicate word indices\n",
      "    return set(filter_list)\n",
      "\n",
      "# Call the function to get the list of word indices\n",
      "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
      "\n",
      "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
      "filtered_text = text_tfidf[:, list(filtered_words)]\n",
      "\n",
      "\n",
      "# Split the dataset according to the class distribution of category_desc\n",
      "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
      "\n",
      "# Fit the model to the training data\n",
      "nb.fit(train_X, train_y)\n",
      "\n",
      "# Print out the model's accuracy\n",
      "print(nb.score(test_X, test_y))\n",
      "\n",
      "the set remove the duplicate word weights\n",
      "\n",
      "\n",
      "      >Dimension reduction\n",
      "\n",
      "Principal component analysis\n",
      "\n",
      "linear transformation to uncorrelated space\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "pca=PCA()\n",
      "df_pca=pca.fit_transform(df)\n",
      "\n",
      "print(df_pca)\n",
      "\n",
      "end of preprocessing journey\n",
      "\n",
      "\n",
      "    >sample    pca\n",
      "\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "# Set up PCA and the X vector for diminsionality reduction\n",
      "pca = PCA()\n",
      "wine_X = wine.drop(\"Type\", axis=1)\n",
      "\n",
      "# Apply PCA to the wine dataset X vector\n",
      "transformed_X = pca.fit_transform(wine_X)\n",
      "\n",
      "# Look at the percentage of variance explained by the different components\n",
      "print(pca.explained_variance_ratio_)\n",
      "\n",
      "\n",
      "   > sample    pca transform  > knn.fit and score\n",
      "\n",
      "\n",
      "# Split the transformed X and the y labels into training and test sets\n",
      "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X,y)\n",
      "\n",
      "# Fit knn to the training data\n",
      "knn.fit(X_wine_train,y_wine_train)\n",
      "\n",
      "# Score knn on the test data and print it out\n",
      "knn.score(X_wine_test,y_wine_test)\n",
      "\n",
      "\n",
      "    >UFO preprocessing\n",
      "\n",
      "\n",
      "missing data: dropna() and notnull()\n",
      "type: astype()\n",
      "\n",
      "train_test_split(X,y,stratify=y)\n",
      "\n",
      "\n",
      "   > sample change the data types for two features\n",
      "\n",
      "# Check the column types\n",
      "print(ufo.dtypes)\n",
      "\n",
      "# Change the type of seconds to float\n",
      "ufo[\"seconds\"] = ufo[\"seconds\"].astype(float)\n",
      "\n",
      "# Change the date column to type datetime\n",
      "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
      "\n",
      "# Check the column types\n",
      "print(ufo[[\"seconds\",\"date\"]].dtypes)\n",
      "\n",
      "  > sample   > find no missing\n",
      "\n",
      "# Check how many values are missing in the length_of_time, state, and type columns\n",
      "print(ufo[[\"length_of_time\", \"state\", \"type\"]].isnull().sum())\n",
      "\n",
      "# Keep only rows where length_of_time, state, and type are not null\n",
      "ufo_no_missing = ufo[ufo[\"length_of_time\"].notnull() & \n",
      "          ufo[\"state\"].notnull() & \n",
      "          ufo[\"type\"].notnull()]\n",
      "\n",
      "# Print out the shape of the new dataset\n",
      "print(ufo_no_missing.shape)\n",
      "\n",
      "output:\n",
      "length_of_time    143\n",
      "state             419\n",
      "type              159\n",
      "\n",
      "\n",
      "   > categorical variables\n",
      "\n",
      "pd.get_dummies\n",
      "\n",
      "standardization\n",
      "var()\n",
      "np.log()\n",
      "\n",
      "    sample    return minutes\n",
      "\n",
      "def return_minutes(time_string):\n",
      "\n",
      "    # Use \\d+ to grab digits\n",
      "    pattern = re.compile(r\"\\d+\")\n",
      "    \n",
      "    # Use match on the pattern and column\n",
      "    num = re.match(pattern, time_string)\n",
      "    if num is not None:\n",
      "        return int(num.group(0))\n",
      "        \n",
      "# Apply the extraction to the length_of_time column\n",
      "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(lambda x: return_minutes(x))\n",
      "\n",
      "# Take a look at the head of both of the columns\n",
      "print(ufo[[\"minutes\",\"length_of_time\"]].head())\n",
      "\n",
      "\n",
      "output:\n",
      "minutes   length_of_time\n",
      "2    None  about 5 minutes\n",
      "4    None       10 minutes\n",
      "7    None        2 minutes\n",
      "8    None        2 minutes\n",
      "9    None        5 minutes\n",
      "In [1]:\n",
      ";\n",
      "\n",
      "  > sample  > ufo var\n",
      "\n",
      "print(ufo.var())\n",
      "\n",
      "seconds    424087.417474\n",
      "lat            39.757593\n",
      "long          379.590986\n",
      "minutes       117.546372\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "# Check the variance of the seconds and minutes columns\n",
      "print(ufo.var())\n",
      "\n",
      "# Log normalize the seconds column\n",
      "ufo[\"seconds_log\"] = np.log(ufo[\"seconds\"])\n",
      "\n",
      "# Print out the variance of just the seconds_log column\n",
      "print(ufo[\"seconds_log\"].var())\n",
      "\n",
      "\n",
      "       Engineering new features\n",
      "\n",
      "Month of the sighting\n",
      "description and vectorize\n",
      "\n",
      ".month and .hour\n",
      "\n",
      "regex\n",
      "\n",
      "\n",
      "   >Sample     encode country and type\n",
      "\n",
      "# Use Pandas to encode us values as 1 and others as 0\n",
      "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda x: 1 if x==\"us\" else 0)\n",
      "\n",
      "# Print the number of unique type values\n",
      "print(len(ufo['type'].unique()))\n",
      "\n",
      "# Create a one-hot encoded set of the type values\n",
      "type_set = pd.get_dummies(ufo['type'])\n",
      "\n",
      "# Concatenate this set back to the ufo DataFrame\n",
      "ufo = pd.concat([ufo, type_set], axis=1)\n",
      "\n",
      "\n",
      "  > sample    set the month and year of the date\n",
      "\n",
      "# Look at the first 5 rows of the date column\n",
      "print(ufo['date'].head())\n",
      "\n",
      "# Extract the month from the date column\n",
      "ufo[\"month\"] = ufo[\"date\"].apply(lambda x: x.month)\n",
      "\n",
      "# Extract the year from the date column\n",
      "ufo[\"year\"] = ufo[\"date\"].apply(lambda x:x.year)\n",
      "\n",
      "# Take a look at the head of all three columns\n",
      "print(ufo[[\"date\",\"month\",\"year\"]].head())\n",
      "\n",
      "\n",
      "  Feature selection and modeling\n",
      "\n",
      "* redundant features\n",
      "* text vector\n",
      "* know your dataset\n",
      "\n",
      "\n",
      "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
      "print(ufo[['seconds','seconds_log','minutes']].corr())\n",
      "\n",
      "seconds      1.000000     0.853371  0.980341\n",
      "seconds_log  0.853371     1.000000  0.824493\n",
      "minutes      0.980341     0.824493  1.000000\n",
      "\n",
      "# Make a list of features to drop\n",
      "to_drop = ['seconds','minutes','city','country']\n",
      "\n",
      "# Drop those features\n",
      "ufo_dropped = ufo.drop(to_drop,axis=1)\n",
      "\n",
      "# Let's also filter some words out of the text vector we created\n",
      "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)\n",
      "\n",
      "   sample  one hot encoding\n",
      "The y labels are the encoded country column, where 1 is us and 0 is ca\n",
      "\n",
      "# Take a look at the features in the X set of data\n",
      "print(X.columns)\n",
      "print(y)\n",
      "# Split the X and y sets using train_test_split, setting stratify=y\n",
      "train_X, test_X, train_y, test_y = train_test_split(X,y)\n",
      "\n",
      "# Fit knn to the training sets\n",
      "knn.fit(train_X,train_y)\n",
      "\n",
      "# Print the score of knn on the test sets\n",
      "print(knn.score(test_X,test_y))\n",
      "\n",
      "   >sample  use a list of filter words\n",
      "\n",
      "# Use the list of filtered words we created to filter the text vector\n",
      "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
      "print(filtered_text)\n",
      "\n",
      "# Split the X and y sets using train_test_split, setting stratify=y \n",
      "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
      "\n",
      "# Fit nb to the training sets\n",
      "nb.fit(train_X,train_y)\n",
      "\n",
      "# Print the score of nb on the test sets\n",
      "print(nb.score(test_X,test_y))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mC:\\Users\\dnishimoto.BOISE\\python\\XGboost for linear regression problems.txt\n",
      "\u001b[30m\n",
      "C:\\Users\\dnishimoto.BOISE\\python\\XGboost for linear regression problems.txt\n",
      "     >XG boost for regression\n",
      "1. predicting contineous or real values\n",
      "\n",
      "Root mean squared error (RMSE)\n",
      "Mean absolute error (MAE)\n",
      "\n",
      "RMSE is \n",
      "error=actual - predicted\n",
      "squaredError=error**2\n",
      "Totaling the squared error\n",
      "Mean of the squared error\n",
      "square root the mean of the squared error\n",
      "\n",
      "punishes larger errors more\n",
      "\n",
      "MAE\n",
      "\n",
      "error=actual - predicted\n",
      "total the absolute error\n",
      "Mean of the total absolute error\n",
      "\n",
      "less frequently used\n",
      "\n",
      "\n",
      "  Objective (loss) functions and base learners\n",
      "\n",
      "objective function (loss) quantifies how far off a prediction is from the actual results\n",
      "\n",
      "Measures the difference between the prediction and the target for some collection of data\n",
      "\n",
      "Goal: Find the model that yields the minimum value of the loss function.\n",
      "\n",
      "Loss Function names\n",
      "\treg:linear - use for regression problems\n",
      "\n",
      "\treg:logistic - use for classification problems when you want decision not probability.\n",
      "\t\n",
      "\treg:logistic - when you want the probability rather than just the decision\n",
      "\n",
      "\n",
      "XGBoost model is composed of many individual models that combine to give a final prediction\n",
      "\n",
      "each of the individual models is combined the base learners that is slight better than random guessing.  The base learners that are better than 50% are combined into a single prediction.\n",
      "\n",
      "the base learners are non-linear\n",
      "\n",
      "there are two kinds of base learners: tree and linear\n",
      "\n",
      "import xgboost as xgb\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "https://github.com/ageron/handson-ml/blob/master/datasets/housing/housing.csv\n",
      "\n",
      "https://github.com/girishkuniyal/Predict-housing-prices-in-Portland\n",
      "\n",
      "https://www.datasethub.com/datasets/idaho/filetypes/csv\n",
      "\n",
      "https://people.ischool.berkeley.edu/~chandangope/project/\n",
      "\n",
      "\n",
      "X_train, X_test, y_train, x_test= train_test_split(X,y , test_size=0.2, random_state=42)\n",
      "\n",
      "\n",
      "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
      "\n",
      "xg_reg.fit(X_train, y_train)\n",
      " \n",
      "\n",
      "preds= xg_reg.predict(X_test)\n",
      "\n",
      "rmse=np.sqrt(mean_squared_error(y_test,y_preds))\n",
      "\n",
      "print(RMSE: %f\" %(rmse))\n",
      "\n",
      "  >Linear base learners\n",
      "- a linear learner\n",
      "- allows you to create a regularized linear regression using XGBoost's powerful learning API\n",
      "\n",
      "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
      "DM_test = xgb.DMatrix(data=X_test,label=y_test)\n",
      "\n",
      "#params={\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
      "params={\"booster\":\"gblinear\",\"objective\":\"reg:squarederror\"}\n",
      "\n",
      "xg_reg=xgb.train(params=params, dtrain=DM_train, num_boost_round=10)\n",
      "\n",
      "pred=xg_reg.predict(DM_test)\n",
      "\n",
      "rmse=np.sqrt(mean_squared_error(y_test,y_pred))\n",
      "\n",
      "print(\"RMSE: %f\" %(rmse))\n",
      "\n",
      "\n",
      "  Sample XGboose (reg:linear)\n",
      "\n",
      "# Create the training and test sets\n",
      "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
      "\n",
      "# Instantiate the XGBRegressor: xg_reg\n",
      "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
      "\n",
      "# Fit the regressor to the training set\n",
      "\n",
      "xg_reg.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labels of the test set: preds\n",
      "preds= xg_reg.predict(X_test)\n",
      "\n",
      "# Compute the rmse: rmse\n",
      "rmse=np.sqrt(mean_squared_error(y_test,preds))\n",
      "print(\"RMSE: %f\" % (rmse))\n",
      "\n",
      " >Sample XGBoost (trees)\n",
      "\n",
      "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
      "\n",
      "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
      "DM_test = xgb.DMatrix(data=X_test,label=y_test)\n",
      "\n",
      "# Create the parameter dictionary: params\n",
      "params={\"booster\":\"gblinear\",\"objective\":\"reg:squarederror\"}\n",
      "\n",
      "# Train the model: xg_reg\n",
      "xg_reg=xgb.train(params=params, dtrain=DM_train, num_boost_round=5)\n",
      "\n",
      "# Predict the labels of the test set: preds\n",
      "preds=xg_reg.predict(DM_test)\n",
      "\n",
      "# Compute and print the RMSE\n",
      "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
      "print(\"RMSE: %f\" % (rmse))\n",
      "\n",
      " >Cross validation (rmse)\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary: params\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
      "\n",
      "# Perform cross-validation: cv_results\n",
      "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "\n",
      "# Print cv_results\n",
      "print(cv_results)\n",
      "\n",
      "# Extract and print final boosting round metric\n",
      "print(cv_results[\"test-rmse-mean\"].tail(1))\n",
      "\n",
      "\n",
      " >Cross validation (mae)\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary: params\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
      "\n",
      "# Perform cross-validation: cv_results\n",
      "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
      "\n",
      "# Print cv_results\n",
      "print(cv_results)\n",
      "\n",
      "# Extract and print final boosting round metric\n",
      "print(cv_results[\"test-mae-mean\"].tail(1))\n",
      "\n",
      "\n",
      "               >regularization\n",
      "\n",
      "regularization penalizes a model as it get more complex\n",
      "want models that are accurate and as simple as possible\n",
      "\n",
      "regularization parameters in xgboost\n",
      "1. gamma - minimum loss reduction allowed for split to occur\n",
      "2. alpha - l1 regularization on leaf weights, large value means more regularization\n",
      "a. causes many leafs in the base learners\n",
      "3. lambda is another name of l2 regularization on leaf weights\n",
      "a. smoother penalty on large numbers and decrease gradually\n",
      "\n",
      "boston_dmatrix = xgb.DMatrix(data=X,label=y)\n",
      "params={\"objective\":\"reg:linear\",\"max_depth\":4}\n",
      "\n",
      "l1_params=[1,10,100]\n",
      "rmses_l1=[]\n",
      "\n",
      "for reg in l1_params:\n",
      "\tparams['alpha']=reg\n",
      "\tcv_results= xgb.cv(dtrain=boston_dmatrix, params=params, nfold=4,\n",
      "\tnum_boost_round=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "\trmse_l1.append(cv_results[\"test-rmse-mean\"].tail(1).values[0])\n",
      "\n",
      "num_boost_round is the number of trees\n",
      "\n",
      "print(\"best rmse as a function of l1:\")\n",
      "\n",
      "print(pd.DataFrame(list(zip(l1_params,rmse_l1)), columns(\"l1\",\"rmse\"]))\n",
      "\n",
      "\n",
      "       >Base learners in XGBoost\n",
      "\n",
      "1. Linear base learner\n",
      "a. Sum of linear terms\n",
      "b. The boosted model is a weighted sum of linear models which is itself linear\n",
      "\n",
      "2. The tree based learner\n",
      "a. decision trees as base models\n",
      "b. The boosted model is the weighted sum of decision trees (non linear)\n",
      "c. almost exclusively used in XGBoost\n",
      "\n",
      "\n",
      "zip creates a generator of parallel values\n",
      "\n",
      "zip([1,2,3],[\"a\",\"b\",\"c\"])\n",
      "output: [1,\"a\"],[2,\"b\"],[3,\"c\"]\n",
      "\n",
      "list() instantiates the full generator and passing that into the dataframe which converts the whole expression.\n",
      "\n",
      " >Sample  to_dict\n",
      "\n",
      "keys = ['name', 'age', 'food']\n",
      "values = ['Monty', 42, 'spam']\n",
      "index=np.arange(0,len(keys)-1)\n",
      "\n",
      "df=pd.DataFrame(list(zip(keys,values)), columns=['keys','values'])\n",
      "df.set_index('keys')\n",
      "print(df.head())\n",
      "\n",
      "data_dict = df.iloc[index].set_index('keys')['values'].to_dict() \n",
      "print(data_dict)\n",
      "\n",
      " >Sample L2 regularization\n",
      "l2 regularization penalty - also known as \"lambda\" - and see its effect on overall model performance on the Ames housing dataset.\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "reg_params = [1, 10, 100]\n",
      "\n",
      "# Create the initial parameter dictionary for varying l2 strength: params\n",
      "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
      "\n",
      "# Create an empty list for storing rmses as a function of l2 complexity\n",
      "rmses_l2 = []\n",
      "\n",
      "# Iterate over reg_params\n",
      "for reg in reg_params:\n",
      "\n",
      "    # Update l2 strength\n",
      "    params[\"lambda\"] = reg\n",
      "    \n",
      "    # Pass this updated param dictionary into cv\n",
      "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "    \n",
      "    # Append best rmse (final round) to rmses_l2\n",
      "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
      "\n",
      "# Look at best rmse per l2 param\n",
      "print(\"Best rmse as a function of l2:\")\n",
      "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\",\"rmse\"]))\n",
      "\n",
      " >Sample  Plot the Trees\n",
      "Have a look at each of the plots. They provide insight into how the model arrived at its final decisions and what splits it made to arrive at those decisions.\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary: params\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
      "\n",
      "# Train the model: xg_reg\n",
      "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
      "\n",
      "# Plot the first tree\n",
      "xgb.plot_tree(xg_reg,num_trees=0)\n",
      "plt.show()\n",
      "\n",
      "# Plot the fifth tree\n",
      "xgb.plot_tree(xg_reg,num_trees=4)\n",
      "plt.show()\n",
      "\n",
      "# Plot the last tree sideways\n",
      "xgb.plot_tree(xg_reg,num_trees=9, rankdir='LR')\n",
      "plt.show()\n",
      "\n",
      "  Sample Plot the importance features\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary: params\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
      "\n",
      "# Train the model: xg_reg\n",
      "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
      "\n",
      "# Plot the feature importances\n",
      "xgb.plot_importance(xg_reg)\n",
      "plt.show()\n",
      "\n",
      "  >Why tuning\n",
      "\n",
      "\n",
      "\n",
      "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
      "untunedParams={\"objective\":\"reg:linear\"}\n",
      "\n",
      "untuned_cv_results=xgb.cv(dtrain=housing_dmatrix, params=untuned_params, nfold=4, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "\n",
      "print(\"Untuned rmse: %f\" %((untuned_cv_results[\"test-mae-mean\"]).tail(1)))\n",
      "\n",
      "    \n",
      "  A more turn parameter resultset\n",
      "\n",
      "tunedParams={\"objective\":\"reg:linear\",\"colsample_bytree\":0.3, \"learning_rate\":0.1, \"max_depth\":5}\n",
      "\n",
      "tuned_cv_results=xgb.cv(dtrain=housing_dmatrix, params=tuned_params, nfold=4, num_boost_round=200, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "\n",
      "print(\"tuned rmse: %f\" %((tuned_cv_results[\"test-mae-mean\"]).tail(1)))\n",
      "\n",
      "\n",
      "  Sample tuning with number of trees\n",
      "\n",
      "# Create the DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
      "\n",
      "# Create the parameter dictionary for each tree: params \n",
      "params = {\"objective\":\"reg:linear\",'max_depth':3}\n",
      "\n",
      "\n",
      "# Create list of number of boosting rounds\n",
      "num_rounds = [5, 10, 15]\n",
      "\n",
      "# Empty list to store final round rmse per XGBoost model\n",
      "final_rmse_per_round = []\n",
      "\n",
      "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
      "for curr_num_rounds in num_rounds:\n",
      "\n",
      "    # Perform cross-validation: cv_results\n",
      "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "    \n",
      "    # Append final round RMSE\n",
      "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
      "\n",
      "# Print the resultant DataFrame\n",
      "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
      "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))\n",
      "\n",
      "  Sample  early stop rounds\n",
      "\n",
      "\n",
      "# Create your housing DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary for each tree: params\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
      "\n",
      "# Perform cross-validation with early stopping: cv_results\n",
      "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,early_stopping_rounds=10, num_boost_round=50, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "\n",
      "# Print cv_results\n",
      "print(\"rmse: %f\" %((cv_results[\"test-rmse-mean\"]).tail(1)))\n",
      "\n",
      "           >Tree Tunable parameters\n",
      "\n",
      "*learning rate: low learning rate will take more trees to reduce residual error (boosting rounds)\n",
      "* gamma: min loss reduction to create new tree split (regularized)\n",
      "* lambda: l2 reg on leaf weights (regularized)\n",
      "* max_depth : max depth per tree\n",
      "* subsample: % samples used per tree (low value - underfitting and high value - overfitting)\n",
      "* colsample_bytree: %features used per tree (features 0 to 1) a small colsample_bytree value means that additional regularization is being added to the model.\n",
      "\n",
      "           >Linear base learner parameters\n",
      "* lambda: L2 reg on weights\n",
      "* alpha: L1 reg on weights\n",
      "* lambda_bias: L2 reg term on bias\n",
      "\n",
      "You can also tune the number of estimators used for both base model types!\n",
      "\n",
      "\n",
      "  Sample Tuning the learning rate (eta)\n",
      "\n",
      "# Create your housing DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
      "\n",
      "# Create the parameter dictionary for each tree (boosting round)\n",
      "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
      "\n",
      "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
      "eta_vals = [0.001,0.01,0.1]\n",
      "best_rmse = []\n",
      "\n",
      "# Systematically vary the eta \n",
      "for curr_val in eta_vals:\n",
      "\n",
      "    params[\"eta\"] = curr_val\n",
      "    \n",
      "    # Perform cross-validation: cv_results\n",
      "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,early_stopping_rounds=5, num_boost_round=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "    \n",
      "    \n",
      "    \n",
      "    # Append the final round rmse to best_rmse\n",
      "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
      "\n",
      "# Print the resultant DataFrame\n",
      "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))\n",
      "\n",
      "   Sample tuning (max depths)\n",
      "\n",
      "# Create your housing DMatrix: housing_dmatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
      "\n",
      "# Create the parameter dictionary\n",
      "params = {\"objective\":\"reg:linear\"}\n",
      "\n",
      "# Create list of max_depth values\n",
      "max_depths = [2, 5, 10, 20]\n",
      "best_rmse = []\n",
      "\n",
      "# Systematically vary the max_depth\n",
      "for curr_val in max_depths:\n",
      "\n",
      "    params[\"max_depth\"] = curr_val\n",
      "    \n",
      "    # Perform cross-validation\n",
      "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
      "                 num_boost_round=10, early_stopping_rounds=5,\n",
      "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "    \n",
      "    # Append the final round rmse to best_rmse\n",
      "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
      "\n",
      "# Print the resultant DataFrame\n",
      "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))\n",
      "\n",
      "  Sample colsample_bytree_vals\n",
      "\n",
      "# Create your housing DMatrix\n",
      "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
      "\n",
      "# Create the parameter dictionary\n",
      "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
      "\n",
      "# Create list of hyperparameter values: colsample_bytree_vals\n",
      "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
      "best_rmse = []\n",
      "\n",
      "# Systematically vary the hyperparameter value \n",
      "for curr_val in colsample_bytree_vals:\n",
      "\n",
      "    params[\"colsample_bytree\"] = curr_val\n",
      "    \n",
      "    # Perform cross-validation\n",
      "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
      "                 num_boost_round=10, early_stopping_rounds=5,\n",
      "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
      "    \n",
      "    # Append the final round rmse to best_rmse\n",
      "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
      "\n",
      "# Print the resultant DataFrame\n",
      "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))\n",
      "\n",
      "\n",
      "  Review of grid search and random search\n",
      "\n",
      "grid search : exhaustive searches of a given hyperparameter\n",
      "\n",
      "\n",
      "   GridSearchCV\n",
      "\n",
      "# Import GridSearchCV\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "gbm_param_grid = {'learning_rate':[0.01,0.1,0.5,0.9]\n",
      "\t'n_estimators':[200],\n",
      "\t'subsample':[0.3,0.5,0.9]}\n",
      "\n",
      "gbm=xgb.XGBRegressor()\n",
      "\n",
      "grid_mse=GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
      "scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
      "\n",
      "\n",
      "grid_mse.fit(X,y)\n",
      "\n",
      "\n",
      "print(\"Best parameters found:\", grid_mse.best_params_)\n",
      "print(\"Lowest RMSE found:, np.sqrt(grid_mse.best_score_)))\n",
      "\n",
      "\n",
      "  Random search \n",
      "* you set the number of iterations you would like to random search to continue\n",
      "* you create the range of hyperparameters values per hyper parameter - randomly draw value from the range of hyperparameters values\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "import random\n",
      "\n",
      "param_dist = {'learning_rate':np.arange(0.05,1.05,.05), 'n_estimators':[200], 'subsample':np.arange(0.05,1.05,.05),}\n",
      "\n",
      "gbm=xgb.XGBRegressor()\n",
      "\n",
      "random_search = RandomizedSearchCV(estimator=gbm,param_distributions=param_dist, n_iter=25,\n",
      "scoring='neg_mean_squared_error',cv=4,verbose=1)\n",
      "random_search.fit(X, y)\n",
      "\n",
      "print(\"Best Parameters\",random_search.best_params_)\n",
      "print(\"Lowest RMSE found:\",np.sqrt(np.abs(random_search.best_score_)))\n",
      "\n",
      "\n",
      "  Sample search GridSearchCV\n",
      "\n",
      "gbm_param_grid = {'colsample_bytree':[0.3,0.7],\n",
      "\t'n_estimators':[50],\n",
      "\t'max_depth':[2,5]}\n",
      "\n",
      "gbm=xgb.XGBRegressor()\n",
      "\n",
      "# Perform grid search: grid_mse\n",
      "grid_mse=GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
      "scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
      "grid_mse.fit(X,y)\n",
      "\n",
      "\n",
      "# Print the best parameters and lowest RMSE\n",
      "print(\"Best parameters found: \", grid_mse.best_params_)\n",
      "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n",
      "\n",
      "\n",
      "  >Sample RandomSearchCV\n",
      "\n",
      "# Create the parameter grid: gbm_param_grid \n",
      "gbm_param_grid = {\n",
      "    'n_estimators': [25],\n",
      "    'max_depth': np.arange(2, 12)\n",
      "}\n",
      "\n",
      "# Instantiate the regressor: gbm\n",
      "gbm = xgb.XGBRegressor(n_estimators=10)\n",
      "\n",
      "# Perform random search: grid_mse\n",
      "randomized_mse = RandomizedSearchCV(estimator=gbm,param_distributions=gbm_param_grid, n_iter=5, scoring='neg_mean_squared_error',cv=4,verbose=1)\n",
      "randomized_mse.fit(X, y)\n",
      "\n",
      "      Review of pipelines using sklearn\n",
      "\n",
      "takes a list of named 2-tuples (name, pipeline_step) as input\n",
      "\n",
      "the pipeline can contain estimator or transformer objects\n",
      "\n",
      "pipeline implement fit/predict models\n",
      "\n",
      "pipelines can be used as input estimators into grid and randomized search and cross_val_score methods\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "https://github.com/eric-bunch/boston_housing\n",
      "\n",
      "\n",
      "rf_pipeline = Pipeline[(\"st_scaler\",StandardScaler()),\n",
      "                       (\"rf_model\",RandomForestRegressor())]\n",
      "\n",
      "\n",
      "\n",
      "scores=cross_val_score(rf_pipeline,X,y,\n",
      "\tscoring=\"neg_mean_squared_error\", cv=10)\n",
      "\n",
      "final_avg_rmse=np.mean(np.sqrt(np.abs(scores)))\n",
      "\n",
      "print(\"Final RMSE:\", final_avg_rmse)\n",
      "\n",
      "  >label encoder and onehotencoder\n",
      "\n",
      "1. LabelEncoder converts a categorical column of strings into integers\n",
      "\n",
      "2. OneHotEncoder: takes a column of integers and encodes them as dummy variables where each variable is a column\n",
      "\n",
      "***Can not be done within a pipeline\n",
      "\n",
      "\n",
      "      Preprocessing with DictVectorizer\n",
      "\n",
      "*Traditionally used in text processing\n",
      "* converts lists of feature mappings into vectors\n",
      "* we need to convert a dataframe into a list of dictionary entries\n",
      "\n",
      "  Sample (LabelEncoder) find the objects\n",
      "\n",
      "# Import LabelEncoder\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "# Fill missing values with 0\n",
      "df.LotFrontage = df.LotFrontage.fillna(0)\n",
      "\n",
      "# Create a boolean mask for categorical columns\n",
      "categorical_mask = (df.dtypes == object)\n",
      "\n",
      "# Get list of categorical column names\n",
      "categorical_columns = df.columns[categorical_mask].tolist()\n",
      "\n",
      "# Print the head of the categorical columns\n",
      "print(df[categorical_columns].head())\n",
      "\n",
      "# Create LabelEncoder object: le\n",
      "le = LabelEncoder()\n",
      "\n",
      "\n",
      "  >XGboost pipeline\n",
      "\n",
      "# Import necessary modules\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "# Fill LotFrontage missing values with 0\n",
      "X.LotFrontage = X.LotFrontage.fillna(0)\n",
      "\n",
      "\n",
      "# Setup the pipeline steps: steps\n",
      "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
      "         (\"xgb_model\", xgb.XGBRegressor())]\n",
      "\n",
      "# Create the pipeline: xgb_pipeline\n",
      "xgb_pipeline = Pipeline(steps)\n",
      "\n",
      "# Fit the pipeline\n",
      "xgb_pipeline.fit(X.to_dict(\"records\"), y)\n",
      "\n",
      "# Apply LabelEncoder to categorical columns\n",
      "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
      "\n",
      "# Print the head of the LabelEncoded categorical columns\n",
      "print(df[categorical_columns].head())\n",
      "\n",
      "  >Sample (OneHotEncoding)\n",
      "\n",
      "# Import OneHotEncoder\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "# Create OneHotEncoder: ohe\n",
      "ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)\n",
      "\n",
      "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
      "df_encoded = ohe.fit_transform(df)\n",
      "\n",
      "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
      "print(df_encoded[:5, :])\n",
      "\n",
      "# Print the shape of the original DataFrame\n",
      "print(df.shape)\n",
      "\n",
      "# Print the shape of the transformed array\n",
      "print(df_encoded.shape)\n",
      "\n",
      "\n",
      "  Sample (DictVectorizer) label and encode at the same time\n",
      "\n",
      "# Import DictVectorizer\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "# Convert df into a dictionary: df_dict\n",
      "df_dict = df.to_dict(\"records\")\n",
      "\n",
      "# Create the DictVectorizer object: dv\n",
      "dv = DictVectorizer(sparse=False)\n",
      "\n",
      "# Apply dv on df: df_encoded\n",
      "df_encoded = dv.fit_transform(df_dict)\n",
      "\n",
      " >Sample (xgboost in the pipeline\n",
      "# Import necessary modules\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "# Fill LotFrontage missing values with 0\n",
      "X.LotFrontage = X.LotFrontage.fillna(0)\n",
      "\n",
      "\n",
      "# Setup the pipeline steps: steps\n",
      "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
      "         (\"xgb_model\", xgb.XGBRegressor())]\n",
      "\n",
      "# Create the pipeline: xgb_pipeline\n",
      "xgb_pipeline = Pipeline(steps)\n",
      "\n",
      "# Fit the pipeline\n",
      "xgb_pipeline.fit(X.to_dict(\"records\"), y)\n",
      "\n",
      "     Incorporating xgboost into pipelines\n",
      "\n",
      "# Import necessary modules\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "# Fill LotFrontage missing values with 0\n",
      "X.LotFrontage = X.LotFrontage.fillna(0)\n",
      "\n",
      "\n",
      "# Setup the pipeline steps: steps\n",
      "steps = [ (\"st_scaler\", StandardScaler()),\n",
      "\t (\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
      "         (\"xgb_model\", xgb.XGBRegressor())]\n",
      "\n",
      "# Create the pipeline: xgb_pipeline\n",
      "xgb_pipeline = Pipeline(steps)\n",
      "\n",
      "# Fit the pipeline\n",
      "xgb_pipeline.fit(X.to_dict(\"records\"), y)\n",
      "\n",
      "\n",
      "scores=cross_val_score(xg_pipeline,X,y,\n",
      "\tscoring=\"neg_mean_squared_error\", cv=10)\n",
      "\n",
      "final_avg_rmse=np.mean(np.sqrt(np.abs(scores)))\n",
      "print(\"Final XGB RMSE:\",final_avg_rmse)\n",
      "\n",
      "   sklearn_pandas\n",
      "1. DataFrameMapper - Interoperability between pandas and scikit-learn\n",
      "\n",
      "2. CategoricalImputer - Allow for imputation of categorical variables before conversion to integers\n",
      "\n",
      "3. sklearn.preprocessing - Imputer - Native imputation of numerical columns in scikit-learn\n",
      "\n",
      "4. sklearn.pipeline:\n",
      "\tfeatureUnion - combine multiple pipelines of features into a single pipeline of features\n",
      "\n",
      "\n",
      "  >Sample Creating the xgboost pipeline\n",
      "\n",
      "# Import necessary modules\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "# Fill LotFrontage missing values with 0\n",
      "X.LotFrontage = X.LotFrontage.fillna(0)\n",
      "\n",
      "# Setup the pipeline steps: steps\n",
      "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
      "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:squarederror\"))]\n",
      "\n",
      "# Create the pipeline: xgb_pipeline\n",
      "xgb_pipeline = Pipeline(steps)\n",
      "\n",
      "xgb_pipeline.fit(X.to_dict(\"records\"), y)\n",
      "\n",
      "# Cross-validate the model\n",
      "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict(\"records\"), y, cv=10, scoring=\"neg_mean_squared_error\")\n",
      "\n",
      "\n",
      "# Print the 10-fold RMSE\n",
      "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))\n",
      "\n",
      "\n",
      "    Sample using categorical imputer\n",
      "\n",
      "Specifically, you'll be able to impute missing categorical values directly using the Categorical_Imputer() class in sklearn_pandas, and the DataFrameMapper() class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.\n",
      "\n",
      "We've also created a transformer called a Dictifier that encapsulates converting a DataFrame using .to_dict(\"records\") without you having to do it explicitly (and so that it works in a pipeline). Finally, we've also provided the list of feature names in kidney_feature_names, the target name in kidney_target_name, the features in X, and the target in y\n",
      "\n",
      "# Import necessary modules\n",
      "from sklearn_pandas import DataFrameMapper\n",
      "from sklearn_pandas import CategoricalImputer\n",
      "\n",
      "# Check number of nulls in each feature column\n",
      "nulls_per_column = X.isnull().sum()\n",
      "print(nulls_per_column)\n",
      "\n",
      "# Create a boolean mask for categorical columns\n",
      "categorical_feature_mask = X.dtypes == object\n",
      "\n",
      "# Get list of categorical column names\n",
      "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
      "\n",
      "# Get list of non-categorical column names\n",
      "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
      "\n",
      "# Apply numeric imputer\n",
      "numeric_imputation_mapper = DataFrameMapper(\n",
      "                                            [([numeric_feature], Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
      "                                            input_df=True,\n",
      "                                            df_out=True\n",
      "                                           )\n",
      "\n",
      "# Apply categorical imputer\n",
      "categorical_imputation_mapper = DataFrameMapper(\n",
      "                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],\n",
      "                                                input_df=True,\n",
      "                                                df_out=True\n",
      "                                               )\n",
      "\n",
      "\n",
      "   Sample FeatureUnion\n",
      "\n",
      "# Import FeatureUnion\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "\n",
      "# Combine the numeric and categorical transformations\n",
      "numeric_categorical_union = FeatureUnion([\n",
      "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
      "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
      "                                         ])\n",
      "\n",
      "   Sample Full Pipeline\n",
      "\n",
      "# Create full pipeline\n",
      "pipeline = Pipeline([\n",
      "                     (\"featureunion\", numeric_categorical_union),\n",
      "                     (\"dictifier\", Dictifier()),\n",
      "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
      "                     (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
      "                    ])\n",
      "\n",
      "# Perform cross-validation\n",
      "cross_val_scores = cross_val_score(pipeline, kidney_data, y, cv=3, scoring=\"roc_auc\")\n",
      "\n",
      "# Print avg. AUC\n",
      "print(\"3-fold AUC: \", np.mean(cross_val_scores))\n",
      "\n",
      "\n",
      "   >Tuning hyperparameter in a pipeline\n",
      "\n",
      "xbg_pipeline = Pipeline[('st_scaler',StandardScaler()),\n",
      "\t\t('xgb_model',xgb.XGBRegressor())]\n",
      "\n",
      "\n",
      "gbm_param_grid={\n",
      "\t'xgb_model__subsample': np.arange(.05,1,.05),\n",
      "\t'xgb_model__max_depth': np.arange(3,20,1),\n",
      "\t'xgb_model__colsample_bytree': np.arange(.1,1.05,.05)}\n",
      "\n",
      "\n",
      "randomized_neg_mse= RandomizedSearchCV(estimator=xgb_pipeline,\n",
      "\tparam_distributions=gbm_param_grid, n_iter=10,\n",
      "\tscoring='neg_mean_squared_error', cv=4)\n",
      "\n",
      "\n",
      "\n",
      "randomized_neg_mse.fit(X, y)\n",
      "\n",
      "print(\"Best rmse: \", np.sqrt(np.abs(randomized_neg_mse.best_score_)))\n",
      "\n",
      "  Sample RandomSearchCV the pipeline\n",
      "\n",
      "# Create the parameter grid\n",
      "gbm_param_grid = {\n",
      "    'clf__learning_rate': np.arange(.05, 1, .05),\n",
      "    'clf__max_depth': np.arange(3,10, 1),\n",
      "    'clf__n_estimators': np.arange(50, 200, 50)\n",
      "}\n",
      "\n",
      "# Perform RandomizedSearchCV\n",
      "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,\n",
      "                                        param_distributions=gbm_param_grid,\n",
      "                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n",
      "\n",
      "# Fit the estimator\n",
      "randomized_roc_auc.fit(X, y)\n",
      "\n",
      "# Compute metrics\n",
      "print(\"Best rmse: \", randomized_roc_auc.best_score_)\n",
      "print(randomized_roc_auc.best_estimator_)\n",
      "\n",
      "\n",
      "example of a pipeline\n",
      "https://stackoverflow.com/questions/52055658/sklearn-pandas-in-a-pipeline-returns-typeerror-builtin-function-or-method-obj\n",
      "\n",
      "https://dunyaoguz.github.io/my-blog/dataframemapper.html\n",
      "\n",
      "\n",
      "Installation\n",
      "pip install https://github.com/scikit-learn/scikit-learn/archive/master.zip\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#,'apple', 'orange', 'banana'\n",
    "search=['labelencoder']\n",
    "search=list(map(lambda x: x.upper(),search))\n",
    "path=os.path.expanduser('~\\python')\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filename=path + \"\\\\\" +filename\n",
    "        #if filename==r\"C:\\Users\\dnishimoto.BOISE\\python\\decision tree machine learning.txt\":\n",
    "        with open(filename) as fin:\n",
    "            text=(fin.read())\n",
    "            text=text.replace('>>',' ')\n",
    "                #print(text)\n",
    "            mywords=text.split(' ')\n",
    "            mywords=map(lambda x: x.upper(),mywords)\n",
    "            mywords=[x.replace('\\n','') for x in mywords if x]\n",
    "            #print(mywords)\n",
    "            if any([x in search  for x in mywords]):\n",
    "                print(Fore.RED+filename)\n",
    "                print(Fore.BLACK)\n",
    "                print(\"{}\\n{}\".format(filename,text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
