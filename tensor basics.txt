import tensorflow as tf

d0 = tf.ones((1,))
#1D
d1 = tf.ones((2,))
#2D
d2 = tf.ones((2,2))
#3d
d3 = tf.ones((2,2,2))

print(d3.numpy())

#defines a 2x3 constant
a= constant(3, shape=[2,3])

#define a 2x2 constant
b= constant([1,2,3,4], shape[2,2])

operation
tf.constant()  constant([1,2,3])
tf.zeros() zeros([2,2])
tf.zeros_like() zeros_like(input_tensor)
tf.ones() ones([2,2])
tf.ones_like() ones_like(input_tensor)
tf.fill() fill([3,3],7)

the variable shape is fixed but the values of the shape can change during run time.

a0 = tf.Variable([1,2,3,4,5,6], dtype=tf.float32)
a1 = tf.Variable([1,2,3,4,5,6], dtype=tf.int16)

b=tf.constant(2,tf.float32)

c0=tf.multiply(a0,b)
c1=a0*b  #tf.multiply is overloaded allowing this expression to be used

>>>>>>sample  >>> convert a np array into a tensor flow constant

# Import constant from TensorFlow
from tensorflow import constant

print(credit_numpy)
# Convert the credit_numpy array into a tensorflow constant
credit_constant = constant(credit_numpy)

# Print constant datatype
print('The datatype is:', credit_constant.dtype)

# Print constant shape
print('The shape is:', credit_constant.shape)

>>>> sample >>> create a variable then convert it to a numpy array

# Define the 1-dimensional variable A1
A1 = Variable([1, 2, 3, 4])

# Print the variable A1
print(A1)

# Convert A1 to a numpy array and assign it to B1
B1 = A1.numpy()

# Print B1
print(B1)


>>>>>>>>>>>basic operations

graphs contain edges and nodes

where the edges are tensors and the nodes are operations

MatMul
Add

Const & Const_1 are fed to Add_1
Const_2 & const_3 are fed to Add resulting in Add_2

Add_1 & Add_2 are fed to MatMul

from tensorflow import constant, add

A0=constant([1])
B0=constant([2])

1 dimensional tensors
A1=constant([1,2])
B1=constant([3,4])

2 dimensional tensors
A2 = constant([1,2],[3,4])
B2 = constant([5,6],[7,8])

C0=add(A0,B0)
C1=add(A1,B1)
C2=add(A2,B2)

add requires that each tensor have the same shape

from tensorflow import ones, matmul, multiply

a0=ones(1)
a31=ones([3,1])
a34=ones([3,4])
a43=ones([4,3])

matmul(A43,A34) but not matmul(A43,A43)

reduce_sum() sums over the dimension of a tensor

A=ones([2,3,4])
or 2*3*4 = 24 ones

x=
1 1 1
1 1 1

reduce_sum(x,0)
[1,1,1]+[1,1,1]

reduce_sum(x,1)
[1,1]+[1,1]+[1,1]

#sum over all dimensions
B= reduce_sum(A) -> 24

B0=reduce_sum(A,0) 3x4 of 2
B1=reduce_sum(A,1) 2x4 of 3
B3=reduce_sum(A,2) 2x3 of 4


>>>> sample >>> tensor multiplication of two tensors

# Define tensors A1 and A23 as constants
A1 = constant([1, 2, 3, 4])
A23 = constant([[1, 2, 3], [1, 6, 4]])

# Define B1 and B23 to have the correct shape
B1 = ones_like(A1)
B23 = ones_like(A23)

# Perform element-wise multiplication
C1 = A1*B1
C23 = A23*B23

# Print the tensors C1 and C23
print('C1: {}'.format(C1.numpy()))
print('C23: {}'.format(C23.numpy()))


>>>>>>sample >>>> tensor matmul

# Define features, params, and bill as constants
features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])
params = constant([[1000], [150]])
bill = constant([[3913], [2682], [8617], [64400]])

# Compute billpred using features and params
billpred =matmul(features,params)

# Compute and print the error
error = bill-billpred 
print(error.numpy())

output:
[[-1687]
 [-3218]
 [-1933]
 [57850]]


wealth=
[11 50]
[7  2 ]
[4  60]
[3  0 ]
[25 10]

>>>>>>>>>advanced operations

gradient() computes the slope of a function at a point
reshape() reshapes a tensor
random() populates tensor with entries drawn from a probability distribution

need to find a optimum

Minimum : lowest value of a loss function
Maximum : highest value of a objective function

gradient : find a point where the gradient is 0

check if the gradient is increasing or decreasing
minimum : change in gradient > 0
maximum : change in gradient < 0

x=tf.Variable(-1.0)

with tf.GradientTape() as tape:
	tape.watch(x)
	y=tf.multiply(x,x)

g=tape.gradient(y,x)
print(g.numpy())

>>>>>>>>>>reshaping 
grayscale 

grays 0 - 255

import tensorflow as tf

#generate a 2x2 matrix
gray = tf.random.uniform([2,2],maxval=255, dtype='int32')

#reshape into a 4x1 vector
gray = tf.reshape(gray,[2*2,1])

color = tf.random.uniform([2,2,3],maxval=255,dtype='int32')

color=tf.reshape(color,[2x2,3])


>>>>>>>sample >>> sign language tensor

# Reshape the grayscale image tensor into a vector
gray_vector = reshape(gray_tensor, (28*28, 1))

# Reshape the color image tensor into a vector
color_vector = reshape(color_tensor, (28*28, 3))


>>>>>>sample >>> compute the gradient for different x along the a parabolia

def compute_gradient(x0):
  	# Define x as a variable with an initial value of x0
	x = Variable(x0)
	with GradientTape() as tape:
		tape.watch(x)
        # Define y using the multiply operation
		y = multiply(x,x)
    # Return the gradient of y with respect to x
	return tape.gradient(y, x).numpy()

# Compute and print gradients at x = -1, 1, and 0
print(compute_gradient(-1.0))
print(compute_gradient(1.0))
print(compute_gradient(0.0))

The slope is negative at x = -1, which means that we can lower the loss by increasing x. The slope at x = 0 is 0, which means that we cannot lower the loss by either increasing or decreasing x. This is because the loss is minimized at x = 0.


>>>> sample use matrix multiplication to predict a letter from an image

You want to determine whether the letter is an X or a K. You don't have a trained neural network, but you do have a simple model, model, which can be used to classify letter.

# Reshape model from a 1x3 to a 3x1 tensor
model = reshape(model, (3, 1))

# Multiply letter by model
output = matmul(letter, model)

# Sum over output and print prediction using the numpy method
prediction = reduce_sum(output)
print(prediction.numpy())

model:
[[ 1.]
 [ 0.]
 [-1.]]
letter

letter is
[[1. 0. 1.]
 [1. 1. 0.]
 [1. 0. 1.]]

>>>>>>>>>>>>>input data

linear model

convert data to a numpy array

import numpy as np
import pandas as pd
housing=pd.read_csv('kc_housing.csv')
housing=np.array(housing)


read_csv
1. filepath_or_buffer
2.sep delimiter between columns
3. delim_whitespace
4. encoding

print(housing[3][3],housing[3][17])

'id', 
'date', 
'price', 
'bedrooms', 
'bathrooms',
'sqft_living', 
'sqft_lot', 
'floors', 
'waterfront', (boolean)
'view',   (boolean)
'condition',
'grade', 
'sqft_above', 
'sqft_basement', 
'yr_built', 
'yr_renovated',
'zipcode', 
'lat', 
'long', 
'sqft_living15', 
'sqft_lot15'


price float
waterfront boolean

price=np.array(housing['price'],np.float32)
waterfront=np.array(housing['waterfront'],np.bool)

or
price=tf.cast(housing['price'],tf.float32)
waterfront=tf.cast(housing['waterfront'],tf.bool)


>>>>sample  >>> load the dataframe

# Import pandas under the alias pd
import pandas as pd

# Assign the path to a string variable named data_path
data_path = 'kc_house_data.csv'

# Load the dataset as a dataframe named housing
housing = pd.read_csv(data_path)

# Print the price column of housing
print(housing['price'])


# Import numpy and tensorflow with their standard aliases
import numpy as np
import tensorflow as tf

# Use a numpy array to define price as a 32-bit float
price = np.array(housing['price'], np.float32)

# Define waterfront as a Boolean using cast
waterfront = tf.cast(housing['waterfront'], tf.bool)

# Print price and waterfront
print(price)
print(waterfront)


>>>>>>>>>>>>>>>>>Loss Functions

how to train models

loss functions tell us how well the model fits the data

we want to minimize the loss function

Mean squared Error (MSE)
Mean absolute Error (MAE)
Huber error

loss functions are accessible from tf.keras.losses()
tf.keras.losses.mse()
tf.keras.losses.mae()
tf.keras.losses.Huber()

MSE
1. strongly penalizes outliers
2. high gradient sensitivity near minimum

MAE
1. Scales linearly with size of error
2. low sensitivity near minimum

Huber
1. Similar to MSE near minimum
2. Similar to MAE away from minimum


import tensorflow as tf
loss=tf.keras.losses.mse(targets, predictions)


def linear_regression(intercept, slope=slope, features=features):
	return intercept+features*slope

def loss_function(intercept, slope, targets=targets, features=features):
	predictions=linear_regression(intercept,slope)

	return tf.keras.losses.mse(targets,predictions)


loss_function(intercept, slope, test_targets, test_features)

>>>>>Sample >>>> Price and prediction error

# Import the keras module from tensorflow
from tensorflow import keras
import matplotlib.pyplot as plt

# Compute the mean squared error (mse)
loss = keras.losses.mse(price, predictions)

# Print the mean squared error (mse)
print(loss.numpy())

plt.clf()
plt.plot(predictions)
plt.plot(price,color='red',alpha=0.2)
plt.show()


You may have noticed that the MAE was much smaller than the MSE, even though price and predictions were the same. This is because the different loss functions penalize deviations of predictions from price differently. MSE does not like large deviations and punishes them harshly.


>>>> sample >>> loss function

# Initialize a variable named scalar
scalar = Variable(1.0, float32)

# Define the model
def model(scalar, features = features):
  	return scalar * features

# Define a loss function
def loss_function(scaler, features = features, targets = targets):
	# Compute the predicted values
	predictions = model(scalar, features)
    
	# Return the mean absolute error loss
	return keras.losses.mae(targets, predictions)

# Evaluate the loss function and print the loss
print(loss_function(scalar).numpy())

>>>>>>>>>>>>>>>>>Linear Regression

assumes that the relationship between two variables can be described by a line

price=intercept + size * slope + error

the difference between the predicted price and the actual price is the error and it can be used to construct the loss function.


def linear_regression(intercept, slope, features=size):
	return intercept+features*slope

def loss_function(intercept, slope, targets=price, features=size):
	# Compute the predicted values
	predictions = linear_regression(intercept, slope)
    
	# Return the mean absolute error loss
	return keras.losses.mae(targets, predictions)


#define an optimization operation

opt=tf.keras.optimizers.Adam()


opt=tf.keras.optimizers.Adam()
for j in range(1000):
    opt.minimize(lambda: loss_function(intercept,slope),\
    var_list=[intercept,slope])
    print(loss_function(intercept,slope))


print(intercept.numpy(),slope.numpy())


>>>>  sample >>>> mae to find loss

# Define a linear regression model
def linear_regression(intercept, slope, features = size_log):
	return intercept+features*slope

# Set loss_function() to take the variables as arguments
def loss_function(intercept,slope, features = size_log, targets = price_log):
	# Set the predicted values
	predictions = linear_regression(intercept, slope, features)
    
    # Return the mean squared error loss
	return keras.losses.mae(targets,predictions)

# Compute the loss for different slope and intercept values
print(loss_function(0.1, 0.1).numpy())
print(loss_function(0.1, 0.5).numpy())


>>>>>>>Sample >>> plot a regression that has optimized to a solution

# Initialize an adam optimizer
opt = keras.optimizers.Adam(0.5)

for j in range(100):
	# Apply minimize, pass the loss function, and supply the variables
	opt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])

	# Print every 10th value of the loss
	if j % 10 == 0:
		print(loss_function(intercept, slope).numpy())

# Plot data and regression line
plot_results(intercept, slope)

>>>>>>>>sample >>>> optimize to find the regression line

# Define the linear regression model
def linear_regression(params, feature1 = size_log, feature2 = bedrooms):
	return params[0] + feature1*params[1] + feature2*params[2]

# Define the loss function
def loss_function(params, targets = price_log, feature1 = size_log, feature2 = bedrooms):
	# Set the predicted values
	predictions = linear_regression(params, feature1, feature2)
  
	# Use the mean absolute error loss
	return keras.losses.mae(targets, predictions)

# Define the optimize operation
opt = keras.optimizers.Adam()

# Perform minimization and print trainable variables
for j in range(10):
	opt.minimize(lambda: loss_function(params), var_list=[params])
	print_results(params)


>>>>>>>>>>>>>>>Batch training

batch training

the complete dataset can not fit into memory
so pass in batches of data

chunksize parameter provides batch size data blocks

import pandas as pd
import numpy as np

for batch in pd.read_csv('kc_housing.csv', chunksize=100):
	price=np.array(batch['price'],np.float32)
	size=np.array(batch['size'],np.float32)

	opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list[intercept, slope])


Batch Training
1. multiple updates per epoch
2. requires division of dataset
3. no limit on dataset size


>>>>>>>>>> sample  linear regression and loss function


# Define the intercept and slope
intercept = Variable(10.0,np.float32)
slope = Variable(0.5, float32)

# Define the model
def linear_regression(intercept, slope, features):
	# Define the predicted values
	return intercept+features*slope

# Define the loss function
def loss_function(intercept, slope, targets, features):
	# Define the predicted values
	predictions = linear_regression(intercept, slope, features)
    
 	# Define the MSE loss
	return keras.losses.mse(targets, predictions)



>>>>> sample optimize the slope and intercept using batches of price and size

# Initialize adam optimizer
opt = keras.optimizers.Adam()

# Load data in batches
for batch in pd.read_csv('kc_house_data.csv',chunksize=100):
	size_batch = np.array(batch['sqft_lot'], np.float32)

	# Extract the price values for the current batch
	price_batch = np.array(batch['price'], np.float32)

	# Complete the loss, fill in the variable list, and minimize
	opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])

# Print trained parameters
print(intercept.numpy(), slope.numpy())


>>>>>>>>>>>>>>>>Dense Layers

credit card default
1. Bill Amount
2. Married
3. Default

input,hidden, output is called forward propagation

input is our features
ouput is our prediction

import tensorflow as tf
inputs = tf.constant[[1,35]])
weights=tf.Variable([[-0.05],[-0.01]])

bias = tf.Variable([0.5])

plays the role of an intercept in a simple regression model.

product=tf.matmul(inputs, weights)

dense=tf.keras.activations.sigmoid(product+bias)

>>>>> define the complete model

import tensorflow as tf

inputs=tf.constant(data, tf.float32)

dense1 = tf.keras.layers.Dense(10, activation='sigmoid')(inputs)

dense2 = tf.keras.layers.Dense(5, activation='sigmoid')(dense1)

output = tf.keras.layers.Dense(1,activation='sigmoid')(dense2)


>>>>>>sample >>>> calculate a dense layer manually

# Initialize bias1
bias1 = Variable(1.0)

# Initialize weights1 as 3x2 variable of ones
weights1 = Variable(ones((3, 2)))

# Perform matrix multiplication of borrower_features and weights1
product1 = matmul(borrower_features,weights1)

# Apply sigmoid activation function to product1 + bias1
dense1 = keras.activations.sigmoid(product1 + bias1)

# Print shape of dense1
print("\n dense1's output shape: {}".format(dense1.shape))


>>>>>>> sample calculate a second dense layer 

# From previous step
bias1 = Variable(1.0)
weights1 = Variable(ones((3, 2)))
product1 = matmul(borrower_features, weights1)
dense1 = keras.activations.sigmoid(product1 + bias1)

# Initialize bias2 and weights2
bias2 = Variable(1.0)
weights2 = Variable(ones((2, 1)))

# Perform matrix multiplication of dense1 and weights2
product2 = matmul(dense1,weights2)

# Apply activation to product2 + bias2 and print the prediction
prediction = keras.activations.sigmoid(product2 + bias2)
print('\n prediction: {}'.format(prediction.numpy()[0,0]))
print('\n actual: 1')


>>>sample >>> shape

# Compute the product of borrower_features and weights1
products1 = matmul(borrower_features,weights1)

# Apply a sigmoid activation function to products1 + bias1
dense1 = keras.activations.sigmoid(products1+bias1)

# Print the shapes of borrower_features, weights1, bias1, and dense1
print('\n shape of borrower_features: ', borrower_features.shape)
print('\n shape of weights1: ', weights1.shape)
print('\n shape of bias1: ', bias1.shape)
print('\n shape of dense1: ', dense1.shape)

shape of borrower_features:  (5, 3)

 shape of weights1:  (3, 2)

 shape of bias1:  (1,)

 shape of dense1:  (5, 2)

borrower_features, is 5x3 because it consists of 5 examples for 3 features. The shape of weights1 is 3x2, as it was in the previous exercise, since it does not depend on the number of examples. Additionally, bias1 is a scalar. Finally, dense1 is 5x2, which means that we can multiply it by the following set of weights, weights2, which we defined to be 2x1 in the previous exercise.


>>>>>Sample constructing an keras model

# Define the first dense layer
dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)

# Define a dense layer with 3 output nodes
dense2 = keras.layers.Dense(3,activation='sigmoid')(dense1)

# Define a dense layer with 1 output node
predictions = keras.layers.Dense(1,activation='sigmoid')(dense2)

# Print the shapes of dense1, dense2, and predictions
print('\n shape of dense1: ', dense1.shape)
print('\n shape of dense2: ', dense2.shape)
print('\n shape of predictions: ', predictions.shape)

output:
shape of dense1:  (100, 7)

 shape of dense2:  (100, 3)

 shape of predictions:  (100, 1)

>>>>>>>>>>>>>Activation Functions

hidden layers performs matrix multiplication and applies an activation function 


import numpy as np
import tensorflow as tf

young, old = 0.3, 0.6
low_bill, high_bill = 0.1, 0.5

young_high=1.0* young + 2*high_bill
young_low=1.0*young + 2*low_bill

old_high=1.0* old + 2.0 * high_bill
old_low=1.0*old + 2.0 * low_bill

print(young_high-young_low)
print(old_high-old_low)

output
0.8
0.8

print(tf.keras.activations.sigmoid(young_high).numpy()-tf.keras.activations.sigmoid(young_low).numpy())

print(tf.keras.activations.sigmoid(old_high).numpy()-tf.keras.activations.sigmoid(old_low).numpy())

output
0.16337568
0.14204389

activation functions
sigmoid (binary classification problems)
relu (all layers but the output layer (0 or max(value))
softmax (output layer for multiple classification)

relu varies between 0 and infinity


inputs= tf.constant(borrower_features,tf.float32)

dense1=tf.keras.layers.Dense(16,activation='relu')(inputs)

dense2=tf.keras.layers.Dense(8,activation='sigmoid')(dense1)

outputs = tf.keras.layers.Dense(4,activation='softmax')(dense2)


>>>>>>> sample >>> input bill_amounts  


# Construct input layer from features
print(bill_amounts)
inputs = constant(bill_amounts)

# Define first dense layer
dense1 = keras.layers.Dense(3, activation='relu')(inputs)

# Define second dense layer
dense2 = keras.layers.Dense(2, activation='relu')(dense1)

# Define output layer
outputs = keras.layers.Dense(1, activation='sigmoid')(dense2)

# Print error for first five examples
error = default[:5] - outputs.numpy()[:5]
print(error)

input
[77479 77057 78102]
 [  326   326   326]
 [13686  1992   604]

output
[[-1.]
 [-1.]
 [-1.]
 [-1.]
 [-1.]]

>>>>>Sample  >>> 10 inputs 8 hidden 6 output

# Construct input layer from borrower features
inputs = constant(borrower_features)

#print(len(borrower_features[0]))

# Define first dense layer
dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)

# Define second dense layer
dense2 = keras.layers.Dense(8, activation='relu')(dense1)

# Define output layer
outputs = keras.layers.Dense(6, activation='softmax')(dense2)

# Print first five predictions
print(outputs.numpy()[:5])


>>>>>>>>>>>>>>>>>>>Gradient Descent Optimizer

tf.keras.optimizer.SGD()
learning_rate


RMS
applies different learning rates to each feature

tf.keras.optimizers.RMSprop()
learning_rate
momentum
decay

allows momentum to both build and decay

adam optimizer

adaptive moment (adam) optimizer

tk.keras.optimizers.Adam()

learning_rate
beta1 (decay)

performs well with default parameter values


def model(bias, weights, features=borrower_features):
	product=tf.matmul(features,weights)
	return tf.keras.activations.sigmoid(product+bias)

def loss_function(bias, weights, targets=default, features=borrower_features):
	predictions=model(bias,weights)
	return tf.keras.loss.binary_crossentropy(targets,predictions)

opt=tk.keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.9)
opt.minimize(lambda: loss_function(bias,weights), var_list=[bias, weights])


>>>>>>>>>sample >>> minimize loss over 100 iterations

# Initialize x_1 and x_2
x_1 = Variable(6.0,float32)
x_2 = Variable(0.3,float32)

# Define the optimization operation
opt = keras.optimizers.SGD(learning_rate=0.01)

for j in range(100):
	# Perform minimization using the loss function and x_1
	opt.minimize(lambda: loss_function(x_1), var_list=[x_1])
	# Perform minimization using the loss function and x_2
	opt.minimize(lambda: loss_function(x_2), var_list=[x_2])

# Print x_1 and x_2 as numpy arrays
print(x_1.numpy(), x_2.numpy())

output
4.3801394 0.42052683


The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as loss_function()


>>> sample rms with momentum

# Initialize x_1 and x_2
x_1 = Variable(0.05,float32)
x_2 = Variable(0.05,float32)

# Define the optimization operation for opt_1 and opt_2
opt_1 = keras.optimizers.RMSprop(learning_rate=.01, momentum=0.99)
opt_2 = keras.optimizers.RMSprop(learning_rate=.01, momentum=0.0)

for j in range(100):
	opt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])
    # Define the minimization operation for opt_2
	opt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])

# Print x_1 and x_2 as numpy arrays
print(x_1.numpy(), x_2.numpy())


>>>>>>>>>>>>Training a network in tensorflow

the eggholder function

there exists a global minimum but it is difficult to identify by inspection.

use random and algorithmic selection of the initial values.

import tensorflow as tf

weights= tf.Variable(tf.random.normal([500],[500])


weights= tf.Variable(tf.random.truncated_normal([500],[500])



#define a dense layer with the default initializer
dense=tf.keras.layers.Dense(32, activation='relu')

dense=tf.keras.layers.Dense(32, activation='relu', \
	kernel_initializer='zeros')


dropout 
this will random drop weights on the network

inputs=np.array(borrow_features, np.float32)
dense1=tf.keras.layers.Dense(32, activation='relu')(inputs)

dense2 = tf.keras.layers.Dense(16,activation='relu')(dense1)

#drop 25 percent of the weights randomly
dropout1=tf.keras.layers.Dropout(0.25)(dense2)

outputs=tf.keras.layers.Dense(1, activation='sigmoid') (dropout1)


>>>>>>>sample initializing the weights of the neural network

# Define the layer 1 weights
w1 = Variable(random.normal([23, 7]))

# Initialize the layer 1 bias
b1 = Variable(ones([7]))

# Define the layer 2 weights
w2 = Variable(random.normal([7,1]))

# Define the layer 2 bias
b2 = Variable(0)

>>>> sample

# Define the model
def model(w1, b1, w2, b2, features = borrower_features):
	# Apply relu activation functions to layer 1
	layer1 = keras.activations.sigmoid(matmul(features, w1) + b1)
    # Apply dropout
	dropout = keras.layers.Dropout(0.25)(layer1)
	return keras.activations.sigmoid(matmul(dropout, w2) + b2)

# Define the loss function
def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):
	predictions = model(w1, b1, w2, b2)
	# Pass targets and predictions to the cross entropy loss
	return keras.losses.binary_crossentropy(targets, predictions)


>>>>>> sample

# Train the model
for j in range(100):
    # Complete the optimizer
	opt.minimize(lambda: loss_function(w1, b1, w2, b2), 
                 var_list=[w1, b1, w2, b2])

# Make predictions with model
model_predictions = model(w1, b1, w2, b2, test_features)

# Construct the confusion matrix
confusion_matrix(test_targets, model_predictions)

>>>>>>>>>>>>>>>>Sequential

28x28 image matrix

16 input
8 hidden
4 output

from tensorflow import keras

model = keras.Sequential()

model.add(keras.layers.Dense(16,activation='relu', input_shape=(28*28,)))

model.add(Dense(8,activation='relu'))

model.add(Dense(4,activation='softmax'))


>>>>>>>>>>>>>>>>Functional api

model1_inputs=tf.keras.Input(shape=(28*28,))
model2_inputs=tf.keras.Input(shape=(10,))

model1_layer1=Dense(12,activation='relu')(model1_inputs)
model1_layer2=Dense(4,activation='softmax')(model1_layer1)


model2_layer1=Dense(12,activation='relu')(model2_inputs)
model2_layer2=Dense(4,activation='softmax')(model2_layer1)

merged=tf.keras.layers.add([model1_layer2,model2_layer2])


model.compile('adam',loss='categorical_crossentropy')

>>>>>>Sign language   >>>  4 images

# Define a Keras sequential model
model=keras.Sequential()

# Define the first dense layer 28x28
model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))

# Define the second dense layer
model.add(keras.layers.Dense(8,activation='relu'))

# Define the output layer
model.add(keras.layers.Dense(4,activation='softmax'))

# Print the model architecture
print(model.summary())



>>>> sign language with dropout rate of .25

# Define the first dense layer
model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))

# Apply dropout to the first layer's output
model.add(keras.layers.Dropout(0.25))

# Define the output layer
model.add(keras.layers.Dense(4, activation='softmax'))

# Compile the model
model.compile('adam', loss='categorical_crossentropy')

# Print a model summary
print(model.summary())


>>>>>>> creating two inputs and Merge the outputs

# For model 1, pass the input layer to layer 1 and layer 1 to layer 2
m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)
m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)

# For model 2, pass the input layer to layer 1 and layer 1 to layer 2
m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)
m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)

# Merge model outputs and define a functional model
merged = keras.layers.add([m1_layer2, m2_layer2])
model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)

# Print a model summary
print(model.summary())


Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 784)]        0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 784)]        0                                            
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 12)           9420        input_1[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 12)           9420        input_2[0][0]                    
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 4)            52          dense_8[0][0]                    
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 4)            52          dense_10[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 4)            0           dense_9[0][0]                    
                                                                 dense_11[0][0]                   
==================================================================================================
Total params: 18,944
Trainable params: 18,944
Non-trainable params: 0
______________________________

>>>>>>>>Training with Keras

train and evaluate

1. load and clean the data
2. define the model
3. train and validate model
4. evaluate model

import tensorflow as tf

model=tf.keras.Sequential()

model.add(tf.keras.layers.Dense(16,activation='relu', input_shape(784,)))

model.add(tf.keras.layers.Dense(4,activation='softmax'))

model.compile('adam',loss='categorical_crossentropy')

model.fit(image_features, image_labels)

batch_size (example 32 by default)
epochs (times trained)
validation_split
1) training
2. validation

validation_split=0.20


performing validation
1. loss
2. val_loss

if training loss becames substantially less than validation loss than overfitting is occurring. 

1. Add dropout
2. or regularize the data

metrics['accuracy']

model.fit(features,labels, epochs=10, validation_split=0.20)

model.evaluate(test)



>>>>>> sample  >>>> compile the model

# Define a sequential model
model=keras.Sequential()

# Define a hidden layer
model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))

# Define the output layer
model.add(keras.layers.Dense(4,activation='softmax'))

# Compile the model
model.compile('SGD', loss='categorical_crossentropy',metrics=['accuracy'])

# Complete the fitting operation
model.fit(sign_language_features, sign_language_labels, epochs=5)


>>>>>> sample >>>> RMSprop optimizer

# Define sequential model
model = keras.Sequential()

# Define the first layer
model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))

# Add activation function to classifier
model.add(keras.layers.Dense(4, activation='softmax'))

# Set the optimizer, loss function, and metrics
model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])

# Add the number of epochs and the validation split
model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=.10)


>>>>>>> sample >>> large number of neurons

# Define sequential model
model=keras.Sequential()

# Define the first layer
model.add(keras.layers.Dense(1024, activation='relu', input_shape=(784,)))
# Add activation function to classifier
model.add(keras.layers.Dense(4, activation='softmax'))

# Finish the model compilation
model.compile(optimizer=keras.optimizers.Adam(lr=0.001), 
              loss='categorical_crossentropy', metrics=['accuracy'])

# Complete the model fit operation
model.fit(sign_language_features, sign_language_labels, epochs=50, validation_split=.5)

if val_loss started to increase before the training process was terminated, then we may have overfitted. when this happens decrease the number of epochs.


>>>>>>>>sample  comparing a small model to a large model

# Evaluate the small model using the train data
small_train = small_model.evaluate(train_features, train_labels)

# Evaluate the small model using the test data
small_test = small_model.evaluate(test_features, test_labels)

# Evaluate the large model using the train data
large_train = large_model.evaluate(train_features, train_labels)

# Evaluate the large model using the test data
large_test = large_model.evaluate(test_features, test_labels)

# Print losses
print('\n Small - Train: {}, Test: {}'.format(small_train, small_test))
print('Large - Train: {}, Test: {}'.format(large_train, large_test))


>>>>>>>>>>>>Estimators api


high level submodule:estimators
mid level: layers, datasets, metrics
low level: python


estimators enforce best practices

1. define feature columns
2. load and transform data
3. define an estimator
4. apply train operation

Defining feature columns:

for an image
features_list[tf.feature_column.numeric_column('image',shape(784,))]


>>>>>>>>>>Loading and transforming data

def input_fn():
    features={"size":[1340,1690,2720],'rooms':[1,3,4]}
    layers=[221900,538000,180000]

    return features, labels

size=tf.feature_column.numeric_column("size")
rooms=tf.feature_column.categorical_column_with_vocabulary_list("rooms",["1","2","3","4","5"],default_value=0)
feature_list=[size,
              tf.feature_column.indicator_column(rooms)]

model0=tf.estimator.DNNRegressor(feature_columns=feature_list,hidden_units=[10,6,6,1])
model0.train(input_fn,steps=20)


model1=tf.estimator.DNNClassifier(feature_columns=feature_list, hidden_units=[32,16,8],n_classes=4)

model0.train(input_fn,steps=20)

>>>>>>>>>sample  >>> define feature columns and build an input_fn

# Define feature columns for bedrooms and bathrooms
bedrooms = feature_column.numeric_column("bedrooms")
bathrooms = feature_column.numeric_column("bathrooms")

# Define the list of feature columns
feature_list = [bedrooms, bathrooms]

def input_fn():
	# Define the labels
	labels = np.array(housing['price'])
	# Define the features
	features = {'bedrooms':np.array(housing['bedrooms']), 
                'bathrooms':np.array(housing['bathrooms'])}
	return features, labels

>>>>> use a linear regressor

# Define the model and set the number of steps
model = estimator.LinearRegressor(feature_columns=feature_list) 
model.train(input_fn, steps=2)





































































