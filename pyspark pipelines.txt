components of a data platform

ingest using singer

deploy spark transformation pipelines

test your code automatically

apply common data cleaning operations

gain insights by combining data with pyspark

operational systems:
1. messaging services
2. payment systems
3. google analytics
4. crm
5. clickstream
6. location services

data lake
1. organized by zones
(operational data is the landing zone)
2. services to clean (clean zone)
3. business zone

extract transform and load

>>>>>>>>>>>>Singer

1. connecting to multiple data sources
2. open source

communicates using json

tap and targets

1. schema
2. state
3. record

import singer
singer.write_schema(schema=json_schema,
	stream_name='DC_employees',
	key_properties=["id"]

import json

json.dumps(json_schema['properties']['age'])

with open('foo.json', mode='w') as fh:
	json.dump(obj=json_schema, fp=fh)

#writes the json-serialized object to the open file handle

>>>>> sample >>> json.dump

# Import json
import json

database_address = {
  "host": "10.0.0.5",
  "port": 8456
}

# Open the configuration file in writable mode
with open("database_config.json", mode='w') as fh:
  # Serialize the object in this file handle
  json.dump(obj=database_address, fp=fh) 


>>>>>>>>> sample singer write_schema

# Complete the JSON schema
schema = {'properties': {
    'brand': {'type': 'string'},
    'model': {'type': 'string'},
    'price': {'type': 'number'},
    'currency': {'type': 'string'},
    'quantity': {'type': 'integer', 'minimum': 1},  
    'date': {'type': 'string', 'format': 'date'},
    'countrycode': {'type': 'string', 'pattern': "^[A-Z]{2}$"}, 
    'store_name': {'type': 'string'}}}


# Write the schema
singer.write_schema(stream_name='products', schema=schema, key_properties=[])


>>>>>>>>>>>ingestion pipeline with Singer

columns=("id","name","age","has_children")

users={
(1,"adrian",32,False),
(2,"ruanne",28,True)
}


singer.write_record(stream_name="DC_employees",
record=dict(zip(columns,users.pop())))

fixed_dict = {"type":"RECORD","stream":"DC_employees"}

record_msg={**fixed_dict,"record":dict(zip(columns, users.pop()))}

print(json.dumps(record_msg))

** unpacks the dictionary


import singer

singer.write_schema(stream_name="foo", schema=...)

singer.write_records(stream_name="foo", records=...)

>>>>>>>>>>introduction to pipes

python my_tap.py | target-csv

python my_tap.py | target-csv --config userconfig.cfg

>>>>>>>>>>>>>keeping track with state messages

last_update_on

extract after last_update_on

update after tap

singer.write_state(value={"max-last-updated-on": some_variable})


You’re running a Singer tap daily at midnight, to synchronize changes between databases. Your tap, called tap-mydelta, extracts only the records that were updated in this database since your last retrieval. To do so, your tap keeps state: it keeps track of the last record it reported on, which can be derived from the table’s last_updated_on field.

>>>> sample >> get from local host rest end point

endpoint = "http://localhost:5000"

# Fill in the correct API key
api_key = "scientist007"

# Create the web API’s URL
authenticated_endpoint = "{}/{}".format("http://localhost:5000", api_key)

print(authenticated_endpoint)

# Get the web API’s reply to the endpoint
api_response = requests.get(authenticated_endpoint).json()
pprint.pprint(api_response)

output:

<script.py> output:
    http://localhost:5000/scientist007
    {'apis': [{'description': 'list the shops available',
               'url': '<api_key>/diaper/api/v1.0/shops'},
              {'description': 'list the items available in shop',
               'url': '<api_key>/diaper/api/v1.0/items/<shop_name>'}]}
    {'apis': [{'url': '<api_key>/diaper/api/v1.0/shops', 'description': 'list the shops available'}, {'url': '<api_key>/diaper/api/v1.0/items/<shop_name>', 'description': 'list the items available in shop'}]}

/shops
items/<shop_name>


>>>> sample get a list of shops

# Create the API’s endpoint for the shops
shops_endpoint = "{}/{}/{}/{}".format(endpoint, api_key, "diaper/api/v1.0", "shops")

shops = requests.get(shops_endpoint).json()
print(shops)

{'shops': ['Aldi', 'Kruidvat', 'Carrefour', 'Tesco', 'DM']}

items_endpoint = "{}/{}/{}/{}/{}".format(endpoint, api_key, "diaper/api/v1.0", "items","Aldi")
items = requests.get(items_endpoint).json()


{'items': [{'countrycode': 'BE', 'brand': 'Diapers-R-Us', 'model': '6months', 'price': 6.8, 'currency': 'EUR', 'quantity': 40, 'date': '2019-02-03'}]}



dm

{'items': [{'brand': 'Huggies',
            'countrycode': 'DE',
            'currency': 'EUR',
            'date': '2019-02-01',
            'model': 'newborn',
            'price': 6.8,
            'quantity': 40},
           {'brand': 'Huggies',
            'countrycode': 'AT',
            'currency': 'EUR',
            'date': '2019-02-01',
            'model': 'newborn',
            'price': 7.2,
            'quantity': 40}]}


>>>>>> sample  >>> extract the schema


# Use the convenience function to query the API
tesco_items = retrieve_products("Tesco")

singer.write_schema(stream_name="products", schema=schema,
                    key_properties=[])


# Write a single record to the stream, that adheres to the schema
singer.write_record(stream_name="products", 
            record={**tesco_items[0], 'store_name': "Tesco"})


>>>>>>>>>>>>>sample  write the store_name to the database


# Use the convenience function to query the API
tesco_items = retrieve_products("Tesco")

singer.write_schema(stream_name="products", schema=schema,
                    key_properties=[])

# Write a single record to the stream, that adheres to the schema
singer.write_record(stream_name="products", 
                    record={**tesco_items[0], "store_name": "Tesco"})

for shop in requests.get(SHOPS_URL).json()["shops"]:
    # Write all of the records that you retrieve from the API
    singer.write_records(
      stream_name="products", # Use the same stream name that you used in the schema
      records=({**tesco_items[0], "store_name": shop}
               for item in retrieve_products(shop))
    )  

>>>>>> tap to drain

tap-marketing-api | target-csv --config data_lake.conf


>>>>>>>>>>> spark 

1. spark sql
2. spark streaming
3. MLlib
4. GraphX

api is pyspark

data processing at scale
interactive analytics

validate hypothesis
machine learning and score models

spark is not used for little data

prices.csv

ratings.csv

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

prices-spark.read.
options(header="true").
csv('mnt/data_lake/landing/prices.csv")

prices.show()


from pprint import pprint

pprint(prices.dtypes)


schema=StructType(
StructField("store",StringType(),nullable=False),

StructField("price",FloatType(), nullable=False)

StructField("date",DateType(),nullable=False
)


prices-spark.read.
options(header="true").
schema(schema).
csv('mnt/data_lake/landing/prices.csv")



# Read a csv file and set the headers
df = (spark.read
      .options(header=True)
      .csv("/home/repl/workspace/mnt/data_lake/landing/ratings.csv"))

df.show()

brand
model
absorption_rate
comfort

ham or spam csv

https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/sms_spam.csv


>>>>>>>>>>>>>> read.csv

# Read a csv file and set the headers
df = (spark.read
      .options(header=True)
      .csv("/home/repl/workspace/mnt/data_lake/landing/ratings.csv"))

df.show()

>>>>>schema

# Define the schema
schema = StructType([
  StructField("brand", StringType(), nullable=False),
  StructField("model", StringType(), nullable=False),
  StructField("absorption_rate", ByteType(), nullable=True),
  StructField("comfort", ByteType(), nullable=True)
])

better_df = (spark
             .read
             .options(header="true")
             # Pass the predefined schema to the Reader
             .schema(schema)
             .csv("/home/repl/workspace/mnt/data_lake/landing/ratings.csv"))
pprint(better_df.dtypes)


>>>>>>>>>>>cleaning data

handling invalid rows

prices=(spark.read
	.options(header='true',mode'DROPMALFORMED')
	.csv('landing/prices.csv'))

the significance of null
1. keep the row
2. fill the blanks with null

prices.fillna(25, subset=['quantity']).show()


>>>>>>>badly chosen placeholders

employees= spark.read.options(header='true').
schema(schema).csv('employees.csv')

>>>> replace with condition in the when function

from pyspark.sql.functions import col, when
from datetime import date, timedelta

one_year_from_now = date.today().replace(year=date.today().year+1)
better_frame = employees.withColumn('end_date',
	when(col('end_date')> one_year_from_now,None).otherwise(col('end_date')))

better_frame.show()

none is translated to null

bytetype range is -128 to 127


>>> fillna to replace missing values

print("BEFORE")
ratings.show()

print("AFTER")
# Replace nulls with arbitrary value on column subset
ratings = ratings.fillna(4, subset=["comfort"])
ratings.show()

>>>>>>> drop invalid rows
PERMISSIVE is the default mode

# Specify the option to drop invalid rows
ratings = (spark
           .read
           .options(header=True, mode='DROPMALFORMED')
           .csv("/home/repl/workspace/mnt/data_lake/landing/ratings_with_invalid_rows.csv"))
ratings.show()

>>>>>  conditionally replacing data

from pyspark.sql.functions import col, when

# Add/relabel the column
categorized_ratings = ratings.withColumn(
    "comfort",
    # Express the condition in terms of column operations
    when(col("comfort") > 3, "sufficient").otherwise("insufficient"))

categorized_ratings.show()


>>>>>>>>>>>>>>>>>>>>>>>Transforming data with spark
deriving insights

standardizing names and normalizing numerical data

filtering rows
selecting and renaming columns
grouping and aggregation
ordering results

prices=spark.read.options(header='true').schema(schema).csv('landing/prices.csv')

filter is passed boolean values

prices_in_beligium = prices.filter(col('countrycode')=='BE).orderBy(col('date'))

col creates column objects

prices.select(

	col('store'),
	col('brand').alias('brandname')
).distinct()

>>>>> grouping and aggregating

(prices
	.groupBy(col('brand'))
	.mean('price')
).show()

(prices
	.groupBy(col('brand'))
	.agg(
		avg('price').alias('average_price')
		count('brand').alias('number_of_times')
	)
)


>>>>> joining data

ratings_with_prices=ratings.join(prices,['brand','model'])

>>>>>>>> sample >>>> select distinct columns

from pyspark.sql.functions import col

# Select the columns and rename the "absorption_rate" column
result = ratings.select([col("brand"),
                       col("model"),
                       col("absorption_rate").alias('absorbency')])

# Show only unique values
result.distinct().show()


>>>>>> sample >>> agg

from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax

aggregated = (purchased
              # Group rows by 'Country'
              .groupBy(col('Country'))
              .agg(
                # Calculate the average salary per group and rename
                avg('Salary').alias('average_salary'),
                # Calculate the standard deviation per group
                stddev_samp('Salary'),
                # Retain the highest salary per group and rename
                sfmax('Salary').alias('highest_salary')
              )
             )

aggregated.show()


+-------+--------------+-------------------+--------------+
    |Country|average_salary|stddev_samp(Salary)|highest_salary|
    +-------+--------------+-------------------+--------------+
    |Germany|       63000.0|                NaN|         63000|
    | France|       48000.0|                NaN|         48000|
    |  Spain|       62000.0| 12727.922061357855|         71000|
    +-------+--------------+-------------------+--------------+
    

Note that the standard deviation column has returned NaN in a few cases. That’s because there weren’t enough data points for these countries (only one record, so you can’t compute a meaningful sample standard deviation), as we’re only loading a small file in this exercise


>>>>>>>>>Packaging your application


python my_pyspark_data_pipeline.py

spark-submit

1. sets up launch environment for use with the cluster manager and the selected deploy mode

spark-submit

	--master "local[*]" \
	--py-files PY_FILES \
	MAIN_PYTHON_FILE \
	app_arguments

zip files
	zip \
		--recurse-path\
		dependencies.zip
		pydiaper

spark-submit \
	--py-files dependencies.zip \
	pydiaper/cleaning/clean_prices.py


>>>>>>>>>>>>>>>>>importance of tests

1. new functionality desired
2. bugs need to get squashed

written expectations of the code

raises confidence that the code is correct now

tests are the most up-to-date form of documentation

testing takes time
testing have a high return on investment

unit tests
service tests
ui test (end to end tests)

>>>>>>writing unit tests

1. Extract
2. Transform
3. Load

transformation is where we add the business logic


prices_with_ratings=spark.read.csv()
exchange_rates=spark.read.csv()

unit_prices_with_ratings = (prices_with_ratings.join() #transform
.withColumn())

transformations operate on dataframes

>>>>>>>>>>>dataframes in memory

from pyspark.sql import Row

purchase=Row("price",
	"quantity",
	"product")

record=purchase(12.99,1,"cake")

df=spark.createDataFrame((record,))

unit_prices_with_ratings=(prices_with_ratings
	.join(exchange_rates,['currency','date'])
	.withColumn('unit_price_in_euro',
	col('price')/col('quantity')
	*col('exchange_rate_to_euro'))


>>>>>>  create reusable well name functions

def link_with_exchange_rates(prices,rates):
	return prices.join(rates,['currency','date'])

def calculate_unit_price_in_euro(df):
	return
	df.withColumn('unit_price_in_euro',
	col('price')/col('quantity')
	*col('exchange_rate_to_euro'))


unit_price_with_ratings=(
	calculate_unit_price_in_euro(
	link_with_exchange_rates(prices,exchange_rates)
	)
)

***each transformation can be tested and reduced

def test_calculate_unit_price_in_euro():
	record=dict(price=10,
		quantity=5,
		exchange_rate_to_euro=2.)

	df=spark.createDataFrame([Row(**record)])

	result=calculate_unit_price_in_euro(df)

	expected_record=Row(**record, unit_price_in_euro=4.)
	expected=spark.createDateFrame([expected_record])
	assertDataFrameEqual(result,expected)

testing framework: pytest

create in-memory dataframes makes testing easier because the data is in plain sight
focus is on a small number of examples

>>>>>> sample >>> in memory dataframe


from datetime import date
from pyspark.sql import Row

Record = Row("country", "utm_campaign", "airtime_in_minutes", "start_date", "end_date")

# Create a tuple of records
data = (
  Record("USA", "DiapersFirst", 28, date(2017, 1, 20), date(2017, 1, 27)),
  Record("Germany", "WindelKind", 31, date(2017, 1, 25), None),
  Record("India", "CloseToCloth", 32, date(2017, 1, 25), date(2017, 2, 2))
)

# Create a DataFrame from these records
frame = spark.createDataFrame(data)
frame.show()



script.py



Light mode



from datetime import date
from pyspark.sql import Row
 
Record = Row("country", "utm_campaign", "airtime_in_minutes", "start_date", "end_date")
 
# Create a tuple of records
data = (
  Record("USA", "DiapersFirst", 28, date(2017, 1, 20), date(2017, 1, 27)),
  Record("Germany", "WindelKind", 31, date(2017, 1, 25), None),
  Record("India", "CloseToCloth", 32, date(2017, 1, 25), date(2017, 2, 2))
)
 
# Create a DataFrame from these records
frame = spark.createDataFrame(data)
frame.show()











Run Code
Submit Answer


IPython Shell


Slides



from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax

aggregated = (purchased
              # Group rows by 'Country'
              .groupBy(col('Country'))
              .agg(
                # Calculate the average salary per group and rename
                avg('Salary').alias('average_salary'),
                # Calculate the standard deviation per group
                stddev_samp('Salary'),
                # Retain the highest salary per group and rename
                sfmax('Salary').alias('highest_salary')
              )
             )

aggregated.show()
from datetime import date
from pyspark.sql import Row

Record = Row("country", "utm_campaign", "airtime_in_minutes", "start_date", "end_date")

# Create a tuple of records
data = (
  Record("USA", "DiapersFirst", 28, date(2017, 1, 20), date(2017, 1, 27)),
  Record("Germany", "WindelKind", 31, date(2017, 1, 25), None),
  Record("India", "CloseToCloth", 32, date(2017, 1, 25), date(2017, 2, 2))
)

# Create a DataFrame from these records
frame = spark.createDataFrame(data)
frame.show()
+-------+------------+------------------+----------+----------+
|country|utm_campaign|airtime_in_minutes|start_date|  end_date|
+-------+------------+------------------+----------+----------+
|    USA|DiapersFirst|                28|2017-01-20|2017-01-27|
|Germany|  WindelKind|                31|2017-01-25|      null|
|  India|CloseToCloth|                32|2017-01-25|2017-02-02|
+-------+------------+------------------+----------+----------+

>>>>>> sample	

pipenv run pytest


from .chinese_provinces_improved import \
    aggregate_inhabitants_by_province
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, \
    StructField, StringType, LongType, BooleanType


def test_aggregate_inhabitants_by_province():
    """The number of inhabitants per province should be aggregated,
    regardless of their distinctive features.
    """

    spark = SparkSession.builder.getOrCreate()

    fields = [
        StructField("country", StringType(), True),
        StructField("province", StringType(), True),
        StructField("inhabitants", LongType(), True),
        StructField("foo", BooleanType(), True),  # distinctive features
    ]

    frame = spark.createDataFrame({
        ("China", "A", 3, False),
        ("China", "A", 2, True),
        ("China", "B", 14, False),
        ("US", "A", 4, False)},
        schema=StructType(fields)
    )
    actual = aggregate_inhabitants_by_province(frame).cache()

    # In the older implementation, the data was first filtered for a specific
    # country, after which you'd aggregate by province. The same province
    # name could occur in multiple countries though.
    # This test is expecting the data to be grouped by country,
    # then province from aggregate_inhabitants_by_province()
    expected = spark.createDataFrame(
        {("China", "A", 5), ("China", "B", 14), ("US", "A", 4)},
        schema=StructType(fields[:3])
    ).cache()

    assert actual.schema == expected.schema, "schemas don't match up"
    assert sorted(actual.collect()) == sorted(expected.collect()),\
        "data isn't equal"




>>> improvements

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, sum

from .catalog import catalog


def extract_demographics(sparksession, catalog):
    return sparksession.read.parquet(catalog["clean/demographics"])


def store_chinese_demographics(frame, catalog):
    frame.write.parquet(catalog["business/chinese_demographics"])


# Improved aggregation function, grouped by country and province
def aggregate_inhabitants_by_province(frame):
    return (frame
            .groupBy("province")
            .agg(sum(col("inhabitants")).alias("inhabitants"))
            )


def main():
    spark = SparkSession.builder.getOrCreate()
    frame = extract_demographics(spark, catalog)
    chinese_demographics = frame.filter(lower(col("country")) == "china")
    aggregated_demographics = aggregate_inhabitants_by_province(
        chinese_demographics
    )
    store_chinese_demographics(aggregated_demographics, catalog)


if __name__ == "__main__":
    main()


>>>>>>>>>>>>continuous testing >>>>>>>>


unittest
pytest
doctest
nose

assert or raise

assert compute == expected

report which test passed and which ones failed

automation is one of the objectives of a data engineer

ci/cd pipeline

continuous integration
1. get code changes integrated with the master branch regularly

continuous delivery
1. Create artifacts (deliverables like documentation, but also programs) that can be deployed into production without breaking things

cicleci - run tests automatically for you

circleci looks for .circleci/config.yml
1. has a section called jobs

jobs:
	test:
		docker:
			-image:circleci/python:3.6.4
		steps:
			-checkout
			-run: pip install -r requirements.txt
			-run: pytest


cicleci
1. checkout code
2. install test & build requirements
3. run tests


order
1. check out your application from version control
2. install your python application dependencies
3. run the test suite of your application
4. create artifacts
5. save the artifacts to location accessible by your company's compute infrastructure



Add flake8 to the development section in the Pipfile, which is in the project’s root folder. This file serves a similar purpose as the requirements.txt files you might have seen in other Python projects. It solves some problems with those though. To add flake8 correctly, look at the line that mentions pytest.


>>>>>>>>>>>>Modern day workflow management


sequence of tasks scheduled to be run
a task can be trigger by a sequence of event

schedule or triggered

scheduled with cron

reads crontab files

#Minutes hours Days Months Day of the week Command

*/15 9-17 * * 1-3,5 log_my_activity
1. one task per line
2. launch my process, log my activity at a specific time
3. every fifteen minutes between normal office hours, ever day of the month, for every month, 
Mon, tues, wednesday, and fridays

your can add comments

other tools
1. luigi
2. azkaban
3. airflow

apache airflow fulfills modern engineering needs
1. create and visualize complex workflows
2. monitor and log workflows
3. scales horizontally (work with other machines)

>>>>>>>>>>>>>The directed Acyclic Graph (DAG)

1. nodes are connected by edges
2. the edge denote a sense of direction on the nodes
3. Acyclic means there is no way to circle back to the same node
4. The nodes are operators

from airflow import DAG

my_dag = DAG(
	dag_id="publish_logs",
	schedule_interval="* * * * *",
	state_date=datetime(2010,1,1)

)

BashOperator (bash script)
Pythonoperator (python script)
SparkSubmitOperator

>>>>>>>>> defining dependencies between task is established using set_downstream and set_upstream operators

task1.set_downstream(task2)
task3.set_upstream(task2)



>>>>>> Dag schedule job

schedule interval: * default

minute
hour
day of the month
day of the week


from datetime import datetime
from airflow import DAG

reporting_dag = DAG(
    dag_id="publish_EMEA_sales_report", 
    # Insert the cron expression
    schedule_interval="0 7 * * 1",
    start_date=datetime(2019, 11, 24),
    default_args={"owner": "sales"}
)

# Specify direction using verbose method
prepare_crust.set_downstream(apply_tomato_sauce)

tasks_with_tomato_sauce_parent = [add_cheese, add_ham, add_olives, add_mushroom]

for task in tasks_with_tomato_sauce_parent:
    # Specify direction using verbose method on relevant task
    apply_tomato_sauce.set_downstream(task)

# Specify direction using bitshift operator
tasks_with_tomato_sauce_parent >> bake_pizza

# Specify direction using verbose method
bake_pizza.set_upstream(prepare_oven)

a.set_downstream(b) means b must be executed after a.
a >> b also means b must be executed after a.
b.set_upstream(a) means a must be executed before b.
b << a also means a must be executed before b.

Set prepare_crust to precede apply_tomato_sauce using the appropriate method.

Set apply_tomato_sauceto precede each of tasks in tasks_with_tomato_sauce_parent using the appropriate method.
Set the tasks_with_tomato_sauce_parent list to precede bake_pizza using either the bitshift operator >> or <<.
Set bake_pizza to succeed prepare_oven using the appropriate method.

>>>>>>>sample

# Specify direction using verbose method
prepare_crust.set_downstream(apply_tomato_sauce)

tasks_with_tomato_sauce_parent = [add_cheese, add_ham, add_olives, add_mushroom]
for task in tasks_with_tomato_sauce_parent:
    # Specify direction using verbose method on relevant task
    apply_tomato_sauce.set_downstream(task)

# Specify direction using bitshift operator
tasks_with_tomato_sauce_parent >> bake_pizza

# Specify direction using verbose method
bake_pizza.set_upstream(prepare_oven)