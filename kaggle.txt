1. problem
2. data
3. model
4. submission
5. leaderboard

download the data
build your models


taxi_train=pd.read_csv('taxi_train.csv')

taxi_train.columns.to_list()

taxi_test=pd.read_csv('taxi_test.csv')

taxi_test.columns.to_list()

submission file:

taxi_sample_submission.csv

taxi_sample_sub=pd.read_csv('taxi_sample_submission.csv')


>>>>> sample

# Import pandas
import pandas as pd

# Read train data
train = pd.read_csv('train.csv')

# Look at the shape of the data
print('Train shape:', train.shape)

# Look at the head() of the data
print(train.head())

 Read the test data
test = pd.read_csv('test.csv')

# Print train and test columns
print('Train columns:', train.columns.tolist())
print('Test columns:', test.columns.tolist())

output:
Train columns: ['id', 'date', 'store', 'item', 'sales']
Test columns: ['id', 'date', 'store', 'item']


# Read the sample submission file
sample_submission = pd.read_csv('sample_submission.csv')

# Look at the head() of the sample submission
print(sample_submission.head())


>>>>>>>>>>>>>>>>>>>>>>>>>Prepare your first submission

taxi_train=pd.read_csv('taxi_train.csv')
taxi_train.columns.to_list()

what is the problem type (regression, classification)


taxi_train.fare_amount.hist(bins=30, alpha=0.5)

from sklearn.linear_model import LinearRegression

lr=LinearRegression()

lr.fit(X=taxi_train[['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']],
	y=taxi_train['fare_amount'])

features=['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']

taxi_test['fare_amount']=lr.predict(taxi_test[features])

>>>>submission file

key
fare_amount

taxi_submission=taxi_test[['key','fare_amount']]

taxi_submission.to_csv('first_sub.csv',index=False)


>>>> sample submission

features=['id', 'date', 'store', 'item', 'sales']

import pandas as pd
from sklearn.ensemble import RandomForestRegressor

# Read the train data
train = pd.read_csv('train.csv')

print(train.columns.to_list())

# Create a Random Forest object
rf = RandomForestRegressor()

# Train a model
rf.fit(X=train[['store', 'item']], y=train['sales'])

# Read test and sample submission data
test = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')

# Show the head() of the sample_submission
print(sample_submission.head())

# Get predictions for the test set
test['sales'] = rf.predict(test[['store', 'item']])

# Write test predictions using the sample_submission format
test[['id', 'sales']].to_csv('kaggle_submission.csv', index=False)


>>>>> public vs private leaderboard

evaluation metric

1. area under the roc (auc) (classification)
2. f1 score (classification)
3. mean log loss (logloss) (classification)
4. mean absolute error (mae) (regression)
5. mean squared error (mse) (regression)
6. mean average precision Ranking (ranking)

test split
1. public test
2. private test


submission[['id','target']].to_csv('submission_1.csv',index=False)


as model complexity increases the train data error goes down but the test data error goes up

private LB
public LB

shake-up

>>>>>>> sample >>> xgboost >>> max_depth=2

import xgboost as xgb

# Create DMatrix on train data
dtrain = xgb.DMatrix(data=train[['store', 'item']],
                     label=train['sales'])

# Define xgboost parameters
params = {'objective': 'reg:linear',
          'max_depth': 2,
          'silent': 1}

# Train xgboost model
xg_depth_2 = xgb.train(params=params, dtrain=dtrain)


>>>>>>>> sample >>> xgboost >>> max_depth=8

import xgboost as xgb

# Create DMatrix on train data
dtrain = xgb.DMatrix(data=train[['store', 'item']],
                     label=train['sales'])

# Define xgboost parameters
params = {'objective': 'reg:linear',
          'max_depth': 8,
          'silent': 1}

# Train xgboost model
xg_depth_8 = xgb.train(params=params, dtrain=dtrain)

dtrain = xgb.DMatrix(data=train[['store', 'item']])
dtest = xgb.DMatrix(data=test[['store', 'item']])

# For each of 3 trained models
for model in [xg_depth_2, xg_depth_8, xg_depth_15]:
    # Make predictions
    train_pred = model.predict(dtrain)     
    test_pred = model.predict(dtest)          
    
    # Calculate metrics
    mse_train = mse(train['sales'], train_pred)                  
    mse_test =mse(test['sales'], test_pred)
    print('MSE Train: {:.3f}. MSE Test: {:.3f}'.format(mse_train, mse_test))

output:

MSE Train: 631.275. MSE Test: 558.522
MSE Train: 183.771. MSE Test: 337.337
MSE Train: 134.984. MSE Test: 355.534

>>>>>>>>>>>>>>Understand the problem

solution workflow
1. understand the problem
2. eda - explore data analysis
3. local validation
4. modeling


data type: tabular, time series, image, text

problem type: classification, regression, ranking

evaluation metric: roc au, f1 score, mae, mse

from sklearn.metrics import roc_auc_score, f1_score, mean_squared_error

def rmsl(y_true, y_pred):
	diffs=np.log(y_true+1) - np.log(y_pred+1)
	squares=np.power(diffs,2)
	err=np.sqrt(np.mean(squares))
	return err


def own_mse(y_true, y_pred):
  	# Raise differences to the power of 2
    squares = np.power(y_true - y_pred, 2)
    # Find mean over all observations
    err = np.mean(squares)
    return err


>>>> sample >>> log loss

import numpy as np

# Import log_loss from sklearn
from sklearn.metrics import log_loss

# Define your own LogLoss function
def own_logloss(y_true, prob_pred):
  	# Find loss for each observation
    terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1 - prob_pred)
    # Find mean over all observations
    err = np.mean(terms) 
    return -err

print('Sklearn LogLoss: {:.5f}'.format(log_loss(y_classification_true, y_classification_pred)))
print('Your LogLoss: {:.5f}'.format(own_logloss(y_classification_true, y_classification_pred)))



>>>>>>>>>>>>>>>>Initial EDA

Exploratory Data Analysis

1. size of the data
2. properties of the target variable
3. properties of the features
4. generate ideas for feature engineering

predict the popularity of an apartment rental listing

target_variable
1. interest_level

two sigma connect

id
bathrooms
bedrooms
building_id
latitude
longitude
manager_id
price
interest_level

df.interest_level.value_counts()

df.describe()
1. count
2. std
3. min
4. 25%
5. 50%
6. 75%
7. max

import matplotlib.pyplot as plt

plt.style.use('ggplot')

prices = df.groupby('interest_level', as_index=False)['price'].median()

fig=plt.figure(figsize=(7,5))

plt.bar(prices.interest_level, prices.price, width=0.5, alpha=0.8)

plt.xlabel('Interest level')
plt.ylabel('Median price')
plt.title('Median listing price across interest level')

plt.show()

>>> sample 

# Shapes of train and test data
print('Train shape:', train.shape)
print('Test shape:', test.shape)

# Train head()
print(train.head())

# Describe the target variable
print(train.fare_amount.describe())

# Train distribution of passengers within rides
print(train.passenger_count.value_counts())


# Calculate the ride distance
train['distance_km'] = haversine_distance(train)

# Draw a scatterplot
plt.scatter(x=train['fare_amount'], y=train['distance_km'], alpha=0.5)
plt.xlabel('Fare amount')
plt.ylabel('Distance, km')
plt.title('Fare amount based on the distance')

# Limit on the distance
plt.ylim(0, 50)
plt.show()

>>>>> sample fare amount on day time

# Create hour feature
train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)
train['hour'] = train.pickup_datetime.dt.hour

# Find median fare_amount for each hour
hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()

# Plot the line plot
plt.plot(hour_price['hour'], hour_price['fare_amount'], marker='o')
plt.xlabel('Hour of the day')
plt.ylabel('Median fare amount')
plt.title('Fare amount based on day time')
plt.xticks(range(24))
plt.show()

>>>>>>>>>>>Local validation

private LB overfitting

holdout set

train data-> train set and holdout set

train set -> training ->model ->predicting -> holdout set   (assess model quality)

>>>>>>>>>>>> cross validation using k-folding

the test on each fold is on data the model has never seen before

from sklearn.model_selection import KFold

kdf=KFold(n_splits=5, shuffle=True, random_state=123)

for train_index, test_index = kdf.split(train):
	cv_train,cv_test=train.iloc[train_index], train.iloc[test_index]


from sklearn.model_selection import StratifiedKFold

str_kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=123)

for train_index, test_index = str_kf.split(train):
	cv_train,cv_test=train.iloc[train_index], train.iloc[test_index]
	
	

>>>>>> sample >>>> KFold

# Import KFold
from sklearn.model_selection import KFold

# Create a KFold object
kf = KFold(n_splits=3, shuffle=True, random_state=123)

# Loop through each split
fold = 0
for train_index, test_index in kf.split(train):
    # Obtain training and testing folds
    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
    print('Fold: {}'.format(fold))
    print('CV train shape: {}'.format(cv_train.shape))
    print('Medium interest listings in CV train: {}\n'.format(sum(cv_train.interest_level == 'medium')))
    fold += 1

>>>>>>> sample >>>> stratified Fold

# Import StratifiedKFold
from sklearn.model_selection import StratifiedKFold

# Create a StratifiedKFold object
str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)

# Loop through each split
fold = 0
for train_index, test_index in str_kf.split(train, train['interest_level']):
    # Obtain training and testing folds
    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
    print('Fold: {}'.format(fold))
    print('CV train shape: {}'.format(cv_train.shape))
    print('Medium interest listings in CV train: {}\n'.format(sum(cv_train.interest_level == 'medium')))
    fold += 1


>>>>>>>>>>>>>>>Validation Usage

data leakage
1. leak in the features: using data that will not be available in the real setting

2. leak in validation strategy - validation strategy differs from the real-world situation


>>>>>> Time K-fold cross validation

from sklearn.model_selection import TimeSeriesSplit

time_kfold=TimeSeriesSpit(n_splits=5)

train=train.sort_values('date')

from train_index, test_index in time_kfold.split(train):
	 cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]


>>>>>> validation pipeline CV_STRATEGY

#list for results

fold_metrics=[]

for train_index, test_index in CV_STRATEGY.split(train):
	 cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]

	model.fit(cv_train)
	predictions=model.predict(cv_test)
	
	metric=evaluate(cv_test, predictions)

	fold_metrics.append(metric)


import numpy as np

mean_score = np.mean(fold_metrics)

overall_score_minimizing = no.mean(fold_metrics)+ np.std(fold_metrics)

overall_score_maximizing = no.mean(fold_metrics)- np.std(fold_metrics)


>>>>> sample >>> timeseries fold

# Create TimeSeriesSplit object
time_kfold = TimeSeriesSplit(n_splits=3)

# Sort train data by date
train = train.sort_values('date')

# Iterate through each split
fold = 0
for train_index, test_index in time_kfold.split(train):
    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]

    
    print('Fold :', fold)
    print('Train date range: from {} to {}'.format(cv_train.date.min(), cv_train.date.max()))
    print('Test date range: from {} to {}\n'.format(cv_test.date.min(), cv_test.date.max()))
    fold += 1



Fold : 0
Train date range: from 2017-12-01 to 2017-12-08
Test date range: from 2017-12-08 to 2017-12-16

Fold : 1
Train date range: from 2017-12-01 to 2017-12-16
Test date range: from 2017-12-16 to 2017-12-24

Fold : 2
Train date range: from 2017-12-01 to 2017-12-24
Test date range: from 2017-12-24 to 2017-12-31


the test date ranges do not overlap


>>>>> sample get the mean validation mse:

from sklearn.model_selection import TimeSeriesSplit
import numpy as np

# Sort train data by date
train = train.sort_values('date')

# Initialize 3-fold time cross-validation
kf = TimeSeriesSplit(n_splits=3)

# Get MSE scores for each cross-validation split
mse_scores = get_fold_mse(train, kf)

print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))

print('MSE by fold: {}'.format(mse_scores))

print('Overall validation MSE: {:.5f}'.format(np.mean(mse_scores) + np.std(mse_scores)))

output:
Mean validation MSE: 955.49186
MSE by fold: [890.30336, 961.65797, 1014.51424]
Overall validation MSE: 1006.38784


>>>>>>>>>>>>>>>>feature engineering

modeling

1. create new features
2. improve models
3. apply tricks
4. preprocess data

feature engineering is creating new features
1. numerical
2. categorical
3. datetime
4. coordinates
5. text
6. images

data = pd.concat([train,test])

train=data[data.id.isin(train_id)]
test=data[data.id.isin(test_id)]

dem['date']=pd.to_datetime(dem['date'])

dem['year']=dem['date'].dt.year
dem['month']=dem['date'].dt.month
dem['day']=dem['date'].dt.day
dem['dayofweek']=dem['date'].dt.dayofweek


>>>>sample >>>> create a new feature

# Look at the initial RMSE
print('RMSE before feature engineering:', get_kfold_rmse(train))

# Find the total area of the house
train['TotalArea'] = train["TotalBsmtSF"] + train["FirstFlrSF"] + train["SecondFlrSF"]

# Look at the updated RMSE
print('RMSE with total area:', get_kfold_rmse(train))

# Find the area of the garden
train['GardenArea'] = train["LotArea"] - train["FirstFlrSF"]
print('RMSE with garden area:', get_kfold_rmse(train))

RMSE before feature engineering: 36029.39
RMSE with total area: 35073.2
RMSE with garden area: 34413.55

# Find total number of bathrooms
train['TotalBath'] = train['FullBath']+train['HalfBath']
print('RMSE with number of bathrooms:', get_kfold_rmse(train))

RMSE with number of bathrooms: 34506.78

Here you see that house area improved the RMSE by almost $1,000. Adding garden area improved the RMSE by another $600. However, with the total number of bathrooms, the RMSE has increased. It means that you keep the new area features, but do not add "TotalBath" as a new feature. Let's now work with the datetime features!


>>>>>> sample >>> new datetime feature

# Concatenate train and test together
taxi = pd.concat([train, test])

# Convert pickup date to datetime object
taxi['pickup_datetime'] = pd.to_datetime(taxi['pickup_datetime'])

# Create a day of week feature
taxi['dayofweek'] = taxi['pickup_datetime'].dt.dayofweek

# Create an hour feature
taxi['hour'] = taxi['pickup_datetime'].dt.hour

# Split back into train and test
new_train = taxi[taxi['id'].isin(train['id'])]
new_test = taxi[taxi['id'].isin(test['id'])]


>>>>>>>>>>>>>>>>categorical features

label encoding
1 a
2 b
3 c
4 a

encoded
1 0
2 1
3 2
4 0


from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

df['cat_encoded'] = le.fit_transform(df['cat'])

to overcome the independency between categories, one hot encoding was developed

ohe=pd.get_dummies(df['cat'], prefix='ohe_cat')

df.drop('cat',axis=1,inplace=True)
df=pd.concat([df,ohe],axis=1)

>>>>> binary features
yes or no

le=LabelEncoder()

binary_feature['binary_encoded']=le.fit_transform(binary_feature['binary_feat'])

>>> other encoders

target encoder

>>>>> sample >>> labelEncoder

# Concatenate train and test together
houses = pd.concat([train, test])

# Label encoder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

# Create new features
houses['RoofStyle_enc'] = le.fit_transform(houses["RoofStyle"])
houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])

# Look at new features
print(houses[['RoofStyle', 'RoofStyle_enc', 'CentralAir', 'CentralAir_enc']].head())



>>>>>> problem with label encoding

The problem with label encoding is that it implicitly assumes that there is a ranking dependency between the categories.


>>>>> sample  value counts between roof style and central air

# Concatenate train and test together
houses = pd.concat([train, test])

# Look at feature distributions
print(houses['RoofStyle'].value_counts(), '\n')
print(houses['CentralAir'].value_counts())


output:

Name: RoofStyle, dtype: int64 

Y    2723
N     196

Name: CentralAir, dtype: int64

>>>>> encode CentralAir as binary 0 or 1

# Concatenate train and test together
houses = pd.concat([train, test])

# Label encode binary 'CentralAir' feature
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
houses['CentralAir_enc'] = le.fit_transform(houses["CentralAir"])

>>>one hot encode

# Create One-Hot encoded features
ohe = pd.get_dummies(houses['RoofStyle'], prefix='RoofStyle')

# Concatenate OHE features to houses
houses = pd.concat([houses, ohe], axis=1)

# Look at OHE features
print(houses[[col for col in houses.columns if 'RoofStyle' in col]].head(3))


>>>>>>>>>>>Target encoding

1. label encoder provides distinct number for each category

2. one-hot encoder creates new features for each category value

target encoding creates a single column

1. calculate mean on the train, apply to the test
2. split train into K folds.  calculate mean on k-1 folds, apply to the k-th fold.  this prevents overfitting
3. add mean target encoded feature to the model


>>>>> sample    categorical  target
def train_mean_target_encoding(train, target, categorical, alpha=5):
    # Create 5-fold cross-validation
    kf = KFold(n_splits=5, random_state=123, shuffle=True)
    train_feature = pd.Series(index=train.index)

    # For each folds split
    for train_index, test_index in kf.split(train):
        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
        # Calculate out-of-fold statistics and apply to cv_test
        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)
        # Save new feature for this particular fold
        train_feature.iloc[test_index] = cv_test_feature       
    return train_feature.values

def test_mean_target_encoding(train, test, target, categorical, alpha=5):
    # Calculate global mean on the train data
    global_mean = train[target].mean()
    
    # Group by the categorical feature and calculate its properties
    train_groups = train.groupby(categorical)
    category_sum = train_groups[target].sum()
    category_size = train_groups.size()
    
    # Calculate smoothed mean target statistics
    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)
    
    # Apply statistics to the test data and fill new categories
    test_feature = test[categorical].map(train_statistics).fillna(global_mean)
    return test_feature.values


def mean_target_encoding(train, test, target, categorical, alpha=5):
  
    # Get the train feature
    train_feature = train_mean_target_encoding(train, target, categorical, alpha)
  
    # Get the test feature
    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)
    
    # Return new features to add to the model
    return train_features, test_features


>>code

# For each folds split
    for train_index, test_index in kf.split(train):
        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
      
        # Calculate out-of-fold statistics and apply to cv_test

        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)
        
        # Save new feature for this particular fold
        train_feature.iloc[test_index] = cv_test_feature       

    return train_feature.values


>>>>> target categorical

# Create 5-fold cross-validation
kf = KFold(n_splits=5, random_state=123, shuffle=True)

# For each folds split
for train_index, test_index in kf.split(bryant_shots):
    cv_train, cv_test = bryant_shots.iloc[train_index], bryant_shots.iloc[test_index]

    # Create mean target encoded feature
    cv_train['game_id_enc'], cv_test['game_id_enc'] = mean_target_encoding(train=cv_train,
                                                                           test=cv_test,
                                                                           target='shot_made_flag',
                                                                           categorical='game_id',
                                                                           alpha=5)
    # Look at the encoding
    print(cv_train[['game_id', 'shot_made_flag', 'game_id_enc']].sample(n=1))


output:

<script.py> output:
           game_id  shot_made_flag  game_id_enc
    7106  20500532             0.0     0.361914
           game_id  shot_made_flag  game_id_enc
    5084  20301100             0.0     0.568395
           game_id  shot_made_flag  game_id_enc
    6687  20500228             0.0      0.48131
           game_id  shot_made_flag  game_id_enc
    5046  20301075             0.0     0.252103
           game_id  shot_made_flag  game_id_enc
    4662  20300515             1.0     0.452637


The main conclusion you should make: while using local cross-validation, you need to repeat mean target encoding procedure inside each folds split separately. Go on to try other problem types beyond binary classification!


>>>> sample

# Create mean target encoded feature
train['RoofStyle_enc'], test['RoofStyle_enc'] = mean_target_encoding(train=train,
                                                                     test=test,
                                                                     target='SalePrice',
                                                                     categorical='RoofStyle',
                                                                     alpha=10)

# Look at the encoding
print(test[['RoofStyle', 'RoofStyle_enc']].drop_duplicates())



output:
<script.py> output:
         RoofStyle  RoofStyle_enc
    0        Gable  171565.947836
    1          Hip  217594.645131
    98     Gambrel  164152.950424
    133       Flat  188703.563431
    362    Mansard  180775.938759
    1053      Shed  188267.663242


So, you observe that houses with the Hip roof are the most pricy, while houses with the Gambrel roof are the cheapest.


>>>>>>> Missing data

Mean/Median imputation

categorical missing data replaced with the most frequent value

new category imputation

df.isnull().head(1)


from sklearn.impute import SimpleImputer

mean_imputer = SimpleImputer(strategy='mean')

constant_imputer = SimpleImputer(strategy='constant', fill_value=-999)


df[['num']] = mean_imputer.fit_transform(df[['num']])


constant_imputer = SimpleImputer(strategy='constant', fill_value='MISS')

>>>> sample >>> find columns with missing data

# Read dataframe
twosigma = pd.read_csv("twosigma_train.csv")

# Find the number of missing values in each column
print(twosigma.isnull().sum())


output:

id                 0
bathrooms          0
bedrooms           0
building_id       13
latitude           0
longitude          0
manager_id         0
price             32
interest_level     0
dtype: int64

 # Look at the columns with the missing values
print(twosigma[['building_id', 'price']].head())


>>> sample >>> imputer mean

# Import SimpleImputer
from sklearn.impute import SimpleImputer

# Create mean imputer
mean_imputer = SimpleImputer(strategy='mean')

# Price imputation
rental_listings[['price']] = mean_imputer.fit_transform(rental_listings[['price']])

>>>> sample >>> simple imputer constant

# Import SimpleImputer
from sklearn.impute import SimpleImputer

# Create constant imputer
constant_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')

# building_id imputation
rental_listings[['building_id']] = constant_imputer.fit_transform(rental_listings[['building_id']])

>>>>>>>>>>>baseline model

taxi_train=pd.read_csv('taxi_train.csv')
taxi_test=pd.read_csv('taxi_test.csv')


from sklearn.model_selection import train_test_split

validation_train, validation_test=train_test_split(taxi_train, test_size=0.3, random_state=123)

taxi_test['fare_amount']=np.mean(taxi_train.fair_amount)

#### mean_sub
taxi_test[['id','fare_amount']].to_csv('mean_sub.csv',index=False)

naive_prediction_groups=taxi_train.groupby('passenger_count').fare_amount.mean()

taxi_test['fare_amount']=taxi_test.passenger_count.map(naive_prediction_groups)

#map- Used for substituting each value in a Series with another value

### mean group sub

taxi_test[['id','fare_amount']].to_csv('mean_group_sub.csv', index=False)

#select only numeric features

features['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']

from sklearn.ensemble import GradientBoostingRegressor

gb=GradientBoostingRegressor()

gb.fit(taxi_train[features], taxi_train.fare_amount)

taxi_test['fare_amount']=gb.predict(taxi_test[features])

### gradient boost

taxi_test[['id','fare_amount']].to_csv('gb_sub.csv',index=False)


model		validation RMSE  public LB RMSE

simple mean	9.986	9.409
group mean	9.978	9.407
gradient boost	5.996	4.595


>>>> sample >>> hold out

import numpy as np
from sklearn.metrics import mean_squared_error
from math import sqrt

# Calculate the mean fare_amount on the validation_train data
naive_prediction = np.mean(validation_train['fare_amount'])

# Assign naive prediction to all the holdout observations
validation_test['pred'] = naive_prediction

# Measure the local RMSE
rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))
print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))

Validation RMSE for Baseline I model: 9.986


>>>> sample Group by hour

# Get pickup hour from the pickup_datetime column
train['hour'] = train['pickup_datetime'].dt.hour
test['hour'] = test['pickup_datetime'].dt.hour

# Calculate average fare_amount grouped by pickup hour 
hour_groups = train.groupby('hour')['fare_amount'].mean()

# Make predictions on the test set
test['fare_amount'] = test.hour.map(hour_groups)

# Write predictions
test[['id','fare_amount']].to_csv('hour_mean_sub.csv', index=False)


>>>> hyperparameter tuning

add hour feature: validation rmse 5.553

add distance feature: validation rmse 5.268

deep learning does not require feature engineering

least squares linear regression
1. loss = (y_i-yhat_i)**2 -> min

ridge regression
loss =  (y_i-yhat_i)**2 + alpha * weights**2

popular approachs
1. Grid Search
2. Random Grid Search
3. Bayesian optimization


>>>>>>>>>>>Grid search

alpha_grid=[0.01,0.1,1,10]

from sklearn.linear_model import Ridge

results={}

for candidate_alpha in alpha_grid:
	ridge_regression=Ridge(alpha=candidate_alpha)

	results[candidate_alpha]=validation_score

>>>>> sample max depth

# Possible max depth values
max_depth_grid = [3,6,9,12,15]
results = {}

# For each value in the grid
for max_depth_candidate in max_depth_grid:
    # Specify parameters for the model
    params = {'max_depth': max_depth_candidate}

    # Calculate validation score for a particular hyperparameter
    validation_score = get_cv_score(train, params)

    # Save the results for each max depth value
    results[max_depth_candidate] = validation_score   
print(results)

output:

{3: 6.50509, 6: 6.52138, 9: 6.64181, 12: 6.8819, 15: 6.99156}



The drawback of tuning each hyperparameter independently is a potential dependency between different hyperparameters. The better approach is to try all the possible hyperparameter combinations.


>>>>>>>>>>>>>>Sample Product with parameters

import itertools

# Hyperparameter grids
max_depth_grid = [3, 5, 7]
subsample_grid = [0.8, 0.9, 1.0]
results = {}

# For each couple in the grid
for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid):
    params = {'max_depth': max_depth_candidate,
              'subsample': subsample_candidate}
    validation_score = get_cv_score(train, params)
    # Save the results for each couple
    results[(max_depth_candidate, subsample_candidate)] = validation_score   
print(results)


{(3, 0.8): 6.33917, (3, 0.9): 6.43642, (3, 1.0): 6.50509, (5, 0.8): 6.26977, (5, 0.9): 6.35116, (5, 1.0): 6.45468, (7, 0.8): 6.1635, (7, 0.9): 6.34018, (7, 1.0): 6.48436}

With max_depth equal to 7 and subsample equal to 0.8, the best RMSE is now $6.16.

(grid_df_class.cv_results_)

applies only to classification

>>>>>>>>Model Ensembling

input data:
1. categorical
2. one hote encoded categorical
3. numerics

inputs into
500 modesl
inputs into 
125 xgboost models

different subsets
40 models and 60 models

input into 
5 models of keras

weighted Rank Average


>>>>> Regression problem

regression classifier

train two models a and b


>>>>> model stacking

1. split train data into two parts
2. train multiple models on part 1
3. make predictions on part 2
4. make predictions on the test data

5. train a new model on part 2 using predictions as features

6. make predictions on the test data using the 2nd level model.

train models A, b, c on part 1

trainid   feature1 feature2 Target A_pred, B_pred, c_pred

4	.10	2.87	1	0.71	0.52	.098

make predictions of the test data as well

testid   feature1 feature2 Target A_pred, B_pred, c_pred


train 2nd level model on part 2 using the train and test data set from part 1

resulting in a stacking prediction

>>>>  sample add new feature gb and rf part 1

from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor

# Train a Gradient Boosting model
gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)

# Train a Random Forest model
rf = RandomForestRegressor().fit(train[features], train.fare_amount)

# Make predictions on the test data
test['gb_pred'] = gb.predict(test[features])
test['rf_pred'] = rf.predict(test[features])

# Find mean of model predictions
test['blend'] = (test['gb_pred'] + test['rf_pred']) / 2
print(test[['gb_pred', 'rf_pred', 'blend']].head(3))


# Split train data into two parts
part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)

# Train a Gradient Boosting model on Part 1
gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)

# Train a Random Forest model on Part 1
rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)

# Make predictions on the Part 2 data
part_2['gb_pred'] = gb.predict(part_2[features])
part_2['rf_pred'] = rf.predict(part_2[features])

# Make predictions on the test data
test['gb_pred'] = gb.predict(test[features])
test['rf_pred'] = rf.predict(test[features])

from sklearn.linear_model import LinearRegression

# Create linear regression model without the intercept
lr = LinearRegression(fit_intercept=False)

# Train 2nd level model on the Part 2 data
lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)

# Make stacking predictions on the test data
test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])

# Look at the model coefficients
print(lr.coef_)

output:
[0.72504358 0.27647395]

Looking at the coefficients, it's clear that 2nd level model has more trust to the Gradient Boosting: 0.7 versus 0.3 for the Random Forest model. 

>>>>>>>>>>>>>>>>>.Save Information

1. save folds to the disk
2. save model runs
3. save model predictions to the disk
4. save performance results


forums

Competition discussion by the participants

Kaggle kernels
scripts and notebooks shared by the participants

cloud computational environment

competitions last 2 to 3 months

>>>>> sample drop column and score

# Drop passenger_count column
new_train_1 = train.drop('passenger_count', axis=1)

# Compare validation scores
initial_score = get_cv_score(train)
new_score = get_cv_score(new_train_1)

print('Initial score is {} and the new score is {}'.format(initial_score, new_score))


 Initial score is 6.50509 and the new score is 6.41902


# Create copy of the initial train DataFrame
new_train_2 = train.copy()

# Find sum of pickup latitude and ride distance
new_train_2['weird_feature'] = new_train_2['pickup_latitude'] + new_train_2['distance_km']

# Compare validation scores
initial_score = get_cv_score(train)
new_score = get_cv_score(new_train_2)

print('Initial score is {} and the new score is {}'.format(initial_score, new_score))

Initial score is 6.50509 and the new score is 6.5121

 In this particular case, dropping the "passenger_count" feature helped, while finding the sum of pickup latitude and ride distance did not. 

Machine learning models

1. talk to business.  Define the problem
2. collect the data
3. select the metric
4. make train and test split
5. create the model
6. move the model to production








 
























































