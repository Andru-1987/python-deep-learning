what is streaming and why does it matter?

batch: larger datasets and more complex analysis, slower moving data

stream: simplier analysis: aggregation/filtering, individual records/ micro batches, data moves fast

iot device: tracks: emission, time, and speed for each vehicle

Kinesis gathers data from iot
lamda builds functions to process the data

amazon s3 - buckets with iam security

amazon sns

amazon kinesis: 1. data firehose 2. data streams 3. data analytics

firehose receives data from producers and delivers it to amazon redshift, s3, and elastic search

we use boto3 to interact with firehose


import boto3

firehose = boto3.client('firehose',
	aws_access_key_id=AWS_KEY_ID,
	aws_secret_access_key=AWS_SECRET,
`	region_name='us-east-1')

response = firehose.list_delivery_streams()

print(response['DeliveryStreamNames'])

#deletes all the deliver streams
for stream_name in response['DeliveryStreamNames']:
	firehose.delete_delivery_stream(DeliveryStreamName=stream_name)


>>>>>>>>>>>

# Import boto3 and create boto3 Firehose client
import boto3
firehose = boto3.client('firehose', 
    aws_access_key_id=AWS_KEY_ID, aws_secret_access_key=AWS_SECRET, 
    region_name='us-east-1', endpoint_url=endpoints['FIREHOSE'])

# Get list of delivery streams
response = firehose.list_delivery_streams()

# Iterate over the response contents and delete every stream
for stream_name in response['DeliveryStreamNames']:
    firehose.delete_delivery_stream(DeliveryStreamName=stream_name)
    print(f"Deleted stream: {stream_name}")

# Print list of delivery streams
print(firehose.list_delivery_streams())


>>>>>>>>>>>>>>>>>>Getting ready for the first stream


vehicle sensors will write to gps-delivery-stream and it delivers data to sd-vehicle-data

security
1.dcUser
2.aws_key_id
3.aws_secret

vehicle sensors write to firehose and firehose delivers data to sd-vehicle-data


s3 = boto3.client('s3', aws_access_key_id AWS_KEY_ID,
		aws_secret_access_key = AWS_SECRET,
		region_name = 'us-east-1')

s3.create_bucket(Bucket='sd-vehicle-data')
dcUser needs add permission to put data into the s3 bucket and to put data on the stream

IAM (identity and access management)
1. users
a. dcUsers
add permissions
attach existing policies directly

add->AmazonKinesisFirehoseFullAccess

dcUser can create, manage and write to firehose stream


The firehose stream can not write to the s3 bucket

Create a role firehose to write to s3 bucket

role: firehoseDeliveryRole -> AmazonS3FullAccess

roles live in iam

a role is a hat that the amazon service puts on

the role has permissions

AmazonS3FullAccess

The role can only be run by the trusted entity

Steps

aws console
1. roles
2. create a role
3. aws service (trusted entity type)
4. kinesis
a. kinesis firehose
b. AmazonS3FullAccess
c. name the role: firehoseDeliveryRole1




>>>>>>>>>>>> Ready for the first stream

# Create the new sd-vehicle-data bucket
s3.create_bucket(Bucket='sd-vehicle-data')

# List the buckets in S3
for bucket_info in s3.list_buckets()['Buckets']:
    
    # Get the bucket_name
    bucket_name = bucket_info['Name']
    
    # Generate bucket ARN.
    arn = "arn:aws:s3:::{}".format(bucket_name)
    
    # Print the ARN
    print(arn)


ARN : amazon resource names

arn:aws:s3:::logs-bucket
arn:aws:s3:::samples-bucket
arn:aws:s3:::sd-vehicle-data

>>>>>>>>>> working with the firehose delivery stream  (create the stream)


we use the role arn when creating our stream

IAM
1. roles
2. firehoseDeliveryRole
3. copy and paste the Role ARN


s3 = boto3.client('s3', aws_access_key_id AWS_KEY_ID,
		aws_secret_access_key = AWS_SECRET,
		region_name = 'us-east-1')


res=firehose.create_delivery_stream(
	deliveryStreamName="gps-delivery-stream",
	deliveryStreamType ="DirectPut",	
	S3DestinationConfiguration={
		"RoleARN":"arn:aws:iam::00000000:role/firehoseDeliveryRole",
		"BucketARN": "arn:aws:s3:::sd-vehicle-data"
	}
)

print(res['DeliveryStreamARN'])

arn:aws:firehose:us-east-1:0000000:deliverystream/gps-delivery-stream


sensor data
{
'record_id': '939ed1d1-1740-420c-8906-445278573c7f'
'timestamp':'4:25:06.00',
'vin' '4ftex4944akl844294',
'lon',106.9447146,
'lat',-6.3385652,
'speed':25
}

res=firehose.put_record(
	DeliveryStreamName='gps-delivery-stream,
	Record={
		'Data':payload+"\n"
	}
	
)

the record is converted into one string

payload = " ".join(
	str(value) for value in record.values()
)

the data arrives in the s3 bucket in folders with year,month,day pattern

>>>>>>>>>. reading data from the s3 bucket

#get the stream key
obj_data = s3.get_object(Bucket='sd-vehicle-data', key='s3 object key ')

vehicle_data = pd.read_csv(
	data['Body'],
	delimiter=" ",
	names=["record_id","timestamp",'vin","lon","lat","speed"]))


<<<<<<<<<

# create_firehose.py: Create firehose stream. No need to edit.
import _setup
firehose, s3 = _setup.ex_vars

# Create s3 bucket
s3.create_bucket(Bucket='sd-vehicle-data')

# Create Firehose delivery stream
res = firehose.create_delivery_stream(
    DeliveryStreamName="gps-delivery-stream",
    DeliveryStreamType="DirectPut",
    # Specify configuration of the destination S3 bucket
    S3DestinationConfiguration = {
        "BucketARN": "arn:aws:s3:::sd-vehicle-data",
        "RoleARN": "arn:aws:iam::0000000:role/firehoseDeliveryRole"
    })
    
# Print the stream ARN
print("Created Firehose Stream ARN: {}".format(res['DeliveryStreamARN']))

Created Firehose Stream ARN: arn:aws:firehose:us-east-1:000000000000:deliverystream/gps-delivery-stream


# OBD2_sensors.py: Write to Firehose stream. EDIT HERE.
import _setup, create_firehose
firehose, s3, records = _setup.ex_vars
for idx, row in records.iterrows(): 

    # Create a payload string that ends with a newline
    payload = ' '.join(str(value) for value in row) 
    payload = payload + "\n"
    print("Sending payload: {}".format(payload))

    # Send the payload string to Firehose stream
    res = firehose.put_record(
        DeliveryStreamName = 'gps-delivery-stream',
        Record = {'Data': payload})

    # Print the written RecordId
    print("Wrote to RecordId: {}".format(res['RecordId']))



>>>>>>> Getting data from the buckets

# analyze_data.py: Analyze written sensor data. EDIT HERE.
import _setup, _run_deps, pandas as pd
firehose, s3, records = _setup.ex_vars

# List the objects that have been written to the S3 bucket
objects = s3.list_objects(Bucket='sd-vehicle-data')['Contents']

# Create list for collecting dataframes from read files.
dfs = []

# For every object, load it from S3
for obj in objects:
    data_file = s3.get_object(Bucket='sd-vehicle-data', Key=obj['Key'])

    # Load it into a dataframe, specifying a delimiter and column names
    dfs.append(pd.read_csv(data_file['Body'], 
                           delimiter = " ", 
                           names=["record_id", "timestamp", "vin", "lon", "lat", "speed"]))

# Concatenate the resulting dataframes.
data = pd.concat(dfs)
print(data.groupby(['vin'])['speed'].max())























