encoder and decoder


model=Sequential()

model.add(Embedding(input_language_size, input_wordvec_dim,
	input_length=input_language_len, mask_zero=True))

#mask_zero inserts a zero token for the vocabulary

model.add(LSTM(128))

model.add(RepeatVector(output_language_len))

#The repeatVector will be used as input for the decoder part the model


#right after the encoder

model.add(LSTM(128, return_sequences=True))

model.add(TimeDistributed(Dense(eng_vocab_size,activation='softmax')))

#comparing the whole sentence

#return_sequences to true and timedistributed the loss function is applied in every token


>>>>

we convert both languages to index of numbers for the encoder and decoder part of the model


from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer()
tokenizer=fit_on_texts(input_texts_list)

#input_texts_list are the sentences in the input language

X=tokenizer.texts_to_sequences(input_texts_list)

X=pad_sequences(X,maxlen=length, padding='post')

tokizer=Tokenizer()
tokenizer.fit_on_texts(output_texts_list)
Y=tokenizer.texts_to_sequences(output_texts_list)
Y=pad_sequences(Y,maxlen=length, padding='post')

ylist=list()

for sequence in Y:
	encoded=to_categorical(sequence,num_classes=vocab_size)
	ylist.append(encoded)

Y = np.array(ylist).reshape(Y.shape[0],Y.shape[1],vocab_size)

#dimensions number of sentences, sentence length, and output vocabulary size


model.fit(X,Y, epochs=N)

#evaluate using nltk.translate.bleu_score

>>>>>>>>>>>>


# Get maximum length of the sentences
pt_length = max([len(sentence.split()) for sentence in pt_sentences])

# Transform text to sequence of numerical indexes
X = input_tokenizer.texts_to_sequences(pt_sentences)

# Pad the sequences
X = pad_sequences(X, maxlen=pt_length, padding='post')

# Print first sentence
print(pt_sentences[0])

# Print transformed sentence
print(X)



>>>>>>>>>>>


# Initialize the variable
Y = transform_text_to_sequences(en_sentences, output_tokenizer)

# Temporary list
ylist = list()
for sequence in Y:
  	# One-hot encode sentence and append to list
    ylist.append(to_categorical(sequence, num_classes=en_vocab_size))

# Update the variable
Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], en_vocab_size)

# Print the raw sentence and its transformed version
print("Raw sentence: {0}\nTransformed: {1}".format(en_sentences[0], Y[0]))


>>>>>>>

# Function to predict many phrases
def predict_many(model, sentences, index_to_word, raw_dataset):
    for i, sentence in enumerate(sentences):
        # Translate the Portuguese sentence
        translation = predict_one(model, sentence, index_to_word)
        
        # Get the raw Portuguese and English sentences
        raw_target, raw_src = raw_dataset[i]
        
        # Print the correct Portuguese and English sentences and the predicted
        print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))

predict_many(model, X_test[:10], en_index_to_word, test)





















