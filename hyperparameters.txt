deeper understanding of the model

30,000 users and 24 attributes

log_reg_clf=LogisticRegression()
log_reg_clf.fit(X_train,y_train)
print(log_reg_clf.coef_)

original_variables=list(X_train.columns)

zipped_together=list(zip(original_variables,log_reg_clf.coef_[0]))
coefs = [list(x) for x in zipped_together]

coefs=pd.DataFrame(coefs, columns=['Variable','Coefficient'])

print(coefs)

coefs.sort_values(by=['Coefficient'], axis=0, inplace=True, ascending=False)

Pay variable are how many months people have delayed their payments

need to know how the classifier works

Random forest has no coefficients

rf_clf = RandomForestClassifier(max_depth=2)
rf_clf.fit(X_train,y_train)

chosen_tree=rf_clf.estimators_[7]

split_column=chosen_tree.tree_.feature[1]
split_column_name=X_train.columns[split_column]
split_value=chosen_tree.tree_.threshold[1]
print(split_column_name,split_value)


>>>>>>>>>>>>>>>sample  >> find the top 3 variables by coefficient values

# Create a list of original variable names from the training DataFrame
original_variables = X_train.columns

# Extract the coefficients of the logistic regression estimator
print(log_reg_clf.coef_)
model_coefficients = log_reg_clf.coef_[0]

# Create a dataframe of the variables and coefficients & print it out
coefficient_df = pd.DataFrame({"Variable" : original_variables, "Coefficient": model_coefficients})
print(coefficient_df)

# Print out the top 3 positive variables
top_three_df = coefficient_df.sort_values(by='Coefficient', axis=0, ascending=False)[0:3]
print(top_three_df)


>>>>>>>>>  sample >>> grab a node on the decision tree

# Extract the 7th (index 6) tree from the random forest
chosen_tree = rf_clf.estimators_[7]

# Visualize the graph using the provided image
imgplot = plt.imshow(tree_viz_image)
plt.show()

# Extract the parameters and level of the top (index 0) node
split_column = chosen_tree.tree_.feature[0]
split_column_name = X_train.columns[split_column]
split_value = chosen_tree.tree_.threshold[0]

# Print out the feature and level
print("This node split on feature {}, at a value of {}".format(split_column_name, split_value))

output: This node split on feature LIMIT_BAL, at a value of 115000.0

if
chosen_tree = rf_clf.estimators_[6]

This node split on feature PAY_AMT5, at a value of 1677.5

>>>>>>>>>>>>>>>>Hyper parameter overview

RandomForestClassifier
n_estimators (number of trees in the forest)
criterion='entropy'

important parameters:
n_estimators (high values)
max_features (how many features to consider before splitting)
max_depth & min_sample_leaf (important to reduce overfitting)
criterion (maybe)

lr=LogisticRegression()
C=1.0
class_weight=None
duel=False
fit_intercept=True
intercept_scaling=1
max_iter=100,
multi_class='warn'
n_jobs=None
penlty='l2'
random_state=None
solver='warn'
tot=0.0001
verbose=0
warm_start=False


>>>sample  >> increase the 

# Print out the old estimator, notice which hyperparameter is badly set
print(rf_clf_old)

# Get confusion matrix & accuracy for the old rf_model
print("Confusion Matrix: \n\n {} \n Accuracy Score: \n\n {}".format(
  	confusion_matrix(y_test, rf_old_predictions),  
  	accuracy_score(y_test, rf_old_predictions)))

# Create a new random forest classifier with better hyperparamaters
rf_clf_new = RandomForestClassifier(n_estimators=500)

# Fit this to the data and obtain predictions
rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test)

# Assess the new model (using new predictions!)
print("Confusion Matrix: \n\n", confusion_matrix(y_test, rf_new_predictions))
print("Accuracy Score: \n\n", accuracy_score(y_test, rf_new_predictions))


>>>> sample >>> k nearest neighbor

# Build a knn estimator for each value of n_neighbours
knn_5 = KNeighborsClassifier(n_neighbors=5)
knn_10 = KNeighborsClassifier(n_neighbors=10)
knn_20 = KNeighborsClassifier(n_neighbors=20)

# Fit each to the training data & produce predictions
knn_5_predictions = knn_5.fit(X_train,y_train).predict(X_test)
knn_10_predictions = knn_10.fit(X_train,y_train).predict(X_test)
knn_20_predictions = knn_20.fit(X_train,y_train).predict(X_test)

# Get an accuracy score for each of the models
knn_5_accuracy = accuracy_score(y_test, knn_5_predictions)
knn_10_accuracy = accuracy_score(y_test, knn_10_predictions)
knn_20_accuracy = accuracy_score(y_test, knn_20_predictions)
print("The accuracy of 5, 10, 20 neighbours was {}, {}, {}".format(knn_5_accuracy, knn_10_accuracy, knn_20_accuracy))


>>>>>>>>>>>>>>>>Automating hyperparameters

LogisticRegression

some values of the parameter solver conflict with the penalty parameter


neighbors_list=[3,5,10,20,50,75]

for test_number in neighbors_list:
	model=KNeighborsClassifier(n_neighbors=test_number)
	predictions=model.fit(X_train,y_train).predict(X_test)
	accuracy=accuracy_score(y_test,predictions)
	accuracy_list.append(accuracy)


results_df=pd.DataFrame({neighbors':neighbors_list, 'accuracy':accuracy_list})

>>>>>>>>>Learning curve

plt.(results_df['neighbors'],results_df['accuracy'])

plt.gca().set(xlabel='n_neighbors', ylabel='Accuracy',title='Accuracy for different n_neighbors')

plt.show()

np.linspace(start,end, number of items)

np.linspace(1,2,5)

[1 1.25 1.5 1.75 2]

>>>>>sample  Gradient boost classifier  >> different learning rates

# Set the learning rates & results storage
learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]
results_list = []

# Create the for loop to evaluate model predictions for each learning rate
for learning_rate in learning_rates:
    model = GradientBoostingClassifier(learning_rate=learning_rate)
    predictions = model.fit(X_train,y_train).predict(X_test)
    # Save the learning rate and accuracy score
    results_list.append([learning_rate, accuracy_score(y_test, predictions)])

# Gather everything into a DataFrame
results_df = pd.DataFrame(results_list, columns=['learning_rate', 'accuracy'])
print(results_df)

output:
learning_rate  accuracy
0          0.001    0.7825
1          0.010    0.8025
2          0.050    0.8100
3          0.100    0.7975
4          0.200    0.7900
5          0.500    0.7775


>>>>> sample >>> plot the learning curve

# Set the learning rates & accuracies list
learn_rates = np.linspace(0.01, 2, num=30)
accuracies = []

# Create the for loop
for learn_rate in learn_rates:
  	# Create the model, predictions & save the accuracies as before
    model = GradientBoostingClassifier(learning_rate=learn_rate)
    predictions = model.fit(X_train,y_train).predict(X_test)
    accuracies.append(accuracy_score(y_test, predictions))

# Plot results    
plt.plot(learn_rates, accuracies)
plt.gca().set(xlabel='learning_rate', ylabel='Accuracy', title='Accuracy for different learning_rates')
plt.show()

>>>>>>>>>>>>Introducing Grid Search>>>>>>>>>>>

learn_rate_list=[0.001,0.01,0.1,0.2,0.3,0.4,0.5]
max_depth_list=[4,6,8,10,12,15,20,25,30]
subsample_list=[0.4,0.6,0.7,0.8,0.9]
max_features_list=['auto','sqrt']

>>>>Sample gradient boost grid search

# Create the function
def gbm_grid_search(learn_rate, max_depth):

	# Create the model
    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth)
    
    # Use the model to make predictions
    predictions = model.fit(X_train, y_train).predict(X_test)
    
    # Return the hyperparameters and score
    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])


# Create the relevant lists
results_list = []
learn_rate_list = [0.01, 0.1, 0.5]
max_depth_list = [2, 4, 6]

# Create the for loop
for learn_rate in learn_rate_list:
    for max_depth in max_depth_list:
        results_list.append(gbm_grid_search(learn_rate,max_depth))

# Print the results
print(results_list) 

output:
[[0.01, 2, 0.78], [0.01, 4, 0.78], [0.01, 6, 0.76], [0.1, 2, 0.74], [0.1, 4, 0.76], [0.1, 6, 0.75], [0.5, 2, 0.73], [0.5, 4, 0.74], [0.5, 6, 0.74]]


>>>>sample  add subsample parameter

results_list = []
learn_rate_list = [0.01, 0.1, 0.5]
max_depth_list = [2,4,6]

# Extend the function input
def gbm_grid_search_extended(learn_rate, max_depth, subsample):

	# Extend the model creation section
    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth, subsample=subsample)
    
    predictions = model.fit(X_train, y_train).predict(X_test)
    
    # Extend the return part
    return([learn_rate, max_depth, subsample, accuracy_score(y_test, predictions)])   


results_list = []

# Create the new list to test
subsample_list =  [0.4 , 0.6]

for learn_rate in learn_rate_list:
    for max_depth in max_depth_list:
    
    	# Extend the for loop
        for subsample in subsample_list:
        	
            # Extend the results to include the new hyperparameter
            results_list.append(gbm_grid_search_extended(learn_rate, max_depth, subsample))
            
# Print results
print(results_list)     

output:
[[0.01, 2, 0.4, 0.73], [0.01, 2, 0.6, 0.74], [0.01, 4, 0.4, 0.73], [0.01, 4, 0.6, 0.75], [0.01, 6, 0.4, 0.72], [0.01, 6, 0.6, 0.78], [0.1, 2, 0.4, 0.74], [0.1, 2, 0.6, 0.74], [0.1, 4, 0.4, 0.73], [0.1, 4, 0.6, 0.73], [0.1, 6, 0.4, 0.74], [0.1, 6, 0.6, 0.76], [0.5, 2, 0.4, 0.64], [0.5, 2, 0.6, 0.67], [0.5, 4, 0.4, 0.72], [0.5, 4, 0.6, 0.71], [0.5, 6, 0.4, 0.63], [0.5, 6, 0.6, 0.64]]


>>>>>>>>>>>>>Grid Search>>>>>>>>>>>>>>

Steps
1. select an estimator
2. select parameters to tune
3. define parameter ranges
4. setup a scoring scheme to see which model is best

GridSearchCV
inputs:
1. estimator
2. param_grid (dictionary of lists)
3. cv
4. scoring
5. refit
6. n_jobs
7. return_train_score


from sklearn import metrics
print(sorted(metrics.SCORERS.keys()))

['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted', 'max_error', 'mutual_info_score', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_gamma_deviance', 'neg_mean_poisson_deviance', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'roc_auc_ovo', 'roc_auc_ovo_weighted', 'roc_auc_ovr', 'roc_auc_ovr_weighted', 'v_measure_score']


refit=true  (the gridsearchcv uses the best model for the fit so you don't need to train another model with the parameters)

import os
print(os.cpu_count())

4 cores

return_train_score

1. logs statistics about the training runs

parameter_grid={'max_depth':[2,4,6,8],'min_samples_leaf':[1,2,4,6]}

rf_class= RandomForestClassifier(criterion='entropy', max_features='auto')


grid_rf_class=GridSearchCV(
    estimator=rf_clf,
    param_grid=parameter_grid,
    scoring='accuracy',
    n_jobs=4,
    cv=10,
    refit=True,
    return_train_score=True)


>>>>>>sample GridSearchCV

# Create a Random Forest Classifier with specified criterion
rf_class = RandomForestClassifier(criterion='entropy')

# Create the parameter grid
param_grid = {'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']} 

# Create a GridSearchCV object
grid_rf_class = GridSearchCV(
    estimator=rf_class,
    param_grid=param_grid,
    scoring='roc_auc',
    n_jobs=4,
    cv=5,
    refit=True, return_train_score=True)
print(grid_rf_class)


>>>>>>>>>>>Understanding grid search output

cv_results_
best_index_
best_params_
best_score_

Extra information
scorer_
n_splits_
refit_time_

cv_results_df=pd.DataFrame(grid_df_class.cv_results_)

pd.set_option("display.max_colwidth",-1)


['mean_fit_time', 
'std_fit_time', 
'mean_score_time', 
'std_score_time',
'param_max_depth', 
'param_min_samples_leaf', 
'params',
'split0_test_score', 
'split1_test_score', 
'split2_test_score',
'split3_test_score', 
'split4_test_score', 
'mean_test_score',
'std_test_score', 
'rank_test_score', 
'split0_train_score',
'split1_train_score', 
'split2_train_score', 
'split3_train_score',
'split4_train_score', 
'mean_train_score', 
'std_train_score']


print(cv_results_df.loc[:,"params"])

cv_results_df=pd.DataFrame(grid_rf_class.cv_results_)

best_row=cv_results_df[cv_results_df["rank_test_score"]==1]

print(best_row)


the test_score columns are repeated for the training_scores


print(type(grid_rf_class.best_estimator_))

print(grid_rf_class.best_estimator_)


>>>>>>sample   >>>> best row

# Read the cv_results property into a dataframe & print it out
cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)
print(cv_results_df)

# Extract and print the column with a dictionary of hyperparameters used
column = cv_results_df.loc[:, ['params']]
print(column)

# Extract and print the row that had the best mean test score
best_row = cv_results_df[cv_results_df['rank_test_score'] == 1 ]
print(best_row)


best_row=cv_results_df[cv_results_df["rank_test_score"]==1]

print(best_row.columns)
print(best_row[['param_max_depth', 'param_min_samples_leaf']])

or 

best_row = cv_results_df.loc[[grid_rf_class.best_index_]]
print(best_row[['param_max_depth', 'param_min_samples_leaf']])


# Print out the ROC_AUC score from the best-performing square
best_score = grid_rf_class.best_score_
print(best_score)

# Create a variable from the row related to the best-performing square
cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)
best_row = cv_results_df.loc[[grid_rf_class.best_index_]]
print(best_row)

# Get the n_estimators parameter from the best-performing square and print
best_n_estimators = grid_rf_class.best_params_["n_estimators"]
print(best_n_estimators)


>>>>>>> sample  >>> best_estimator_

# See what type of object the best_estimator_ property is
print(type(grid_rf_class.best_estimator_))

# Create an array of predictions directly using the best_estimator_ property
predictions = grid_rf_class.best_estimator_.predict(X_test)

# Take a look to confirm it worked, this should be an array of 1's and 0's
print(predictions[0:5])

# Now create a confusion matrix 
print("Confusion Matrix \n", confusion_matrix(y_test, predictions))

# Get the ROC-AUC score
predictions_proba = grid_rf_class.best_estimator_.predict_proba(X_test)[:,1]
print("ROC-AUC Score \n", roc_auc_score(y_test, predictions_proba))


>>>>>>>>>>>>>>>>>>>>Random Search >>>>>>>>>>>>>>

not every hyperparameter is as important
probability


100 squares
we have 5 squares with the best performance
then hitting a best square is 5 out 100
or 0.05

We have a 1-0.05^n chance to missing everything

a hit would be (1-(1-0.05)**n)

1-(1-0.05)**n)>0.95 gives n>=59


how to create a random sample grid manually

learn_rate_list=np.linspace(0.001,2,150)
min_samples_left_list=list(range(1,51))

#builds a cross product between the learning rate and the min sample leaf values

for itertools import product
combination_list=[list(x) for x in
	product(learn_rate_list, min_sample_leaf_list)]

#select 100 models from our larger set

random_combination_index= np.random.choice(
	range(0,len(combination_list)),100,
	replace=False)

combination_random_chosen=[combinations_list[x] for x in random_combinations_index]


>>>>>sample >>> how to random select different learning rate and min sample combinations

# Create a list of values for the learning_rate hyperparameter
learn_rate_list = list(np.linspace(0.01,1.5,200))

# Create a list of values for the min_samples_leaf hyperparameter
min_samples_list = list(range(10,41))

# Combination list
combinations_list = [list(x) for x in product(learn_rate_list, min_samples_list)]

# Sample hyperparameter combinations for a random search.
random_combinations_index = np.random.choice(range(0, len(combinations_list)), 250, replace=False)
combinations_random_chosen = [combinations_list[x] for x in random_combinations_index]

# Print the result
print(combinations_random_chosen)


>>>>>>sample   >>> manually create a random combination choice between three features


# Create lists for criterion and max_features
criterion_list = ['gini','entropy']
max_feature_list = ['auto','sqrt','Log2',None]

# Create a list of values for the max_depth hyperparameter
max_depth_list = list(range(3,56))

# Combination list
combinations_list = [list(x) for x in product(criterion_list,max_feature_list,max_depth_list)]

# Sample hyperparameter combinations for a random search
combinations_random_chosen = random.sample(combination_list, 150)

# Print the result
print(combinations_random_chosen)


>>>>>>>>sample

# Confirm how many hyperparameter combinations & print
number_combs = len(combinations_list)
print(number_combs)

# Sample and visualise specified combinations
for x in [50, 500, 1500]:
    sample_and_visualize_hyperparameters(x)
    
# Sample all the hyperparameter combinations & visualise
sample_and_visualize_hyperparameters(number_combs)


>>>>>>>>>>>>>>>>>>>Random Search CV


RandonSearchCV
inputs:
1. estimator
2. param_distributions (how to sample)
3. fit_params
4. cv
5. scoring
6. n_jobs
7. return_train_score
8. pre_dispatch
9. verbose
10. random_state
11. n_iter (number of samples for the random search to take from your grid)


learn_rate_list=np.linspace(0.001,2,150)
min_samples_leaf_list=list(range(1,51))

parameter_grid={
	'learning_rate':learn_rate_list,
	'min_samples_leaf': min_samples_leaf_list
}

number_models=10
random_GBM_class=RandomizedSearchCV(
	estimator=GradientBoostingClassifier(),
	param_distributions=parameter_grid,
	n_iter=number_models,
	scoring='accuracy',
	n_jobs=4,
	cv=10,
	refit=True,
	return_train_score=True)

random_GBM_class.fit(X_train,y_train)
predictions=random_GBM_class.predict(X_test)

print(accuracy_score(y_test,predictions));
print(grid_rf_class.best_params_)
print(grid_rf_class.best_score_)

rand_x = list(random_GBM_class.cv_results_['param_learning_rate'])
rand_y = list(random_GBM_class.cv_results_['param_min_samples_leaf'])

x_lims=[np.min(learn_rate_list), np.max(learn_rate_list)]
y_lims=[np.min(min_samples_leaf_list), np.max(min_samples_leaf_list)]

plt.scatter(rand_y,rand_x,c=['blue']*10)
plt.gca().set(xlabel='learn_rate', ylabel='min_sample_leaf', title='Random Search Hyperparameters')
plt.show()


>>>>> sample   >> randomSearchCV()  >>gradientboostclassifier


# Create the parameter grid
param_grid = {'learning_rate': np.linspace(.1,2,150), 'min_samples_leaf': list(range(20,65))} 

# Create a random search object
random_GBM_class = RandomizedSearchCV(
    estimator = GradientBoostingClassifier(),
    param_distributions= param_grid,
    n_iter = 10,
    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)

# Fit to the training data
random_GBM_class.fit(X_train, y_train)

# Print the values used for both hyperparameters
print(random_GBM_class.cv_results_['param_learning_rate'])
print(random_GBM_class.cv_results_['param_min_samples_leaf'])


>>>>>> sample random SEarch CV >> randomforestclassifier 

# Create the parameter grid
param_grid = {'max_depth': list(range(5,26)), 'max_features': ['auto' , 'sqrt']} 

# Create a random search object
random_rf_class = RandomizedSearchCV(
    estimator = RandomForestClassifier (n_estimators=80),
    param_distributions = param_grid, n_iter = 5, return_train_score = True )

# Fit to the training data
random_rf_class.fit(X_train, y_train)

# Print the values used for both hyperparameters
print(random_rf_class.cv_results_['param_max_depth'])
print(random_rf_class.cv_results_['param_max_features'])

>>>>> sample >>> randomSearchCV() >>  randomForestClassifier

# Create the parameter grid
param_grid = {'max_depth': list(range(5,26)), 'max_features': ['auto' , 'sqrt']} 

# Create a random search object
random_rf_class = RandomizedSearchCV(
    estimator = RandomForestClassifier(n_estimators=80),
    param_distributions = param_grid, n_iter = 5,
    scoring='roc_auc', n_jobs=4, cv = 3, refit=True, return_train_score = True)

# Fit to the training data
random_rf_class.fit(X_train, y_train)

# Print the values used for both hyperparameters
print(random_rf_class.cv_results_['param_max_depth'])
print(random_rf_class.cv_results_['param_max_features'])


>>>>>>>>>>>>>>>>>Comparing Grid and Random search

>>> sample


# Sample grid coordinates
grid_combinations_chosen = combinations_list[0:300]

# Create a list of sample indexes
sample_indexes = list(range(0,len(combinations_list)))

# Randomly sample 300 indexes
random_indexes = np.random.choice(sample_indexes, 300, replace=False)

# Use indexes to create random sample
random_combinations_chosen = [combinations_list[index] for index in random_indexes ]

# Call the function to produce the visualization
visualize_search(grid_combinations_chosen, random_combinations_chosen)


>>>>>>>>>Informed Search >> Coarse to fine

Learn while the hyperparameter is searching
One model -> Measure -> Learn

Process:
1. Start with a random search
2. Find Promising areas
3. Undertake a grid search in the smaller area
4. Continue the process until an optimal score is obtained or the area has become to small


max_depth_list between 1 and 65
min_sample_list between 3 and 17
learn_rate_list 150 value between 0.01 and 150

combinations_list=[list(x) from x in product(max_depth_list, min_sample_list, learn_rate_list)]

print(len(combinations_list))

output: 134400

max_depth better between 8 and 30
min_samples_leaf better below 8
visualize coarse to fine using max_depth and accuracy scoring


others are learn rate (worst above 1.3


>>>sample  >>> max_depth, min_samples_leaf, learn rate

1000 samples

# Confirm the size of the combinations_list
print(len(combinations_list))

print(results_df.columns)
# Sort the results_df by accuracy and print the top 10 rows
print(results_df.sort_values(by='accuracy', ascending=False).head(10))

# Confirm which hyperparameters were used in this search
print(results_df.columns)

# Call visualize_hyperparameter() with each hyperparameter in turn
visualize_hyperparameter('max_depth')
visualize_hyperparameter('min_samples_leaf')
visualize_hyperparameter('learn_rate')

>>>>>>sample  >>>> max depth and learn rate

# Create some combinations lists & combine
max_depth_list = list(range(1, 20))
learn_rate_list = np.linspace(0.001, 1, 50)

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Bayesian Statistics

bayes rule:

1. iteratively update our beliefs about some outcome


P(A|B) = P(B|A)P(A)/P(B)

LHS= the probability of A, given B has occurred.  B is some evidence.

P(A) is the prior. The initial hypothesis about the event.  It is different to P(A|B), the P(A|B) is the probability given new evidence.

P(B) is the marginal likelihood and it is the probability of observing this new evidence.


P(B|A) is the likelihood which the probability of observing the evidence, given the event we care about


5% of people in the general population have a certain disease P(D)

10% of people are predisposed

>>>>>>>>>>>>>Hyperopt

process:
1. set the domain of our grid
2. set the optimization algorithm
3. objective function to minimize


space={
	'max_depth':hp.quniform('max_depth,2,10,2),
	'min_samples_lear': hp.quniform('min_samples_leaf',2,8,2),
	'learning_rate': hp.uniform('learning_rate',0.01,1,55)
}

quniform means quantified or percentiled by the 3rd number

def objective(params):
	params={'max_depth': int(params['max_depth']),
	'min_samples_leaf': int(params['min_samples_leaf']),
	'learning_rate': params['learning_rate']}
	gbm_clf=GradientBoostingClassifier(n_estimators=500,**params)
	best_score=cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=10, n_jobs=2).mean()

loss=1-best_score
return loss


>>>>>>>>>>>>>>>>sample closing out accounts

7% (0.07) of people are likely to close their account next month
15% (0.15) of people with accounts are unhappy with your product (you don't know who though!)
35% (0.35) of people who are likely to close their account are unhappy with your product

# Assign probabilities to variables 
p_unhappy = 0.15
p_unhappy_close = 0.35

# Probabiliy someone will close
p_close = 0.07

# Probability unhappy person will close
p_close_unhappy = (p_unhappy_close * p_close) / p_unhappy

print(p_close_unhappy)

output:
0.16333333333333336


There's a 16.3% chance that a customer, given that they are unhappy, will close their account.


>>>>>>sample >>>> hyper opt

# Set up space dictionary with specified hyperparameters
space = {'max_depth': hp.quniform('max_depth',2,10,2),'learning_rate': hp.uniform('learning_rate',0.001,0.9)}

# Set up objective function
def objective(params):
    params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}
    gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) 
    best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()
    loss = 1 - best_score
    return loss

# Run the algorithm
best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)
print(best)

output:

0%|          | 0/20 [00:00<?, ?it/s, best loss: ?]
  5%|5         | 1/20 [00:00<00:04,  4.68it/s, best loss: 0.26759418985474637]
 10%|#         | 2/20 [00:00<00:03,  4.63it/s, best loss: 0.2549063726593165] 
 15%|#5        | 3/20 [00:00<00:03,  4.83it/s, best loss: 0.2549063726593165]
 20%|##        | 4/20 [00:00<00:03,  5.07it/s, best loss: 0.2549063726593165]
 25%|##5       | 5/20 [00:01<00:03,  3.84it/s, best loss: 0.2549063726593165]
 30%|###       | 6/20 [00:01<00:03,  3.96it/s, best loss: 0.2549063726593165]
 35%|###5      | 7/20 [00:01<00:02,  4.39it/s, best loss: 0.2549063726593165]
 40%|####      | 8/20 [00:01<00:02,  4.62it/s, best loss: 0.2549063726593165]
 45%|####5     | 9/20 [00:01<00:02,  4.91it/s, best loss: 0.2549063726593165]
 50%|#####     | 10/20 [00:02<00:01,  5.20it/s, best loss: 0.2549063726593165]
 55%|#####5    | 11/20 [00:02<00:01,  5.42it/s, best loss: 0.2549063726593165]
 60%|######    | 12/20 [00:02<00:01,  5.04it/s, best loss: 0.2549063726593165]
 65%|######5   | 13/20 [00:02<00:01,  4.40it/s, best loss: 0.2549063726593165]
 70%|#######   | 14/20 [00:03<00:02,  2.78it/s, best loss: 0.2525688142203555]
 75%|#######5  | 15/20 [00:03<00:01,  3.30it/s, best loss: 0.2525688142203555]
 80%|########  | 16/20 [00:03<00:01,  3.63it/s, best loss: 0.2525688142203555]
 85%|########5 | 17/20 [00:04<00:01,  2.72it/s, best loss: 0.24246856171404285]
 90%|######### | 18/20 [00:04<00:00,  3.18it/s, best loss: 0.24246856171404285]
 95%|#########5| 19/20 [00:04<00:00,  3.57it/s, best loss: 0.24246856171404285]
100%|##########| 20/20 [00:05<00:00,  3.87it/s, best loss: 0.24246856171404285]
100%|##########| 20/20 [00:05<00:00,  3.95it/s, best loss: 0.24246856171404285]
{'learning_rate': 0.11310589268581149, 'max_depth': 6.0}


>>>>>>>>Informed Search Genetic algorithms

genetics:
1. surviving creatures exist
2. strong creatures survive
3. crossover occurrs with offspring
4. there are random mutations

hypertuning:
1. create some models
2. pick the best by scoring function
a. these are the ones to survive
3. create new models 
4. has some advantage of randomness

>>>>>>>>>>>>>>>>>TPOT

TPOT is a python automated machine learning tool.


generations: iterations to run training
population_size: the number of models to keep after each iteration
offspring_size: number of models to produce in each iteration
mutation_rate: the proportion of pipelines to apply randomness
crossover_rate: the proportion of pipelines to breed each iteration
scoring: the function to determine the best models
cv: cross-validation strategy to use

from tpot import TPOTClassifier

tpot=TPOTClassifier(generations=3, population_size=5, verbosity=2, offspring_size=10, scoring='accuracy', cv=5)

tpot.fit(X_train,y_train)

print(tpot.score(X_test,y_test))


>>>>>>>>>>>>sample >>>> tpotclassifier

# Assign the values outlined to the inputs
number_generations = 3
population_size = 4
offspring_size = 3
scoring_function = 'accuracy'

# Create the tpot classifier
tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size,
                          offspring_size=offspring_size, scoring=scoring_function,
                          verbosity=2, random_state=2, cv=2)

# Fit the classifier to the training data
tpot_clf.fit(X_train,y_train)

# Score on the test set
print(tpot_clf.score(X_test, y_test))

output:
Best pipeline: BernoulliNB(input_matrix, alpha=0.1, fit_prior=True)
0.76

Nice work! You can see in the output the score produced by the chosen model (in this case a version of Naive Bayes) over each generation, and then the final accuracy score with the hyperparameters chosen for the final model. This is a great first example of using TPOT for automated hyperparameter tuning. You can now extend on this on your own and build great machine learning models!

>>>>>sample  decisiontreeclassifier

# Create the tpot classifier 
tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,
                          verbosity=2, random_state=42)

# Fit the classifier to the training data
tpot_clf.fit(X_train, y_train)

# Score on the test set
print(tpot_clf.score(X_test, y_test))

Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.
Generation 1 - Current best internal CV score: 0.7675066876671917
Generation 2 - Current best internal CV score: 0.7675066876671917

Best pipeline: KNeighborsClassifier(MaxAbsScaler(input_matrix), n_neighbors=57, p=1, weights=distance)
0.75


Randomstate=122


Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.
Generation 1 - Current best internal CV score: 0.8075326883172079
Generation 2 - Current best internal CV score: 0.8075326883172079

random_state=99

Best pipeline: RandomForestClassifier(SelectFwe(input_matrix, alpha=0.033), bootstrap=False, criterion=gini, max_features=1.0, min_samples_leaf=19, min_samples_split=10, n_estimators=100)
0.78


You can see that TPOT is quite unstable when only running with low generations, population size and offspring. The first model chosen was a Decision Tree, then a K-nearest Neighbor model and finally a Random Forest. Increasing the generations, population size and offspring and running this for a long time will assist to produce better models and more stable results. Don't hesitate to try it yourself on your own machine!


Best pipeline: MLPClassifier(PCA(LogisticRegression(SelectPercentile(input_matrix, percentile=37), C=20.0, dual=False, penalty=l2), iterated_power=1, svd_solver=randomized), alpha=0.1, learning_rate_init=0.001)
0.8207777777777778