>>>>>>>>>>>XG boost for regression
1. predicting contineous or real values

Root mean squared error (RMSE)
Mean absolute error (MAE)

RMSE is 
error=actual - predicted
squaredError=error**2
Totaling the squared error
Mean of the squared error
square root the mean of the squared error

punishes larger errors more

MAE

error=actual - predicted
total the absolute error
Mean of the total absolute error

less frequently used


>>>>Objective (loss) functions and base learners

objective function (loss) quantifies how far off a prediction is from the actual results

Measures the difference between the prediction and the target for some collection of data

Goal: Find the model that yields the minimum value of the loss function.

Loss Function names
	reg:linear - use for regression problems

	reg:logistic - use for classification problems when you want decision not probability.
	
	reg:logistic - when you want the probability rather than just the decision


XGBoost model is composed of many individual models that combine to give a final prediction

each of the individual models is combined the base learners that is slight better than random guessing.  The base learners that are better than 50% are combined into a single prediction.

the base learners are non-linear

there are two kinds of base learners: tree and linear

import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

https://github.com/ageron/handson-ml/blob/master/datasets/housing/housing.csv

https://github.com/girishkuniyal/Predict-housing-prices-in-Portland

https://www.datasethub.com/datasets/idaho/filetypes/csv

https://people.ischool.berkeley.edu/~chandangope/project/


X_train, X_test, y_train, x_test= train_test_split(X,y , test_size=0.2, random_state=42)


xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)

xg_reg.fit(X_train, y_train)
 

preds= xg_reg.predict(X_test)

rmse=np.sqrt(mean_squared_error(y_test,y_preds))

print(RMSE: %f" %(rmse))

>>>>>Linear base learners
- a linear learner
- allows you to create a regularized linear regression using XGBoost's powerful learning API

DM_train = xgb.DMatrix(data=X_train, label=y_train)
DM_test = xgb.DMatrix(data=X_test,label=y_test)

#params={"booster":"gblinear","objective":"reg:linear"}
params={"booster":"gblinear","objective":"reg:squarederror"}

xg_reg=xgb.train(params=params, dtrain=DM_train, num_boost_round=10)

pred=xg_reg.predict(DM_test)

rmse=np.sqrt(mean_squared_error(y_test,y_pred))

print("RMSE: %f" %(rmse))


>>>>Sample XGboose (reg:linear)

# Create the training and test sets
X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)

# Instantiate the XGBRegressor: xg_reg
xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)

# Fit the regressor to the training set

xg_reg.fit(X_train, y_train)

# Predict the labels of the test set: preds
preds= xg_reg.predict(X_test)

# Compute the rmse: rmse
rmse=np.sqrt(mean_squared_error(y_test,preds))
print("RMSE: %f" % (rmse))

>>>Sample XGBoost (trees)

# Convert the training and testing sets into DMatrixes: DM_train, DM_test

DM_train = xgb.DMatrix(data=X_train, label=y_train)
DM_test = xgb.DMatrix(data=X_test,label=y_test)

# Create the parameter dictionary: params
params={"booster":"gblinear","objective":"reg:squarederror"}

# Train the model: xg_reg
xg_reg=xgb.train(params=params, dtrain=DM_train, num_boost_round=5)

# Predict the labels of the test set: preds
preds=xg_reg.predict(DM_test)

# Compute and print the RMSE
rmse = np.sqrt(mean_squared_error(y_test,preds))
print("RMSE: %f" % (rmse))

>>>Cross validation (rmse)

# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":4}

# Perform cross-validation: cv_results
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics="rmse", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)

# Extract and print final boosting round metric
print(cv_results["test-rmse-mean"].tail(1))


>>>Cross validation (mae)

# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":4}

# Perform cross-validation: cv_results
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics="mae", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)

# Extract and print final boosting round metric
print(cv_results["test-mae-mean"].tail(1))


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>regularization

regularization penalizes a model as it get more complex
want models that are accurate and as simple as possible

regularization parameters in xgboost
1. gamma - minimum loss reduction allowed for split to occur
2. alpha - l1 regularization on leaf weights, large value means more regularization
a. causes many leafs in the base learners
3. lambda is another name of l2 regularization on leaf weights
a. smoother penalty on large numbers and decrease gradually

boston_dmatrix = xgb.DMatrix(data=X,label=y)
params={"objective":"reg:linear","max_depth":4}

l1_params=[1,10,100]
rmses_l1=[]

for reg in l1_params:
	params['alpha']=reg
	cv_results= xgb.cv(dtrain=boston_dmatrix, params=params, nfold=4,
	num_boost_round=10, metrics="rmse", as_pandas=True, seed=123)
	rmse_l1.append(cv_results["test-rmse-mean"].tail(1).values[0])

num_boost_round is the number of trees

print("best rmse as a function of l1:")

print(pd.DataFrame(list(zip(l1_params,rmse_l1)), columns("l1","rmse"]))


>>>>>>>>>>>>>>>Base learners in XGBoost

1. Linear base learner
a. Sum of linear terms
b. The boosted model is a weighted sum of linear models which is itself linear

2. The tree based learner
a. decision trees as base models
b. The boosted model is the weighted sum of decision trees (non linear)
c. almost exclusively used in XGBoost


zip creates a generator of parallel values

zip([1,2,3],["a","b","c"])
output: [1,"a"],[2,"b"],[3,"c"]

list() instantiates the full generator and passing that into the dataframe which converts the whole expression.

>>>Sample  to_dict

keys = ['name', 'age', 'food']
values = ['Monty', 42, 'spam']
index=np.arange(0,len(keys)-1)

df=pd.DataFrame(list(zip(keys,values)), columns=['keys','values'])
df.set_index('keys')
print(df.head())

data_dict = df.iloc[index].set_index('keys')['values'].to_dict() 
print(data_dict)

>>>Sample L2 regularization
l2 regularization penalty - also known as "lambda" - and see its effect on overall model performance on the Ames housing dataset.

# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

reg_params = [1, 10, 100]

# Create the initial parameter dictionary for varying l2 strength: params
params = {"objective":"reg:linear","max_depth":3}

# Create an empty list for storing rmses as a function of l2 complexity
rmses_l2 = []

# Iterate over reg_params
for reg in reg_params:

    # Update l2 strength
    params["lambda"] = reg
    
    # Pass this updated param dictionary into cv
    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics="rmse", as_pandas=True, seed=123)
    
    # Append best rmse (final round) to rmses_l2
    rmses_l2.append(cv_results_rmse["test-rmse-mean"].tail(1).values[0])

# Look at best rmse per l2 param
print("Best rmse as a function of l2:")
print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=["l2","rmse"]))

>>>Sample  Plot the Trees
Have a look at each of the plots. They provide insight into how the model arrived at its final decisions and what splits it made to arrive at those decisions.

# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":2}

# Train the model: xg_reg
xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)

# Plot the first tree
xgb.plot_tree(xg_reg,num_trees=0)
plt.show()

# Plot the fifth tree
xgb.plot_tree(xg_reg,num_trees=4)
plt.show()

# Plot the last tree sideways
xgb.plot_tree(xg_reg,num_trees=9, rankdir='LR')
plt.show()

>>>>Sample Plot the importance features

# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":4}

# Train the model: xg_reg
xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)

# Plot the feature importances
xgb.plot_importance(xg_reg)
plt.show()

>>>>>Why tuning



housing_dmatrix = xgb.DMatrix(data=X,label=y)
untunedParams={"objective":"reg:linear"}

untuned_cv_results=xgb.cv(dtrain=housing_dmatrix, params=untuned_params, nfold=4, metrics="rmse", as_pandas=True, seed=123)

print("Untuned rmse: %f" %((untuned_cv_results["test-mae-mean"]).tail(1)))

    
>>>>A more turn parameter resultset

tunedParams={"objective":"reg:linear","colsample_bytree":0.3, "learning_rate":0.1, "max_depth":5}

tuned_cv_results=xgb.cv(dtrain=housing_dmatrix, params=tuned_params, nfold=4, num_boost_round=200, metrics="rmse", as_pandas=True, seed=123)

print("tuned rmse: %f" %((tuned_cv_results["test-mae-mean"]).tail(1)))


>>>>Sample tuning with number of trees

# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary for each tree: params 
params = {"objective":"reg:linear",'max_depth':3}


# Create list of number of boosting rounds
num_rounds = [5, 10, 15]

# Empty list to store final round rmse per XGBoost model
final_rmse_per_round = []

# Iterate over num_rounds and build one model per num_boost_round parameter
for curr_num_rounds in num_rounds:

    # Perform cross-validation: cv_results
    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics="rmse", as_pandas=True, seed=123)
    
    # Append final round RMSE
    final_rmse_per_round.append(cv_results["test-rmse-mean"].tail().values[-1])

# Print the resultant DataFrame
num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))
print(pd.DataFrame(num_rounds_rmses,columns=["num_boosting_rounds","rmse"]))

>>>>Sample  early stop rounds


# Create your housing DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary for each tree: params
params = {"objective":"reg:linear", "max_depth":4}

# Perform cross-validation with early stopping: cv_results
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,early_stopping_rounds=10, num_boost_round=50, metrics="rmse", as_pandas=True, seed=123)

# Print cv_results
print("rmse: %f" %((cv_results["test-rmse-mean"]).tail(1)))

>>>>>>>>>>>>>>>>>>>>>>>Tree Tunable parameters

*learning rate: low learning rate will take more trees to reduce residual error (boosting rounds)
* gamma: min loss reduction to create new tree split (regularized)
* lambda: l2 reg on leaf weights (regularized)
* max_depth : max depth per tree
* subsample: % samples used per tree (low value - underfitting and high value - overfitting)
* colsample_bytree: %features used per tree (features 0 to 1) a small colsample_bytree value means that additional regularization is being added to the model.

>>>>>>>>>>>>>>>>>>>>>>>Linear base learner parameters
* lambda: L2 reg on weights
* alpha: L1 reg on weights
* lambda_bias: L2 reg term on bias

You can also tune the number of estimators used for both base model types!


>>>>Sample Tuning the learning rate (eta)

# Create your housing DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary for each tree (boosting round)
params = {"objective":"reg:linear", "max_depth":3}

# Create list of eta values and empty list to store final round rmse per xgboost model
eta_vals = [0.001,0.01,0.1]
best_rmse = []

# Systematically vary the eta 
for curr_val in eta_vals:

    params["eta"] = curr_val
    
    # Perform cross-validation: cv_results
    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,early_stopping_rounds=5, num_boost_round=10, metrics="rmse", as_pandas=True, seed=123)
    
    
    
    # Append the final round rmse to best_rmse
    best_rmse.append(cv_results["test-rmse-mean"].tail().values[-1])

# Print the resultant DataFrame
print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=["eta","best_rmse"]))

>>>>>>Sample tuning (max depths)

# Create your housing DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary
params = {"objective":"reg:linear"}

# Create list of max_depth values
max_depths = [2, 5, 10, 20]
best_rmse = []

# Systematically vary the max_depth
for curr_val in max_depths:

    params["max_depth"] = curr_val
    
    # Perform cross-validation
    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,
                 num_boost_round=10, early_stopping_rounds=5,
                 metrics="rmse", as_pandas=True, seed=123)
    
    # Append the final round rmse to best_rmse
    best_rmse.append(cv_results["test-rmse-mean"].tail().values[-1])

# Print the resultant DataFrame
print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=["max_depth","best_rmse"]))

>>>>Sample colsample_bytree_vals

# Create your housing DMatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary
params={"objective":"reg:linear","max_depth":3}

# Create list of hyperparameter values: colsample_bytree_vals
colsample_bytree_vals = [0.1, 0.5, 0.8, 1]
best_rmse = []

# Systematically vary the hyperparameter value 
for curr_val in colsample_bytree_vals:

    params["colsample_bytree"] = curr_val
    
    # Perform cross-validation
    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,
                 num_boost_round=10, early_stopping_rounds=5,
                 metrics="rmse", as_pandas=True, seed=123)
    
    # Append the final round rmse to best_rmse
    best_rmse.append(cv_results["test-rmse-mean"].tail().values[-1])

# Print the resultant DataFrame
print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=["colsample_bytree","best_rmse"]))


>>>>Review of grid search and random search

grid search : exhaustive searches of a given hyperparameter


>>>>>>GridSearchCV

# Import GridSearchCV
from sklearn.model_selection import GridSearchCV

gbm_param_grid = {'learning_rate':[0.01,0.1,0.5,0.9]
	'n_estimators':[200],
	'subsample':[0.3,0.5,0.9]}

gbm=xgb.XGBRegressor()

grid_mse=GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,
scoring='neg_mean_squared_error', cv=4, verbose=1)


grid_mse.fit(X,y)


print("Best parameters found:", grid_mse.best_params_)
print("Lowest RMSE found:, np.sqrt(grid_mse.best_score_)))


>>>>Random search 
* you set the number of iterations you would like to random search to continue
* you create the range of hyperparameters values per hyper parameter - randomly draw value from the range of hyperparameters values

from sklearn.model_selection import RandomizedSearchCV
import random

param_dist = {'learning_rate':np.arange(0.05,1.05,.05), 'n_estimators':[200], 'subsample':np.arange(0.05,1.05,.05),}

gbm=xgb.XGBRegressor()

random_search = RandomizedSearchCV(estimator=gbm,param_distributions=param_dist, n_iter=25,
scoring='neg_mean_squared_error',cv=4,verbose=1)
random_search.fit(X, y)

print("Best Parameters",random_search.best_params_)
print("Lowest RMSE found:",np.sqrt(np.abs(random_search.best_score_)))


>>>>Sample search GridSearchCV

gbm_param_grid = {'colsample_bytree':[0.3,0.7],
	'n_estimators':[50],
	'max_depth':[2,5]}

gbm=xgb.XGBRegressor()

# Perform grid search: grid_mse
grid_mse=GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,
scoring='neg_mean_squared_error', cv=4, verbose=1)
grid_mse.fit(X,y)


# Print the best parameters and lowest RMSE
print("Best parameters found: ", grid_mse.best_params_)
print("Lowest RMSE found: ", np.sqrt(np.abs(grid_mse.best_score_)))


>>>>>Sample RandomSearchCV

# Create the parameter grid: gbm_param_grid 
gbm_param_grid = {
    'n_estimators': [25],
    'max_depth': np.arange(2, 12)
}

# Instantiate the regressor: gbm
gbm = xgb.XGBRegressor(n_estimators=10)

# Perform random search: grid_mse
randomized_mse = RandomizedSearchCV(estimator=gbm,param_distributions=gbm_param_grid, n_iter=5, scoring='neg_mean_squared_error',cv=4,verbose=1)
randomized_mse.fit(X, y)

>>>>>>>>>>>>Review of pipelines using sklearn

takes a list of named 2-tuples (name, pipeline_step) as input

the pipeline can contain estimator or transformer objects

pipeline implement fit/predict models

pipelines can be used as input estimators into grid and randomized search and cross_val_score methods

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

https://github.com/eric-bunch/boston_housing


rf_pipeline = Pipeline[("st_scaler",StandardScaler()),
                       ("rf_model",RandomForestRegressor())]



scores=cross_val_score(rf_pipeline,X,y,
	scoring="neg_mean_squared_error", cv=10)

final_avg_rmse=np.mean(np.sqrt(np.abs(scores)))

print("Final RMSE:", final_avg_rmse)

>>>>>label encoder and onehotencoder

1. LabelEncoder converts a categorical column of strings into integers

2. OneHotEncoder: takes a column of integers and encodes them as dummy variables where each variable is a column

***Can not be done within a pipeline


>>>>>>>>>>>>Preprocessing with DictVectorizer

*Traditionally used in text processing
* converts lists of feature mappings into vectors
* we need to convert a dataframe into a list of dictionary entries

>>>>Sample (LabelEncoder) find the objects

# Import LabelEncoder
from sklearn.preprocessing import LabelEncoder

# Fill missing values with 0
df.LotFrontage = df.LotFrontage.fillna(0)

# Create a boolean mask for categorical columns
categorical_mask = (df.dtypes == object)

# Get list of categorical column names
categorical_columns = df.columns[categorical_mask].tolist()

# Print the head of the categorical columns
print(df[categorical_columns].head())

# Create LabelEncoder object: le
le = LabelEncoder()


>>>>>XGboost pipeline

# Import necessary modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import DictVectorizer

# Fill LotFrontage missing values with 0
X.LotFrontage = X.LotFrontage.fillna(0)


# Setup the pipeline steps: steps
steps = [("ohe_onestep", DictVectorizer(sparse=False)),
         ("xgb_model", xgb.XGBRegressor())]

# Create the pipeline: xgb_pipeline
xgb_pipeline = Pipeline(steps)

# Fit the pipeline
xgb_pipeline.fit(X.to_dict("records"), y)

# Apply LabelEncoder to categorical columns
df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))

# Print the head of the LabelEncoded categorical columns
print(df[categorical_columns].head())

>>>>>Sample (OneHotEncoding)

# Import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder

# Create OneHotEncoder: ohe
ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)

# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded
df_encoded = ohe.fit_transform(df)

# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe
print(df_encoded[:5, :])

# Print the shape of the original DataFrame
print(df.shape)

# Print the shape of the transformed array
print(df_encoded.shape)


>>>>Sample (DictVectorizer) label and encode at the same time

# Import DictVectorizer
from sklearn.feature_extraction import DictVectorizer

# Convert df into a dictionary: df_dict
df_dict = df.to_dict("records")

# Create the DictVectorizer object: dv
dv = DictVectorizer(sparse=False)

# Apply dv on df: df_encoded
df_encoded = dv.fit_transform(df_dict)

>>>Sample (xgboost in the pipeline
# Import necessary modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import DictVectorizer

# Fill LotFrontage missing values with 0
X.LotFrontage = X.LotFrontage.fillna(0)


# Setup the pipeline steps: steps
steps = [("ohe_onestep", DictVectorizer(sparse=False)),
         ("xgb_model", xgb.XGBRegressor())]

# Create the pipeline: xgb_pipeline
xgb_pipeline = Pipeline(steps)

# Fit the pipeline
xgb_pipeline.fit(X.to_dict("records"), y)

>>>>>>>>>>Incorporating xgboost into pipelines

# Import necessary modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import DictVectorizer

# Fill LotFrontage missing values with 0
X.LotFrontage = X.LotFrontage.fillna(0)


# Setup the pipeline steps: steps
steps = [ ("st_scaler", StandardScaler()),
	 ("ohe_onestep", DictVectorizer(sparse=False)),
         ("xgb_model", xgb.XGBRegressor())]

# Create the pipeline: xgb_pipeline
xgb_pipeline = Pipeline(steps)

# Fit the pipeline
xgb_pipeline.fit(X.to_dict("records"), y)


scores=cross_val_score(xg_pipeline,X,y,
	scoring="neg_mean_squared_error", cv=10)

final_avg_rmse=np.mean(np.sqrt(np.abs(scores)))
print("Final XGB RMSE:",final_avg_rmse)

>>>>>>sklearn_pandas
1. DataFrameMapper - Interoperability between pandas and scikit-learn

2. CategoricalImputer - Allow for imputation of categorical variables before conversion to integers

3. sklearn.preprocessing - Imputer - Native imputation of numerical columns in scikit-learn

4. sklearn.pipeline:
	featureUnion - combine multiple pipelines of features into a single pipeline of features


>>>>>Sample Creating the xgboost pipeline

# Import necessary modules
from sklearn.feature_extraction import DictVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

# Fill LotFrontage missing values with 0
X.LotFrontage = X.LotFrontage.fillna(0)

# Setup the pipeline steps: steps
steps = [("ohe_onestep", DictVectorizer(sparse=False)),
         ("xgb_model", xgb.XGBRegressor(max_depth=2, objective="reg:squarederror"))]

# Create the pipeline: xgb_pipeline
xgb_pipeline = Pipeline(steps)

xgb_pipeline.fit(X.to_dict("records"), y)

# Cross-validate the model
cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict("records"), y, cv=10, scoring="neg_mean_squared_error")


# Print the 10-fold RMSE
print("10-fold RMSE: ", np.mean(np.sqrt(np.abs(cross_val_scores))))


>>>>>>>>Sample using categorical imputer

Specifically, you'll be able to impute missing categorical values directly using the Categorical_Imputer() class in sklearn_pandas, and the DataFrameMapper() class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.

We've also created a transformer called a Dictifier that encapsulates converting a DataFrame using .to_dict("records") without you having to do it explicitly (and so that it works in a pipeline). Finally, we've also provided the list of feature names in kidney_feature_names, the target name in kidney_target_name, the features in X, and the target in y

# Import necessary modules
from sklearn_pandas import DataFrameMapper
from sklearn_pandas import CategoricalImputer

# Check number of nulls in each feature column
nulls_per_column = X.isnull().sum()
print(nulls_per_column)

# Create a boolean mask for categorical columns
categorical_feature_mask = X.dtypes == object

# Get list of categorical column names
categorical_columns = X.columns[categorical_feature_mask].tolist()

# Get list of non-categorical column names
non_categorical_columns = X.columns[~categorical_feature_mask].tolist()

# Apply numeric imputer
numeric_imputation_mapper = DataFrameMapper(
                                            [([numeric_feature], Imputer(strategy="median")) for numeric_feature in non_categorical_columns],
                                            input_df=True,
                                            df_out=True
                                           )

# Apply categorical imputer
categorical_imputation_mapper = DataFrameMapper(
                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],
                                                input_df=True,
                                                df_out=True
                                               )


>>>>>>Sample FeatureUnion

# Import FeatureUnion
from sklearn.pipeline import FeatureUnion

# Combine the numeric and categorical transformations
numeric_categorical_union = FeatureUnion([
                                          ("num_mapper", numeric_imputation_mapper),
                                          ("cat_mapper", categorical_imputation_mapper)
                                         ])

>>>>>>Sample Full Pipeline

# Create full pipeline
pipeline = Pipeline([
                     ("featureunion", numeric_categorical_union),
                     ("dictifier", Dictifier()),
                     ("vectorizer", DictVectorizer(sort=False)),
                     ("clf", xgb.XGBClassifier(max_depth=3))
                    ])

# Perform cross-validation
cross_val_scores = cross_val_score(pipeline, kidney_data, y, cv=3, scoring="roc_auc")

# Print avg. AUC
print("3-fold AUC: ", np.mean(cross_val_scores))


>>>>>>>Tuning hyperparameter in a pipeline

xbg_pipeline = Pipeline[('st_scaler',StandardScaler()),
		('xgb_model',xgb.XGBRegressor())]


gbm_param_grid={
	'xgb_model__subsample': np.arange(.05,1,.05),
	'xgb_model__max_depth': np.arange(3,20,1),
	'xgb_model__colsample_bytree': np.arange(.1,1.05,.05)}


randomized_neg_mse= RandomizedSearchCV(estimator=xgb_pipeline,
	param_distributions=gbm_param_grid, n_iter=10,
	scoring='neg_mean_squared_error', cv=4)



randomized_neg_mse.fit(X, y)

print("Best rmse: ", np.sqrt(np.abs(randomized_neg_mse.best_score_)))

>>>>Sample RandomSearchCV the pipeline

# Create the parameter grid
gbm_param_grid = {
    'clf__learning_rate': np.arange(.05, 1, .05),
    'clf__max_depth': np.arange(3,10, 1),
    'clf__n_estimators': np.arange(50, 200, 50)
}

# Perform RandomizedSearchCV
randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,
                                        param_distributions=gbm_param_grid,
                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)

# Fit the estimator
randomized_roc_auc.fit(X, y)

# Compute metrics
print("Best rmse: ", randomized_roc_auc.best_score_)
print(randomized_roc_auc.best_estimator_)


example of a pipeline
https://stackoverflow.com/questions/52055658/sklearn-pandas-in-a-pipeline-returns-typeerror-builtin-function-or-method-obj

https://dunyaoguz.github.io/my-blog/dataframemapper.html


Installation
pip install https://github.com/scikit-learn/scikit-learn/archive/master.zip





