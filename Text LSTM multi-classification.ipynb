{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense,SpatialDropout1D\n",
    "from tensorflow.keras.layers import LSTM, Embedding\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.feature_selection import chi2, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv('https://raw.githubusercontent.com/kushalchauhan98/bcn-cnn-text-classification/master/hm_train.csv')\n",
    "#df.to_csv('hm_train.csv')\n",
    "df=pd.read_csv('hm_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter=df['cleaned_hm'].str.len()<=100\n",
    "df=df[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        I went on a successful date with someone I fel...\n",
      "1        I was happy when my son got 90% marks in his e...\n",
      "2             I went to the gym this morning and did yoga.\n",
      "4        I went with grandchildren to butterfly display...\n",
      "5                                  I meditated last night.\n",
      "                               ...                        \n",
      "60315              when i got a high level on a video game\n",
      "60316    I got together with my best friend and baked c...\n",
      "60317                  I went to a restaurant with friends\n",
      "60318    The other day on Mechanical Turk I made over f...\n",
      "60319    Finished the semester today and aced majority ...\n",
      "Name: cleaned_hm, Length: 43354, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"cleaned_hm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (43354, 7)\n",
      "[[0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n",
      "['achievement', 'affection', 'bonding', 'enjoy_the_moment', 'exercise', 'leisure', 'nature']\n",
      "Shape of data tensor: (43354, 250)\n"
     ]
    }
   ],
   "source": [
    "#rint(df)\n",
    "y=df['predicted_category']\n",
    "\n",
    "y = pd.get_dummies(df['predicted_category']).values\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "LABELS=sorted(df['predicted_category'].unique())\n",
    "\n",
    "print(y)\n",
    "print(LABELS)\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['cleaned_hm'].values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['cleaned_hm'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.10, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(LABELS), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "106/549 [====>.........................] - ETA: 5:13 - loss: 1.4370 - accuracy: 0.4800"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCategory(pred,LABELS):\n",
    "    returnVal=[]\n",
    "    curr_index=0\n",
    "    curr_val=0\n",
    "    for item in pred:\n",
    "        val=max(item)\n",
    "        index=list(item).index(val)\n",
    "        #print(val,LABELS[index])\n",
    "        if val>curr_val:\n",
    "            curr_index=index\n",
    "            curr_val=val\n",
    "            \n",
    "    return LABELS[curr_index]\n",
    "    #returnVal.append(LABELS[np.argmax(np.array(item))])\n",
    "    #return returnVal\n",
    "        \n",
    "for key,item in df.iterrows():\n",
    "    data=[]\n",
    "    data.append(item['cleaned_hm'])\n",
    "    seq = tokenizer.texts_to_sequences(data)\n",
    "    padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    pred = model.predict(padded)\n",
    "    print(pred)\n",
    "    category=LABELS[np.argmax(pred)]\n",
    "    print(data, \"[predicted category]\", category, \"[actual category]\",item['predicted_category'])\n",
    "    #break\n",
    "    #print(pred, LABELS[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT=['cleaned_hm']\n",
    "TARGET='predicted_category'\n",
    "NUMERIC=[]\n",
    "#def combine_text_columns(data_frame, to_drop=NUMERIC + LABELS):\n",
    "def combine_text_columns(data_frame, text_labels=TEXT):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    # Drop non-text columns that are in the df\n",
    "    #to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data =data_frame[text_labels]\n",
    "     \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords=spacy.lang.en.stop_words.STOP_WORDS\n",
    "chi_k = 300\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "get_text_data = FunctionTransformer(combine_text_columns,validate=False)\n",
    "\n",
    "# Preprocess the numeric data: get_numeric_data\n",
    "#get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC], validate=False)\n",
    "\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                #('numeric_features', Pipeline([\n",
    "                #    ('selector', get_numeric_data),\n",
    "                #    ('imputer', SimpleImputer())\n",
    "                #])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "                    #('vectorizer',CountVectorizer(stop_words=stopwords,token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2))),('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(RandomForestClassifier(n_estimators=15))),\n",
    "        #('nb', MultinomialNB()),\n",
    "        #('lr', OneVsRestClassifier(LogisticRegression(C=100)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[TEXT]\n",
    "y=df[TARGET]\n",
    "\n",
    "#print(X)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.3, random_state=42)\n",
    "\n",
    "pl.fit(X_train,y_train)\n",
    "predictions=pl.predict(X_train)\n",
    "\n",
    "accuracy = pl.score(X_train, y_train)\n",
    "print(\"\\nAccuracy:{:.2f}% \".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pl.predict(X_test)\n",
    "\n",
    "index=0\n",
    "for key,item in X_test.iterrows():\n",
    "    #print(item['cleaned_hm'])\n",
    "    sentence=item['cleaned_hm']\n",
    "    result=predictions[index]\n",
    "    #actual=y_test[index]\n",
    "    index+=1\n",
    "    print(sentence,\"[[result]]\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
