sales=pd.DataFrame({
    'weekday':['Sun','Sun','Mon','Mon'],
    'city':['Austin','Dallas','Austin','Dallas'],
    'bread':[139,237,326,456],
    'butter':[20,45,70,98]
    
})
print(sales)
print(sales.groupby('weekday').count())

count is an aggregation reduction function

others are:
1. mean()
2. std()
3. sum()
4. first(), last()
5. min(), max()


print(sales)
print(sales.groupby('weekday').count())
print(sales.groupby('weekday')['bread'].sum())
sales.groupby('weekday')['bread'].sum().plot.pie()
print(sales.groupby('weekday')['bread','butter'].sum())
print(sales.groupby(['city','weekday']).mean())

 weekday    city  bread  butter
0     Sun  Austin    139      20
1     Sun  Dallas    237      45
2     Mon  Austin    326      70
3     Mon  Dallas    456      98
         city  bread  butter
weekday                     
Mon         2      2       2
Sun         2      2       2
weekday
Mon    782
Sun    376
Name: bread, dtype: int64
         bread  butter
weekday               
Mon        782     168
Sun        376      65
                bread  butter
city   weekday               
Austin Mon        326      70
       Sun        139      20
Dallas Mon        456      98
       Sun        237      45


>>>>>>>>>>>>>>>unique

sales['weekday'].unique()

output:
[Mon,Sun]
>>>>>>category

sales['weekday']=sales['weekday'].astype('category')

* uses less memory and it runs faster

>>>>>>> sample groupby

# Group titanic by 'pclass'
by_class = titanic.groupby('pclass')

# Aggregate 'survived' column of by_class by count
count_by_class =by_class['survived'].count()

# Print count_by_class
print(count_by_class)

# Group titanic by 'embarked' and 'pclass'
by_mult = titanic.groupby(['embarked','pclass'])

# Aggregate 'survived' column of by_mult by count
count_mult = by_mult['survived'].count()

# Print count_mult
print(count_mult)

pclass  name  sex  age  sibsp  ...  cabin  embarked  boat  body  home.dest
survived                                 ...                                        
pclass  name  sex  age  sibsp  ...  cabin  embarked  boat  body  home.dest
survived                                 ...                                        
pclass
1    323
2    277
3    709
Name: survived, dtype: int64
embarked  pclass
C         1         141
          2          28
          3         101
Q         1           3
          2           7
          3         113
S         1         177
          2         242
          3         495
Name: survived, dtype: int64



In [1]:


>>>>>> groupby

# Read life_fname into a DataFrame: life
life = pd.read_csv(life_fname, index_col='Country')

# Read regions_fname into a DataFrame: regions
regions = pd.read_csv(regions_fname, index_col='Country')

# Group life by regions['region']: life_by_region
life_by_region = life.groupby(regions['region'])

# Print the mean over the '2010' column of life_by_region
print(life_by_region['2010'].mean())

region
America                       74.037350
East Asia & Pacific           73.405750
Europe & Central Asia         75.656387
Middle East & North Africa    72.805333
South Asia                    68.189750
Sub-Saharan Africa            57.575080

 
>>>>>>>>>>>>>>>Group by and aggregation


print(sales.groupby('city')[['bread','butter']].max())

bread  butter
city                 
Austin    326      70
Dallas    456      98

print(sales.groupby('city')[['bread','butter']].agg(['max','sum','mean','count'])


def data_range(series):
	return series.max() - series.min()


print(sales.groupby('weekday')[['bread','butter']].agg([data_range])


#for each weekday find the sum for bread and the data_range for butter

print("\n\naggregate by column ", sales.groupby('weekday')[['bread','butter']].agg({'bread':'sum','butter':data_range}))



>>>>>> print the median fare for each class

# Group titanic by 'pclass': by_class
by_class = titanic.groupby('pclass')

# Select 'age' and 'fare'
by_class_sub = by_class[['age','fare']]

# Aggregate by_class_sub by 'max' and 'median': aggregated
aggregated = by_class_sub.agg(['max','median'])
print(aggregated)

# Print the maximum age in each class
print(aggregated.loc[:, ('age','max')])

# Print the median fare in each class
print(aggregated.loc[:,('fare','median')])

         age             fare         
         max median       max   median
pclass                                
1       80.0   39.0  512.3292  60.0000
2       70.0   29.0   73.5000  15.0458
3       74.0   24.0   69.5500   8.0500

pclass (max age per pclass)  
1    80.0
2    70.0
3    74.0

Name: (age, max), dtype: float64
pclass  (median fare per pclass)

1    60.0000
2    15.0458
3     8.0500
Name: (fare, median), dtype: float64


>>>>>>>>>>>sample aggregation functions using a dictionary for the column mapping to function

# Read the CSV file into a DataFrame and sort the index: gapminder
gapminder = pd.read_csv('gapminder.csv',index_col=['Year','region','Country']).sort_index()

# Group gapminder by 'Year' and 'region': by_year_region
by_year_region = gapminder.groupby(level=['Year','region'])

# Define the function to compute spread: spread
def spread(series):
    return series.max() - series.min()

# Create the dictionary: aggregator
aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}

# Aggregate by_year_region using the dictionary: aggregated
aggregated = by_year_region.agg(aggregator)

# Print the last 6 entries of aggregated 
print(aggregated.tail(6))


population  child_mortality       gdp
Year region                                                             
2013 America                     9.629087e+08        17.745833   49634.0
     East Asia & Pacific         2.244209e+09        22.285714  134744.0
     Europe & Central Asia       8.968788e+08         9.831875   86418.0
     Middle East & North Africa  4.030504e+08        20.221500  128676.0
     South Asia                  1.701241e+09        46.287500   11469.0
     Sub-Saharan Africa          9.205996e+08        76.944490   32035.0


>>>>>>>>>>>>sample group sales units by day
# Read file: sales
'Company', 'Product', 'Units'


sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)

#print(sales.index.strftime('%a'))
# Create a groupby object: by_day
by_day = sales.groupby(sales.index.strftime('%a'))
print(*by_day)
# Create sum: units_sum
units_sum = by_day['Units'].sum()

# Print units_sum
print(units_sum)


>>>>>>>>>>>>>>>>>>Groupby and Transformation

def zscore(series):
	return (series - series.mean())/series.std()

auto.groupby('yr')['mpg'].transform(zscore).head()


def zscore_with_year_and_name(group):
	df=pd.DataFrame(
		{'mpg': zscore(group['mpg']),
		'year':group['yr'],
		'name':group['name']})
	return df

auto.groupby('yr').apply(zscore_with_year_and_name).head()








Company   Product  Units
Date                                           
2015-02-02 08:30:00      Hooli  Software      3
2015-02-02 21:00:00  Mediacore  Hardware      9
2015-02-09 09:00:00  Streeplex   Service     19
2015-02-09 13:00:00  Mediacore  Software      7
2015-02-16 12:00:00      Hooli  Software     10) ('Sat',                              Company   Product  Units
Date                                                 
2015-02-07 23:00:00  Acme Coporation  Hardware      1
2015-02-21 05:00:00        Mediacore  Software      3
2015-02-21 20:30:00            Hooli  Hardware      3) ('Thu',                              Company   Product  Units
Date                                                 
2015-02-05 02:00:00  Acme Coporation  Software     19
2015-02-05 22:00:00            Hooli   Service     10
2015-02-19 11:00:00        Mediacore  Hardware     16
2015-02-19 16:00:00        Mediacore   Service     10
2015-02-26 09:00:00        Streeplex   Service      4) ('Tue',                      Company   Product  Units
Date                                         
2015-02-03 14:00:00  Initech  Software     13) ('Wed',                              Company   Product  Units
Date                                                 
2015-02-04 15:30:00        Streeplex  Software     13
2015-02-04 22:00:00  Acme Coporation  Hardware     14
2015-02-11 20:00:00          Initech  Software      7
2015-02-11 23:00:00            Hooli  Software      4
2015-02-25 00:30:00          Initech   Service     10)
Mon    48
Sat     7
Thu    59
Tue    13
Wed    48
Name: Units, dtype: int64



>>>>>>>>>>>>>>>>>>>>>>>Detecting outliers with zscore

# Import zscore
from scipy.stats import zscore

# Group gapminder_2010: standardized
standardized = gapminder_2010.groupby('region')['life','fertility'].transform(zscore)

# Construct a Boolean Series to identify outliers: outliers
outliers = (standardized['life'] < -3) | (standardized['fertility'] > 3)

# Filter gapminder_2010 by the outliers: gm_outliers
gm_outliers = gapminder_2010.loc[outliers]

# Print gm_outliers
print(gm_outliers)


                 fertility    life  population  child_mortality     gdp                 region
Country                                                                                   
Guatemala        3.974  71.100  14388929.0             34.5  6849.0                America
Haiti            3.350  45.000   9993247.0            208.8  1518.0                America
Tajikistan       3.780  66.830   6878637.0             52.6  2110.0  Europe & Central Asia
Timor-Leste      6.237  65.952   1124355.0             63.8  1777.0    East Asia & Pacific

Using z-scores like this is a great way to identify outliers in your data.

>>>>>>>>>>>>>>>>>> sample impute using the median for age   transform

# Create a groupby object: by_sex_class
by_sex_class = titanic.groupby(['sex','pclass'])

# Write a function that imputes median
def impute_median(series):
    return series.fillna(series.median())

# Impute age and assign to titanic['age']
titanic.age = by_sex_class['age'].transform(impute_median)

# Print the output of titanic.tail(10)
print(titanic.tail(10))

      pclass  survived                                     name     sex   age  ...  cabin  embarked boat   body home.dest
1299       3         0                      Yasbeck, Mr. Antoni    male  27.0  ...    NaN         C    C    NaN       NaN
1300       3         1  Yasbeck, Mrs. Antoni (Selini Alexander)  female  15.0  ...    NaN         C  NaN    NaN       NaN
1301       3         0                     Youseff, Mr. Gerious    male  45.5  ...    NaN         C  NaN  312.0       NaN
1302       3         0                        Yousif, Mr. Wazli    male  25.0  ...    NaN         C  NaN    NaN       NaN
1303       3         0                    Yousseff, Mr. Gerious    male  25.0  ...    NaN         C  NaN    NaN       NaN
1304       3         0                     Zabour, Miss. Hileni  female  14.5  ...    NaN         C  NaN  328.0       NaN
1305       3         0                    Zabour, Miss. Thamine  female  22.0  ...    NaN         C  NaN    NaN       NaN
1306       3         0                Zakarian, Mr. Mapriededer    male  26.5  ...    NaN         C  NaN  304.0       NaN
1307       3         0                      Zakarian, Mr. Ortin    male  27.0  ...    NaN         C  NaN    NaN       NaN
1308       3         0                       Zimmerman, Mr. Leo    male  29.0  ...    NaN         S  NaN    NaN       NaN


def disparity(gr):
    # Compute the spread of gr['gdp']: s
    s = gr['gdp'].max() - gr['gdp'].min()
    # Compute the z-score of gr['gdp'] as (gr['gdp']-gr['gdp'].mean())/gr['gdp'].std(): z
    z = (gr['gdp'] - gr['gdp'].mean())/gr['gdp'].std()
    # Return a DataFrame with the inputs {'z(gdp)':z, 'regional spread(gdp)':s}
    return pd.DataFrame({'z(gdp)':z , 'regional spread(gdp)':s})


# Group gapminder_2010 by 'region': regional
regional = gapminder_2010.groupby('region')

# Apply the disparity function on regional: reg_disp
reg_disp = regional.apply(disparity)

# Print the disparity of 'United States', 'United Kingdom', and 'China'
print(reg_disp.loc[['United States','United Kingdom','China']])


 z(gdp)  regional spread(gdp)
Country                                       
United States   3.013374               47855.0
United Kingdom  0.572873               89037.0
China          -0.432756               96993.0


>>>>>>>>>>>>>>>>>>>>>>>>Groupby and filtering

auto.groupby('yr')['mpg'].mean()


splitting = auto.groupby('yr')

print(splitting.groups.keys())

the output are the years

for group_name, group in splitting:
	avg = group['mpg'].mean()
	print(group_name, avg)


#filter only groups for chevrolet

for group_name, group in splitting:
	avg = group.loc[group[name].str.contains('chevrolet'),'mpg'].mean()
	print(group_name, avg)

#create a dictionary comprehension

chevy_means = {year: group.loc[group['name'].str.contains('chevrolet'),'mpg'].mean()
	for year, group in splitting}


chevy= auto['name'].str.contains('chevrolet')

auto.groupby(['yr',chevy])['mpg'].mean()

compares chevrolet mpg against all of its competitors


>>>>>>> sample gender survival rates

# Create a groupby object using titanic over the 'sex' column: by_sex
by_sex = titanic.groupby('sex')

# Call by_sex.apply with the function c_deck_survival
c_surv_by_sex = by_sex.apply(c_deck_survival)

# Print the survival rates
print(c_surv_by_sex)

sex
female    0.913043
male      0.312500

>>>>>> filtering a groupby

# Read the CSV file into a DataFrame: sales
sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)

# Group sales by 'Company': by_company
by_company = sales.groupby('Company')

# Compute the sum of the 'Units' of by_company: by_com_sum
by_com_sum = by_company['Units'].sum()
print(by_com_sum)

# Filter 'Units' where the sum is > 35: by_com_filt
by_com_filt = by_company.filter(lambda g:g['Units'].sum()>35)
print(by_com_filt)

Company
Acme Coporation    34
Hooli              30
Initech            30
Mediacore          45
Streeplex          36


Company   Product  Units
Date                                           
2015-02-02 21:00:00  Mediacore  Hardware      9
2015-02-04 15:30:00  Streeplex  Software     13
2015-02-09 09:00:00  Streeplex   Service     19
2015-02-09 13:00:00  Mediacore  Software      7
2015-02-19 11:00:00  Mediacore  Hardware     16
2015-02-19 16:00:00  Mediacore   Service     10
2015-02-21 05:00:00  Mediacore  Software      3
2015-02-26 09:00:00  Streeplex   Service      4


>>>>>>>>  group by using a filter

# Create the Boolean Series: under10
under10 = (titanic['age']<10).map({True:'under 10',False: 'over 10'})

# Group by under10 and compute the survival rate
survived_mean_1 = titanic.groupby(under10)['survived'].mean()
print(survived_mean_1)

# Group by under10 and pclass and compute the survival rate
survived_mean_2 = titanic.groupby([under10,'pclass'])['survived'].mean()
print(survived_mean_2)

age
over 10     0.366748
under 10    0.609756
Name: survived, dtype: float64
age       pclass
over 10   1         0.617555
          2         0.380392
          3         0.238897
under 10  1         0.750000
          2         1.000000
          3         0.446429

the pclass had a strong influence on who survived

>>>>>>>>>>>>>>>>>olympic medals


# Select the 'NOC' column of medals: country_names
country_names = medals['NOC']
# Count the number of medals won by each country: medal_counts
medal_counts = country_names.value_counts()

# Print top 15 countries ranked by medals
print(medal_counts.head(15))


gender and event gender

# Select columns: ev_gen
ev_gen = medals[['Event_gender','Gender']]

# Drop duplicate pairs: ev_gen_uniques
ev_gen_uniques = ev_gen.drop_duplicates()

# Print ev_gen_uniques
print(ev_gen_uniques)

Event_gender Gender
0                M    Men
348              X    Men
416              W  Women
639              X  Women
23675            W    Men


>>>>>>>>>>>>>>>filtering

## Create the Boolean Series: sus
sus = (medals.Event_gender == 'W') & (medals.Gender == 'Men')

# Create a DataFrame with the suspicious row: suspect
suspect = medals[sus]

# Print suspect
print(suspect)

City  Edition      Sport Discipline            Athlete  NOC Gender     Event Event_gender   Medal
23675  Sydney     2000  Athletics  Athletics  CHEPCHUMBA, Joyce  KEN    Men  marathon            W  Bronze

>>>>>>>>>>>>>>>>>>>idxmax() and idxmin()

idxmax() row or column label where maximum value is located

idxmin() row or column label where minimum value is located

weather.T.idxmax(axis='columns')
weather.T.idxmin(axis='columns')


>>>>> filter the # Create a Boolean Series that is True when 'Edition' is between 1952 and 1988: during_cold_war
during_cold_war = (medals['Edition'] >= 1952) & (medals['Edition'] <= 1988)

# Extract rows for which 'NOC' is either 'USA' or 'URS': is_usa_urs
is_usa_urs = medals.NOC.isin(['USA','URS'])

# Use during_cold_war and is_usa_urs to create the DataFrame: cold_war_medals
cold_war_medals = medals.loc[during_cold_war & is_usa_urs]

# Group cold_war_medals by 'NOC'
country_grouped = cold_war_medals.groupby('NOC')

# Create Nsports
Nsports = country_grouped['Sport'].nunique().sort_values(ascending=False)

# Print Nsports
print(Nsports)

NOC
    URS    21
    USA    20

>>>>>>>>sample >>> medals during the cold war

# Create the pivot table: medals_won_by_country
medals_won_by_country = medals.pivot_table(index=['Edition'],columns=['NOC'],values=['Athlete'],aggfunc=['count'])

# Slice medals_won_by_country: cold_war_usa_urs_medals
cold_war_usa_urs_medals = medals_won_by_country.loc[1952:1988, ['USA','URS']]

# Create most_medals 
most_medals = cold_war_usa_urs_medals.idxmax(axis='columns')

# Print most_medals.value_counts()
print(most_medals.value_counts())

Edition
URS    8
USA    2

>>>>>>>>>>>>unstack is a reshaping of the multi-index
grouped=df.groupby(['Year'])['Name'].count().plot()

france_medals = france_grps['Athlete'].count().unstack()

>>>>>>sample usa medals

medals.Medal = pd.Categorical(values=medals.Medal,
categories=['Bronze','Silver','Gold'])

# Create the DataFrame: usa
usa = medals[medals['NOC']=='USA']

# Group usa by ['Edition', 'Medal'] and aggregate over 'Athlete'
usa_medals_by_year = usa.groupby(['Edition','Medal'])['Athlete'].count()

# Reshape usa_medals_by_year by unstacking
usa_medals_by_year = usa_medals_by_year.unstack(level='Medal')

# Plot the DataFrame usa_medals_by_year
usa_medals_by_year.plot()
plt.show()
