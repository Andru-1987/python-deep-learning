exploratory data analysis
* John Tukey

"Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone"

df_swing=pd.read_csv('2008_swing_states.csv')
df_swing[['state','county','dem_share']]

*explore the data better

>>histograph

import matplotlib.pyplot as plt
_ = plt.hist(df_swing['dem_share'], bin)
_ = plt.xlabel('percent of vote')
_ = plt.ylabel('number of counties')
plt.show()

import seaborn as sns

sns.set() # get the seaborn defaults

>>

# Import plotting modules
import matplotlib.pyplot as plt
import seaborn as sns

# Set default Seaborn style
sns.set()

# Compute number of data points: n_data
n_data = len(versicolor_petal_length)

# Number of bins is the square root of number of data points: n_bins
n_bins = np.sqrt(n_data)

n_bins = int(n_bins)

# Plot histogram of versicolor petal lengths
_ = plt.hist(versicolor_petal_length, bins=n_bins)

_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
plt.show()

>>>bee swarm plot
import seaborn as sns

_= sns.swarmplot(x='state', y='dem_share', data=df_swing)
_= plt.xlabel('state')
_= plt.ylabel('percent of vote')
plt.show()

>>>

# Create bee swarm plot with Seaborn's default settings
import seaborn as sns
# very interesting

print(df.columns)
_= sns.swarmplot(x='species', y='petal length (cm)', data=df)
_= plt.xlabel('species')
_= plt.ylabel('petal length (cm)')

plt.show()

>>>

Empirical cumulative distribution function (ECDF)

x=voting percentage
y=percent of the counties

import numpy as np
x=np.sort(df_swing['dem_share'])
y=np.arange(1,len(x)+1) / len(x)

_=plt.plot(x,y,marker='.', linestyle='none')
_=plt.xlabel('percent of vote')
_=plt.ylabel('ecdf')

plt.margins(0.02)
plt.show()

>>
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    # Number of data points: n
    n =len(data)

    # x-data for the ECDF: x
    x = np.sort(data)

    # y-data for the ECDF: y
    y = np.arange(1,n+1) / n

    return x, y

# Compute ECDFs

x_set, y_set=ecdf(setosa_petal_length)
x_vers, y_vers = ecdf(versicolor_petal_length)
x_virg,y_virg=ecdf(virginica_petal_length)

# Plot all ECDFs on the same plot

_=plt.plot(x_set,y_set,marker='.', linestyle='none')
_=plt.plot(x_vers,y_vers,marker='.', linestyle='none')
_=plt.plot(x_virg,y_virg,marker='.', linestyle='none')


# Annotate the plot
plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Display the plot
plt.margins(0.02)
plt.show()

>>> mean

import numpy as np

np.mean(dem_share_PA)

sum of all the data divided by n

the median is the middle data of sorted data

np.median(dem_share_PA)

The median is the 50 percentile

np.percentile(df_swing['dem_share'],[25,50,75])

returns the values matching the percentile

box plot : 25th, 50th, and 75th percentile
the whiskers extend 1.5 the IQR

outliers are not necessarily errors

>>box plot

import matplotlib.pyplot as plt
import seaborn as sns

_ = sns.boxplot(x='east_west', y='dem_share',
data=df_all_states)
_ = plt.xlabel('region')
_ = plt.ylabel('percent of vote')
plt.show()


>>percentiles

# Specify array of percentiles: percentiles
percentiles = np.array([2.5, 25, 50, 75, 97.5])

# Compute percentiles: ptiles_vers
ptiles_vers = np.percentile(versicolor_petal_length, percentiles)

# Print the result
print(ptiles_vers)


# Plot the ECDF
_ = plt.plot(x_vers, y_vers, '.')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Overlay percentiles as red diamonds.
_ = plt.plot(ptiles_vers, percentiles/100, marker='D', color='red',
         linestyle='none')

# Show the plot
plt.show()

>>Box plot

# Create box plot with Seaborn's default settings

_ = sns.boxplot(x='species', y='petal length (cm)',
data=df)
_ = plt.xlabel('species')
_ = plt.ylabel('Petal length (cm)')
plt.show()

>>>>>>Variance and Standard deviation

1.) Variablity represents the spread of the data
2.) Variance is a way to quantify the spread of the data.  Variance is the mean squared distance of the data from their mean

(x-x_mean) ** 2 / n

np.sqrt(np.var(dem_share_FL))

is equivalent to np.std(dem_share_FL)
the standard deviation


>>>

# Array of differences to mean: differences
differences = versicolor_petal_length - np.mean(versicolor_petal_length)

# Square the differences: diff_sq
diff_sq = differences**2

# Compute the mean square difference: variance_explicit
variance_explicit = np.mean(diff_sq)

# Compute the variance using NumPy: variance_np
variance_np = np.var(versicolor_petal_length)

# Print the results
print(variance_explicit, variance_np)

>>

# Compute the variance: variance

variance=np.var(versicolor_petal_length)
# Print the square root of the variance

print(np.sqrt(variance))
# Print the standard deviation
print(np.std(versicolor_petal_length))


>>> Covariance

1) A measure of how two quantities vary together
2) if the distance from the x mean and the distance from the y mean are positive than the point is positively correlated

1/m (x-x_mean)*(y-y_mean)  if positive then positively correlated

if y is below the mean then the variable is negatively correlated

pearson correlation = covariance/(std of x)*(std of y)

equals

pearson correlation = variability due to codependence/ independant variability

-1 no correlation and 1 totally correlated

>>>

# Make a scatter plot
_ = plt.plot(versicolor_petal_length, versicolor_petal_width,
             marker='.', linestyle='none')

# Label the axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('petal width (cm)')

# Show the result
plt.show()

>>dataset -variance, covariance, and negative covariance

Look at the spread in the x-direction in the plots: The plot with the largest spread is the one that has the highest variance.
High covariance means that when x is high, y is also high, and when x is low, y is also low.
Negative covariance means that when x is high, y is low, and when x is low, y is high.

>>>sample

For example, we have two sets of data x and y, np.cov(x, y) returns a 2D array where entries [0,1] and [1,0] are the covariances. Entry [0,0] is the variance of the data in x, and entry [1,1] is the variance of the data in y. This 2D output array is called the covariance matrix, since it organizes the self- and covariance.

# Compute the covariance matrix: covariance_matrix
covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width)

# Print covariance matrix
print(covariance_matrix)

# Extract covariance of length and width of petals: petal_cov
petal_cov = covariance_matrix[0,1]

# Print the length/width covariance
print(petal_cov)


>>>sample pearson_r

def pearson_r(x,y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat=np.corrcoef(x,y)
#covariance/(std of x)*(std of y)
    # Return entry [0,1]
    return corr_mat[0,1]

# Compute Pearson correlation coefficient for I. versicolor: r

r=pearson_r(versicolor_petal_length,versicolor_petal_width)


# Print the result

print (r)

#where -1 no correlation and 1 totally correlated

>>ECDF

x=np.sort(grouped['CostPerSquareFoot'].values)
y=np.arange(1,len(x)+1)/len(x)

_=plt.plot(x,y,marker='.', linestyle='none')
_=plt.xlabel('Cost Sq Ft')
_=plt.ylabel('ECDF')

>>>Probablity

1. probablistic logic helps us describe uncertainty
2. statistic inference is the process whereby we go from measured data to probabilistic conclusions based on expectation

np.random.random() 
#draws a number between 0 and 1

np.random.seed(42)
#allows you to have reproducable code
#integer fed into random number generator algorithm

random_numbers=np.random.random(size=4)
random_numbers

heads=random_numbers<0.5
heads
np.sum(heads)


n_all_heads=0

for _ in range(1000):
	heads=np.random.random(size=4) <0.5
	n_heads=np.sum(heads)
	if n_heads==4:
		n_all_heads+=1

n_all_heads/10000


>>Hacker statistics

1. figure out how to simulate the data
2. simulate the data many many times
3. compute the probability as an approximate fraction fo the trials with the outcome of interest


>>Sample

# Seed the random number generator

np.random.seed(42)
# Initialize random numbers: random_numbers
random_numbers=np.empty(100000)

# Generate random numbers by looping over range(100000)
for i in range(100000):
    random_numbers[i] = np.random.random()

# Plot a histogram
ax = plt.hist(random_numbers)

# Show the plot
plt.show()

>>Sample

def perform_bernoulli_trials(n, p):
    """Perform n Bernoulli trials with success probability p
    and return number of successes."""
    # Initialize number of successes: n_success
    n_success = 0

    # Perform trials
    for i in range(n):
        # Choose random number between zero and one: random_number
        random_number = np.random.random()

        # If less than p, it's a success  so add one to n_success
        if random_number < p:
            n_success += 1

    return n_success

# Seed random number generator
np.random.seed(42)

# Initialize the number of defaults: n_defaults

n_defaults=np.empty(1000)
# Compute the number of defaults
for i in range(1000):
    n_defaults[i] = perform_bernoulli_trials(100,0.05)


# Plot the histogram with default number of bins; label your axes
plt.clf()
_ = plt.hist(n_defaults, normed=True)
_ = plt.xlabel('number of defaults out of 100 loans')
_ = plt.ylabel('probability')

# Show the plot
plt.show()




	
































