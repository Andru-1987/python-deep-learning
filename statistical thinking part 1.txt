exploratory data analysis
* John Tukey

"Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone"

df_swing=pd.read_csv('2008_swing_states.csv')
df_swing[['state','county','dem_share']]

*explore the data better

>>histograph

import matplotlib.pyplot as plt
_ = plt.hist(df_swing['dem_share'], bin)
_ = plt.xlabel('percent of vote')
_ = plt.ylabel('number of counties')
plt.show()

import seaborn as sns

sns.set() # get the seaborn defaults

>>

# Import plotting modules
import matplotlib.pyplot as plt
import seaborn as sns

# Set default Seaborn style
sns.set()

# Compute number of data points: n_data
n_data = len(versicolor_petal_length)

# Number of bins is the square root of number of data points: n_bins
n_bins = np.sqrt(n_data)

n_bins = int(n_bins)

# Plot histogram of versicolor petal lengths
_ = plt.hist(versicolor_petal_length, bins=n_bins)

_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
plt.show()

>>>bee swarm plot
import seaborn as sns

_= sns.swarmplot(x='state', y='dem_share', data=df_swing)
_= plt.xlabel('state')
_= plt.ylabel('percent of vote')
plt.show()

>>>

# Create bee swarm plot with Seaborn's default settings
import seaborn as sns
# very interesting

print(df.columns)
_= sns.swarmplot(x='species', y='petal length (cm)', data=df)
_= plt.xlabel('species')
_= plt.ylabel('petal length (cm)')

plt.show()

>>>

Empirical cumulative distribution function (ECDF)

x=voting percentage
y=percent of the counties

import numpy as np
x=np.sort(df_swing['dem_share'])
y=np.arange(1,len(x)+1) / len(x)

_=plt.plot(x,y,marker='.', linestyle='none')
_=plt.xlabel('percent of vote')
_=plt.ylabel('ecdf')

plt.margins(0.02)
plt.show()

>>
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    # Number of data points: n
    n =len(data)

    # x-data for the ECDF: x
    x = np.sort(data)

    # y-data for the ECDF: y
    y = np.arange(1,n+1) / n

    return x, y

# Compute ECDFs

x_set, y_set=ecdf(setosa_petal_length)
x_vers, y_vers = ecdf(versicolor_petal_length)
x_virg,y_virg=ecdf(virginica_petal_length)

# Plot all ECDFs on the same plot

_=plt.plot(x_set,y_set,marker='.', linestyle='none')
_=plt.plot(x_vers,y_vers,marker='.', linestyle='none')
_=plt.plot(x_virg,y_virg,marker='.', linestyle='none')


# Annotate the plot
plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Display the plot
plt.margins(0.02)
plt.show()

>>> mean

import numpy as np

np.mean(dem_share_PA)

sum of all the data divided by n

the median is the middle data of sorted data

np.median(dem_share_PA)

The median is the 50 percentile

np.percentile(df_swing['dem_share'],[25,50,75])

returns the values matching the percentile

box plot : 25th, 50th, and 75th percentile
the whiskers extend 1.5 the IQR

outliers are not necessarily errors

>>box plot

import matplotlib.pyplot as plt
import seaborn as sns

_ = sns.boxplot(x='east_west', y='dem_share',
data=df_all_states)
_ = plt.xlabel('region')
_ = plt.ylabel('percent of vote')
plt.show()


>>percentiles

# Specify array of percentiles: percentiles
percentiles = np.array([2.5, 25, 50, 75, 97.5])

# Compute percentiles: ptiles_vers
ptiles_vers = np.percentile(versicolor_petal_length, percentiles)

# Print the result
print(ptiles_vers)


# Plot the ECDF
_ = plt.plot(x_vers, y_vers, '.')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Overlay percentiles as red diamonds.
_ = plt.plot(ptiles_vers, percentiles/100, marker='D', color='red',
         linestyle='none')

# Show the plot
plt.show()

>>Box plot

# Create box plot with Seaborn's default settings

_ = sns.boxplot(x='species', y='petal length (cm)',
data=df)
_ = plt.xlabel('species')
_ = plt.ylabel('Petal length (cm)')
plt.show()

>>>>>>Variance and Standard deviation

1.) Variablity represents the spread of the data
2.) Variance is a way to quantify the spread of the data.  Variance is the mean squared distance of the data from their mean

(x-x_mean) ** 2 / n

np.sqrt(np.var(dem_share_FL))

is equivalent to np.std(dem_share_FL)
the standard deviation


>>>

# Array of differences to mean: differences
differences = versicolor_petal_length - np.mean(versicolor_petal_length)

# Square the differences: diff_sq
diff_sq = differences**2

# Compute the mean square difference: variance_explicit
variance_explicit = np.mean(diff_sq)

# Compute the variance using NumPy: variance_np
variance_np = np.var(versicolor_petal_length)

# Print the results
print(variance_explicit, variance_np)

>>

# Compute the variance: variance

variance=np.var(versicolor_petal_length)
# Print the square root of the variance

print(np.sqrt(variance))
# Print the standard deviation
print(np.std(versicolor_petal_length))


>>> Covariance

1) A measure of how two quantities vary together
2) if the distance from the x mean and the distance from the y mean are positive than the point is positively correlated

1/m (x-x_mean)*(y-y_mean)  if positive then positively correlated

if y is below the mean then the variable is negatively correlated

pearson correlation = covariance/(std of x)*(std of y)

equals

pearson correlation = variability due to codependence/ independant variability

-1 no correlation and 1 totally correlated

>>>

# Make a scatter plot
_ = plt.plot(versicolor_petal_length, versicolor_petal_width,
             marker='.', linestyle='none')

# Label the axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('petal width (cm)')

# Show the result
plt.show()

>>dataset -variance, covariance, and negative covariance

Look at the spread in the x-direction in the plots: The plot with the largest spread is the one that has the highest variance.
High covariance means that when x is high, y is also high, and when x is low, y is also low.
Negative covariance means that when x is high, y is low, and when x is low, y is high.

>>>sample

For example, we have two sets of data x and y, np.cov(x, y) returns a 2D array where entries [0,1] and [1,0] are the covariances. Entry [0,0] is the variance of the data in x, and entry [1,1] is the variance of the data in y. This 2D output array is called the covariance matrix, since it organizes the self- and covariance.

# Compute the covariance matrix: covariance_matrix
covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width)

# Print covariance matrix
print(covariance_matrix)

# Extract covariance of length and width of petals: petal_cov
petal_cov = covariance_matrix[0,1]

# Print the length/width covariance
print(petal_cov)


>>>sample pearson_r

def pearson_r(x,y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat=np.corrcoef(x,y)
#covariance/(std of x)*(std of y)
    # Return entry [0,1]
    return corr_mat[0,1]

# Compute Pearson correlation coefficient for I. versicolor: r

r=pearson_r(versicolor_petal_length,versicolor_petal_width)


# Print the result

print (r)

#where -1 no correlation and 1 totally correlated

>>ECDF

x=np.sort(grouped['CostPerSquareFoot'].values)
y=np.arange(1,len(x)+1)/len(x)

_=plt.plot(x,y,marker='.', linestyle='none')
_=plt.xlabel('Cost Sq Ft')
_=plt.ylabel('ECDF')

>>>Probablity

1. probablistic logic helps us describe uncertainty
2. statistic inference is the process whereby we go from measured data to probabilistic conclusions based on expectation

np.random.random() 
#draws a number between 0 and 1

np.random.seed(42)
#allows you to have reproducable code
#integer fed into random number generator algorithm

random_numbers=np.random.random(size=4)
random_numbers

heads=random_numbers<0.5
heads
np.sum(heads)


n_all_heads=0

for _ in range(1000):
	heads=np.random.random(size=4) <0.5
	n_heads=np.sum(heads)
	if n_heads==4:
		n_all_heads+=1

n_all_heads/10000


>>Hacker statistics

1. figure out how to simulate the data
2. simulate the data many many times
3. compute the probability as an approximate fraction fo the trials with the outcome of interest


>>Sample

# Seed the random number generator

np.random.seed(42)
# Initialize random numbers: random_numbers
random_numbers=np.empty(100000)

# Generate random numbers by looping over range(100000)
for i in range(100000):
    random_numbers[i] = np.random.random()

# Plot a histogram
ax = plt.hist(random_numbers)

# Show the plot
plt.show()

>>Sample

def perform_bernoulli_trials(n, p):
    """Perform n Bernoulli trials with success probability p
    and return number of successes."""
    # Initialize number of successes: n_success
    n_success = 0

    # Perform trials
    for i in range(n):
        # Choose random number between zero and one: random_number
        random_number = np.random.random()

        # If less than p, it's a success  so add one to n_success
        if random_number < p:
            n_success += 1

    return n_success

# Seed random number generator
np.random.seed(42)

# Initialize the number of defaults: n_defaults

n_defaults=np.empty(1000)
# Compute the number of defaults
for i in range(1000):
    n_defaults[i] = perform_bernoulli_trials(100,0.05)


# Plot the histogram with default number of bins; label your axes
plt.clf()
_ = plt.hist(n_defaults, normed=True)
_ = plt.xlabel('number of defaults out of 100 loans')
_ = plt.ylabel('probability')

# Show the plot
plt.show()

def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    # Number of data points: n
    n =len(data)

    # x-data for the ECDF: x
    x = np.sort(data)

    # y-data for the ECDF: y
    y = np.arange(1,n+1) / n

    return x, y

# Compute ECDF: x, y

x,y=ecdf(n_defaults)
# Plot the ECDF with labeled axes

_=plt.plot(x,y,marker='.', linestyle='none')
_=plt.xlabel('Bank Defaults')
_=plt.ylabel('ECDF')


# Show the plot

plt.show()

# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money

n_lose_money = np.sum(n_defaults >= 10)

# Compute and print probability of losing money
print('Probability of losing money =', n_lose_money / len(n_defaults))


>>>>The binomial distribution


Probability mass function (PMF)
1. The set of probabilities of discrete outcomes
2.  the values are discrete because only certain values can be obtained.

dice: 1,2,3,4,5,6 each with a 1/6 probability
1. discrete uniform probablity pmf

probability distribution is a mathematical descriptiion of outcomes.

The binomial distribution story

The number r of successes in n bernouli trials with probability p of sucess is binomially distributed.

The number r of heads in 4 flips with probability 0.5 of heads is binomially distributed.

print(np.random.binomial(100,0.5,size=10))
#size  tells the function how many random numbers to sample out the distribution

n=60
p=0.1
samples=np.random.binomial(n,p,size=10000)

x,y = ecdf(samples)
_ = plt.plot(x,y, marker='.', linestyle='none')
plt.margins(0.02)
_ = plt.xlabel('number of successes')
_ = plt.ylabel('CDF')
plt.show()


>>>Sample

n=100
p=0.05
n_defaults=np.random.binomial(n,p,size=10000)

# Compute CDF: x, y
x,y = ecdf(n_defaults)

_ = plt.plot(x,y, marker='.', linestyle='none')
plt.margins(0.02)
_ = plt.xlabel('number of successes')
_ = plt.ylabel('CDF')
plt.show()

# Compute bin edges: bins
bins = np.arange(0, max(n_defaults) + 1.5) - 0.5

# Generate histogram
plt.clf()
_ = plt.hist(n_defaults, bins=bins, normed=True)
_ = plt.xlabel('number of defaults out of 100 loans')
_ = plt.ylabel('probability')
_
# Label axes


# Show the plot
plt.show()


>>>>Poisson Process

1.  The time of the next event is completely independent of when the prevous event happened
a. natural births in a given hospital
b. hit on a website during a given hour
c. meteor strikes
d. molecular collision in a gas
e. avaition incidents
f. buses in poissonville

poisson distribution

1. The number r of arrivals of a poisson process in a given time interval with a average rate of ? arrivals per interval is poisson distribution

2. The number r of hits on a website in one hour with an average hit rate of 6 hits per hour is poisson distribute

Poisson distribution
1. limit of binomial distribution of low probablity of success and large number of trials
2. That is, for rare events

https://www.cnbc.com/2020/02/07/junk-bond-scare-is-rising-no-one-cares-people-are-buying-everything.html

https://www.cnbc.com/2020/04/06/investing-in-hunt-for-returns-investors-are-buying-junk-bonds.html

samples = np.random.poisson(6, size=10000)
x,y=ecdf(samples)
_=plt.plot(x,y, marker='.', linestyle='none')
plt.margins(0.02)
_= plt.xlabel('number of successes')
_= plt.ylabel('cdf')
plt.show()

>>sample

#the Poisson distribution is a limit of the Binomial distribution for rare events

#Say we do a Bernoulli trial every minute for an hour, each with a success probability of 0.1. We would do 60 trials, and the number of successes is Binomially distributed, and we would expect to get about 6 successes. 


>>>Sample 1

# Draw 10,000 samples out of Poisson distribution: samples_poisson

samples_poisson = np.random.poisson(10, size=10000)

# Print the mean and standard deviation
print('Poisson:     ', np.mean(samples_poisson),
                       np.std(samples_poisson))

# Specify values of n and p to consider for Binomial: n, p

n=[20,100,1000]
p=[0.5,0.1,0.01]

# Draw 10,000 samples for each n,p pair: samples_binomial
for i in range(3):
    samples_binomial = np.random.binomial(n[i],p[i],size=10000)

    # Print results
    print('n =', n[i], 'Binom:', np.mean(samples_binomial),
                                 np.std(samples_binomial))

>>Sample 2

# Draw 10,000 samples out of Poisson distribution: n_nohitters

n_nohitters = np.random.poisson(251/115, size=10000)

# Compute number of samples that are seven or greater: n_large
n_large = np.sum(n_nohitters>=7)

# Compute probability of getting seven or more: p_large

p_large=n_large/10000

# Print the result
print('Probability of seven or more no-hitters:', p_large)


>>>density functions

continous variables can take on any values, not just discrete values.

normal distribution

1. probability density function (pdf)
a. continous analog to the pmf
b. mathematical description of the relative likelihood of observing a value of a continous variable

cdf - accumulative distribution function

Normal distribution is famous
1. describes a continous variable whose PDF has a single symmetric peak.
a. The mean determines where the center of the peak is
b. The standard deviation is a measure of how wide the peak is.


we can use a histogram to compare the data to a normal probability of distribution pdf


import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

mean=np.mean(michelson_speed_of_light)
std= np.std(michelson_speed_of_light)

samples= np.random.normal(mean,std, size=10000)
x,y=ecdf(michaelson_speed_of_light)
x_theory,y_theory=ecdf(samples)

sns.set()

_=plt.plot(x_theory,y_theory)
_=plt.plot(x,y,marker='.', linestyle='none')
_=plt.xlabel('speed of light (km/s)')
_=plt.ylabel('cdf')
plt.show()

The michelson data is approximately normally distributed

>>Sample of the distribution

# Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10
samples_std1 = np.random.normal(20, 1, size=100000)
samples_std3 = np.random.normal(20, 3, size=100000)
samples_std10 = np.random.normal(20, 10, size=100000)

# Make histograms
_ = plt.hist(samples_std1, bins=100, normed=True, histtype='step')
_ = plt.hist(samples_std3, bins=100, normed=True, histtype='step')
_ = plt.hist(samples_std10, bins=100, normed=True, histtype='step')

# Make a legend, set limits and show plot
_ = plt.legend(('std = 1', 'std = 3', 'std = 10'))
plt.ylim(-0.01, 0.42)
plt.show()

# Generate CDFs
x_std1,y_std1=ecdf(samples_std1)
x_std3,y_std3=ecdf(samples_std3)
x_std10,y_std10=ecdf(samples_std10)



# Plot CDFs

_=plt.plot(x_std1,y_std1)
_=plt.plot(x_std3,y_std3)
_=plt.plot(x_std10,y_std10)


# Make a legend and show the plot
_ = plt.legend(('std = 1', 'std = 3', 'std = 10'), loc='lower right')
plt.show()

>>>Normal distribution and properties and warnings
1. The normal distribution is often referred to as the guassian distribution

>>sample 3
# Compute mean and standard deviation: mu, sigma
mu = np.mean(belmont_no_outliers)
sigma = np.std(belmont_no_outliers)

# Sample out of a normal distribution with this mu and sigma: samples
samples = np.random.normal(mu, sigma, size=10000)

# Get the CDF of the samples and of the data
x_theor, y_theor = ecdf(samples)
x, y = ecdf(belmont_no_outliers)

# Plot the CDFs and show the plot
_ = plt.plot(x_theor, y_theor)
_ = plt.plot(x, y, marker='.', linestyle='none')
_ = plt.xlabel('Belmont winning time (sec.)')
_ = plt.ylabel('CDF')
plt.show()

# Take a million samples out of the Normal distribution: samples
samples = np.random.normal(mu, sigma, size=1000000)

# Compute the fraction that are faster than 144 seconds: prob
prob = np.sum(samples <= 144) / len(samples)

# Print the result
print('Probability of besting Secretariat:', prob)

https://www.cnbc.com/2020/03/20/junk-bond-default-rate-expected-to-triple-in-next-12-months-sp-says.html

S&P Global Ratings said the default rate for high-yield, or junk, bonds is heading to 10% over the next 12 months, more than triple the rate of 3.1% that closed out 2019. 

find junk bonds by cuspip
https://managingfundswithpythonandsql.wordpress.com/

:/Index Holdings/High Yield Archiv

https://github.com/fedspendingtransparency/usaspending-website/wiki

>>>>Exponential distribution

1. The waiting time between arrivals of a poisson process is exponentially distributed

2. It has a single parameter the mean waiting time

nuclear incidents - timing of one is independent of all others. time of days between nuclear incidents

mean= np.mean(inter_times)
samples = np.random.exponential(mean, size=10000)
x,y=ecdf(inter_times)
x_theor,y_theor = ecdf(samples)

_ = plt.plot(x_theor, y_theor)
_ = plt.plot(x, y, marker='.', linestyle='none')
_ = plt.xlabel('time (days)')
_ = plt.ylabel('CDF')



speculative grade default rates

https://www.schwab.com/resource-center/insights/content/corporate-defaults-what-investors-should-know-when-a-bond-issuer-goes-bankrupt

1984 3%
1988 3%
1992 12%
1996 4%
2000 8%
2002 12%
2004 6%
2008 2%
2010 12%
2012 3%
2016 5%
2020 2%


if the incidents are evenly distributed the event can be modeled as a poisson process.

if you can simulate a story you can get its distribution.


>>sample

def successive_poisson(tau1, tau2, size=1):
    """Compute time for arrival of 2 successive Poisson processes."""
    # Draw samples out of first exponential distribution: t1
    t1 = np.random.exponential(tau1, size)

    # Draw samples out of second exponential distribution: t2
    t2 = np.random.exponential(tau2, size)

    return t1 + t2


#Recall, from the earlier exercise, that tau1 denotes the mean waiting time for a no-hitter, while tau2 denotes the mean waiting time for hitting the cycle.

#The mean waiting time for a no-hitter is 764 games, and the mean waiting time for hitting the cycle is 715 games.


waiting_times = successive_poisson(764, 715, size=100000)

# Make the histogram
_ = plt.hist(waiting_times, bins=100, histtype='step',
             normed=True)

# Label axes
_ = plt.xlabel('total waiting time (games)')
_ = plt.ylabel('PDF')

# Show the plot
plt.show()

# Notice that the PDF is peaked, unlike the waiting time for a single Poisson process. For fun (and enlightenment), I encourage you to also plot the CDF.






















	
































