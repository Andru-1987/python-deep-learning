exploratory data analysis
* John Tukey

"Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone"

df_swing=pd.read_csv('2008_swing_states.csv')
df_swing[['state','county','dem_share']]

*explore the data better

>>histograph

import matplotlib.pyplot as plt
_ = plt.hist(df_swing['dem_share'], bin)
_ = plt.xlabel('percent of vote')
_ = plt.ylabel('number of counties')
plt.show()

import seaborn as sns

sns.set() # get the seaborn defaults

>>

# Import plotting modules
import matplotlib.pyplot as plt
import seaborn as sns

# Set default Seaborn style
sns.set()

# Compute number of data points: n_data
n_data = len(versicolor_petal_length)

# Number of bins is the square root of number of data points: n_bins
n_bins = np.sqrt(n_data)

n_bins = int(n_bins)

# Plot histogram of versicolor petal lengths
_ = plt.hist(versicolor_petal_length, bins=n_bins)

_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
plt.show()

>>>bee swarm plot
import seaborn as sns

_= sns.swarmplot(x='state', y='dem_share', data=df_swing)
_= plt.xlabel('state')
_= plt.ylabel('percent of vote')
plt.show()

>>>

# Create bee swarm plot with Seaborn's default settings
import seaborn as sns
# very interesting

print(df.columns)
_= sns.swarmplot(x='species', y='petal length (cm)', data=df)
_= plt.xlabel('species')
_= plt.ylabel('petal length (cm)')

plt.show()

>>>

Empirical cumulative distribution function (ECDF)

x=voting percentage
y=percent of the counties

import numpy as np
x=np.sort(df_swing['dem_share'])
y=np.arange(1,len(x)+1) / len(x)

_=plt.plot(x,y,marker='.', linestyle='none')
_=plt.xlabel('percent of vote')
_=plt.ylabel('ecdf')

plt.margins(0.02)
plt.show()

>>
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    # Number of data points: n
    n =len(data)

    # x-data for the ECDF: x
    x = np.sort(data)

    # y-data for the ECDF: y
    y = np.arange(1,n+1) / n

    return x, y

# Compute ECDFs

x_set, y_set=ecdf(setosa_petal_length)
x_vers, y_vers = ecdf(versicolor_petal_length)
x_virg,y_virg=ecdf(virginica_petal_length)

# Plot all ECDFs on the same plot

_=plt.plot(x_set,y_set,marker='.', linestyle='none')
_=plt.plot(x_vers,y_vers,marker='.', linestyle='none')
_=plt.plot(x_virg,y_virg,marker='.', linestyle='none')


# Annotate the plot
plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Display the plot
plt.margins(0.02)
plt.show()

>>> mean

import numpy as np

np.mean(dem_share_PA)

sum of all the data divided by n

the median is the middle data of sorted data

np.median(dem_share_PA)

The median is the 50 percentile

np.percentile(df_swing['dem_share'],[25,50,75])

returns the values matching the percentile

box plot : 25th, 50th, and 75th percentile
the whiskers extend 1.5 the IQR

outliers are not necessarily errors

>>box plot

import matplotlib.pyplot as plt
import seaborn as sns

_ = sns.boxplot(x='east_west', y='dem_share',
data=df_all_states)
_ = plt.xlabel('region')
_ = plt.ylabel('percent of vote')
plt.show()


>>percentiles

# Specify array of percentiles: percentiles
percentiles = np.array([2.5, 25, 50, 75, 97.5])

# Compute percentiles: ptiles_vers
ptiles_vers = np.percentile(versicolor_petal_length, percentiles)

# Print the result
print(ptiles_vers)


# Plot the ECDF
_ = plt.plot(x_vers, y_vers, '.')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Overlay percentiles as red diamonds.
_ = plt.plot(ptiles_vers, percentiles/100, marker='D', color='red',
         linestyle='none')

# Show the plot
plt.show()

>>Box plot

# Create box plot with Seaborn's default settings

_ = sns.boxplot(x='species', y='petal length (cm)',
data=df)
_ = plt.xlabel('species')
_ = plt.ylabel('Petal length (cm)')
plt.show()

>>>>>>Variance and Standard deviation

1.) Variablity represents the spread of the data
2.) Variance is a way to quantify the spread of the data.  Variance is the mean squared distance of the data from their mean

(x-x_mean) ** 2 / n

np.sqrt(np.var(dem_share_FL))

is equivalent to np.std(dem_share_FL)
the standard deviation


>>>

# Array of differences to mean: differences
differences = versicolor_petal_length - np.mean(versicolor_petal_length)

# Square the differences: diff_sq
diff_sq = differences**2

# Compute the mean square difference: variance_explicit
variance_explicit = np.mean(diff_sq)

# Compute the variance using NumPy: variance_np
variance_np = np.var(versicolor_petal_length)

# Print the results
print(variance_explicit, variance_np)

>>

# Compute the variance: variance

variance=np.var(versicolor_petal_length)
# Print the square root of the variance

print(np.sqrt(variance))
# Print the standard deviation
print(np.std(versicolor_petal_length))


>>> Covariance

1) A measure of how two quantities vary together
2) if the distance from the x mean and the distance from the y mean are positive than the point is positively correlated

1/m (x-x_mean)*(y-y_mean)  if positive then positively correlated

if y is below the mean then the variable is negatively correlated

pearson correlation = covariance/(std of x)*(std of y)

equals

pearson correlation = variability due to codependence/ independant variability

-1 no correlation and 1 totally correlated

>>>

# Make a scatter plot
_ = plt.plot(versicolor_petal_length, versicolor_petal_width,
             marker='.', linestyle='none')

# Label the axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('petal width (cm)')

# Show the result
plt.show()

>>dataset -variance, covariance, and negative covariance

Look at the spread in the x-direction in the plots: The plot with the largest spread is the one that has the highest variance.
High covariance means that when x is high, y is also high, and when x is low, y is also low.
Negative covariance means that when x is high, y is low, and when x is low, y is high.

>>>sample

For example, we have two sets of data x and y, np.cov(x, y) returns a 2D array where entries [0,1] and [1,0] are the covariances. Entry [0,0] is the variance of the data in x, and entry [1,1] is the variance of the data in y. This 2D output array is called the covariance matrix, since it organizes the self- and covariance.

# Compute the covariance matrix: covariance_matrix
covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width)

# Print covariance matrix
print(covariance_matrix)

# Extract covariance of length and width of petals: petal_cov
petal_cov = covariance_matrix[0,1]

# Print the length/width covariance
print(petal_cov)


>>>sample pearson_r

def pearson_r(x,y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat=np.corrcoef(x,y)
#covariance/(std of x)*(std of y)
    # Return entry [0,1]
    return corr_mat[0,1]

# Compute Pearson correlation coefficient for I. versicolor: r

r=pearson_r(versicolor_petal_length,versicolor_petal_width)


# Print the result

print (r)

#where -1 no correlation and 1 totally correlated

>>ECDF

x=np.sort(grouped['CostPerSquareFoot'].values)
y=np.arange(1,len(x)+1)/len(x)

_=plt.plot(x,y,marker='.', linestyle='none')
_=plt.xlabel('Cost Sq Ft')
_=plt.ylabel('ECDF')









	
































