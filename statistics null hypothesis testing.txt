>>test statistics and p-values

null hypothesis using permutation testing.  A null hypothesis is the commonly accepted fact.  Researchers work to reject or disaprove the null hypothesis.  Researchers come up with an alternate hypothesis to explain the phenomenon then reject the null hypothesis.

how reasonable the asessment of data is assuming a hypothesis is true

a test statistic is a number that can be computed from observed data and from data you simulate under the null hypothesis

test statistic serves as a basis of comparison between the data and the simulated data

your test should be pertinent to the question your asking.

permutation replicate is the difference in the comparison of the single value.

sampling
np.mean(perm_sample_PA) - np.mean(perm_sample_OH)

1.122

actual data
np.mean(dem_share_PA) - np.mean(dem_share_OH)

1.158

histograph of the percentage differences between PA and OH

the difference in this case was between -4 and 4 percentage difference.

the actual data difference was 1.158 as a red line on the histogram.

to the right of the red line there were 23 percent with at least a 1.158 difference.  The right of the red line is called a p value


p-value

The probability of obtaining a value of your test statistic that is at least as extreme as what was observed, under the assumption the null hypothesis is true.

the p-value is not the probability that the null hypothesis is true

if the p-value is small than the data is statistical significantly different.

null hypothesis significant testing (nhst)


>>Sample

def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""

    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)

    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1,data_2)

        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)

    return perm_replicates

# Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. In the next exercise, we will test the hypothesis that the two frogs have the same distribution of impact forces.

# Make bee swarm plot
_ = sns.swarmplot(x='ID',y='impact_force',data=df)

# Label axes
_ = plt.xlabel('frog')
_ = plt.ylabel('impact force (N)')

# Show the plot
plt.show()


def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""

    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1)-np.mean(data_2)

    return diff

# Compute difference of mean impact force from experiment: empirical_diff_means
empirical_diff_means = diff_of_means(force_a,force_b)

# Draw 10,000 permutation replicates: perm_replicates
perm_replicates = draw_perm_reps(force_a,force_b,
                                 diff_of_means, size=10000)

# Compute p-value: p
p = np.sum(perm_replicates >= empirical_diff_means) / len(perm_replicates)

# Print the result
print('p-value =', p)

p-value=0.0063


The p-value tells you that there is about a 0.6% chance that you would get the difference of means observed in the experiment if frogs were exactly the same. A p-value below 0.01 is typically said to be "statistically significant," but: warning! warning! warning! You have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be "statistically significant," but they are definitely not the same!


pipleline for hypothesis testing

1. clearly state the null hypothesis
2. define your testing statistic
3. generate many sets of simulated data assuming the null hypothesis is true
4. Compute the test statistic for each simulated data set
5. The p-value is the fraction of your simulated data set for which the test statistic is at least as extreme as for the real data.


we want to know if there is something michelson speed of light 299,852 km/s and newcomb 299,860 km/s are correlated

we only have the newcomb mean

Null hypothesis

1. The true mean speed of light in Michelsons experiments was actually Newcombs reported value.

>>>How using bootstrapping to simulate data under the null hypothesis

1. we shift michelsons data with the same mean as newcombs data

newcomb_value=299860 #km/s

michelson_shifted = michelson_speed_of_light -
np.mean(michelson_speed_of_light) + new newcomb_value

def diff_from_newcomb(data, newcomb_value=299860):
	return np.mean(data)-newcomb_value

diff_from_newcomb= diff_from_newcomb(michelson_speed_of_light)

bs_replicates = draw_bs_reps(michelson_shifted, np.mean, 10000)

p_value= np.sum(bs_replicates <= newcomb_value)/10000


>>>Sample


Force B
[0.172 0.142 0.037 0.453 0.355 0.022 0.502 0.273 0.72  0.582 0.198 0.198
 0.597 0.516 0.815 0.402 0.605 0.711 0.614 0.468]


# Make an array of translated impact forces: translated_force_b
translated_force_b = force_b - np.mean(force_b) + 0.55

# Take bootstrap replicates of Frog B's translated impact forces: bs_replicates
bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)

# Compute fraction of replicates that are less than the observed Frog B force: p
p = np.sum(bs_replicates <= np.mean(force_b)) / 10000

# Print the p-value
print('p = ', p)


# Compute mean of all forces: mean_force

print (forces_concat)

mean_force = np.mean(forces_concat)

# Generate shifted arrays
force_a_shifted = force_a - np.mean(force_a) + mean_force
force_b_shifted = force_b - np.mean(force_b) + mean_force

# Compute 10,000 bootstrap replicates from shifted arrays
bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, 10000)
bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, 10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_replicates_a - bs_replicates_b

print(empirical_diff_means)
# Compute and print p-value: p
p = np.sum(bs_replicates >= empirical_diff_means) / len(bs_replicates)

print('p-value =', p)



Null hypothesis test

>>>>>>>>>>>>>>>>>>>>>>A/B testing>>>>>>>>>>>>>>>

500 views for each splash page A and B

page a had 45 click throughs
page b had 67 click throughs

maybe it is observed chance.  what is the probability


import numpy as np

# clickthrough_A, clickthrough_B array of 1s or 0s where 1 is a click through

def diff_frac(data_A, data_B):
	frac_A=np.sum(data_A) / len(data_A)
	frac_B=np.sum(data_B) / len(data_B)
	return frac_B-frac_A

diff_frac_obs=diff_frac(clickthrough_A,clickthrough_B)

def ecdf(data):
    #Compute ECDF for a one-dimensional array of measurements.
    # Number of data points: n
    n =len(data)

    # x-data for the ECDF: x
    x = np.sort(data)

    # y-data for the ECDF: y
    y = np.arange(1,n+1) / n

    return x, y

def permutation_replicate(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)

    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    return perm_replicates

def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""

    # Concatenate the data sets: data
    data = np.concatenate((data1,data2))

    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)

    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]

    return perm_sample_1, perm_sample_2


>>> code

perm_replicates=np.empty(10000)

for i in range(10000):
	perm_replicates[i]=permutation_replicate(
	clickthrough_A, clickthrough_B, diff_frac)

p_value=np.sum(perm_replicates>=diff_frac_obs)/10000

>>>AB testing

* used by organizations to see if a strategy change gives better results

* a low p value lead to a change in performance

A change in click through rate may be statistically significant but not profit if only a few people visited the site.

>>>Sample civil rights act of 1964

153 house democrats
136 republicans
91 democrats voted
35 republicans voted

# Construct arrays of data: dems, reps
dems = np.array([True] * 153 + [False] * 91)
reps = np.array([True] * 136 + [False] * 35)

def frac_yea_dems(dems, reps):
    """Compute fraction of Democrat yea votes."""
    frac = np.sum(dems) / len(dems)
    return frac

# Acquire permutation samples: perm_replicates
perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, 10000)

# Compute and print p-value: p
p = np.sum(perm_replicates <= 153/244) / len(perm_replicates)
print('p-value =', p)

the p-value=0.0002

Did party affiliation make a difference in the vote?

yes because the p-value was small


>>>sample >> not

 Compute the observed difference in mean inter-no-hitter times: nht_diff_obs
nht_diff_obs = diff_of_means(nht_dead, nht_live)

# Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates
perm_replicates = draw_perm_reps(nht_dead, nht_live,
                                 diff_of_means, size=10000)

# Compute and print the p-value: p
p = np.sum(perm_replicates <= nht_diff_obs) / len(perm_replicates)
print('p-val =', p)


, the pitcher was no longer allowed to spit on or scuff the ball, an activity that greatly favors pitchers.


>>>>>>>>>>>>>>>>How to do a hypothesis Test on Correlation and execute it.

pearson correlation = covariance/(std of x)*(std of y)

-1 negatively correlated and 1 totally correlated

* posit null hypothesis that there are no correlation between the two variables


* use pearson correlation, p as test statistic
* compute p-value as a fraction of replicates that have p at least as large as observed

given an observed value of .54

no replicia has a value greater than .54
this suggest a very low p value
suggesting counties with high vote counts voted for the candidate


>>>>Sample >> the affect on insectide on bee reproduction

# Compute x,y values for ECDFs
x_control, y_control = ecdf(control)
x_treated, y_treated = ecdf(treated)

# Plot the ECDFs
plt.plot(x_control, y_control, marker='.', linestyle='none')
plt.plot(x_treated, y_treated, marker='.', linestyle='none')

# Set the margins
plt.margins(0.02)

# Add a legend
plt.legend(('control', 'treated'), loc='lower right')

# Label axes and show plot
plt.xlabel('millions of alive sperm per mL')
plt.ylabel('ECDF')
plt.show()

>>>>>>>Boot strapping bees and insecticide
Null test: hypothesis: On average, male bees treated with neonicotinoid insecticide have the same number of active sperm per milliliter of semen than do untreated male bees. You will use the difference of means as your test statistic.

# Compute the difference in mean sperm count: diff_means

diff_means = np.mean(control) - np.mean(treated)

# Compute mean of pooled data: mean_count
mean_count = np.mean(np.concatenate((control, treated)))

# Generate shifted data sets
control_shifted = control - np.mean(control) + mean_count
treated_shifted = treated - np.mean(treated) + mean_count

# Generate bootstrap replicates
bs_reps_control = draw_bs_reps(control_shifted,
                               np.mean, size=10000)
bs_reps_treated = draw_bs_reps(treated_shifted,
                               np.mean, size=10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_reps_control - bs_reps_treated

# Compute and print p-value: p
p = np.sum(bs_replicates >= np.mean(control) - np.mean(treated)) \
            / len(bs_replicates)
print('p-value =', p)


>>>>>>>>>>>>>Case study
study of small birds (finches)
1. geospiza fortis
2. geospiza scandens

princeton university
dryad digital repository

1. beak length
2. beak depth

check how the beak depth of g. scandens has changed over time.

1975 to 2012

estimates of the mean beak depth for the representive years

hypothesis test if the beaks did get deeper





























