how to fit a glm in python

import statsmodels.api as sm

support for formulas

import statsmodels.formula.api as smf

use glm() directly

from statsmodels.formula.api import glm


model_glm = glm(formula = 'Salary ~ Experience',
                data = salary,
                family = sm.families.Gaussian()).fit()

# View model coefficients
print(model_glm.params)


glm.summary() --> summarize the model

glm.predict() --> make predictions

>>>>>>>>>>>>>>>>>  Salary prediction based on education

print("the scatter plot represents the mean age by income")
grouped= df.groupby('AGE')
mean_income_by_age=grouped['REALINC'].mean()

df2=pd.DataFrame()
df2['AGE']=np.linspace(18,85)
df2['AGE2']=df2['AGE']**2
df2['EDUC']=12
df2['EDUC2']=df2['EDUC']**2

df3=pd.DataFrame()
df3['AGE']=np.linspace(18,85)
df3['AGE2']=df3['AGE']**2
df3['EDUC']=16
df3['EDUC2']=df3['EDUC']**2

df4=pd.DataFrame()
df4['AGE']=np.linspace(18,85)
df4['AGE2']=df4['AGE']**2
df4['EDUC']=18
df4['EDUC2']=df4['EDUC']**2

#print("Max real income",df["REALINC"].max())
#print(df.describe())
results= smf.ols("REALINC ~ EDUC+EDUC2+AGE+AGE2", data=df).fit()

print("Make predictions using the prediction dataframe")
pred12=results.predict(df2)
pred16=results.predict(df3)
pred18=results.predict(df4)

plt.plot(df2['AGE'],pred12,label='High school')
plt.plot(df2['AGE'],pred16,label='Bachelor')
plt.plot(df2['AGE'],pred18,label='Masters')
plt.plot(mean_income_by_age,'o',alpha=0.5)
plt.xlabel('Age')
plt.ylabel('Income')
plt.legend()
plt.show()


>>>>>>>>>>>>>>>>>>>>>>> Describe the model

formula based:

from statsmodel.formula.api import glm
model=glm(formula,data,family)

array based:
import statsmodels.api as sm
X=sm.add_constant(X)
model=sm.glm(y,X,family)

the course will use the formula based method.

formula argument:
	response~explanatory variable(s)
		output~input(s)

formula='y~x1+x2'

C(x1) : categorical variables
-1: remove intercept

x1:x2: an interaction term between x1 and x2

x1*x2" an interaction term between x1 and x2 and the individual variables

np.log(x1) apply vectorized functions to model variables

>>>>>>>>>>>>>>>>>>. Family Argument

Gaussian (link=sm.families.links.identity) -> default family
Binomial (link=sm.families.links.logit)
1. probit, cauchy, log, cloglog
Poisson (link=sm.families.links.log)
1. identity and sqrt

print(model_GLM.summary())
includes confidence intervals and p scores
coef includes the intercept and coefficient values for the generalized polynomial

>>>>>>>>>>>>>>> coefficients


model_GLM.params   prints the regression coefficients

model_GLM.conf_int() prints confidence intervals

model_GLM.conf(alpha=0.05,cols=None)   alpha is the confidence percent.


>>>>>>>>>>>>>> predictions

model_GLM.predict(test_data)


>>>>>>>>>>>>>>>sample data

The dataset which you will use is on the contamination of groundwater with arsenic in Bangladesh where we want to model the household decision on switching the current well.

switch: 1 if the change of the current well occurred; 0 otherwise
arsenic: The level of arsenic contamination in the well
distance: Distance to the closest known safe well
education: Years of education of the head of the household

http://www.stat.columbia.edu/~gelman/arm/examples/
http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/arsenic_chap5.R


>>>> sample >> setup the formula


model_formula='switch ~ distance'


link_function = sm.families.links.logit()
model_family = sm.families.Binomial(link = link_function)


wells_fit = glm(formula = model_formula, 
                 data = df, 
                 family = model_family).fit()


# View the results of the wells_fit model
print(wells_fit.summary())

# Extract coefficients from the fitted model wells_fit
intercept, slope = wells_fit.params

# Print coefficients
print('Intercept =', intercept)
print('Slope =', slope)

# Extract and print confidence intervals
print(wells_fit.conf_int())


>>>>>>>>>>>>>>>>>>>>logistic regression

binary response data: two class category 0 or 1

examples:
credit scoring -> default/non-default
passing a test -> pass/fail
fraud detection -> fraud/non-fraud
choice of a product -> product abc/product xyz


binary data can appear in two forms: ungrouped and grouped

ungrouped: single event, flip of a coin, two possible outcomes 0 or 1  Bernoulli(p) or Binomial(n=1,p)

grouped occurs with multiple events occurring at the same time, flipping multiple coins, number of successes in a given n number of trials  

binomial(n,p)

>>>>>>>>>>>>>Odds and odds ratio

odds = event occurring/ event not occurring

odds ratio = odd1/odd2

for example

win win win lose

odds = win win win/lose  3 to 1

odds are not probabilities

probabilities are frequencies

odds = probability / (1-probability)

probability = odds/(1- odds)


step 1. probability model

E(y) = u = P(y=1) = B0 + b1x1

step 2. logistic function

f(z) = 1/ ( 1+ exp(-z))


u= 1/ (1+exp(-(b0+b1x1))= exp(b0+b1x1)/1+exp(b0+b1x1)

1-u = 1/(1+exp(b0+b1x1))

odds= u/(1-u) = exp(B0+b1x1)

log transformation -> logistic regression

	logit(u) =log(u/1-u) = B0 + b1x1


>>>> logistic regression >>> multi variant

http://www.science.smith.edu/~jcrouser/SDS293/labs/lab4-py.html

>>>>>>intrepreting coefficients


coef: intercept and weight

Beta > 0
coefficient beta is increasing for arsenic and switch  (ascending curve)

and

Beta < 0
coefficient beta is decreasing for distance and switch (descending curve)

>>>>>>>>linear vs logistic

coef
Intercept=-3.6947
Weight=1.8151

linear:

glm('y ~ weight,
	data=crab,
	family=sm.families.Gaussian())

14+0.32*weight

For every one-unit increase in weight by 0.32

logistic:

glm('y~ weight',
	data=crab,
	family=sm.families.Binomial())
log(odds) -3.69 + 1.8*weight

for every one unit increase in weight by 1.8


>>>> log odds interpretation

odds = u/(1-u) where u is probablity

log(u/(1-u))=B0 +b1x1

increase x by one-unit
log(u/(1-u))=B0 +b1(x1+1)=B0+b1x1+b1

take the exponental
log(u/(1-u))=exp(B0+b1x1)exp(B1)

conclusion ->the odds are multipled by exp(b1)		


>>>>> sample

log(odds) -3.69 + 1.8*weight

the odds of satelittle crab multiply by exp(1.8151) =6.14 for an unit increase in weight

baseline odds:
exp(-3.6947) = 0.0248 are the odds when weight=0

too calculate the rate of change on probability, we calculate the slope at x

where slow b x u(1-u) where u is probability


x=1.5

intercept, slope = model_GLM.params

probability = odds/(1- odds)

est_prob=np.exp(intercept + slope * x) / (1 + np.exp(intercept + slope * x))

#compute the incremental change in estimated probablity given x

ic_prob = slope * est_prob * (1-est_prob)


>>>>>> change in probabiltiy

x_values=np.linspace(.1,10,50)
intercept, slope = wells_fit.params

ic_prob=[]
for x in x_values:
    est_prob=np.exp(intercept + slope * x) / (1 + np.exp(intercept + slope * x))

    #compute the incremental change in estimated probablity given x
    ic_prob.append(slope * est_prob * (1-est_prob))
    
plt.plot(ic_prob)
plt.ylabel("Change in probability")
plt.xlabel("Arsenic levels")
plt.show()


>>>>>>>>>>>sample  >>> switch affect on the odds by distance

# Load libraries and functions
import statsmodels.api as sm
from statsmodels.formula.api import glm
import numpy as np

# Fit logistic regression model
model_GLM = glm(formula = "switch ~ distance100",
                data = wells,
                family = sm.families.Binomial()).fit()

# Extract model coefficients
print('Model coefficients: \n', model_GLM.params)

# Compute the multiplicative effect on the odds
print('Odds: \n', np.exp(model_GLM.params))

Model coefficients: 
     Intercept      0.605959
    distance100   -0.621882
    dtype: float64
    Odds: 
     Intercept      1.833010
    distance100    0.536933
    dtype: float64


The odds of switching the well is 1/2 for a 1-unit (100m) increase in distance, so for every one switch (household switches to the nearest safe well) there would be 2 households who would not switch to the nearest safe well.

>>>>>> sample  >>>> intercept and slope

# Define x at 1.5
x = 1.5

# Extract intercept & slope from the fitted model
intercept, slope = wells_GLM.params

output:
0.6059593596187388 -0.6218819312605801



<<<<<< get the probability at x

# Define x at 1.5
x = 1.5

# Compute and print the estimated probability
est_prob = np.exp(intercept + slope * x) / (1 + np.exp(intercept + slope * x))

print('Estimated probability at x = 1.5: ', round(est_prob, 4))


slope_tan = slope * est_prob * (1 - est_prob)
print('The rate of change in probability: ', round(slope_tan,4))


output:
Estimated probability at x = 1.5:  0.419
The rate of change in probability:  -0.1514

So at the distance100 value of 1.5 the estimated probability is 0.419 with the rate of change in the estimated probability of negative 0.1514. This means that for every 1oop m increase in distance100 at the distance100 value of 1.5 the probability of well switch decreases by 15 to 14%.


>>>>>>>>>>>>>Estimation of beta coefficients

where the parameters maximize the probability of the observed data.

the estimation beta coefficient is where the likelihood takes on its maximum value

likelihood is the probability given estimate coefficients

maximum likelihood estimation (mle)

estimated coefficient beta

* log likelihood takes on the maximum value

iteratively reweighted least squares (IRLS)

use
1. std err
2. z value
3. p value
4. confidence intervals


>>>>>>>>>>>>>Standard error

the standard error is the standard deviation
1. the flatter the peak, the location of the of maximum harder to define
2. larger se

whereas sharper peak the location of the maximum is more clearly defined
2. smaller se

standard error
1. the square root of the variance

2. the diagonal of the variance-covariance matrix
print(model_GLM.cov_params())

	  intercept      weight
intercept 0.774762	 -0.325087
weight    -0.325087	0.141983

#compute the standard error for weight  
standard error is the weight coefficient, sqrt

std_error = np.sqrt(0.141803)


>>>>>>>>>significance testing

z-statistic
z=b/se

z is the coefficient divided by the standard error

a z value greater than 2, we say the z value is statistically significant

horseshoe crab model
y~weight

z=1.8151/0.377 =4.819

weight is statistically signficant

variance is used to determine signifance

>>>>>>>>>>> confidence intervals and odds changes
confidence intervals determine the level of certainty on the estimates

[beta - 1.96 * SE, B+1.96 * se]

             coef     std err
intercept    -3.6947  0.880
weight       1.8151   0.377


[1.8151 - 1.96 * 0.377, 1.81151 + 1.96 * 3.77]

[1.07618,2.55402]

shows the change in the log(odds) while maintaining 95 confidence 

using the model_GLM.conf_int())
            0          1
intercept   -5.419897  -1.969555
weight      1.076826   2.553463

0 is the lower bound
1 is the upper bound

confidence intervals for odds

print (np.exp(model_GLM.conf_int()))

             0          1
intercept    0.004428   0.139519
weight       2.9335348  12.851533

an increase in one unit of weight increases the odds by 2.93 at a most 12.85


>>>>> sample

In the video we analyzed the horseshoe crab model by predicting y with weight. In this exercise you will assess the significance of the estimated coefficients but with width as explanatory variable instead. 


>>>>>>>> sample

# Import libraries and th glm function
import statsmodels.api as sm
from statsmodels.formula.api import glm

# Fit logistic regression and save as crab_GLM
crab_GLM = glm('y ~ width', data = crab, family = sm.families.Binomial()).fit()

# Print model summary
print(crab_GLM.summary())

Generalized Linear Model Regression Results                  
    ==============================================================================
    Dep. Variable:                      y   No. Observations:                  173
    Model:                            GLM   Df Residuals:                      171
    Model Family:                Binomial   Df Model:                            1
    Link Function:                  logit   Scale:                          1.0000
    Method:                          IRLS   Log-Likelihood:                -97.226
    Date:                Fri, 12 Feb 2021   Deviance:                       194.45
    Time:                        13:05:48   Pearson chi2:                     165.
    No. Iterations:                     4   Covariance Type:             nonrobust
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    Intercept    -12.3508      2.629     -4.698      0.000     -17.503      -7.199
    width          0.4972      0.102      4.887      0.000       0.298       0.697
    ==============================================================================



In [1]:

width          0.4972 

Yes, the estimate is positive meaning that the fit is upward sloping which means that width increases the chance of a satellite



>>>>> sample >>> covariance and std error and z

# Extract coefficients
intercept, slope = crab_GLM.params

# Estimated covariance matrix: crab_cov
crab_cov = crab_GLM.cov_params()
print(crab_cov)

# Compute standard error (SE): std_error
std_error = np.sqrt(crab_cov.loc['width', 'width'])
print('SE: ', round(std_error, 4))

# Compute Wald statistic
wald_stat = slope/std_error
print('Wald statistic: ', round(wald_stat,4))


OUTPUT:
Intercept     width
    Intercept   6.910158 -0.266848
    width      -0.266848  0.010350
    SE:  0.1017
    Wald statistic:  4.8875


the probabiliy increase 1% for every one unit of width
the variable is statistically significant
and it may be approaching a maximum

>>>>>>>>>>>confidence interval

# Extract and print confidence intervals
print(crab_GLM.conf_int())


confidence interval of log(odd)
 0         1
Intercept -17.503010 -7.198625
width       0.297833  0.696629

exponential

print(conf_int()))
                      0         1
Intercept  2.503452e-08  0.000748
width      1.346936e+00  2.006975




upper_conf=np.exp(.4972+1.96*0.1017)
print(upper_conf)

lower_conf=np.exp(.4972-1.96-0.1017)
print(lower_conf)


lower_conf -> 1.34
upper_conf -> 2.00

We can conclude that a 1 cm increase in width of a female crab has at least 35% increase odds (from lower bound) and at most it doubles the odds (from upper bound) that a satellite crab is present.

print((1-(lower_conf/upper_conf))*100) -> 32.8%



>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Model coefficients

coefficient beta represent the increase or decrease of the sigmoid curve

b>0 -> ascending curve

b<0 -> descending curve



>>>>>>>>>>>>>>>>>>>>computing and describing predictions

probabilities
or
classes

x,y -> fitted model -> predictions

model_GLM.predict(exog=new_data)

compute model predictions for dataset new_data

probability > 0.5 >= yes

probabilty < 0.5 < no

crab['fitted']=model.fittedvalues.values

cut_off=0.4

crab['pred_class'] = np.where(crab['fitted']>cut_off,1,0)

crab['pred_class'].value_counts()

performance is measured by a confusion matrix

TN FP
FN TP

print(pd.crosstab(y_actual, y_predicted,
  rownames['Actual'], colnames=['Predicted'],
  margins=True))


>>>>>>>>>>>>>>sample >>>> plot the switch inter terms of arsenic with logistic regression

# Plot distance and switch and add overlay with the logistic fit
sns.regplot(x = 'arsenic', y = 'switch', 
            y_jitter = 0.03,
            data = wells, 
            logistic = True,
            ci = None)

# Display the plot
plt.show()


# Compute predictions for the test sample wells_test and save as prediction
prediction = wells_fit.predict(exog = wells_test)

# Add prediction to the existing data frame wells_test and assign column name prediction
wells_test['prediction'] = prediction

print(prediction)

# Examine the first 5 computed predictions
print(wells_test[['switch','arsenic','prediction']].head())


# Define the cutoff
cutoff = 0.5

# Compute class predictions: y_prediction
y_prediction = np.where(prediction > cutoff, 1, 0)

print(y_prediction)

# Compute class predictions y_pred
y_prediction = np.where(prediction > cutoff, 1, 0)


>>>>> use the cutoff to generate the confusion matrix

# Assign actual class labels from the test sample to y_actual
y_actual = wells_test['switch']

# Compute and print confusion matrix using crosstab function
conf_mat = pd.crosstab(y_actual, y_prediction, 
                       rownames=['Actual'], 
                       colnames=['Predicted'], 
                       margins = True)
                      
# Print the confusion matrix
print(conf_mat)


>>>>>>>>>>>>>>count data and poisson distribution

count the number of occurrences in a specified unit of time, distance, area, or volume

1. number of goals in a soccer match
2. number of earthquakes
3. number of crab satelittles
4. number of awards won by a person
5. number of bike crossing over the bridge

poisson random variable
1. events occur independently and randomly

P(y)= l**ye**(-l)/y!

l=mean and variance

y is always position, discrete not continueous, and lower bound at zero, but no upper bound


import seaborn as sns
sns.distplot('y')

>>>>>>>>>>>poisson regression model

response variable

y ~ poisson(l)

mean of the response

E(y) = l

Poisson regression model

log(l) = B0 + b1X1

contineous and or categorical -> poisson regression model
categorical -> log-linear model

import statsmodel.api as sm
from statsmodels.formula.api import glm

glm('y~x', data=my_data,
	family=sm.families.Poisson())


>>>>>>>> distplot >>> skewed distribution

# Import libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Plot sat variable
sns.distplot(crab['sat'])

# Display the plot
plt.show()

>>>>>>>> GLM poisson sats by weight of the crab

# Import libraries
import statsmodels.api as sm
from statsmodels.formula.api import glm


# Fit Poisson regression of sat by weight
model = glm('sat ~ weight', data =crab, family = sm.families.Poisson()).fit()

# Display model results
print(model.summary())

 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                    sat   No. Observations:                  173
Model:                            GLM   Df Residuals:                      171
Model Family:                 Poisson   Df Model:                            1
Link Function:                    log   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -458.08
Date:                Fri, 12 Feb 2021   Deviance:                       560.87
Time:                        23:04:18   Pearson chi2:                     536.
No. Iterations:                     5   Covariance Type:             nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.4284      0.179     -2.394      0.017      -0.779      -0.078
weight         0.5893      0.065      9.064      0.000       0.462       0.717
==============================================================================


In [1]:

crab  sat  y  weight  width  color  spine         width_C
0       1    8  1   3.050   28.3      2      3  [28.25, 29.25)
1       2    0  0   1.550   22.5      3      3    [0.0, 23.25)
2       3    9  1   2.300   26.0      1      1  [25.25, 26.25)
3       4    0  0   2.100   24.8      3      3  [24.25, 25.25)
4       5    4  1   2.600   26.0      3      3  [25.25, 26.25)
5       6    0  0   2.100   23.8      2      3  [23.25, 24.25)
6       7    0  0   2.350   26.5      1      1  [26.25, 27.25)
7       8    0  0   1.900   24.7      3      2  [24.25, 25.25)
8       9    0  0   1.950   23.7      2      1  [23.25, 24.25)
9      10    0  0   2.150   25.6      3      3  [25.25, 26.25)
10     11    0  0   2.150   24.3      3      3  [24.25, 25.25)
11     12    0  0   2.650   25.8      2      3  [25.25, 26.25)
12     13   11  1   3.050   28.2      2      3  [27.25, 28.25)
13     14    0  0   1.850   21.0      4      2    [0.0, 23.25)
14     15   14  1   2.300   26.0      2      1  [25.25, 26.25)
15     16    8  1   2.950   27.1      1      1  [26.25, 27.25)
16     17    1  1   2.000   25.2      2      3  [24.25, 25.25)
17     18    1  1   3.000   29.0      2      3  [28.25, 29.25)
18     19    0  0   2.200   24.7      4      3  [24.25, 25.25)
19     20    5  1   2.700   27.4      2      3  [27.25, 28.25)
20     21    4  1   1.950   23.2      2      2    [0.0, 23.25)
21     22    3  1   2.300   25.0      1      2  [24.25, 25.25)
22     23    1  1   1.600   22.5      2      1    [0.0, 23.25)
23     24    2  1   2.600   26.7      3      3  [26.25, 27.25)
24     25    3  1   2.000   25.8      4      3  [25.25, 26.25)
25     26    0  0   1.300   26.2      4      3  [25.25, 26.25)
26     27    3  1   3.150   28.7      2      3  [28.25, 29.25)
27     28    5  1   2.700   26.8      2      1  [26.25, 27.25)
28     29    0  0   2.600   27.5      4      3  [27.25, 28.25)
29     30    0  0   2.100   24.9      2      3  [24.25, 25.25)
..    ...  ... ..     ...    ...    ...    ...             ...
143   144    0  0   2.000   24.3      2      1  [24.25, 25.25)
144   145    0  0   2.400   25.8      2      3  [25.25, 26.25)
145   146    8  1   2.100   25.0      4      3  [24.25, 25.25)
146   147    4  1   3.725   31.7      2      1   [29.25, 34.5)
147   148    4  1   3.025   29.5      2      3   [29.25, 34.5)
148   149   10  1   1.900   24.0      3      3  [23.25, 24.25)
149   150    9  1   3.000   30.0      2      3   [29.25, 34.5)
150   151    4  1   2.850   27.6      2      3  [27.25, 28.25)
151   152    0  0   2.300   26.2      2      3  [25.25, 26.25)
152   153    0  0   2.000   23.1      2      1    [0.0, 23.25)
153   154    0  0   1.600   22.9      2      1    [0.0, 23.25)
154   155    0  0   1.900   24.5      4      3  [24.25, 25.25)
155   156    4  1   1.950   24.7      2      3  [24.25, 25.25)
156   157    0  0   3.200   28.3      2      3  [28.25, 29.25)
157   158    2  1   1.850   23.9      2      3  [23.25, 24.25)
158   159    0  0   1.800   23.8      3      3  [23.25, 24.25)
159   160    4  1   3.500   29.8      3      2   [29.25, 34.5)
160   161    4  1   2.350   26.5      2      3  [26.25, 27.25)
161   162    3  1   2.275   26.0      2      3  [25.25, 26.25)
162   163    8  1   3.050   28.2      2      3  [27.25, 28.25)
163   164    0  0   2.150   25.7      4      3  [25.25, 26.25)
164   165    7  1   2.750   26.5      2      3  [26.25, 27.25)
165   166    0  0   2.200   25.8      2      3  [25.25, 26.25)
166   167    0  0   1.800   24.1      3      3  [23.25, 24.25)
167   168    2  1   2.175   26.2      3      3  [25.25, 26.25)
168   169    3  1   2.750   26.1      3      3  [25.25, 26.25)
169   170    4  1   3.275   29.0      3      3  [28.25, 29.25)
170   171    0  0   2.625   28.0      1      1  [27.25, 28.25)
171   172    0  0   2.625   27.0      4      3  [26.25, 27.25)
172   173    0  0   2.000   24.5      2      2  [24.25, 25.25)
 


>>>>>>>>>>>interpreting model fit

parameter estimation

1. maximum likelihood estimation (mle)
2. iteratively reweighted least squares (irls)


poisson regression model

log (l) = B0+B1*X1

l=exp(B0) + exp(b1x1))

exp(b0) when x=0 is the intercept

exp(B1)
1. the multiplicative effect on the mean l for 1 unit increase in x


interpreting the coefficient effedt

if b1>0
  exp(b1) >1
  l is exp(b1) time larger than when x=0

if b1<0
   exp(b1 < 1
   l is exp(b1) times smaller than when x=0

if b1=0
    exp(b1)=0
   l=exp(b0)
   multiplicative factor is 1
   y and x are not related

  
model.params
1. intercept -0.428405
2. weight     0.589304

B1>0

np.exp(0.589304) -> 1.803

for each unit of l multiple it by 1.803

80% increase in the number of satelites

confidence interval

print(model.conf_int())

intercept -0.779112 -0.077699
weight    0.461873   0.716735


print(np.exp(model.conf_int()))

intercept 0.458813 0.925243
weight    1.587044 2.047737

for every on unit increase in weigth the satelites will increase 1.58704 times and at most 2.047737 times.


>>>>>>>> sample >>>> glm poisson

# Import libraries
import statsmodels.api as sm
from statsmodels.formula.api import glm

# Fit Poisson regression of sat by width
model = glm('sat ~ width', data = crab, family = sm.families.Poisson()).fit()

# Display model results
print(model.summary())

Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                    sat   No. Observations:                  173
Model:                            GLM   Df Residuals:                      171
Model Family:                 Poisson   Df Model:                            1
Link Function:                    log   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -461.59
Date:                Sat, 13 Feb 2021   Deviance:                       567.88
Time:                        10:33:03   Pearson chi2:                     544.
No. Iterations:                     5   Covariance Type:             nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -3.3048      0.542     -6.095      0.000      -4.368      -2.242
width          0.1640      0.020      8.216      0.000       0.125       0.203
==============================================================================



In [1]:


>>>>>>> sample >>> calculate the estimated lambda

# Compute average crab width
mean_width = np.mean(crab['width'])

# Print the compute mean
print('Average width: ', round(mean_width, 3))

# Extract coefficients
intercept, slope = model.params

# Compute the estimated mean of y (lambda) at the average width
est_lambda = np.exp(intercept) * np.exp(slope * mean_width)

# Print estimated mean of y
print('Estimated mean of y at average width: ', round(est_lambda, 3))

output:

Average width:  26.299
Estimated mean of y at average width:  2.744

Great work! The Poisson regression model states that at the mean value of female crab width of 26.3 the expected mean number of satellite crabs present is 2.74.

>>>>>>> the multiplicative effect
 
# Extract coefficients
intercept, slope = model.params

# Compute and print the multiplicative effect
print(np.exp(slope))

output
1.1782674386452303

To conclude a 1-unit increase in female crab width the number of satellite crabs will increase, which will be multiplied by 1.18.

>>>>>>>>>confidence interval

# Compute confidence intervals for the coefficients
model_ci = model.conf_int()

# Compute and print the confidence intervals for the multiplicative effect on the mean
print(np.exp(model_ci))

 0         1
Intercept  0.012683  0.106248
width      1.133051  1.225289

The multiplicative effect on the mean response for a 1-unit increase in width is at least 1.13 and at most 1.22.

see the multiplicative effect above.  it should fall within the confidence interval range

lambda represents the mean and the variance

>>>>>>>>>>>>>>> The problem of overdispersion

what if the variance is not equal to the mean


y_mean = crab['sat'].mean()
2.919
y_variance = crab['sat'].var()
9.912

the variance is 3 times the mean

this affect is called overdispersion

under dispersion occurs when the variance < mean

consequences:
1. small standard errors
2. small p-value


we can measure dispersion using the pearson coefficient
pearson chi2 and df residuals

ratio=crab_fit.pearson_chi2/crab_fit.df_resid

print(ratio)

ratio = 1 > approximately poisson
ratio < 1 underdispersion
ratio > 1 overdispersion

negative binomial regression

import statsmodels.api as sm
from statsmodels.formula.api import glm

model = glm('y ~ x', data = my_data,
	family = sm.families.NegativeBinomal(alpha=1)).fit()



# Compute and print sample mean of the number of satellites: sat_mean
sat_mean = np.mean(crab.sat)

print('Sample mean:', round(sat_mean, 3))

# Compute and print sample variance of the number of satellites: sat_var
sat_var = np.var(crab.sat)
print('Sample variance:', round(sat_var, 3))

# Compute ratio of variance to mean
print('Ratio:', round(sat_var/sat_mean, 3))

Sample mean: 2.919
Sample variance: 9.855
Ratio: 3.376

over dispersion

Notice that the variance is 3.37 times the mean.

>>>>>>>>sample

# Expected number of zero counts
exp_zero_cnt = ((sat_mean**0)*np.exp(-sat_mean))/math.factorial(0)

# Print exp_zero_counts
print('Expected zero counts given mean of ', round(sat_mean,3), 
      'is ', round(exp_zero_cnt,3)*100)

# Number of zero counts in sat variable
actual_zero_ant = sum(crab['sat'] == 0)

# Number of observations in crab dataset
num_obs = len(crab)

# Print the percentage of zero count observations in the sample
print('Actual zero counts in the sample: ', round(actual_zero_ant / num_obs,3)*100)


# Print the percentage of zero count observations in the sample
print('Actual zero counts in the sample: ', round(actual_zero_ant / num_obs,3)*100)
Expected zero counts given mean of  2.919 is  5.4
Actual zero counts in the sample:  35.8



Notice that given the mean parameter there should be 5.4% observations with zero count, but in the crab sample there are 35.8% observations with zero count, indicating the presence of overdispersion.


print(crab_pois.pearson_chi2 / crab_pois.df_resid)

3.182204743877364
 
over dispersion

There is overdispersion present since the ratio is greater than 1, meaning that the coefficient estimates should not be interpreted directly

>>>>>>>>>>>> Negative Binomal compared with Poisson



# Define the formula for the model fit
formula = 'sat ~ width'

# Fit the GLM negative binomial model using log link function
crab_NB = smf.glm(formula = formula, data = crab, 
				  family = sm.families.NegativeBinomial()).fit()

# Print Poisson model's summary
print(crab_NB.summary())

# Print the negative binomial model's summary
print(crab_pois.summary())



Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                    sat   No. Observations:                  173
Model:                            GLM   Df Residuals:                      171
Model Family:        NegativeBinomial   Df Model:                            1
Link Function:                    log   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -375.80
Date:                Sat, 13 Feb 2021   Deviance:                       206.41
Time:                        18:40:52   Pearson chi2:                     155.
No. Iterations:                     6   Covariance Type:             nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -4.0323      1.129     -3.572      0.000      -6.245      -1.820
width          0.1913      0.042      4.509      0.000       0.108       0.274
==============================================================================



                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                    sat   No. Observations:                  173
Model:                            GLM   Df Residuals:                      171
Model Family:                 Poisson   Df Model:                            1
Link Function:                    log   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -461.59
Date:                Sat, 13 Feb 2021   Deviance:                       567.88
Time:                        18:40:52   Pearson chi2:                     544.
No. Iterations:                     5   Covariance Type:             nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -3.3048      0.542     -6.095      0.000      -4.368      -2.242
width          0.1640      0.020      8.216      0.000       0.125       0.203
==============================================================================



Notice how standard error increased to 0.042, reflecting overdispersion which was not captured with the Poisson model.


# Compute confidence intervals for crab_Pois model
print('Confidence intervals for the Poisson model')
print(crab_pois.conf_int())

# Compute confidence intervals for crab_NB model
print('Confidence intervals for the Negative Binomial model')
print(crab_NB.conf_int())

Poisson

 0         1
Intercept -4.367531 -2.241983
width      0.124914  0.203176
Confidence intervals for the Negative Binomial model

Negative Bionomial
                  0         1
Intercept -6.244509 -1.820000
width      0.108155  0.274472


>>>>>>>>>>>>>>plotting a regresion model




plt.subplots(figsize=(8,5))

sns.regplot('width','sat',
data=crab_df,
fit_reg=False,
y_jitter=0.3
)

sns.regplot('width,'sat',
data=crab_df,
fity_reg=True,
y_jitter=0.3,
line_kws={'color':'green',
'label':'LM fit'}
) 


crab_df['fit_values']=model.fittedvalues

sns.scatterplot('width','fit_values',
	data=crab_df,
	color='red',
	label='Poisson')


>>>>>>>>>>>> sample regplot

# Import libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Plot the data points and linear model fit
sns.regplot('width','sat', data = crab,
            y_jitter = 0.3,
            fit_reg = True,
            line_kws = {'color':'green', 
                        'label':'LM Fit'})

# Print plot
plt.show()


# Add fitted values to the fit_values column of crab dataframe
crab['fit_values'] = model.fittedvalues

# Poisson regression fitted values
sns.scatterplot('width','fit_values', data = crab,
           color = 'Red', label = 'Poisson')

# Print plot          
plt.show()

Great, now you can compare both fits on one graph! Similary as for the model with weight variable the linear and Poisson fits are close in the mid range of width values, but diverge on smaller and larger values.


>>>>>>>>>>>>>>>>>>>>>>>>>>>Multiple linear regression


logit(y)=B0+B1X1

consider you have additional variables

logit(y)=B0+B1X1+B2X2+...BpXp

the betas affect log(odds) where y=1

the np.exp(betas) have a multiplicative affect on the odds


model=glm('y ~ x1+x2+x3+x4',
	data=my_data,
	family=sm.families.Binomial()).fit()

formula='switch ~ distance100 + arsenic'

model=glm(formula,
	data=my_data,
	family=sm.families.Binomial()).fit()


model.summary

z:
distance100 -8.59
arsenic 11.13

both coefficients are statistically significant with P values less than 5%

switching the well is less like the further the distance
switching is high if the well is high in arsenic

coeff
distance100 -0.89
arsenic 0.4608

a unit-change in distance 100 corresponds to a negative difference of 0.89 in the logit

a unit-change in arsenic corresponds to a positive difference of 0.46 in the logit

what is the impact of adding an additional coefficient to the model?


multicollinearity occurs when variables are correlated with each other.

the higher the correlation the more structure is presence
>>>>>>>>>>>>>>>>>collinearity
presence of collinearity?
1. coefficients is not significant, but variable is highly correlated with y

2. adding/removing a varaible significantly changes coefficients

3. not logical sign of the coefficient

4. variables have high pairwise correlation

>>>>the variance inflation factor (VIF)

it describes how inflated the variance of the coefficient is

VIF > 2.5

from statsmodels.stats.outliers_influence 
import variance_inflation_factor


Recall from the video that the rule of thumb threshold is VIF at the level of 2.5, meaning if the VIF is above 2.5 you should consider there is effect of multicollinearity on your fitted model.



>>>>>>> multi variant

# Import statsmodels
import statsmodels.api as sm
from statsmodels.formula.api import glm

# Define model formula
formula = 'y ~ width+color'

# Fit GLM
model = glm(formula, data = crab, family = sm.families.Binomial()).fit()

# Print model summary
print(model.summary())


From model summary note that for each one-level increase in color of the female crab, the estimated odds multiply by exp(-0.509)=0.6, i.e. the odds for dark crabs are 60% than those for medium 


>>>> width + weight

# Import statsmodels
import statsmodels.api as sm
from statsmodels.formula.api import glm

# Define model formula
formula = 'y ~ weight + width'

# Fit GLM
model = glm(formula, data= crab, family = sm.families.Binomial()).fit()

# Print model summary
print(model.summary())

coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -9.3547      3.528     -2.652      0.008     -16.270      -2.440
weight         0.8338      0.672      1.241      0.214      -0.483       2.150
width          0.3068      0.182      1.686      0.092      -0.050       0.663
=====================


multiplicative effect

Intercept    0.000087
weight       2.302031
width        1.359054


for every one unit increase the weight has 2.3 increase in odds
and the width a 1.35 increase

neither is statistically signficant.tThere is evident presence of multicollinearity! Let's measure it in the next exercise.


>>>>>>> measuring collinearity

# Import functions
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Get variables for which to compute VIF and add intercept term
X = crab[['weight','width','color']]
X['Intercept'] = 1

# Compute and view VIF
vif = pd.DataFrame()
vif["variables"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# View results using print
print(vif)

variables         VIF
0     weight    4.691018
1      width    4.726378
2      color    1.076594
3  Intercept  414.163343

weight and width are collinear and color is not


With VIF well above 2.5 for weight and width means that there is multicollinearity present in the model and you can not use both variables in the model.

>>>>>>>>>> Comparing models

how to determine if single variable model compares to the multivariant model

deviance tests the null hypothesis that the multivariable model is more correct

goodness of fit measures if the model is more accurate

d=-2LL(B)

-2 times the log life of the model fit

lower deviance means better model fit

null deviance is the deviance of only the intercept term

evaluate
1. adding a random noise variable would, on average, decrease deviance by 1

2. adding p predictors to the model deviance should decrease by more than p

print (model.null_deviance)

print (model.deviance)

4118.0992 null deviance
4076.23768 deviance

reduction by 41.86
therefore distance100 improved the fit

print(-2*model.llf)
4076.2378 compute deviance from log life

>>>>>>>>>>>model complexity

model_1 and model_2

L1>L2  (may be a better fit)

consider the number of parameters higher in model_2

model_2 could be overfitting the training set

overfitting means that model_2 is not generalizing well for unseen data

reducing model complexity can reduce overfitting and improve generalization


>>>>>>>>>>>.sample deviance to measure overfitting

# Import statsmodels
import statsmodels.api as sm
from statsmodels.formula.api import glm

# Define model formula
formula = 'switch ~ distance100+arsenic'

# Fit GLM
model_dist_ars = glm(formula, data = wells, family = sm.families.Binomial()).fit()

# Compare deviance of null and residual model
diff_deviance = model_dist_ars.null_deviance - model_dist_ars.deviance

# Print the computed difference in deviance
print(diff_deviance)

output:
187.43094886399467

Having both distance100 and arsenic in the model reduces deviance by 187 compared to the intercept only model.


>>>>>>> compare deviance for each feature

# Compute the difference in adding distance100 variable
diff_deviance_distance = model_dist.null_deviance - model_dist.deviance

# Print the computed difference in deviance
print('Adding distance100 to the null model reduces deviance by: ', 
      round(diff_deviance_distance,3))

# Compute the difference in adding arsenic variable
diff_deviance_arsenic = model_dist.deviance - model_dist_ars.deviance

# Print the computed difference in deviance
print('Adding arsenic to the distance model reduced deviance further by: ', 
      round(diff_deviance_arsenic,3))



Adding distance100 to the null model reduces deviance by:  41.861
Adding arsenic to the distance model reduced deviance further by:  145.57

Adding distance100 to the null model reduces deviance by 41.9 and with an addition of arsenic the deviance further reduces by 145. Having such large reduction than expected reduction by 1 we can conclude that the multivariate model has improved the model fit.

>>>>>>>>>>>>>>>Model forumla

from patsy import dmatrix

dmatrix('x1 + x2')

variable transformation

formula='y ~ x1 + np.log(x2)'

#centering and standardizing removing noise
formua="y~ center(x1) + standardize(x2)"

formula="y~ my_owntransformation(x2)"

x1=np.array([1,2,3])
x2=np.array([4,5,6])

dmatrix('I(x1+x2)')

if np.array is missing then it add concatenation not element wise addition


dmatrix('color',data=crab)

crab['color'].value_counts()

2 95
3 44
4 22
1 12

we see four levels

dmatrix('C(color), data=crab)

we can see the four levels in the resulting matrix


>>>>>>>>>>>>>>>. dmatrix

# Import function dmatrix()
from patsy import dmatrix

# Construct model matrix with arsenic
model_matrix = dmatrix('arsenic', data = wells, return_type = 'dataframe')
print(model_matrix.head())


>>>>>>>> dmatrix arsenic and distance 100

# Import function dmatrix()
from patsy import dmatrix

# Construct model matrix with arsenic and distance100
model_matrix = dmatrix('arsenic + distance100', data = wells, return_type = 'dataframe')
print(model_matrix.head())


>>>>> log to formula

# Import function dmatrix
import numpy as np
from patsy import dmatrix

# Construct model matrix for arsenic with log transformation
dmatrix('np.log(arsenic)', data = wells,
       return_type = 'dataframe').head()


>>>>>>> glm with log of arsenic

# Import statsmodels
import statsmodels.api as sm
from statsmodels.formula.api import glm
import numpy as np

# Define model formula
formula = 'switch ~ np.log(arsenic)'

# Fit GLM
model_log_ars = glm(formula, data = wells, 
                     family = sm.families.Binomial()).fit()

# Print model summary
print(model_log_ars.summary())



=================================================================================
                      coef    std err          z      P>|z|      [0.025      0.975]
-----------------------------------------------------------------------------------
Intercept           0.0962      0.041      2.325      0.020       0.015       0.177
np.log(arsenic)     0.7076      0.064     11.050      0.000       0.582       0.833
==========



diff_deviance = model_dist_ars.null_deviance - model_dist_ars.deviance

diff_deviance=model_log_ars.null_deviance - model_log_ars.deviance
print(diff_deviance)

28.81139965355123

>>>>>>>>>> categorical

# Import function dmatrix
from patsy import dmatrix

# Construct and print model matrix for color as categorical variable
print(dmatrix('C(color)', data =crab,
     	   return_type = 'dataframe').head())


>>>>>>>> categorical color and treatment

# Import function dmatrix
from patsy import dmatrix

# Construct and print the model matrix for color with reference group 3
print(dmatrix('C(color, Treatment(3))', 
     	  data = crab,
     	  return_type = 'dataframe').head())

Treatment(3))[T.1]  C(color, Treatment(3))[T.2]  C(color, Treatment(3))[T.4]
0        1.0                          0.0                          1.0                          0.0
1        1.0                          0.0                          0.0                          0.0
2        1.0                          1.0                          0.0                          0.0
3        1.0                          0.0                          0.0                          0.0
4        1.0                          0.0                          0.0                          0.0


analyzes color against treatment group 3

Notice the change in columns where now the medium dark category is the reference group, where its mean behavior is represented by the intercept.


>>>>>>>>> categorical and interaction terms present

logistic regression explains the relationship between the categorical data and the binary output

nominial variable order is not important
ordinal variable order is important

Logistic Model:
logit(y=1/X) = B0 + b1X1+B2X2

where y equal 1 conditional on X where X represents explanatory variables X1 and X2

suppose X1=0 then estimate B0 and B2 terms

if the lines are not parallel then there is the presence of interaction

parallel where the effect of x1 on y depends on the level of x2 and vice versa

logit(y=1|X) = b0+b1x1+b2x2 + b3x1x2


b3 is a new variable for allowing interactions

X1=0  -> B0 + b2x2
x1=1 -> (b0+b1) +(b2+b3)x2

b1 is the difference between the two intercepts
b3 is the difference between the two slopes

>>>>>>>>>>>>>>> sample  sat and categorical color and treatment

# Construct model matrix
model_matrix = dmatrix('C(color, Treatment(4))' , data = crab, 
                       return_type = 'dataframe')

# Print first 5 rows of model matrix dataframe
print(model_matrix.head())

# Fit and print the results of a glm model with the above model matrix configuration
model = glm('y ~ C(color, Treatment(4))', data = crab, 
            family = sm.families.Binomial()).fit()

print(model.summary())


Intercept  C(color, Treatment(4))[T.1]  C(color, Treatment(4))[T.2]  C(color, Treatment(4))[T.3]
0        1.0                          0.0                          1.0                          0.0
1        1.0                          0.0                          0.0                          1.0
2        1.0                          1.0                          0.0                          0.0
3        1.0                          0.0                          0.0                          1.0
4        1.0                          0.0                          0.0                          1.0


 coef    std err          z      P>|z|      [0.025      0.975]
-----------------------------------------------------------------------------------------------
Intercept                      -0.7621      0.458     -1.665      0.096      -1.659       0.135
C(color, Treatment(4))[T.1]     1.8608      0.809      2.301      0.021       0.276       3.446
C(color, Treatment(4))[T.2]     1.7382      0.512      3.393      0.001       0.734       2.742
C(color, Treatment(4))[T.3]     1.1299      0.551      2.051      0.040       0.050       2.210
===============================================================================================

logit = -0.7621 + 1.8608*color1 + 1.7382*color2 + 1.1299*color3

color 2 has a 5.86 multiplicative affect on log(odd)


>>>>>>  sample dmatrix and glm

# Construct model matrix
model_matrix = dmatrix('C(color, Treatment(4)) + width' , data = crab, 
                       return_type = 'dataframe')

# Print first 5 rows of model matrix
print(model_matrix.head())

# Fit and print the results of a glm model with the above model matrix configuration
model = glm('y ~ C(color, Treatment(4)) + width', data = crab, 
            family = sm.families.Binomial()).fit()

print(model.summary())

>>>>>>> interaction between the arsenic and distance 100 variables

# Import libraries
import statsmodels.api as sm
from statsmodels.formula.api import glm

# Fit GLM and print model summary
model_int = glm('switch ~ center(distance100) + center(arsenic) + center(distance100):center(arsenic)', 
                data = wells, family = sm.families.Binomial()).fit()

# View model results
print(model_int.summary())

>>>>>>>>>>>> sample crab color


Intercept  C(color, Treatment(4))[T.1]  C(color, Treatment(4))[T.2]  C(color, Treatment(4))[T.3]  width
0        1.0                          0.0                          1.0                          0.0   28.3
1        1.0                          0.0                          0.0                          1.0   22.5
2        1.0                          1.0                          0.0                          0.0   26.0
3        1.0                          0.0                          0.0                          1.0   24.8
4        1.0                          0.0                          0.0                          1.0   26.0

coef    std err          z      P>|z|      [0.025      0.975]
-----------------------------------------------------------------------------------------------
Intercept                     -12.7151      2.762     -4.604      0.000     -18.128      -7.302
C(color, Treatment(4))[T.1]     1.3299      0.853      1.560      0.119      -0.341       3.001
C(color, Treatment(4))[T.2]     1.4023      0.548      2.557      0.011       0.327       2.477
C(color, Treatment(4))[T.3]     1.1061      0.592      1.868      0.062      -0.054       2.267
width                           0.4680      0.106      4.434      0.000       0.261       0.675
===============================================================================================
In [1]:
     

logit=-12.7151 + 1.3299 * color1 + 1.4023 *color2 + 1.1061*color3 + 0.4680 * width


A one-unit increase in width has multiplicative effect of 1.5967 on the odds that the satellite is nearby for all color groups.



>>>>>>>>>>>>>>>>interaction of the variables center(distance100):center(arsenic)

# Import libraries
import statsmodels.api as sm
from statsmodels.formula.api import glm

# Fit GLM and print model summary
model_int = glm('switch ~ center(distance100) + center(arsenic) + center(distance100):center(arsenic)', 
                data = wells, family = sm.families.Binomial()).fit()

# View model results
print(model_int.summary())


coef    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------------------------------
Intercept                               0.3511      0.040      8.810      0.000       0.273       0.429
center(distance100)                    -0.8737      0.105     -8.337      0.000      -1.079      -0.668
center(arsenic)                         0.4695      0.042     11.159      0.000       0.387       0.552
center(distance100):center(arsenic)    -0.1789      0.102     -1.748      0.080      -0.379       0.022
=======================================================================================================
In [1]:



The interaction term decreases the importance of arsenic as explanatory variable given one unit increase in distance100 values.




Model Data-> logistic regression (binary) logit(y) = B0+b1x1  increases log odds by b1
Linear Model -> Continueous -> Poisson Regression (count)   Log(lambda)=B0+B1X1  multiples l by exp(b1)


glm linear model
1. family=sm.families.Gaussian()

glm logistic regression()
2. family=sm.families.Binomal()


glm poisson regression
family=sm.families.Poisson()


other courses:
an introduction to categorical data analysis
regression modeling strategies
apply predictive modeling


horseshoe 173 row dataset
http://users.stat.ufl.edu/~aa/cda/data.html
Here y is whether a female crab has a satellite (1=yes, 0=no) and weight is in grams, rather than kg as in the text. Also, color has values 1-5 with 1=light; there were no crabs of color 1, so in the text, color was re-coded as color - 1 to give values 1, 2, 3, 4.)


