uci machine learning repository

http get requests

BeautifulSoup

urllib to fetch data from the web

urlopen()

from urllib.request import urlretrieve

url="http://https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
urlretrieve(url,"winequalitiy-white.csv")


>>>> Sample   >>>> load wine color red from the repository and load it into a dataframe

# Import package
from urllib.request import urlretrieve
import pandas as pd

url="https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv"

urlretrieve(url,"winequality-red.csv")

df = pd.read_csv('winequality-red.csv', sep=';')
print(df.head())

#plot the acidity

# Plot first column of df
pd.DataFrame.hist(df.ix[:, 0:1])
plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')
plt.ylabel('count')
plt.show()


????????Sample  >>>> read_excel >>> print keys and head

# Import package
import pandas as pd

# Assign url of file: url
url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'

# Read in all sheets of Excel file: xls
xls = pd.read_excel(url, sheet_name=None)

# Print the sheetnames to the shell
print(xls.keys())

# Print the head of the first sheet (using its name, NOT its index)
print(xls['1700'].head())

>>>>>>>>>> httprequest get

from urllib.request import urlopen, Request

url="https://www.wikipedia.org"

request=Request(url)
response= urlopen(request)
html=response.read()
response.close

>>>Import requests

url="https://www.wikipedia.org/"

r=requests.get(url)

text=r.text


>>>>> sample >>> scrap a page using request and urlopen

# Import packages

from urllib.request import urlopen, Request

# Specify the url
url = "https://campus.datacamp.com/courses/1606/4135?ex=2"

# This packages the request: request
request=Request(url)
response= urlopen(request)
html=response.read()

# Sends the request and catches the response: response


# Print the datatype of response
print(type(response))

# Be polite and close the response!
response.close()
print(html)


>>>>>> sample >>>> requests get

import requests

url="http://www.datacamp.com/teach/documentation"

r=requests.get(url)

text=r.text

print(text)

>>>>>>>>>>>>>>>>Scrapping the web with python

beautifulsoup

from urllib.request import urlretrieve,Request,urlopen
from bs4 import BeautifulSoup

url="https://www.woodysmithhyundai.com/"

r=requests.get(url)

text=r.text

soup=BeautifulSoup(text)

print(soup.prettify())

print(soup.title)
print(soup.get_text())

for link in soup.find_all("a"):
	print(link.get("href"))


>>>> Sample >>> parse html  >>> prettify()

# Import packages
import requests
from urllib.request import urlretrieve,Request,urlopen

from bs4 import BeautifulSoup

# Specify url: url
url="https://www.python.org/~guido/"

# Package the request, send the request and catch the response: r

r=requests.get(url)
# Extracts the response as html: html_doc
html=r.text

# Create a BeautifulSoup object from the HTML: soup

soup=BeautifulSoup(html)
# Prettify the BeautifulSoup object: pretty_soup

pretty_soup=soup.prettify()
# Print the response
print(pretty_soup)

>>>>> sample >>> scrape for text and title

# Import packages
import requests
from bs4 import BeautifulSoup

# Specify url: url
url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Extract the response as html: html_doc
html_doc = r.text

# Create a BeautifulSoup object from the HTML: soup
soup=BeautifulSoup(html_doc)

# Get the title of Guido's webpage: guido_title

title=soup.title
# Print the title of Guido's webpage to the shell

print(title)

# Get Guido's text: guido_text

guido_text=soup.get_text()

>>>>> sample >>> print all href links in the page

# Import packages
import requests
from bs4 import BeautifulSoup

# Specify url
url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Extracts the response as html: html_doc
html_doc = r.text

# create a BeautifulSoup object from the HTML: soup
soup = BeautifulSoup(html_doc)

# Print the title of Guido's webpage
print(soup.title)

# Find all 'a' tags (which define hyperlinks): a_tags

a_tags=soup.find_all("a")


for link in a_tags:
    value=str(link.get("href"))
    #print(type(value))
    if "http://" in value:
        print(link.get("href"))


>>>>>>>>>>>>>>>>>>>APIS and JSON
application programming interfaces

open movie database

json : javascript object notation
real-time server-to-browser communication
human readable

json are stored in a dictionary in python

objects: string, value, dictionaries, or arrays

import json

with open("snakes.json", "r") as json_file:
	json_data=json.load(json_file)


print (type(json_data))

for key,value in json_data.items():
	print(key+':'+,value)


>>> Sample >>> open a json file and read the key value pairs

# Load JSON: json_data
with open("a_movie.json") as json_file:
    json_data=json.load(json_file)

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])


>>>>>>>>>>Apis and interacting with the world wide web

Much of your data will be avaialable through apis

import requests

url="http://www.omdbapi.com/?t=hackers'

r=requests.get(url)
json_data=r.json()

for key,value in json_data.items():
	print(key+':',value)

>>>>> sample >>> query title hackers

# Import requests package
import requests

# Assign URL to variable: url
url='http://www.omdbapi.com/?t=hackers&apikey=72bc447a'

# Package the request, send the request and catch the response: r
r = requests.get(url)
json_data=r.json()

# Print the text of the response
for key,value in json_data.items():
	print(key+':',value)


>>>> sample >>> print a part of the json tree

# Import package
import requests

# Assign URL to variable: url
url="https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza"

# Package the request, send the request and catch the response: r
r = requests.get(url)
json_data=r.json()

print(json_data)
# Print the text of the response
for key,value in json_data.items():
	print(key+':',value)
# Decode the JSON data into a dictionary: json_data


# Print the Wikipedia page extract
pizza_extract = json_data['query']['pages']['24768']['extract']

print(pizza_extract)

>>>>>>>>>>>>>>>>>>>>Interacting with twitter

stream data from twitter
1. use filters for incoming tweets
2. api authentication and oauth
3. tweepy python package


Application Settings
1. Consumer key
2. consumer secret
3. access level
4. owner
5. ower id

Your access token
Access Token
Access Token Secret
Access level
Owner
Owner ID

tweets are returned as  jsons

conda install -c conda-forge tweepy

import tweepy, json


access_token="..."
access_token_secret="..."
consumer_key="..."
consumer_secret="..."

auth=tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)


class MyStreamListener(tweepy.StreamListener)
	def __init__(self, api=None):
		super(MyStreamListener, self).__init__()
		self.num_tweets=0
		self.file=open("tweets.txt","w")
	def on_status(self,status):
		tweet=status._json
		self.file.write(json.dumps(tweet)+'\\n')
		tweet_list.append(status)
		self.num_tweets+=1
		if self.num_tweets<100:
			return True
		else
			return False
		self.file.close()

l=MyStreamListener()
stream=tweep.Stream(auth,l)

stream.filter(track=['apples','oranges'])

>>>>>Sample >>> oauth tweet stream

# Import package
import tweepy, json

# Store OAuth authentication credentials in relevant variables
access_token = "1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy"
access_token_secret = "X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx"
consumer_key = "nZ6EA0FxZ293SxGNg8g8aP0HM"
consumer_secret = "fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i"

# Pass OAuth details to tweepy's OAuth handler
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)

# Initialize Stream listener
l = MyStreamListener()

# Create your Stream object with authentication
stream = tweepy.Stream(auth, l)

# Filter Twitter Streams to capture data by the keywords:
stream.filter(['clinton', 'trump', 'sanders', 'cruz'])

>>>> sample >>> tweets.txt >>> load the json 

# Import package
import json

# String of path to file: tweets_data_path

tweets_data_path="tweets.txt"
# Initialize empty list to store tweets: tweets_data
tweets_data=[]


# Open connection to file
tweets_file = open(tweets_data_path, "r")

# Read in tweets and store in list: tweets_data
for line in tweets_file:
    tweet=json.loads(line)
    tweets_data.append(tweet)
    

# Close connection to file
tweets_file.close()

# Print the keys of the first tweet dict
print(tweets_data[0].keys())




dict_keys(['in_reply_to_user_id', 'created_at', 'filter_level', 'truncated', 'possibly_sensitive', 'timestamp_ms', 'user', 'text', 'extended_entities', 'in_reply_to_status_id', 'entities', 'favorited', 'retweeted', 'is_quote_status', 'id', 'favorite_count', 'retweeted_status', 'in_reply_to_status_id_str', 'in_reply_to_user_id_str', 'id_str', 'in_reply_to_screen_name', 'coordinates', 'lang', 'place', 'contributors', 'geo', 'retweet_count', 'source'])


# Import package
import pandas as pd

# Build DataFrame of tweet texts and languages
df = pd.DataFrame(tweets_data, columns=["user","text"])

# Print head of DataFrame
print(df.head())


>>>>> Sample  >>> count by Word in text

# Initialize list to store tweet counts
[clinton, trump, sanders, cruz] = [0, 0, 0, 0]

# Iterate through df, counting the number of tweets in which
# each candidate is mentioned
for index, row in df.iterrows():
    clinton += word_in_text('clinton', row['text'])
    trump += word_in_text('trump',row['text'])
    sanders += word_in_text('sanders', row['text'])
    cruz += word_in_text('cruz', row['text'])


>>>> sample >>> plot the results

# Import packages

import seaborn as sns
import matplotlib.pyplot as plt

# Set seaborn style
sns.set(color_codes=True)

# Create a list of labels:cd
cd = ['clinton', 'trump', 'sanders', 'cruz']

# Plot the bar chart
ax = sns.barplot(x=cd,y=[clinton, trump,sanders,cruz])
ax.set(ylabel="count")
plt.show()





	















