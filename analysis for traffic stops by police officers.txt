the dataset for stops by police officers in the state of rhode island.

1. State
2. stop_date
3. stop_time
4. county_name (contains nan values)
5. driver_gender
6. driver_race


ri=pd.read_csv('police.csv')
ri.isnull()

ri.isnull().sum()
county_name=91741

ri.shape()
output: 91741,15

drop county_name column

ri.drop('county_name',axis='columns', inplace=True)

.dropna() : drops rows based on the presence of missing values.



>>>> sample >>> dropping a column

# Examine the shape of the DataFrame
print(ri.shape)

# Drop the 'county_name' and 'state' columns
ri.drop(['county_name', 'state'], axis='columns', inplace=True)

# Examine the shape of the DataFrame (again)
print(ri.shape)

>>>> sample >>> drop na subset

# Count the number of missing values in each column
print(ri.isnull().sum())

# Drop all rows that are missing 'driver_gender'
ri.dropna(subset=['driver_gender'], inplace=True)

# Count the number of missing values in each column (again)
print(ri.isnull().sum())

# Examine the shape of the DataFrame
print(ri.shape)


removing columns and rows that will not be useful.


>>>>>>Examining the data types
read_csv creates an inferred datatype

print(ri.dtypes)

dtype:
1.object
2.bool
3.int
4.float
5.datetime
6.category

datatype affect opeations you can perform

math operations can be performed on int and floats

datetime 
category uses less memory and runs faster
bool enables logical and mathematical operations


apple
1. date
2. time
3. price

apple.price.dtype
output dtype('O') means object

apple['price']= apple.price.astype('float')


>>>>>Sample >>> convert object dtype to bool

# Examine the head of the 'is_arrested' column
print(ri.is_arrested.dtype)

# Change the data type of 'is_arrested' to 'bool'
ri['is_arrested'] = ri.is_arrested.astype(bool)

# Check the data type of 'is_arrested' 
print(ri.is_arrested.dtype)


>>>> sample >>> value_counts and unique

# Count the unique values in 'violation'
print(ri['violation'].unique())

# Express the counts as proportions
print(ri['violation'].value_counts(normalize=True))

['Equipment' 'Speeding' 'Other' 'Moving violation' 'Registration/plates'
 'Seat belt']
Speeding               48423
Moving violation       16224
Equipment              10921
Other                   4409
Registration/plates     3703
Seat belt               2856
Name: violation, dtype: int64

>>>>>normalized=True  >> output

Speeding               0.559571
Moving violation       0.187483
Equipment              0.126202
Other                  0.050950
Registration/plates    0.042791
Seat belt              0.033004
Name: violation, dtype: float64


>>>>>sample >>>> women have more speeding violations

# Create a DataFrame of female drivers
female = ri[ri['driver_gender']=='F']

# Create a DataFrame of male drivers
male = ri[ri['driver_gender']=='M']

print(female.violation.value_counts(normalize=True))

# Compute the violations by male drivers (as proportions)
print(male.violation.value_counts(normalize=True))

output:

Speeding               0.658114
Moving violation       0.138218
Equipment              0.105199
Registration/plates    0.044418
Other                  0.029738
Seat belt              0.024312
Name: violation, dtype: float64

Speeding               0.522243
Moving violation       0.206144
Equipment              0.134158
Other                  0.058985
Registration/plates    0.042175
Seat belt              0.036296
Name: violation, dtype: float64


Filtering a dataframe using multiple conditions

female = ri[ri.driver_gender=='F']
female.shape

or

female = ri[
(ri.driver_gender=='F') &
(ri.is_arrested==True)
]
female.shape


each condition is surround by parentheses and the & separates the conditions

only female drivers who were arrested

| represents the or condition

|| represents the and condition


>>sample >>> filtering

ri[(ri.driver_gender=='F') & (ri.violation=='Speeding')]


>>> Sample >>> Stop outcomes


# Create a DataFrame of female drivers stopped for speeding
female_and_speeding = ri[(ri.driver_gender=='F') & (ri.violation=='Speeding')]

# Create a DataFrame of male drivers stopped for speeding
male_and_speeding = ri[(ri.driver_gender=='M') & (ri.violation=='Speeding')]

print("male")
# Compute the stop outcomes for female drivers (as proportions)
print(female_and_speeding.stop_outcome.value_counts(normalize=True))
print("female")
# Compute the stop outcomes for male drivers (as proportions)
print(male_and_speeding.stop_outcome.value_counts(normalize=True))

Output::  (95% of stops resulting in a ticket)

male
Citation            0.952192
Warning             0.040074
Arrest Driver       0.005752
N/D                 0.000959
Arrest Passenger    0.000639
No Action           0.000383
Name: stop_outcome, dtype: float64

female
Citation            0.944595
Warning             0.036184
Arrest Driver       0.015895
Arrest Passenger    0.001281
No Action           0.001068
N/D                 0.000976
Name: stop_outcome, dtype: float64


>>>>>>>Does gender affect the vehicles that are searched?


ri.isnull().sum()

true is 1
false is 0
then sum the rows

the mean of a boolean series represents the percentage of True values

ri.is_arrested.value_counts(normalized=True)
.03
ri.is_arrested.mean()
.03


find the unique districts

ri.district.unique()

print(df_sas[df_sas['District'].isin(districts)]['ArrestInt'].mean())


print(df_sas.groupby('District')['ArrestInt'].mean())

print(df_sas.groupby(['District','Ward'])['ArrestInt'].mean())

>>>>>Sample  >> search_conducted

# Check the data type of 'search_conducted'
print(ri['search_conducted'].dtype)

# Calculate the search rate by counting the values
print(ri['search_conducted'].value_counts(normalize=True))

# Calculate the search rate by taking the mean
print(ri.search_conducted.mean())


output
bool
False    0.961785
True     0.038215
Name: search_conducted, dtype: float64
0.0382153092354627


>>>>Sample >>> female

# Calculate the search rate for female drivers
print(ri[ri.driver_gender=='F'].search_conducted.mean())

output: 0.019180617481282074 (female)
output: 0.04542557598546892 (male)

>>>Sample >>> groupby

# Calculate the search rate for both groups simultaneously
print(ri.groupby('driver_gender').search_conducted.mean())

>>>Sample >>> groupby multiple column

print(ri.groupby(['driver_gender','violation']).search_conducted.mean())

driver_gender  violation          
F              Equipment              0.039984
               Moving violation       0.039257
               Other                  0.041018
               Registration/plates    0.054924
               Seat belt              0.017301
               Speeding               0.008309

M              Equipment              0.071496
               Moving violation       0.061524
               Other                  0.046191
               Registration/plates    0.108802
               Seat belt              0.035119
               Speeding               0.027885
Name: search_conducted, dtype: float64


>>>>>>>>>>>>>>>Gender affect frisking

ri.search_type.value_counts(dropna=False)
1. Incident to Arrest
2. Probable cause
3. Inventory
4. Reasonable Suspicion
5. Protective Frisk
6. Incident to Arrest, Inventory
7. Incident to Arrest, Probable Cause


ri['inventory']=ri.search_type.str.contains('Inventory',na=False)

na=False means return a false when it finds a missing value
ri.inventory.sum()


search=ri[ri.searched_conducted==True]
searched.inventory.mean()


>>>Sample   >>> search type count, frisk in the search_type

# Count the 'search_type' values
print(len(ri.search_type.unique()))

# Check if 'search_type' contains the string 'Protective Frisk'
ri['frisk'] = ri.search_type.str.contains('Protective Frisk', na=False)

# Check the data type of 'frisk'
print(ri['frisk'].dtype)

# Take the sum of 'frisk'
print(ri['frisk'].sum())


>>>Sample >>> search conduction  >> frisk average per gender

# Create a DataFrame of stops in which a search was conducted
searched = ri[ri.search_conducted == True]

# Calculate the overall frisk rate by taking the mean of 'frisk'
print(searched.frisk.mean())

# Calculate the frisk rate for each gender
print(searched.groupby("driver_gender").frisk.mean())

>>>>>>>>>>>>Does the time of day affect arrest rate

analyzing datetime data

apple
1. price
2. volume (shares traded)
3. date_and_time


dt.month
dt.week
dt.dayofweek
dt.hour

apple.set_index('date_and_time', inplace=True)
apple.index.month
apple.price.mean()

month_price=apple.groupby(apple.index.month).price.mean()

monthly_price.plot()
plt.xlabel('Month')
plt.ylabel('Price')

df_sas['Year']=pd.DatetimeIndex(df_sas['date']).year
arrest_year=df_sas.groupby(['Year'])['ArrestInt'].sum()


>>>>>Sample  >>> arrest rate as a time of day

# Calculate the overall arrest rate
print(ri.is_arrested.mean())

# Calculate the hourly arrest rate
print(ri.groupby(ri.index.hour).is_arrested.mean())

# Save the hourly arrest rate
hourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean()


>>>>Sample >>> plot arrest time

# Import matplotlib.pyplot as plt
import matplotlib.pyplot as plt

# Create a line plot of 'hourly_arrest_rate'
hourly_arrest_rate.plot()

# Add the xlabel, ylabel, and title
plt.xlabel('Hour')
plt.ylabel('Arrest Rate')
plt.title('Arrest Rate by Time of Day')

# Display the plot
plt.show()

>>>>>>>>>>>>>>>>>>Are drug related stops on the rise
1. We will use a subplot to see how two variables change over time

2. Resampling is when you change the frequency of the time series

monthly_price=apple.price.resample('M').mean()

resample by month

the output is the last day of the month rather than a number

monthly_volume=apple.volume.resample('M').mean()


pd.concat([monthly_price,monthly_volume],axis='columns')

concatenates along a specified axis

monthly.plot(subplots=True)
plt.show()


>>>>Sample >>>> drug related stops >> resampling

# Calculate the annual rate of drug-related stops
print(ri.drugs_related_stop.resample('A').mean())

# Save the annual rate of drug-related stops
annual_drug_rate = ri.drugs_related_stop.resample('A').mean()

# Create a line plot of 'annual_drug_rate'
annual_drug_rate.plot(subplots=True)

# Display the plot
plt.show()

>>>>Sample >>> concatenate the two columns

# Calculate and save the annual search rate
annual_search_rate = ri.search_conducted.resample('A').mean()

# Concatenate 'annual_drug_rate' and 'annual_search_rate'
annual = pd.concat([annual_drug_rate,annual_search_rate], axis='columns')

# Create subplots from 'annual'
annual.plot(subplots=True)

# Display the subplots
plt.show()


>>>>>>>>What violations are caught in each district

result=df_sas.groupby(['Year','Month','fbi_code'])['ArrestInt'].sum().reset_index()
#print(top20.columns)
mask=result['ArrestInt']>30
fbi_codes=result[mask]['fbi_code'].unique()

filter=df_sas['fbi_code'].isin(fbi_codes) 
fbi_codes=df_sas['fbi_code'].unique()

arrest_breakdown=df_sas[filter].groupby(['Year','Month','fbi_code'])['ArrestInt'].sum().reset_index()
keys=arrest_breakdown.keys()
#print(arrest_breakdown)

g = sns.factorplot(data=arrest_breakdown, x='Year', y='ArrestInt', 
                  hue='fbi_code',  kind='point',size=8,aspect=2)

plt.show()

>>>>>>>>>>>>>>>> cross tab

table=pd.crosstab(ri.driver_race, ri_driver_gender)

creates a pivot table building a frequency table

ri[(ri.driver_race=='Asian') & (ri.driver_gender=='F')].shape


range=table.loc['Asian':'Hispanic']

range.plot(kind='bar')
plt.show()


>>> stack bar plot

range.plot(kind='bar', stacked=True)
plt.show()


>>>>  Sample

# Create a frequency table of districts and violations
print(pd.crosstab(ri.district,ri.violation))

# Save the frequency table as 'all_zones'
all_zones = pd.crosstab(ri.district,ri.violation)

# Select rows 'Zone K1' through 'Zone K3'
print(all_zones.loc['Zone K1':'Zone K3'])

# Save the smaller table as 'k_zones'
k_zones = all_zones.loc['Zone K1':'Zone K3']

k_zone.plot(kind='bar', stacked=True)
plt.show()

>>>>>>>>>>How long might you be stopped

apple
date_and_time
price
volume
change

change when  change

True if the price went up

calculate how often the price went up taking the column mean


Stefan Jansen
https://www.amazon.com/dp/B08D9SP6MB/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1

mapping = {'up':True, 'down':False}
apple['is_up']=apple.chage.map(mapping)

apple.is_up.mean()

>>>>>how often searches occur after each violation

search_rate=ri.groupby('violation').search_conducted.mean()

search_rate.plot(kind='bar')
plt.show()

search rate is on the y axis
the violation is on the x axis


search_rate.plot(kind='barh')
plt.show()

>>>>sample >>>> mapping

# Print the unique values in 'stop_duration'
print(ri.stop_duration.unique())

# Create a dictionary that maps strings to integers
mapping = {
    '0-15 Min': 8,
    '16-30 Min': 23,
    '30+ Min': 45
    }

# Convert the 'stop_duration' strings to integers using the 'mapping'
ri['stop_minutes'] = ri.stop_duration.map(mapping)

# Print the unique values in 'stop_minutes'
print(ri['stop_minutes'].unique())


>>>>>>sample >>>  groupby  >> average >>> sort


# Calculate the mean 'stop_minutes' for each value in 'violation_raw'
print(ri.groupby('violation_raw')['stop_minutes'].mean())

# Save the resulting Series as 'stop_length'
stop_length = ri.groupby('violation_raw')['stop_minutes'].mean()

# Sort 'stop_length' by its values and create a horizontal bar plot
stop_length.sort_values().plot(kind='barh')

# Display the plot
plt.show()


>>>>>>Exploring the weather dataset

do weather conditions affect police behavior

noaa : national centers for environmental information

single station in rhode islands to give weather information

weather = pd.read_csv('weather.csv')

weather.head(3)

TAVG, TMIN, TMAX: Temperature
AWND. WSF2: Wind Speed (average, fastest wind speed in a 2 minute interval)
WT01, WT022: Bad Weather conditions

https://mesonet.agron.iastate.edu/request/download.phtml?network=ID_ASOS

increased convinced the data is trustworthy


station:three or four character site identifier
valid:timestamp of the observation
tmpf:Air Temperature in Fahrenheit, typically @ 2 meters
dwpf:Dew Point Temperature in Fahrenheit, typically @ 2 meters
relh:Relative Humidity in %
drct:Wind Direction in degrees from north
sknt:Wind Speed in knots 
p01i:One hour precipitation for the period from the observation time to the time of the previous hourly precipitation reset. This varies slightly by site. Values are in inches. This value may or may not contain frozen precipitation melted by some device on the sensor or estimated by some other means. Unfortunately, we do not know of an authoritative database denoting which station has which sensor.
alti:Pressure altimeter in inches
mslp:Sea Level Pressure in millibar
vsby:Visibility in miles
gust:Wind Gust in knots
skyc1:Sky Level 1 Coverage
skyc2:Sky Level 2 Coverage
skyc3:Sky Level 3 Coverage
skyc4:Sky Level 4 Coverage
skyl1:Sky Level 1 Altitude in feet
skyl2:Sky Level 2 Altitude in feet
skyl3:Sky Level 3 Altitude in feet
skyl4:Sky Level 4 Altitude in feet
wxcodes:Present Weather Codes (space seperated)
feel:Apparent Temperature (Wind Chill or Heat Index) in Fahrenheit
ice_accretion_1hr:Ice Accretion over 1 Hour (inches)
ice_accretion_3hr:Ice Accretion over 3 Hours (inches)
ice_accretion_6hr:Ice Accretion over 6 Hours (inches)
peak_wind_gust:Peak Wind Gust (from PK WND METAR remark) (knots)
peak_wind_drct:Peak Wind Gust Direction (from PK WND METAR remark) (deg)
peak_wind_time:Peak Wind Gust Time (from PK WND METAR remark)
metar:unprocessed reported observation in METAR format

weather[['AWND','WSF2']].describe()

create a box plot
weather[['AWND','WSF2']].plot(kind='box')
plt.show()


take the fast wind speed minus the average wind speed

weather['WDIFF']= weather.WSF2-weather.AWND

weather.WDIFF.plot(kind='hist', bins =20)
plt.show()

>>>>Sample  >>> box plot temperatures

# Read 'weather.csv' into a DataFrame named 'weather'
df=pd.read_csv('weather.csv')

# Describe the temperature columns
print(df[['TMIN','TAVG','TMAX']].describe())

# Create a box plot of the temperature columns
df[['TMIN','TAVG','TMAX']].plot(kind='box')

# Display the plot
plt.show()

>>>>>Sample >> histogram to confirm temperature range distribution

# Create a 'TDIFF' column that represents temperature difference
weather['TDIFF']=weather.TMAX- weather.TMIN

# Describe the 'TDIFF' column
print(weather['TDIFF'].describe())

# Create a histogram with 20 bins to visualize 'TDIFF'
weather['TDIFF'].plot(kind='hist',bins=20)

# Display the plot
plt.show()


>>>>>>>>>>>>>>>>>>>>>>>>>Categorizing the weather


slicing columns of the original dataframe

temp=weather.loc[:,'TAVG':'TMAX']


temp.sum(axis='columns').head()  

this sums all the columns

ri.stop_duration.unique()

mapping = {
    '0-15 Min': 'short',
    '16-30 Min': 'medium',
    '30+ Min': 'long'
    }

# Convert the 'stop_duration' strings to integers using the 'mapping'
ri['stop_length'] = ri.stop_duration.map(mapping)

ri.stop_length.dtype
outputs object type because it contains string data

ri.stop_length.unique()


cats=['short','medium','long']
ri.stop_length.astype('category',ordered=True,categories=cats)


1. stores more efficiently
2. allows a logical order

ri.stop_length.memory_usage(deep=True)  #memory used to store the column

cats=['short','medium','long']

ri[ri.stop_length>'short']
output data with categories of medium or long stop_length

ri.groupby('stop_length').is_arrested.mean()


>>>> Sample >>> bad weather conditions

# Copy 'WT01' through 'WT22' to a new DataFrame
WT = weather.loc[:,'WT01':'WT22']

# Calculate the sum of each row in 'WT'
weather['bad_conditions'] = WT.sum(axis='columns')

# Replace missing values in 'bad_conditions' with '0'
weather['bad_conditions'] = weather.bad_conditions.fillna(0).astype('int')

# Create a histogram to visualize 'bad_conditions'


weather['bad_conditions'].plot(kind='hist')
plt.show()
# Display the plot

>>>> sample  >>> bad_conditions by category

# Count the unique values in 'bad_conditions' and sort the index
print(weather.bad_conditions.value_counts().sort_index())

# Create a dictionary that maps integers to strings
mapping = {0:'good', 1:'bad', 2:'bad', 3:'bad', 4:'bad', 5:'worse', 6:'worse', 7:'worse', 8:'worse', 9:'worse'}

# Convert the 'bad_conditions' integers to strings using the 'mapping'
weather['rating'] = weather.bad_conditions.map(mapping)

# Count the unique values in 'rating'
print(weather['rating'].unique())

print(weather.rating.value_counts())

output:
bad      1836
good     1749
worse     432
Name: rating, dtype: int64

>>>>Sample  >>> create a column as a category

# Create a list of weather ratings in logical order
cats=['good','bad','worse']

# Change the data type of 'rating' to category
weather['rating'] = weather.rating.astype('category', ordered=True, categories=cats)

# Examine the head of 'rating'
print(weather['rating'].head())


>>>>>>>>>>>>>>Merging Datasets

reset_index returns the index to an autonumber


high=high_low[['DATE','HIGH']]

we only need the high column

apple_high=pd.merge(left=apple, right=high, left_on='date', right_on='DATE', how=left)

apple_high.set_index('date_and_time', inplace=True)



<<<<<<<Sample >>> reset the index to the autonumber,  extract the DATE and rating columns from the weather dataframe

# Reset the index of 'ri'
ri.reset_index(inplace=True)

# Examine the head of 'ri'
print(ri.head())

# Create a DataFrame from the 'DATE' and 'rating' columns
weather_rating=weather[['DATE','rating']]

# Examine the head of 'weather_rating'
print(weather_rating.head())


>>>>>>Sample >>> merge columns on stop_date and date

# Examine the shape of 'ri'
print(ri.shape)

# Merge 'ri' and 'weather_rating' using a left join
ri_weather = pd.merge(left=ri, right=weather_rating, left_on='stop_date', right_on='DATE', how='left')

# Examine the shape of 'ri_weather'
print(ri_weather.shape)

# Set 'stop_datetime' as the index of 'ri_weather'
ri_weather.set_index('stop_datetime', inplace=True)


https://datatofish.com/multiple-linear-regression-python/


>>> weather and behavior

search_rate = ri.groupby(['violation','driver_gender']).search_conducted.mean()

multi-index
pandas.core.indexes.multi.multiindex (second dimension)

level=0
level=1

search_rate.loc['Equipment'] #level 0
search_rate.loc['Equipment','M'] #level 1

search_rate.unstack()

results in a dataframe

ri.pivot_table(index='violation',
	columns='driver_gender',
	values='search_conducted')

>>>>>Sample
print(ri_weather.is_arrested.mean())
0.0355690117407784

overall arrest rate


# Calculate the arrest rate for each 'rating'
print(ri_weather.groupby('rating').is_arrested.mean())

stop_minutes  
rating                
good     0.033715
bad      0.036261
worse    0.041667

>>>>Sample  >>> create a multi index series  >> violation and rating for is_arrested mean

# Calculate the arrest rate for each 'violation' and 'rating'
print(ri_weather.groupby(['violation','rating']).is_arrested.mean())

violation            rating
*Equipment            good      0.059007
                     bad       0.066311
                     worse     0.097357
*Moving violation     good      0.056227
                     bad       0.058050
                     worse     0.065860
*Other                good      0.076966
                     bad       0.087443
                     worse     0.062893
*Registration/plates  good      0.081574
                     bad       0.098160
                     worse     0.115625
Seat belt            good      0.028587
                     bad       0.022493
                     worse     0.000000
Speeding             good      0.013405
                     bad       0.013314
                     worse     0.016886

>>>>sample >>> slicing the multi-index series

# Save the output of the groupby operation from the last exercise
arrest_rate = ri_weather.groupby(['violation', 'rating']).is_arrested.mean()

# Print the 'arrest_rate' Series
print(arrest_rate)

# Print the arrest rate for moving violations in bad weather
print(arrest_rate.loc['Moving violation','bad'])

# Print the arrest rates for speeding violations in all three weather conditions
print(arrest_rate.loc['Speeding'])


>>>>sample >>> unstack and pivot

# Unstack the 'arrest_rate' Series into a DataFrame
print(arrest_rate.unstack())

# Create the same DataFrame using a pivot table
print(ri_weather.pivot_table(index='violation', columns='rating', values='is_arrested'))

practice answering questions using data

https://openpolicing.stanford.edu

































































