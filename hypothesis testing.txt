a/b testing to test different advertising scenarios
1. users were split into control and treatment group

The treatment group (no ad) got 43.4% more purchases than the control group (with ad)

was this result statistically significant or just random chance

The mean annual compensation of the population of data scientist is $110,000

mean_comp_sampling = stack_overflow['converted_comp'].mean()

is the mean signicantly difference from the sampling set

generating a bootstrap distribution

stack_overflow.sample(frac=1, replace=True)['convert_comp']

so_boot_distn=[]

for i in range(5000):
	so_boot_distn.append(
		np.mean(
			stack_overflow.sample(frac=1, replace=True)['convert_comp']
		)
	)


import matplotlib.pyplot as plt
plt.hist(so_boot_distn, bins=50)
plt.show()

std_error= np.std(so_boot_distn, ddof=1)

z-scores

standardize value = (value - mean ) / standard deviation

z= sample stat - hypoth.param.value / std_error

hypothesis testing is used to determine whether sample statistic is close to or far away from expected hypothesized values.


probability density function:
normal distribution
1. mean=0
2. standard deviation =1

this is often call the z distribution


# Print the late_shipments dataset
print(late_shipments)

# Calculate the proportion of late shipments
late_prop_samp = (late_shipments['late'] == 'Yes').mean()
# Print the results
print(late_prop_samp)

0.061


# Hypothesize that the proportion is 6%
late_prop_hyp = 0.06

# Calculate the standard error
std_error = np.std(late_shipments_boot_distn,ddof=1)

# Find z-score of late_prop_samp
z_score = (late_prop_samp - late_prop_hyp)/std_error

# Print z_score
print(z_score)

0.13353771933071554

The z-score is a standardized measure of the difference between the sample statistic and the hypothesized statist

p-values

hypothesis tests are like criminal trials
1. the defendant committed the crime 
2. the defendant did not not commit the crime

two possible verdicts
1. guilty
2. not guilty

the initial assumption is the defendant is not guilty

prosecution must present evidence beyond reasonable doubt for a guilty verdict

Stackoverflow

age_first_code_cut classifies when stack overflow user first start programming

adult : means they started at 14 or older
child : means they started before 14

suppose previous reseach suggests that 35% of software developers start programming as children

a hypothesis is a statement about an unknown population parameter

a hypothesis test compares two competing hypotheses
1. the null hypothesis (H0) is the existing idea
2. the alterative hypothesis (Ha) is the new challenger idea of the reseacher

h0: the porportion of data scientist starting programming as children is 35%
ha: the proportion of data scientist starting programming as children is greater than 35%

either Ha or H0 is true (not both)
1. H0 is assumed to be true

The test ends in either reject H0 or fail to reject H0

The beyond a reasonable doubt is the the significance level.

tails of a distribution are the left and right tails of the distribution

hypothesis tests check if the sample statistics lie in the tails of the null distribution

if we are checking for a [difference sample and a hypothesis value] then then we look for extreme values in the left and right tails.

if the alternate hypothesis uses language like less or fewer than we do a left-tailed test

if the alternate hypothesis uses language like greater than we do a right-tailed test

HA:  The proportion of data scientist starting programming as children is greater than 35%
1. right tailed test

p-values measure the support or strength of the null hypothesis

p-values: probability of obtaining a result, assuming the null hypothesis is true.

large p-value, large support for H0.  Statistically likely not in the tail of the null distribution.

small p-value, strong evidence against H0. Statistically likely in the tail of the null distribution

p-value is between 0 and 1

>>>>> calculating the p-value

1. calculate the z-score

prop_child_samp = (stack_overflow['age_first_code_cut']=='child').mean()

h0
prop_child_hyp=0.35

std_error = np.std(first_code_boot_distn, ddof=1)

z_score = (prop_child_samp - prop_child_hyp) / std_error

from scipy.stats import norm

1-norm.cdf(z_score, loc=0, scale=1)

left-tailed test use norm.cdf()
right-tailed test use 1- norm.cdf()

3 out of 100,000


>>>>>> rules

If the alternative hypothesis is about a difference between the sample statistic and the null statistic, then you need a two-tailed test.

If the alternative hypothesis is about the sample statistic being less than the null statistic, then you need a left-tailed test.

If the alternative hypothesis is about the sample statistic being greater than the null statistic, then you need a right-tailed test.


# Calculate the z-score of late_prop_samp
z_score = (late_prop_samp-late_prop_hyp )/std_error

# Calculate the p-value
p_value = 1-norm.cdf(z_score, loc=0, scale=1)
                 
# Print the p-value
print(p_value) 

>>>>>>>>>>>>>>statistically significance

p-values quantify evidence for the null hypothesis

large p-values indicate a lack of evidence for the alternate hypothesis (fail to reject null hypothesis) - stick with the null hypothesis

small p-values -> reject null hypothesis,  favor the alternative hypothesis

>>>> what is the cutoff point between a small p-value and a large p-value?  the significance level.

The significance level is the threshhold point for "beyond a reasonable doubt" (alpha)

common alpha values are 0.2,0.1, 0.05, and 0.01

0.05 is popular

if the p-value <= alpha then reject h0, else fail to reject h0

alpha should be set prior to conducting the hypothesis test

alpha=0.05

prop_child_samp = (stack_overflow['age_first_code_cut']=='child').mean()
prop_child_hyp=0.35
std_error = np.std(first_code_boot_distn, ddof=1)
z_score = (prop_child_samp - prop_child_hyp) / std_error
p_value = 1-norm.cdf(z_score,loc=0, scale=1)

p_value<=alpha
True (reject h0 in favor of Ha)

confidence intervals
1. for a significance level of alpha, it is common to choose a a confidence interval level of 1-alpha

alpha = 0.05 -> 95% confidence interval

import numpy as np
lower=np.quantile(first_code_boot_distn, 0.025)
upper=np.quantile(first_code_boot_distn, 0.975)

0.37 and 0.41  range of values


types of errors

			did not commit the crime	committed the crime
Verdict not guilty   	correct				got away
verdict guilty		wrong conviction		correct


		actual H0		actual Ha
chosen H0	correct			false negative
chosen HA	false positive		correct


>>>>>>

# Calculate 95% confidence interval using quantile method
lower = np.quantile(late_shipments_boot_distn,0.025)
upper = np.quantile(late_shipments_boot_distn,0.975)

# Print the confidence interval
print((lower, upper))

(0.047, 0.076)

When you have a confidence interval width equal to one minus the significance level, if the hypothesized population parameter is within the confidence interval, you should fail to reject the null hypothesis.


>>>>>> performing t-tests

two-sample problems
1. compare sample statistics across groups of a variable
2. converted_comp is a numerical variable
3. age_first_code_cut is a categorical variable with levels (child and adult)

Are users who first programmed as a child compensated higher than those that started as adults?

H0: The mean compensation is the same for those that coded first as a child and those that coded first as an adult

Ha: The mean compensation is greater for those that coded first as a child compared to those that coded first as an adult

Ha u child > u adult
Ha u child - u adult > 0

stack_overflow.groupby('age_first_code_cut')['converted_comp'].mean()

sample mean estimates the population mean denoted by x bar

x bar - a sample mean

x bar child - sample mean compensation for coding first as a child
x bar adult - sample mean compensation for coding first as a adult

x bar child = 134k
x bar adult = 111k

x bar child - x bar adult = a test statistic

t= (difference in sample stats - difference in population parameters) / standard error

t = (x bar child - x bar adult) - ( mean child - mean adjust) / SE(x bar child - x bar adult)

SE is the standard error

SE(x bar child - x bar adult) = square root ( standard deviation child **2 / number of children + standard deviation adult **2/ number of adults)

H0: meam child - mean adult =0

t = (x bar child - x bar adult)/ square root ( standard deviation child **2 / number of children + standard deviation adult **2/ number of adults)

xbar=stack_overflow.groupby('age_first_code_cut')['converted_comp'].mean()
adult=111313
child:132419
s=stack_overflow.groupby('age_first_code_cut')['converted_comp'].std()
adult=271546
child=255585
n=stack_overflow.groupby('age_first_code_cut')['converted_comp'].count()
adult=1376
child=885

numerator=xbar_child - xbar_adult
denominator=np.sqrt(s_child**2/n_child + s_adult**2/n_adult)
t_stat=numerator/denominator

1.86999313316221844


late_shipments dataset has been split into a "yes" group, where late == "Yes" and a "no" group where late == "No". The weight of the shipment is given in the weight_kilograms variable.

The sample means for the two groups are available as xbar_no and xbar_yes. The sample standard deviations are s_no and s_yes. The sample sizes are n_no and n_yes. numpy is also loaded as np.

>>> late_shipments

['id', 'country', 'managed_by', 'fulfill_via', 'vendor_inco_term', 'shipment_mode', 'late_delivery', 'late', 'product_group', 'sub_classification', 'vendor', 'item_description',
       'molecule_test_type', 'brand', 'dosage', 'dosage_form', 'unit_of_measure_per_pack', 'line_item_quantity', 'line_item_value', 'pack_price', 'unit_price', 'manufacturing_site',
       'first_line_designation', 'weight_kilograms', 'freight_cost_usd', 'freight_cost_groups', 'line_item_insurance_usd']


# Calculate the numerator of the test statistic
numerator = xbar_yes-xbar_no

# Calculate the denominator of the test statistic
denominator = np.sqrt(s_yes**2/n_yes+s_no**2/n_no)

# Calculate the test statistic
t_stat = numerator/denominator

# Print the test statistic
print(t_stat)

2.3936661778766433

>>>>>>>> calculating p-values from t-statistics

t-distributions
1. t statistics follow a t-distribution
2. have a parameter name degrees of freedom

plot using the probability density function

as we increase the degrees of freedom the t-distribution gets closer to the normal distribution

a normal distribution is a t-distribution with infinite degrees of freedom

degrees of freedom: maximum number of logically independent values in the data sample

>>>> calculating the degrees of freedom
1. dataset has 5 independent observations
2. four of the values are 2,6,8,5  totalling 21
3. the mean is 5  therefore 5x5=25 
4. the last value must be 4
5. here, there are 4 degrees of freedom

degrees of freedom = n child + n adult -2  (number of observations less 2)

suppose

h0: the porportion of data scientist starting programming as children is 35%
ha: the proportion of data scientist starting programming as children is greater than 35%

greater than
use the right-tailed test

signficance level

alpha=0.1

if p<=alpha then reject H0

p value
p_value=1-norm.cdf(z_score)

>>>>>>>>>>>>>t-statistics

use the t-statistics when using multiple sample statistics to estimate a population parameter

numerator = xbar_child - xbar_adult
denominator= np.sqrt(s_child**2/n_child + s_adult**2/n_adult)
t_stat=numerator/denominator

1.86

degree_of_freedom = n_child + n_adult -2
2259

use t-distribution cdf not normal cdf

from scipy.stats import t

1-t.cdf(t_stat, df=degrees_of_freedom)

0.030811302165157595

p<=alpha of .1 so reject H0 in favor Ha

evidence that stack overflow data scientist who started coding a child earn more

# Calculate the degrees of freedom
degrees_of_freedom = n_no+n_yes-2

# Calculate the p-value from the test stat
p_value = t.cdf(t_stat,df=degrees_of_freedom)

# Print the p_value
print(p_value)

0.008432382146249523

alpha=0.05
print(p_value<=alpha)
true

>>>>>>>>>> Paired t-tests

hypothesis

question: Was the percentage of Republican candidate votes lower in 2008 than 2012


H0 u2008 - u2012 =0  (hunch is wrong)
Ha u2008 - u2012 <0

alpha=0.05

Data is paired -> each voter percentage refers to the same county

sample_data['diff']=sample_data['repub_percent_08'] - sample_data['repub_percent_12']

sample_data['diff'].hist(bins=20)

most occurrences were between 0 and -10

xbar_diff=sample_data['diff'].mean()

-2.877109041242944


t= (x diff - u diff) / sqrt( std diff **2/n diff)
df=n diff -1

n_diff=len(sample_data)

100

s_diff=sample_diff['diff'].std()

assuming the H0 is true : H0 : u 2008-u2012=0 (resulting in mu diff=0)

t_stat=(xbar_diff-0) / np.sqrt(s_diff**2/n_diff)

-5.601043121928489

degrees_of_freedom  n_diff - 1

p_value=t.cdf(t_stat, df=n_diff-1)

9.6 10^-8

p_value >= alpha  is false  (reject H0 in favor Ha)

import pinqouin

pingouin.ttest(x=sample_data['diff'], y=0, alternative="less")  y=0 is the mu diff=0 for the H0

or

pingouin.ttest(x=sample_data['repub_percent_08'],
	y=sample_data['repub_percent_12'],
	paired=True,
	alternative="less")

>>>>>>>>

# Calculate the differences from 2012 to 2016
sample_dem_data['diff'] = sample_dem_data['dem_percent_12']-sample_dem_data['dem_percent_16']

# Print sample_dem_data
print(sample_dem_data)

# Find the mean of the diff column
xbar_diff = sample_dem_data['diff'].mean()

# Print xbar_diff
print(xbar_diff)

6.829312660106834

s_diff = sample_dem_data['diff'].std()

5.040139140132317

sample_dem_data['diff'].hist(bins=20)
plt.show()

# Conduct a t-test on diff
test_results = pingouin.ttest(x=sample_dem_data['diff'], 
                              y=0, 
                              alternative="two-sided")

                              
# Print the test results
print(test_results)

T  dof alternative       p-val         CI95%  cohen-d        BF10  power
T-test  30.298  499   two-sided  3.601e-115  [6.39, 7.27]    1.355  2.246e+111    1.0

reject H0 in favor of Ha


# Conduct a paired t-test on dem_percent_12 and dem_percent_16
paired_test_results = pingouin.ttest(x=sample_dem_data['dem_percent_12'],
	                y=sample_dem_data['dem_percent_16'],
	                paired=True,
                              alternative="two-sided")


 T  dof alternative       p-val         CI95%  cohen-d        BF10  power
T-test  30.298  499   two-sided  3.601e-115  [6.39, 7.27]    0.454  2.246e+111    1.0


>>>>>> A Nova tests  what is there are more than two groups to compare

stack overflow has five categories of job satisfaction
1. very satisfied
2. slightly satisfied
3. slightly dissatisfied
4. neither
5. very dissatisfied

stack_overflow['job_sat'].value_counts()

visualizing multiple distributions
1. Is mean annual compensation different from different levels of job satisification

sns.boxplot(x="converted_comp",y="job_sat", data=stackoverflow)
plt.show()


A test for differences between groups

alpha=0.2

pingouin.anova(data=stackoverflow,
	dv="converted_comp",
	between="job_sat")

dv is the dependent variable

p_value is stored in the p-unc column

0.001315 < alpha

at least two categories have significantly different compensations

pingouin.pairwise_tests(data=stack_overflow,
	dv="converted_comp",
	between="job_sat",
	padjust="none"
)


pingouin.pairwise_tests(data=stack_overflow,
	dv="converted_comp",
	between="job_sat",
	padjust="bonf"
)

>>> bonf to prevent the pairs from producing a false positive

padjust:
none - no correction
bonf - one-step bonferroni correction
sidak - one step sidak correction
holm - step-down method using bonferroni adjustments
fdr-bh - benjamini/hochberg fdr correction
fdr-by - benjamin/yekutieli fdr correction


# Calculate the mean pack_price for each shipment_mode
xbar_pack_by_mode = late_shipments.groupby("shipment_mode")['pack_price'].mean()

# Print the grouped means
print(xbar_pack_by_mode)

shipment_mode
Air            39.712
Air Charter     4.227
Ocean           6.432

three shipment modes (shipment_mode): "Air", "Air Charter", and "Ocean".

columns: Late_delivery  unit_of_measure_per_pack  line_item_quantity  line_item_value  pack_price  unit_price  weight_kilograms  freight_cost_usd  line_item_insurance_usd

# Calculate the standard deviation of the pack_price for each shipment_mode
s_pack_by_mode =late_shipments.groupby("shipment_mode")['pack_price'].std()

# Print the grouped standard deviations
print(s_pack_by_mode)

shipment_mode
Air            48.933
Air Charter     0.993
Ocean           5.303

# Boxplot of shipment_mode vs. pack_price
sns.boxplot(x="pack_price",y="shipment_mode",data=late_shipments)
plt.show()


>>>> nova

# Run an ANOVA for pack_price across shipment_mode
anova_results = pingouin.anova(data=late_shipments,
	dv="pack_price",
	between="shipment_mode")

# Print anova_results
print(anova_results)


 Source  ddof1  ddof2       F      p-unc    np2
0  shipment_mode      2    997  21.865  5.089e-10  0.042

alpha 0.1

reject the null hypothesis

There is a significant difference in pack prices between the shipment modes. 


# Perform a pairwise t-test on pack price, grouped by shipment mode
pairwise_results = pingouin.pairwise_tests(data=late_shipments,
	dv="pack_price",
	between="shipment_mode",
	padjust="bonf"
)


# Print pairwise_results
print(pairwise_results)


>>>>>>>>>>>>> One-sample proportion tests

Is a claim about an unknown population proportion feasible?

1. Standard error of sample statistic from bootstrap distribution
2. Compute a standardized test statistic
3. calculate a p-value
4. decide which hypothesis made most sense

p: population proportion
p-hat: sample proportion (sample statistic)
p0: hypothesized population proportion


z-score= sample statistic - mean(p-hat)/SE(p-hat) = (p-hat - p)/se(p-hat)


Assume H0 is true, p=p0, so

z= (p-hat - p0)/SE(p-hat)

Under H0

SE(p-hat)= sqrt(p0*(1-p0)/n)  where n is the sample size

z=(p-hat - p0)/sqrt(p0*(1-p0)/n)


>>>> t distribution has fatter tails than a normal distribution and helps prevent mistakenly rejecting the null hypothesis.

numerator = xbar_child - xbar_adult
denominator= np.sqrt(s_child**2/n_child + s_adult**2/n_adult)
t_stat=numerator/denominator

s_child,s_adult=stack_overflow.groupby('age_first_code_cut')['converted_comp'].std()

s is calculated from x
s estimates the population standard deviation

uncertainty is our estimate of the paramter

Stack overflow age categories

h0: proportion of stack overflow users are under thirty =0.5
ha: porportion of stack overflow users are under thirty <>0.5

alpha=0.01

stack_overflow['age_cat'].value_counts(normalize=True)

under 30 -> 0.53
at least 30 ->0.46


p_hat= (stack_overflow['age_cat'] == 'Under 30').mean()

p_hat -> 0.53

p_0 -> 0.5

n= len(stack_overflow)

n->2261

numerator = p_hat - p_0
denominator = np.sqrt(p_0 * (1-p_0)/n)
z_score=numerator / denominator

z_score -> 3.38

from scipy.stats import norm

left tailed (less than)

p_value = norm.cdf(z_score)

right tailed (greater than)

p_value = 1- norm.cdf(z_score)

two  tailed (not equal)

p_value = norm.cdf(-z_score) + 1-norm.cdf(z_score)


p_value = 2*(1-norm.cdf(z_score))

p_value -> .000709

p_value <= alpha

true  (reject the null hypothesis)

users under thirty <> 0.5

The t-test is needed for tests of mean(s) since you are estimating two unknown quantities, which leads to more variability.

>>>>>

late shipments was "greater than" 6%

# Hypothesize that the proportion of late shipments is 6%
p_0 = 0.06

# Calculate the sample proportion of late shipments
p_hat = (late_shipments['late']=="Yes").mean()

# Calculate the sample size
n = len(late_shipments)

# Print p_hat and n
print(p_hat, n)

0.061 1000

# Calculate the numerator and denominator of the test statistic
numerator = p_hat-p_0
denominator = np.sqrt(p_0*(1-p_0)/n)

# Calculate the test statistic
z_score = numerator/denominator

# Print the result
print(z_score)

0.13315591032282698

"greater than" alternative hypothesis.

p_value =  1- norm.cdf(z_score)

# Print the p-value
print(p_value)

0.44703503936503364


>>>>>>>> Two sample Proportion tests

1. compare the proportions in two different populations

Yes- Is a hobbist

H0: Proportion of hobbyist users is the same for those under thirty as those at least thirty
Ha: Porportion of hobbyist users is different for those under thirty to those at least thirty

H0: p>=30 - p<30 =0
Ha p>=30 - p<30 != 0

alpha=0.05

z=((p>=30 - p<30) -0) / SE(p>=30 - p<30)


p_hats = stack_overflow.groupby("age_cat")["hobbyist"].value_counts(normalize=True)

At least 30  Yes 0.77
	     No 0.226
Under 30     Yes 0.84
	     No 0.15

n=stack_overlow.groupby("age_cat")['hobbyist'].count()

At least 30  - 1050
Under 30 - 1211


p_hat_at_least_30 = p_hats[("at least 30", "Yes")]
p_hat_under_30 = p_hats[("Under 30","Yes")]

n_at_least_30 = n["At least 30"]
n_under_30 = n["Under 30"]


p_hat(n_at_least_30 * p_hat_at_least_30 + n_under_30* p_hat_under_30)/ (n_at_least_30 + n_under_30)

std_errr = np.sqrt(p_hat * (1-p_hat) / n_at_least_30 +
		p_hat * (1-p_hat) / n_under_30)

z_score = (p_hat_at_least_30 - p_hat_under_30) / std_error

-4.222

>>>>>>

n_hobbyist = np.array([812, 1021])
n_rows= np.array([812+238, 1021+190])

form statsmodels.stats.proportion import proportions_ztest
z_score, p_value = proportions_ztest(count=n_hobbyists, nobs=n_rows,alternative="two-sided")

-4.2, 2.40-05

2.40-05 is less than 0.05 so we reject the H0 

>>>>>>>
Freight costs are stored in the freight_cost_group column, and the categories are "expensive" and "reasonable".


# Calculate the pooled estimate of the population proportion
p_hat = (p_hats["reasonable"] * ns["reasonable"] + p_hats["expensive"] * ns["expensive"]) / (ns["reasonable"] + ns["expensive"])

# Calculate p_hat one minus p_hat
p_hat_times_not_p_hat = p_hat * (1 - p_hat)

# Divide this by each of the sample sizes and then sum
p_hat_times_not_p_hat_over_ns = p_hat_times_not_p_hat / ns["expensive"] + p_hat_times_not_p_hat / ns["reasonable"]

# Calculate the standard error
std_error = np.sqrt(p_hat_times_not_p_hat_over_ns)

# Print the result
print(std_error)

# Calculate the z-score
z_score = (p_hats["expensive"]-p_hats["reasonable"])/std_error

# Print z_score
print(z_score)

3.119

p_value = 1-norm.cdf(z_score)

# Print p_value
print(p_value)

0.00090721

This tiny p-value leads us to suspect there is a larger proportion of late shipments for expensive freight compared to reasonable freight.

>>>>>

# Count the late column values for each freight_cost_group
late_by_freight_cost_group = late_shipments.groupby("freight_cost_group")["late"].value_counts()

# Print the counts
print(late_by_freight_cost_group)

freight_cost_group  late
expensive           No      500
                    Yes      45
reasonable          No      439
                    Yes      16


results=late_shipments.pivot_table(index=['freight_cost_group'], columns='late', aggfunc='size', fill_value=0)
success_expensive=results.loc["expensive"]["Yes"]
fail_expensive=results.loc["expensive"]["No"]
success_reasonable=results.loc["reasonable"]["Yes"]
fail_reasonable=results.loc["reasonable"]["No"]

success_counts = np.array([success_expensive, success_reasonable])

n = np.array([success_expensive + fail_expensive, success_reasonable + fail_reasonable])

from statsmodels.stats.proportion import proportions_ztest

print(stat, p_value)

stat, p_value = proportions_ztest(count=success_counts, nobs=n,
                                  alternative="larger")


>>>>>>> chi-squared test of independence

statistical independence - proportion of successes in the response variable is the same across all categories of the explanatory variable

import pingouin
expected, observed, stats = pingouin.chi2_independence(data=stack_overflow, x='hobbyist', y='age_cat', correction = False)

print(stats)

>>>>>>

age_category:
1. Under 30
2. At Least 30

job_sat:
Very satisfied
Slightly satisfied
Slightly dissatisified
Neither
Very dissatisfied

h0: Age categories are independent of job satisification levels
ha: Age categories are not independent of job satisification levels

alpha=0.1

assuming independence, how far away are the observed results from the expected values

props=stack_overflow.groupby('job_sat')['age_cat'].value_counts(normalize=True)
wide_props=props.unstack()

wide_props.plot(kind="bar",stacked=True)

most 30+ workers are either very dissatified with their job or very satified with their job

expected, observed, stats = pingouin.chi2_independence(data=stack_overflow, x='job_sat', y='age_cat', correction = False)

pearson pval = .235

pval is greater than alpha at 0.1
 
failed to reject the null hypothesis, therefore age categories are independent of job satisification.

degrees of freedom
subtract 1 from each of the category count and multiple them

(2-1) *(5-1)=4

squared numbers are non-negative
chi-square tests are almost always right-tailed

>>>>>>>
# Proportion of freight_cost_group grouped by vendor_inco_term
props = late_shipments.groupby("vendor_inco_term")["freight_cost_group"].value_counts(normalize=
True)

# Print props
print(props)


vendor_inco_term  freight_cost_group
CIP               reasonable             34
                  expensive              22
DDP               expensive              55
                  reasonable             45
EXW               expensive             430
                  reasonable            302
FCA               reasonable             73
                  expensive              38

wide_props = props.unstack()

wide_props.plot(kind="bar",stacked=True)

expected, observed, stats = pingouin.chi2_independence(data=late_shipments, x='freight_cost_group', y='vendor_inco_term', correction = False)

   test  lambda    chi2  dof       pval  cramer  power
0  pearson     1.0  28.941  3.0  2.304e-06    0.17  0.998


Reject the null hypothesis and conclude that vendor_inco_term and freight_cost_group are associated.


>>>>>chi square goodness of fit test

compare a single categorical variable to a hypothesis distribution


How do you feel when you discover that you've already visited the top resource in a coding problem?

purple_link_counts=stack_overflow['purple_link'].value_counts().rename_axis('purple_link').reset_index(name='n').sort_values('purple_link')


hypothesized = pd.DataFrame({
'purple_link':['Amused','Annoyed','Hello, old friend','Indifferent'],
'prop':[1/6,1/6,1/2,1/6]
})

H0: The sample matches the hypothesized distribution

Ha: The sample does not match the hypothesized distribution

X**2 measure how far observed results are from expectation in each group

alpha=0.01

n_total=len(stack_overflow)
hypothesized["n"]= hypothesized["prop"] * n_total

plt.bar(purple_link_counts['purple_link'], purple_link_counts[n], color='red',label='Observed')

plot.bar(hypothesized['purple_link'],hypothesizd['n'],alpha=0.5, color='blue',label='Hypothesized')
plt.legend()
plt.show()

>>>>> chi-squared goodness of fit test

how well you hypothesized date fits with observed data

from scipy.stats import chisquare

chisquare(f_obs=purple_link_counts['n'], f_exp=hypothesized['n'])

pvalue is 1.126 e-09 or very small  (reject H0  and the sample data does not match the hypothesized distribution)

>>>>>>

n_total = len(late_shipments)

999

hypothesize 4 values
CIP: 0.05
DDP: 0.1
EXW: 0.75
FCA: 0.1

hypothesized = pd.DataFrame({
'vendor_inco_term':['CIP','DDP','EXW','FCA'],
'prop':[0.05,0.1,0.75,0.1]
})

observed:
EXW    732
FCA    111
DDP    100
CIP     56


# Find the number of rows in late_shipments
n_total = len(late_shipments)

hypothesized = pd.DataFrame({
'vendor_inco_term':['CIP','DDP','EXW','FCA'],
'prop':[0.05,0.1,0.75,0.1]
})

# Create n column that is prop column * n_total
hypothesized["n"]= hypothesized["prop"] * n_total

# Print the modified hypothesized DataFrame
print(hypothesized)

>>>>

# Find the number of rows in late_shipments
n_total = len(late_shipments)

# Create n column that is prop column * n_total
hypothesized["n"] = hypothesized["prop"] * n_total

incoterm_counts=late_shipments['vendor_inco_term'].value_counts().rename_axis('vendor_inco_term').reset_index(name='n').sort_values('vendor_inco_term')


# Plot a red bar graph of n vs. vendor_inco_term for incoterm_counts
plt.bar(incoterm_counts['vendor_inco_term'], incoterm_counts['n'],color='red')

# Add a blue bar plot for the hypothesized counts
plt.bar(hypothesized['vendor_inco_term']
,hypothesized['n']
,color="blue"
,alpha=0.5
,label="Hypothesized")

plt.legend()
plt.show()

Power_divergenceResult(statistic=2.3633633633633613, pvalue=0.5004909543758689)

accept the H0 hypothesis


>>>>> assumptions in hypothesis testing

assumption:
	1. The samples are random subsets of larger populations
	2. Each observation (row) in the dataset is independent
	3. The sample is big enough to mitigate uncertainty, so that the central limit theorem applies

consequence
	1. Sample is not representative of population
	2. Increase chance of false negative/positive error
	3. Wider confidence intervals

how to check this
	1. Understand how your data was collected
	2. Speak to the data collector/domain expert
	3. Understand how our data was collected

Large sample size:t-test
	1. at least 30 observations in the sample


	two samples
	n1 >=30, n2 >=30

Paired samples
	1. at least 30 pairs of observations across the sample

One sample
	1. Number of successes in sample is greater than or equal to 10
	2. Number of failures in sample is greater than or equal to 10


chi-squared test only requires 5 successes and 5 failures

sanity check
	1. if the bootstrap distribution doesn't look normal, assumptions likely aren't valid

revisit data collection to check for randomness, independence, and sample size


# Count the freight_cost_group values
counts = late_shipments["freight_cost_group"].value_counts()

# Print the result
print(counts)

# Inspect whether the counts are big enough
print((counts >= 30).all())


# Count the late values
counts = late_shipments["late"].value_counts()

# Print the result
print(counts)

# Inspect whether the counts are big enough
print((counts >= 10).all())


# Count the values of freight_cost_group grouped by vendor_inco_term
counts = late_shipments.groupby("vendor_inco_term")["freight_cost_group"].value_counts()

# Print the result
print(counts)

# Inspect whether the counts are big enough
print((counts >= 5).all())

# Count the shipment_mode values
counts = late_shipments["shipment_mode"].value_counts()

# Print the result
print(counts)

# Inspect whether the counts are big enough
print((counts >= 30).all())

>>>>>>> Non-parametric test


z-test, t-test, anova are all parametric tests

parametric tests assume a normal distribution

require sufficient large sample sizes

>>>>>> republican votes
state
county
rep_vote_percent_2008
rep_vote_percent_2012


5 pairs is not enough to meet the sample size condition for the paired

1. at least 30 pairs of observations across the samples

alpha=0.01

>>> less left tail test

pingouin.ttest(x=repub_votes_potus_08_12_small["repub_percent_08"],
y=repub_votes_potus_08_12_small["repub_percent_12"],
	paired=True,
	alternative="less")

p-val=0.002096 is less than alpha

reject h0

h0 2008 election had the same voter percentage as the 2012 election.
ha the 2008 had a smaller percentage of votes was less than the 2012 election

>>>>>> non-parameteric tests

x=[1,15,3,10,6]

non-parametric tests are more reliable than parameteric tests for small sample sizes and when data isn't normally distributed

Wilcoxon-signed rank test

1. works on ranked absolute differences between the pairs of data

repub_votes_small['diff']=repub_votes_small['repub_percent_08']-repub_votes_small['repub_percent_12']
repub_votes_small['diff']=repub_votes_small['diff'].abs()

from scipy.stats import rankdata

repub_votes_small['rank_abs_diff']=rankdata(repub_votes_small['diff'])

W=np.min([T_minus, T_plus])


pingouin.wilcoxon(x=repub_votes_potus_08_12_small["repub_percent_08"],
y=repub_votes_potus_08_12_small["repub_percent_12"],
	alternative="less")

W-val 0
p-value 0.03125  or 3%  which is 10 times larger than the p-value of the t-test

Failed to reject H0, since 0.03125 > 0.01 (alpha)

>>>>>>>>>>

# Conduct a paired t-test on dem_percent_12 and dem_percent_16
paired_test_results = pingouin.ttest(x=sample_dem_data["dem_percent_12"],
y=sample_dem_data["dem_percent_16"],
	paired=True,
	alternative="greater")

# Print paired t-test results
print(paired_test_results)


h0: the votes percentage for 2012 and 2016 are the same
ha: the voting percentage for 2012 are greater than the voting percentages for 2016

             T  dof alternative       p-val        CI95%  cohen-d        BF10  power
T-test  30.298  499     greater  1.800e-115  [6.46, inf]    0.454  4.491e+111    1.0

# Conduct a Wilcoxon test on dem_percent_12 and dem_percent_16
wilcoxon_test_results = pingouin.wilcoxon(x=sample_dem_data["dem_percent_12"],
y=sample_dem_data["dem_percent_16"],
	alternative="greater")



# Print Wilcoxon test results
print(wilcoxon_test_results)

             W-val alternative      p-val    RBC   CLES
Wilcoxon  122849.0     greater  8.902e-78  0.962  0.645


>>>>> Non-parameteric ANOVA

An ANOVA test is a type of statistical test used to determine if there is a statistically significant difference between two or more categorical groups by testing for differences of means using variance.

examine the converted_comp=converted compensation and age first code cut

age_vs_comp= stack_overflow[['converted_com','age_first_code_cut']]

age_vs_comp_wide= age_vs_comp.pivot(columns='age_first_code_cut','values='converted_comp')

results in columns: age_first_code_cut, adult, child

alpha=0.01

pingouin.mwu(x=age_vs_comp_wide['child'],
	y=age_vs_comp_wide['adult'],
	alternative='greater')

>>>> a right tail test

p-val 1.9 e-19

ha: those programmers that code first as a child have a higher income than those who code as adults

reject h0

>>>> non-parameteric with more than one group
krustal works on block data

pingouin.kruskal(data=stack_overflow,
	dv='converted_comp', between='job_sat')

p-unc is the p-value
5.777e-15

accept the Ha


>>>> mwu

# Select the weight_kilograms and late columns
weight_vs_late = late_shipments[['weight_kilograms','late']]


# Convert weight_vs_late into wide format
weight_vs_late_wide = weight_vs_late.pivot(columns='late', 
                                           values='weight_kilograms')


# Run a two-sided Wilcoxon-Mann-Whitney test on weight_kilograms vs. late
wmw_test = pingouin.mwu(x=weight_vs_late_wide ["No"],
y=weight_vs_late_wide ["Yes"],
	alternative="two-sided")



# Print the test results
print(wmw_test)

       U-val alternative      p-val    RBC   CLES
MWU  19134.0   two-sided  1.371e-05  0.332  0.334

The small p-value here leads us to suspect that a difference does exist in the weight of the shipment and whether or not it was late. The Wilcoxon-Mann-Whitney test is useful when you cannot satisfy the assumptions for a parametric test comparing two means, like the t-test.


# Run a Kruskal-Wallis test on weight_kilograms vs. shipment_mode
kw_test = pingouin.kruskal(data=late_shipments,
    dv="weight_kilograms",
    between="shipment_mode"
)



# Print the results
print(kw_test)

 Source  ddof1        H      p-unc
    Kruskal  shipment_mode      2  125.097  6.849e-28

he Kruskal-Wallis test yielded a very small p-value, so there is evidence that at least one of the three groups of shipment mode has a different weight distribution than the others. Th Kruskal-Wallis test is comparable to an ANOVA, which tests for a difference in means across multiple groups.









































































































































































