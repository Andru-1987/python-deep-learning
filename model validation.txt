model validation consists of
1. ensuring your model performs as expected on new data
2. testing model performance on holdout datasets
3. selecting the best model, parameters, and accuracy metrics
4. acheiving the best accuracy for the data given

python

X_train,X_test,y_train,y_test=train_test_split(pd.DataFrame(features),labels,test_size=.3)
model=RandomForestRegressor( random_state=1111)

model.fit(X_train, y_train)

predictions=model.predict(X_test)


score=mae(y_true=y_test, y_pred=predictions)
print("{0:.2f}".format(score))

for i, item in enumerate(model.feature_importances_):
    print("{0:s} : {1:2f}".format(NUMERIC[i],item))

candy ranking

>>>>Sample  >>> train vs test accuracy

# The model is fit using X_train and y_train
model.fit(X_train, y_train)

# Create vectors of predictions
train_predictions = model.predict(X_train)
test_predictions = model.predict(X_test)

# Train/Test Errors
train_error = mae(y_true=y_train, y_pred=train_predictions)
test_error = mae(y_true=y_test, y_pred=test_predictions)

# Print the accuracy for seen and unseen data
print("Model error on seen data: {0:.2f}.".format(train_error))
print("Model error on unseen data: {0:.2f}.".format(test_error))

>>>>>>>>> Regression models

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier

rfr=RandomForestRegressor(random_state=111)
rfc=RandomForestClassifier(random_state=111)

n_estimators: number of trees in the forest

max_depth: the maximum depth of the trees
random_state: random seed

rfr=RandomForestRegressor(n_estimators=50, max_depth=10)

>>>>>sample  >>> initializing the random forest classifier

# Set the number of trees
rfr.n_estimators = 100

# Add a maximum depth
rfr.max_depth = 6

# Set the random state
rfr.random_state = 1111

# Fit the model
rfr.fit(X_train,y_train)

for i, item in enumerate(rfr.feature_importances_):
	print("{0:s}: {1:2f}".format(X.columns[i],item))

prints the column and its importance

>>>>>>>sample  >>> print the feature importance

 Fit the model using X and y
rfr.fit(X_train, y_train)

# Print how important each column is to the model
for i, item in enumerate(rfr.feature_importances_):
      # Use i and item to print out the feature importance of each column
    print("{0:s}: {1:.2f}".format(X_train.columns[i], item))


>>>>>>>>>>>>>sample classification Random Forest

rfc.predict_proba(X_test)
rfc.get_params()
rfc.score(X_test,y_test)


# Fit the rfc model. 
rfc.fit(X_train, y_train)

# Create arrays of predictions
classification_predictions = rfc.predict(X_test)
probability_predictions = rfc.predict_proba(X_test)

# Print out count of binary predictions
print(pd.Series(classification_predictions).value_counts())

# Print the first value from probability_predictions
print('The first predicted probabilities are: {}'.format(probability_predictions[0]))

output:
1    563
0    204
dtype: int64
The first predicted probabilities are: [0.26524423 0.73475577]


>>>>>>>sample  >>>> print information about the randomforest model

rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111)

# Print the classification model
print(rfc)

# Print the classification model's random state parameter
print('The random state is: {}'.format(rfc.random_state))

# Print all parameters
print('Printing the parameters dictionary: {}'.format(rfc.get_params()))

output:

RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=6, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,
            oob_score=False, random_state=1111, verbose=0,
            warm_start=False)
The random state is: 1111
Printing the parameters dictionary: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': 6, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 1111, 'verbose': 0, 'warm_start': False}
In [1]:


>>>>>>sample >>> create a randomforestclassifier

from sklearn.ensemble import RandomForestClassifier

# Create a random forest classifier
rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111)

# Fit rfc using X_train and y_train
rfc.fit(X_train, y_train)

# Create predictions on X_test
predictions = rfc.predict(X_test)
print(predictions[0:5])

print(rfc.score(X_test, y_test))

>>>>>>>>>>>>>>>>train test and validate

X=df.iloc[:,1:10]
y=df.iloc[:,10]
y=y.apply(lambda x: 1 if x==True else 0)

X_train, X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=1111)


holdout sample for testing
80:20 split


validation holdout sample

X_temp, X_test,y_temp,y_test=train_test_split(X,y,test_size=0.2, random_state=1111)

X_train, X_val,y_train,y_val=train_test_split(X_temp,y_temp,test_size=0.25, random_state=1111)

>>>>>>>>Sample  training test datasets

# Create dummy variables using pandas
X = pd.get_dummies(tic_tac_toe.iloc[:,0:9])
y = tic_tac_toe.iloc[:, 9]

# Create training and testing datasets. Use 10% for the test set
X_train,X_test,y_train,y_test  = train_test_split(X, y, test_size=.10, random_state=1111)


>>>>>sample create holdout (raining, validation, and testing datasets)

# Create temporary training and final testing datasets
X_temp, X_test, y_temp, y_test  =\
    train_test_split(X, y, test_size=.20, random_state=1111)

# Create the final training and validation datasets
X_train,X_val,y_train,y_val =\
    train_test_split(X_temp, y_temp, test_size=.25, random_state=1111)

>>>>>>>>>>>>>>>>Accuracy metrics (regression)

regression are used for continueous variables.

mean absolute error (absolute between predictions and actuals)/n

mean squared error (MSE)

(prediction - actual)**2/n

* larger errors will have a larger impact on the model.

* allows outlier errors to contribute more to the overall error

MAE and MSE are in different units and should not be compared.

rfr=RandomForestRegressor(n_estimators=500, random_state=1111)

#Mean absolute Error

rfr.fit(X_train, y_train)
test_predictions=rfr.predict(X_test)
sum(abs(y_test-test_predictions))/len(test_predictions)

or use
mean_absolute_error(y_test, test_predictions)

#Mean squared error

from sklearn.metrics import mean_squared_error

sum(abs(y_test-test_predictions)**2)/len(test_predictions)

or

mean_squared_error(y_test,test_predictions)


>>>>testing just chocolate

chocolate_preds=rfr.predict(X_test[X_test[:,1]==1])
mean_absolute_error(y_test[X_test[:,1]==1],chocolate_preds)


nonchocolate_preds=rfr.predict(X_test[X_test[:,1]==0])
mean_absolute_error(y_test[X_test[:,1]==0],nonchocolate_preds)


>>>>sample Mean absolute error calculations

from sklearn.metrics import mean_absolute_error

# Manually calculate the MAE
n = len(predictions)
mae_one = sum(abs(y_test - predictions)) / n
print('With a manual calculation, the error is {}'.format(mae_one))

# Use scikit-learn to calculate the MAE
mae_two = mean_absolute_error(y_test, predictions)
print('Using scikit-lean, the error is {}'.format(mae_two))

>>>> sample >>> mean squared error

from sklearn.metrics import mean_squared_error

n = len(predictions)
# Finish the manual calculation of the MSE
mse_one = sum((y_test - predictions)**2) / n
print('With a manual calculation, the error is {}'.format(mse_one))

# Use the scikit-learn function to calculate MSE
mse_two = mean_squared_error(y_test,predictions)
print('Using scikit-lean, the error is {}'.format(mse_two))

>>>>sample

east_teams = labels == 'E'

# Create arrays for the true and predicted values
true_east = y_test[east_teams]
preds_east = predictions[east_teams]

# Print the accuracy metrics
print('The MAE for East teams is {}'.format(
    mae(true_east, preds_east)))


output:
The MAE for East teams is 6.733333333333333


>>>>>>>>>>>>>Classification metrics

precision
recall
accuracy
specificity
f1-score


precision, recall, accuracy

confusion matrix 
1. True Negative s(TN) predict 1 and actual 1
2. True Positive (TP) predict 0 and actual 0
3. False Positive (FP) predict 1 actual 0
4. False Negative (FN) predict 0 actual 1

from sklearn.metrics import confusion_matrix

cm=confusion_matrix(y_test,test_predictions)
print(cm)

Accuracy=TN+TP/total number of observations


Precision=TP / (TP+FP)

Recall=TP/(TP+FN)
1) reduces the risk of make a wrong prediction

accuracy_score(y_test, test_prediction)
precision_score(y_test, test_prediction)
recall_score(y_test, test_prediction)


>>>>sample  >>> calculate accuracy, precision, recall

# Calculate and print the accuracy
accuracy = (324+ 491) / (953)
print("The overall accuracy is {0: 0.2f}".format(accuracy))

# Calculate and print the precision
precision = (491) / (491 + 15)
print("The precision is {0: 0.2f}".format(precision))

# Calculate and print the recall
recall = (491) / (491 + 123)
print("The recall is {0: 0.2f}".format(recall))



Prediction: 0	Prediction: 1
Actual: 0	324 (TN)	15 (FP)
Actual: 1	123 (FN)	491 (TP)


>>>>sample  >>> confusion matrix

from sklearn.metrics import confusion_matrix

# Create predictions
test_predictions = rfc.predict(X_test)

# Create and print the confusion matrix
cm = confusion_matrix(y_test, test_predictions)
print(cm)

# Print the true positives (actual 1s that were predicted 1s)
print("The number of true positives is: {}".format(cm[1, 1]))



>>>>sample >>> calculating precision: tp/tp+fp

from sklearn.metrics import precision_score

test_predictions = rfc.predict(X_test)

# Create precision or recall score based on the metric you imported
score = precision_score(y_test, test_predictions)

# Print the final result
print("The precision value is {0:.2f}".format(score))


>>>>>>>>>>>>>>>>>bias variance tradeoff

variance following the training data to closely but fails to generalize

overfitting occurs when the model pays attention to noise in the data

bias

bias occurs when the model fails to find the relationship between the data and the response

bias leads to high errors in both the training and testing datasets and it is associated with underfit.

underfitting occurs when the model could not find the patterns in the data.

rfc=RandomForestClassifier(n_estimator=100,max_depth=4)
rfc.fit(X_train, y_train)

print("Training: {0:.2f}".format(y_train,train_predictions))}

output .84

print("Testing: {0:.2f}".format(y_test,test_predictions))}

output:.77

training accuracy is higher than the testing accuracy

the testing is probabily underfitting

the max_depth of 4 is probably not deep enough

>>>>>>>sample    regressor  mae

# Update the rfr model
rfr = RandomForestRegressor(n_estimators=25,
                            random_state=1111,
                            max_depth=2)
rfr.fit(X_train, y_train)

# Print the training and testing accuracies 
print('The training error is {0:.2f}'.format(
  mae(y_train, rfr.predict(X_train))))
print('The testing error is {0:.2f}'.format(
  mae(y_test, rfr.predict(X_test))))


>>>>>>>>sample multiple accuracy scores - changing estimators

rom sklearn.metrics import accuracy_score

test_scores, train_scores = [], []
for i in [1, 2, 3, 4, 5, 10, 20, 50]:
    rfc = RandomForestClassifier(n_estimators=i, random_state=1111)
    rfc.fit(X_train, y_train)
    # Create predictions for the X_train and X_test datasets.
    train_predictions = rfc.predict(X_train)
    test_predictions = rfc.predict(X_test)
    # Append the accuracy score for the test and train predictions.
    train_scores.append(round(accuracy_score(y_train, train_predictions), 2))
    test_scores.append(round(accuracy_score(y_test, test_predictions), 2))
# Print the train and test scores.
print("The training scores were: {}".format(train_scores))
print("The testing scores were: {}".format(test_scores))


>>>>>>>>>>holdout set for validation


cross validation

>>>sample


# Create two different samples of 200 observations 
sample1 = tic_tac_toe.sample(200, random_state=1111)
sample2 = tic_tac_toe.sample(200, random_state=1171)

# Print the number of common observations 
print(len([index for index in sample1.index if index in sample2.index]))

output: 40 samples in common


print(sample1['Class'].value_counts())
print(sample2['Class'].value_counts())

>>>>>>>>>>>>>>five fold cross validation

number of cross validation splits
shuffle data before splitting
random_state


X=np.array(range(40))
y=np.array)([0]*20+[1]*20)

kf=KFold(n_splits=5)

splits=kf.split(X)

produces a list of indices


rfr=RandomForestRegressor(n_estimators=25, random_state=1111)
errors=[]

for train_index, val_index in splits:
	X_train, y_train = X[train_index], y[train_index]
	X_val, y_val=X[val_index], y[val_index]

	rfr.fit(X_train, y_train)
	predictions=rfr.predict(X_test)
	errors.append(<some_accuracy_metric>)


>>>>>>>>Sample  >>> creating the train and val split indices

from sklearn.model_selection import KFold

# Use KFold
kf = KFold(n_splits=5,shuffle=True,random_state=1111)

# Create splits
splits = kf.split(X)

# Print the number of indices
for train_index, val_index in splits:
    print("Number of training indices: %s" % len(train_index))
    print("Number of validation indices: %s" % len(val_index))

>>>>>>sample >>> cross validation accuracy

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

rfc = RandomForestRegressor(n_estimators=25, random_state=1111)

# Access the training and validation indices of splits
for train_index, val_index in splits:
    # Setup the training and validation data
    X_train, y_train = X[train_index], y[train_index]
    X_val, y_val = X[val_index], y[val_index]
    # Fit the random forest model
    rfc.fit(X_train, y_train)
    # Make predictions, and print the accuracy
    predictions = rfc.predict(X_val)
    print("Split accuracy: " + str(mean_squared_error(y_val, predictions)))


from sklearn.metrics import mean_absolute_error, make_scorer

mae_scorer = make_scorer(mean_absolute_error)

mse= make_scorer(mean_squared_error)

cv_results=cross_val_score(rfc, X,y,cv=5, scoring=mse)

print(cv_results)

print("the mean:{}".format(cv_results.mean()))
print("the std:{}".format(cv_results.std()))

output: 150.9
51.67



>>>>> sample >>> cross_val_score


rfc = RandomForestRegressor(n_estimators=25, random_state=1111)
mse = make_scorer(mean_squared_error)

# Set up cross_val_score
cv = cross_val_score(estimator=rfc,
                     X=X_train,
                     y=y_train,
                     cv=10,
                     scoring=mse)

# Print the mean error
print(cv.mean())

>>>>>>leave one out cross-validation

use a single point for a complete validation set

present the average error

best error estimated possible for a single end point

n=X.shape[0]

mse=make_scorer(mean_squared_error)

cv_results=cross_val_score(estimator,X,y,scoring=mse, cv=n


>>>> sample >>>  cross_val_score

from sklearn.metrics import mean_absolute_error, make_scorer

# Create scorer
mae_scorer = make_scorer(mean_absolute_error)

rfr = RandomForestRegressor(n_estimators=15, random_state=1111)

# Implement LOOCV
scores = cross_val_score(rfr, X=X, y=y, cv=len(X), scoring=mae_scorer)

# Print the mean and standard deviation
print("The mean of the errors is: %s." % np.mean(scores))
print("The standard deviation of the errors is: %s." % np.std(scores))


>>>>>>>>>>>>>>>>hyper parameter tuning


regression parameters
lr.coeff_, lr.intercept

hyper parameters
n_estimators: number of trees
max_depth: maximum depth of the tree
max_features: max number of features
min_sample_split: minimum number of samples required to make a split

depth=[4.6.9.10,12]
samples=[2,4,6,8]
feature=[2,4,6,8,10]


rfc=RandomForestRegressor(
	n_estimators=100,
	max_depth=depth[0],
	min_sample_split=samples[3],
	max_features=features[1])

rfr.get_params()

16 different hyperparameters

>>>>sample

# Review the parameters of rfr
print(rfr.get_params())
output:
{'bootstrap': True, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 'warn', 'n_jobs': None, 'oob_score': False, 'random_state': 1111, 'verbose': 0, 'warm_start': False}
In [1]:


# Maximum Depth
max_depth = [4,8,12]

# Minimum samples for a split
min_samples_split = [2,5,10]

# Max features 
max_features = [4,6,8,10]


from sklearn.ensemble import RandomForestRegressor

# Fill in rfr using your variables
rfr = RandomForestRegressor(
    n_estimators=100,
    max_depth=random.choice(max_depth),
    min_samples_split=random.choice(min_samples_split),
    max_features=random.choice(max_features))

# Print out the parameters
print(rfr.get_params())


output:

{'bootstrap': True, 'criterion': 'mse', 'max_depth': 4, 'max_features': 10, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}
In [1]:
;

grid searching is only possible with a limited number of searching and a limited number of parameters


random searching
bayesian optimization


from sklearn.model_selection import RandomizedSearchCV

random_search=RandomizedSearchCV()

param_dist={"max_mepth":[4,6,8,None],
	"max_features":range(2,11),
	"min_samples_split":range(2,11)}


n_iter: number of models to run
scoring: scoring method to use

from sklearn.metrics import make_scorer, mean_absolute_error

scorer=make_scorer(mean_absolute_error)

random_search=RandomizedSearchCV(estimator=rfr,
param_distributions=param_dist,
n_iter=40,
cv=5)

random_search.fit(X,y)


>>>>>>>sample  >>> random search regressor


rom sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import make_scorer, mean_squared_error

# Finish the dictionary by adding the max_depth parameter
param_dist = {"max_depth": [2,4,6,8],
              "max_features": [2, 4, 6, 8, 10],
              "min_samples_split": [2, 4, 8, 16]}

# Create a random forest regression model
rfr =RandomForestRegressor(n_estimators=10, random_state=1111)

# Create a scorer to use (use the mean squared error)
scorer = make_scorer(mean_squared_error)


# Import the method for random search
from sklearn.model_selection import RandomizedSearchCV

# Build a random search using param_dist, rfr, and scorer
random_search =\
    RandomizedSearchCV(
        estimator=rfr,
        param_distributions=param_dist,
        n_iter=10,
        cv=5,
        scoring=scorer)

>>>>>>>>>selecting the final model


random_search.best_score_
random_search.best_params_
random_search.best_estimator_
random_search.results_

random_search.results_['mean_test_score']
random_search.results_['params']

max_depth=[item['max_depth'] for item in random_search.cv_results_['params']]

scores=list(random_search.results_['mean_test_score'])

d=pd.DataFrame([max_depth,scores]).T
d.columns=['Max Depth','Score']

d.groupby(['Max Depth']).mean()

>>>>>>sample >>> random search CV

{'n_estimators': [10, 25, 50], 'max_depth': range(2, 12, 2), 'min_samples_split': range(2, 12, 2)}
In [1]:

from sklearn.metrics import precision_score, make_scorer

# Create a precision scorer
precision = make_scorer(precision_score)
# Finalize the random search
rs = RandomizedSearchCV(
  estimator=rfc, param_distributions=param_dist,
  scoring = precision,
  cv=5, n_iter=10, random_state=1111)
rs.fit(X, y)

# print the mean test scores:
print('The accuracy for each run was: {}.'.format(rs.cv_results_['mean_test_score']))
# print the best model score:
print('The best accuracy for a single model was: {}'.format(rs.best_score_))


The accuracy for each run was: [0.86446668 0.75302055 0.67570816 0.88459939 0.88381178 0.86917588
 0.68014695 0.81721906 0.87895856 0.92917474].
The best accuracy for a single model was: 0.9291747446879924




