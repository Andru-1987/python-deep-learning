>>>Dimensions reduction PCA

1. express data in a compress formed
2. a big deal in big data sets
3. remove less-informative "noise" features which cause problems for prediction tasks e.g. classification regression.


>>>Principal component Analysis (PCA)

1. A fundamental dimension reduction technique
2. Rotates data samples to be aligned with axes
3. shifts data so they have mean 0

pca is sckit-learn component like kmeans or standardscaler

fit learns the transformation from given data
transform applies the learned transformation

PCA features are not correlated
1. features of dataset are often correlated like total_phenols
and od280

2. PCA aligns the data with axes
3. resulting PCA features are not linearly correlated
4. Pearson correlation is used to determine linear correlation

>>> Covariance

1) A measure of how two quantities vary together
2) if the distance from the x mean and the distance from the y mean are positive than the point is positively correlated

1/m (x-x_mean)*(y-y_mean)  if positive then positively correlated

if y is below the mean then the variable is negatively correlated

pearson correlation = covariance/(std of x)*(std of y)

pearson is a value between -1 and 1

value of 0 means no linear correlation

Principal components
1. Principal components = directions of variance
2. PCA aligns principal components with the axes

print(model.components_)

>>>Sample pearson correlation of grains width and length

# Perform the necessary imports
import matplotlib.pyplot as plt
from scipy.stats import pearsonr


# Assign the 0th column of grains: width
width = grains[:,0]

# Assign the 1st column of grains: length
length = grains[:,1]

# Scatter plot width vs length
plt.scatter(width,length)
plt.axis('equal')
plt.show()

# Calculate the Pearson correlation
correlation, pvalue = pearsonr(width,length)

# Display the correlation
print(correlation)

>>>>Sample

from sklearn.datasets import load_wine
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

data = load_wine()
X=data.data
y=data.target
print(data.feature_names)

model=PCA()
model.fit(X)
pca_features=model.transform(X)
#print(pca_features)
#transformed returns an array with the same number of rows and columns as the original sample

xs = pca_features[:,0]

# Assign 1st column of pca_features: ys
ys = pca_features[:,1]

# Scatter plot xs vs ys
plt.scatter(xs, ys)
plt.axis('equal')
plt.show()


correlation, pvalue = pearsonr(xs,ys)

print("the pca correlation ",correlation)

>>>>Intrinsic dimension

1. Intrinsic dimension is the number of features needed to approximate the dataset
2. Essential idea behind dimension reduction
3. What is the most compact representation of the samples


iris data
take three measurements: sepal length, sepal width, petal width

when viewed in 3 dimensional space the features mostly lye on a single plane.  Therefore, the data can be represented using two features without losing to much data.

1. Intrinsic dimensions equal the number of pca features with significant variance.

2. PCA features are ordered by variance descending

3. Intrinsic dimension is the number of PCA features with significant variance.



import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

pca=PCA()
pca.fit(samples)

features = range(pca.n_components_)

plt.bar(features, pca.explained_variance_)
plt.xticks(features)
plt.ylabel('variance')
plt.xlabel('pca features')
plt.show()


>>>>>Sample

# Make a scatter plot of the untransformed points
plt.scatter(grains[:,0], grains[:,1])

# Create a PCA instance: model
model = PCA()

# Fit model to points
model.fit(grains)

# Get the mean of the grain samples: mean
mean = model.mean_

# Get the first principal component: first_pc
first_pc = model.components_[0,:]

# Plot first_pc as an arrow, starting at mean
plt.arrow(mean[0],mean[1], first_pc[0], first_pc[1], color='red', width=0.01)

# Keep axes on same scale
plt.axis('equal')
plt.show()

>>>Standard Scalar


# Perform the necessary imports
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt

# Create scaler: scaler
scaler = StandardScaler()

# Create a PCA instance: pca
pca = PCA()

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, pca)

# Fit the pipeline to 'samples'
pipeline.fit(samples)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xlabel('PCA feature')
plt.ylabel('variance')
plt.xticks(features)
plt.show()


>>>Dimension reduction with PCA

1. same data with less features
2. assumes low variance are noise
3. specify how many features to keep

PCA(n_components=2)

Intrinsic dimension is a good choice

from sklearn.decomposition import PCA

pca=PCA(n_components=2)
pca.fit(samples)
transformed=pca.transform(samples)
print(transformed.shape)

import matplotlib.pyplot as plt

xs=transformed[:,0]
ys=transformed[:,1]
plt.scatter(xs,ys,c=species)
plt.show()


>>>Word frequency arrays
1. rows represent documents, columns represent words
2. entries measure presence of each word in each document
3. most entries have a measure of 0

sparse arrays and csr_matrix

scipy.sparse.csr_matrix

csr_matrix remembers only the non-zero entries (saves space)

use scikit-learn TruncatedSVD because pca does not support csr_matrix

from sklearn.decomposition import TruncatedSVD

model=TruncatedSVD(n_components=3)
model.fit(documents)
TruncatedSVD(algorithm='randomize',...)
transformed=model.transform(documents)

>>>Sample

# Import PCA
from sklearn.decomposition import PCA

# Create a PCA model with 2 components: pca
pca = PCA(n_components=2)

# Fit the PCA instance to the scaled samples
pca.fit(scaled_samples)

# Transform the scaled samples: pca_features
pca_features = pca.transform(scaled_samples)

# Print the shape of pca_features
print(pca_features.shape)

>>Sample


from sklearn.feature_extraction.text import TfidfVectorizer

# Create a TfidfVectorizer: tfidf
tfidf = TfidfVectorizer() 

# Apply fit_transform to document: csr_mat
csr_mat = tfidf.fit_transform(documents)

# Print result of toarray() method
print(csr_mat.toarray())

# Get the words: words
words = tfidf.get_feature_names()

# Print words
print(words)

>>>>Sample Kmeans pipeline

# Perform the necessary imports
from sklearn.cluster import KMeans
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import TruncatedSVD

# Create a TruncatedSVD instance: svd
model=TruncatedSVD(n_components=50)

# Create a KMeans instance: kmeans
kmeans = KMeans(n_clusters=6)

# Create a pipeline: pipeline
pipeline = make_pipeline(model,kmeans)

>>Sample 

# Import pandas
import pandas as pd

# Fit the pipeline to articles
pipeline.fit(articles)

# Calculate the cluster labels: labels
labels = pipeline.predict(articles)

# Create a DataFrame aligning labels and titles: df
df = pd.DataFrame({'label': labels, 'article': titles})

# Display df sorted by cluster label
print(df.sort_values('label'))

