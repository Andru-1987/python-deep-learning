optimal parameters

1. Parameter values the bring the model in closest agreement with the data.

2. we use numpy to find optimal parameters for us

Packages to do statistical inference
1. scipy.stats
2. statsmodels


hacker statistics applies to a wide variety of problems

parameters to match to the best theoritical data model and your data

>>Sample

# Seed random number generator
np.random.seed(42)

# Compute mean no-hitter time: tau
tau = np.mean(nohitter_times)

# Draw out of an exponential distribution with parameter tau : inter_nohitter_time

inter_nohitter_time = np.random.exponential(tau, 100000)

# Plot the PDF and label axes
_ = plt.hist(inter_nohitter_time,
             bins=50, normed=True, histtype='step')
_ = plt.xlabel('Games between no-hitters')
_ = plt.ylabel('PDF')

# Show the plot
plt.show()

No hitter times - mean 764

[ 843 1613 1101  215  684  814  278  324  161  219  545  715  966  624
   29  450  107   20   91 1325  124 1468  104 1309  429   62 1878 1104
  123  251   93  188  983  166   96  702   23  524   26  299   59   39
   12    2  308 1114  813  887  645 2088   42 2090   11  886 1665 1084
 2900 2432  750 4021 1070 1765 1322   26  548 1525   77 2181 2752  127
 2147  211   41 1575  151  479  697  557 2267  542  392   73  603  233
  255  528  397 1529 1023 1194  462  583   37  943  996  480 1497  717
  224  219 1531  498   44  288  267  600   52  269 1086  386  176 2199
  216   54  675 1243  463  650  171  327  110  774  509    8  197  136
   12 1124   64  380  811  232  192  731  715  226  605  539 1491  323
  240  179  702  156   82 1397  354  778  603 1001  385  986  203  149
  576  445  180 1403  252  675 1351 2983 1568   45  899 3260 1025   31
  100 2055 4043   79  238 3931 2351  595  110  215    0  563  206  660
  242  577  179  157  192  192 1848  792 1693   55  388  225 1134 1172
 1555   31 1582 1044  378 1687 2915  280  765 2819  511 1521  745 2491
  580 2072 6450  578  745 1075 1103 1549 1520  138 1202  296  277  351
  391  950  459   62 1056 1128  139  420   87   71  814  603 1349  162
 1027  783  326  101  876  381  905  156  419  239  119  129  467]

# Create an ECDF from real data: x, y
x, y = ecdf(nohitter_times)

# Create a CDF from theoretical samples: x_theor, y_theor
x_theor, y_theor = ecdf(inter_nohitter_time)

# Overlay the plots
plt.plot(x_theor, y_theor)
plt.plot(x, y, marker='.', linestyle='none')

# Margins and axis labels
plt.margins(.20)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

# Show the plot
plt.show()


>>>linear regression finds correlation in the data

x=total votes in thousands
y=percent of votes for obama

the distance between the slope line and a point is called a residual. points below the line will have a negative residual.

The line will be where the sum of squares of residuals (which is the distance) is minimal.

np.polyfit();

#a line is a one degree polynomial

slope, intercept= np.polyfit(total_votes,
dem_share, 1)

#slope is rise over run
#a slope of 4 means we get 4 more voters for every 100,000 total votes

>>Sample

correlation between illiteracy and fertility

# Plot the illiteracy rate versus fertility
_ = plt.plot(illiteracy,fertility, marker='.', linestyle='none')

# Set the margins and label axes
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

# Show the plot
plt.show()

# Show the Pearson correlation coefficient
print(pearson_r(illiteracy, fertility))


# Plot the illiteracy rate versus fertility
_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

# Perform a linear regression using np.polyfit(): a, b
a, b = np.polyfit(illiteracy,fertility,1)

# Print the results to the screen
print('slope =', a, 'children per woman / percent illiterate')
print('intercept =', b, 'children per woman')

# Make theoretical line to plot
x = np.array([0,100])
y = a * x + b

# Add regression line to your plot
_ = plt.plot(x,y)

# Draw the plot
plt.show()

#It is optimizing the sum of the squares of the residuals, also known as RSS(for residual sum of squares)

# Specify slopes to consider: a_vals
a_vals = np.linspace(0, 0.1, 200)

# Initialize sum of square of residuals: rss
rss = np.empty_like(a_vals)

# Compute sum of square of residuals for each value of a_vals
for i, a in enumerate(a_vals):
    rss[i] = np.sum((fertility - a*illiteracy - b)**2)

# Plot the RSS
plt.plot(a_vals, rss, '-')
plt.xlabel('slope (children per woman / percent illiterate)')
plt.ylabel('sum of square of residuals')

plt.show()

>>>EDA

explore your data first

# Perform linear regression: a, b
a, b = np.polyfit(x,y,1)

# Print the slope and intercept
print(a, b)

# Generate theoretical x and y data: x_theor, y_theor
x_theor = np.array([3, 15])
y_theor =a * x_theor + b

# Plot the Anscombe data and theoretical line
_ = plt.plot(x,y, marker='.', linestyle='none')
_ = plt.plot(x_theor,y_theor)

# Label the axes
plt.xlabel('x')
plt.ylabel('y')

# Show the plot
plt.show()

>>sample 2

#Now, to verify that all four of the Anscombe data sets have the same slope and intercept from a linear regression, you will compute the slope and intercept for each set. 

# Iterate through x,y pairs
for x, y in zip(anscombe_x, anscombe_y):
    # Compute the slope and intercept: a, b
    a, b = np.polyfit(x,y,1)

    # Print the result
    print('slope:', a, 'intercept:', b)

>>bootstrap replicates

how to simulate the sampling an infinite number of times
we need to think probablistic

how
1. resample the data
2. recompute the function of interest like mean

we random select from the data, a data point, and store it in the resampling data.

bootstrapping is the resampling of data to perform statistical inference

import numpy as np

sample=np.random.choice([1,2,3,4,5],size=5)

mean=np.mean(sample)
median=np.median(sample)
std=np.std(sample)


>>>Sample

for i in range(50):
    # Generate bootstrap sample: bs_sample
    bs_sample = np.random.choice(rainfall, size=rainfall.size)

    # Compute and plot ECDF from bootstrap sample
    x, y = ecdf(bs_sample)
    _ = plt.plot(x, y, marker='.', linestyle='none',
                 color='gray', alpha=0.1)

# Compute and plot ECDF from original data
x, y = ecdf(rainfall)
_ = plt.plot(x, y, marker='.')

# Make margins and label axes
plt.margins(0.02)
_ = plt.xlabel('yearly rainfall (mm)')
_ = plt.ylabel('ECDF')

# Show the plot
plt.show()


>>Bootstrap confidence intervals
1. pdf is the probability distribution function
2. normed sets the probability to be no more than 1

def bootstrap_replicate_1d(data, func):
	bs_sample=np.random.choice(data, len(data))
	return func(bs_sample)

bs_replicates = np.empty(10000)

for i in range(10000):
	bs_replicates[i] = bootstrap_replicate_1d(
		data, np.mean)

_=plt.hist(bs_replicates,bins=30, normed=True)
_=plt.xlabel('mean of default')
_=plt.ylabel('pdf')
plt.show()

>>>compute the 95 percent confidence interval of the mean

if we repeated measurements over and over again, p% of the observed values would lie within the p% confidence interval.


conf_interval=np.percentile(bs_replicates,[2.5,97.5])

def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""

    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)

    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data,func)

    return bs_replicates

>>Sample

# Take 10,000 bootstrap replicates of the mean: bs_replicates

bs_replicates = draw_bs_reps(rainfall, np.mean, size=10000)

# Compute and print SEM
sem = np.std(rainfall) / np.sqrt(len(rainfall))
print(sem)

# Compute and print standard deviation of bootstrap replicates
bs_std = np.std(bs_replicates)
print(bs_std)

# Make a histogram of the results
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel('mean annual rainfall (mm)')
_ = plt.ylabel('PDF')

# Show the plot
plt.show()

>>>Sample 2 using variance

# Generate 10,000 bootstrap replicates of the variance: bs_replicates
bs_replicates = draw_bs_reps(rainfall, np.var, size=10000)

# Put the variance in units of square centimeters
bs_replicates/=100

# Make a histogram of the results
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel('variance of annual rainfall (sq. cm)')
_ = plt.ylabel('PDF')

# Show the plot
plt.show()

#if the tail to right of left is longer than the data is not normally distributed.  use np.var to determine if an normally distributed curve exists.

>>Sample 3 - confidence interval

# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates
bs_replicates = draw_bs_reps(nohitter_times, np.mean, size=10000)

# Compute the 95% confidence interval: conf_int
conf_int = np.percentile(bs_replicates,[2.5,97.5])

# Print the confidence interval
print('95% confidence interval =',conf_int, 'games')

# Plot the histogram of the replicates
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel(r'$\tau$ (games)')
_ = plt.ylabel('PDF')

# Show the plot
plt.show()

>>> Paired bootstrap

Nonparameter inference

the linear regression model is a parametric estimate
1. we can get the confidence intervals on the slope.  We need to think probabilistically.

we can resample pairs
1. compute the slope and intercept from the resampled data
2. each slope and intercept is a bootstrap replicate
3. we can get the confidence intervals from percentiles of the bootstrap replicates

we can access the pairs by indices.

we can generate the index of the pair by using np.arange(7)

we resample the indices

indices= np.arange(len(total_votes))

bs_inds= np.random.choice(indices, len(indices))

#slice out the data from the original data using the resampled indices

bs_total_votesm=total_votes[bs_inds]
bs_dem_share= dem_share[bs_inds]

#compute a linear regression using np.polyfit

bs_slope, bs_intercept=np.polyfit(bs_total_votes, bs_dem_share,1)

# Make theoretical line to plot
x = np.array([0,100])
y = bs_slope * x + bs_intercept

# Add regression line to your plot
_ = plt.plot(x,y)

# Draw the plot
plt.show()

>>sample
#find the optimal measurement.  you will discover how that parameter is likely to change after repeated measurements.


def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""

    # Set up array of indices to sample from: inds
    inds = np.arange(len(x))

    # Initialize replicates: bs_slope_reps, bs_intercept_reps
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)

    # Generate replicates
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)

    return bs_slope_reps, bs_intercept_reps


# Generate replicates of slope and intercept using pairs bootstrap

bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy,fertility,1000)

# Compute and print 95% CI for slope
print(np.percentile(bs_slope_reps, [2.5,97.5]))

# Plot the histogram
_ = plt.hist(bs_slope_reps, bins=50, normed=True)
_ = plt.xlabel('slope')
_ = plt.ylabel('PDF')
plt.show()

# Generate array of x-values for bootstrap lines: x
x = np.array([0,100])
plt.clf()
# Plot the bootstrap lines
for i in range(100):
    _ = plt.plot(x, 
                 bs_slope_reps[i]*x + bs_intercept_reps[i],
                 linewidth=0.5,alpha=0.2, color='red')


# Plot the data
_ = plt.plot(illiteracy,fertility, marker='.', linestyle='none')

# Label axes, set the margins, and show the plot
_ = plt.xlabel('illiteracy')
_ = plt.ylabel('fertility')
plt.margins(0.02)
plt.show()


>>>Formulating and simulating an hypothesis

permutation
1. random reordering of entries in an array
2. we assume the two entities we are comparing are identically distributed

3. make a single array with all the entities in it

import numpy as np

dem_share_both = np.concatenate(dem_share_PA, dem_share_OH))

dem_share_perm=np.random.permutation(dem_share_both)

#permutation samples
perm_sample_PA= dem_share_perm[:len(dem_share_PA)]
perm_sample_OH = dem_share_perm[len(dem_share_PA):]

>>>Sample


def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""

    # Concatenate the data sets: data
    data = np.concatenate((data1,data2))

    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)

    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]

    return perm_sample_1, perm_sample_2

>>>sample 2

for _ in range(50):
    # Generate permutation samples
    perm_sample_1, perm_sample_2 = permutation_sample(
                                    rain_june, rain_november)

    # Compute ECDFs
    x_1, y_1 = ecdf(perm_sample_1)
    x_2, y_2 = ecdf(perm_sample_2)

    # Plot ECDFs of permutation sample
    _ = plt.plot(x_1, y_1, marker='.', linestyle='none',
                 color='red', alpha=0.02)
    _ = plt.plot(x_2, y_2, marker='.', linestyle='none',
                 color='blue', alpha=0.02)

# Create and plot ECDFs from original data
x_1, y_1 = ecdf(rain_june)
x_2, y_2 = ecdf(rain_november)
_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red')
_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue')

# Label axes, set margin, and show plot
plt.margins(0.02)
_ = plt.xlabel('monthly rainfall (mm)')
_ = plt.ylabel('ECDF')
plt.show()

>>test statistics and p-values

null hypothesis using permutation testing.  A null hypothesis is the commonly accepted fact.  Researchers work to reject or disaprove the null hypothesis.  Researchers come up with an alternate hypothesis to explain the phenomenon then reject the null hypothesis.

how reasonable the asessment of data is assuming a hypothesis is true

a test statistic is a number that can be computed from observed data and from data you simulate under the null hypothesis

test statistic serves as a basis of comparison between the data and the simulated data

your test should be pertinent to the question your asking.

permutation replicate is the difference in the comparison of the single value.

sampling
np.mean(perm_sample_PA) - np.mean(perm_sample_OH)

1.122

actual data
np.mean(dem_share_PA) - np.mean(dem_share_OH)

1.158

histograph of the percentage differences between PA and OH

the difference in this case was between -4 and 4 percentage difference.

the actual data difference was 1.158 as a red line on the histogram.

to the right of the red line there were 23 percent with at least a 1.158 difference.  The right of the red line is called a p value


p-value

The probability of obtaining a value of your test statistic that is at least as extreme as what was observed, under the assumption the null hypothesis is true.

the p-value is not the probability that the null hypothesis is true

if the p-value is small that the data is statistical significantly different.

null hypothesis significant testing (nhst)


>>Sample

def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""

    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)

    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1,data_2)

        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)

    return perm_replicates

# Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. In the next exercise, we will test the hypothesis that the two frogs have the same distribution of impact forces.

# Make bee swarm plot
_ = sns.swarmplot(x='ID',y='impact_force',data=df)

# Label axes
_ = plt.xlabel('frog')
_ = plt.ylabel('impact force (N)')

# Show the plot
plt.show()


def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""

    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1)-np.mean(data_2)

    return diff

# Compute difference of mean impact force from experiment: empirical_diff_means
empirical_diff_means = diff_of_means(force_a,force_b)

# Draw 10,000 permutation replicates: perm_replicates
perm_replicates = draw_perm_reps(force_a,force_b,
                                 diff_of_means, size=10000)

# Compute p-value: p
p = np.sum(perm_replicates >= empirical_diff_means) / len(perm_replicates)

# Print the result
print('p-value =', p)

p-value=0.0063


The p-value tells you that there is about a 0.6% chance that you would get the difference of means observed in the experiment if frogs were exactly the same. A p-value below 0.01 is typically said to be "statistically significant," but: warning! warning! warning! You have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be "statistically significant," but they are definitely not the same!


pipleline for hypothesis testing

1. clearly state the null hypothesis
2. define your testing statistic
3. generate many sets of simulated data assuming the null hypothesis is true
4. Compute the test statistic for each simulated data set
5. The p-value is the fraction of your simulated data set for which the test statistic is at least as extreme as for the real data.


we want to know if there is something michelson speed of light 299,852 km/s and newcomb 299,860 km/s are correlated

we only have th newcomb mean

Null hypothesis

1. The true mean speed of light in Michelsons experiments was actually Newcombs reported value.

>>>How using bootstrapping to simulate data under the null hypothesis

1. we shift michelsons data with the same mean as newcombs data

newcomb_value=299860 #km/s

michelson_shifted = michelson_speed_of_light -
np.mean(michelson_speed_of_light) + new newcomb_value

def diff_from_newcomb(data, newcomb_value=299860):
	return np.mean(data)-newcomb_value

diff_from_newcomb= diff_from_newcomb(michelson_speed_of_light)

bs_replicates = draw_bs_reps(michelson_shifted, np.mean, 10000)

p_value= np.sum(bs_replicates <= newcomb_value)/10000


>>>Sample


Force B
[0.172 0.142 0.037 0.453 0.355 0.022 0.502 0.273 0.72  0.582 0.198 0.198
 0.597 0.516 0.815 0.402 0.605 0.711 0.614 0.468]


# Make an array of translated impact forces: translated_force_b
translated_force_b = force_b - np.mean(force_b) + 0.55

# Take bootstrap replicates of Frog B's translated impact forces: bs_replicates
bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)

# Compute fraction of replicates that are less than the observed Frog B force: p
p = np.sum(bs_replicates <= np.mean(force_b)) / 10000

# Print the p-value
print('p = ', p)


# Compute mean of all forces: mean_force

print (forces_concat)

mean_force = np.mean(forces_concat)

# Generate shifted arrays
force_a_shifted = force_a - np.mean(force_a) + mean_force
force_b_shifted = force_b - np.mean(force_b) + mean_force

# Compute 10,000 bootstrap replicates from shifted arrays
bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, 10000)
bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, 10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_replicates_a - bs_replicates_b

print(empirical_diff_means)
# Compute and print p-value: p
p = np.sum(bs_replicates >= empirical_diff_means) / len(bs_replicates)

print('p-value =', p)


























