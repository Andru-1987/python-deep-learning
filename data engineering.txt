
what is data engineering
1. Data is scattered
2. Data is not optimized for analyses
3. Legacy code is causing corrupt data

Data engineer
1. gather data from different sources
2. optimize database for analyses
3. remove corrupt data

definition:
1. An engineer that develops, constructs, tests, and maintains architectures such as databases and large-scale processing systems.

2. processing large amounts of data

3. setting up clusters of machines


Data Engineer Jobs
1. develop scalable data architecture
2. streamline data acquistion
3. set up processes to bring together data
4. clean corrupt data

Data Scientist
1. mining data for patterns
2. statistical modeling
3. build predictive models using machine learning
4. monitor business process
5. clean outliers in data

>>>>>>>>>>>>>>>>Tools of the data engineer

tools for loading data
1. expert users of databases
2. support application

Processing
1. clean data
2. aggregate data
3. join data


df=spark.read.parquet('users.parquet')
outliers=df.filter(df['age']>100)
print(outliers.count())


Scheduling

1. plan jobs with specific intervals
2. resolve dependency requirements of jobs

example

JoinProductOrder needs to run after CleanProduct and CleanOrder run

existing tools:
database: mysql, postgresql

processing: spark, hive

scheduling: airflow, oozie, cron


>>>>cloud providers

data engineers are heavy users of the cloud

1. clusters of machines are required
2. cover electrical and maintenance costs
3. peaks and quiet moments optimization

cloud is reliable
data needs to be replicated

The three big cloud providers are : AWS, Azure, and google

aws had 32% of the market share in 2018
azure has 17% market share in 2018
google has 10% market share in 2018

storage
* upload files
* images
(AWS S3, Azure Blog Storage, Google Cloud Storage)

computation
(virtual machines)
(AWS EC2, Azure Virtual Machines, Google Compute Engine)

databases
(AWS RDS, Azure SQL Database, Google Cloud SQL)

>>>>>>>>>>>>>>>>>>>>Databases

1. holds data
2. organizes data
3. retrieve/search data

structured
* relational database

unstructured
* schemaless and more like files
* videos and photos

semi-structured data
* json

SQL
* tables
* database schema
* relational databases

NoSQL
* non-relational database
* structured or unstructured
* key-value stores
* document db (json objects)
1. redis
2. mongodb


Star Schedule
dimension tables
fact tables


>>>>>>> sample  >>> read_sql with db_engine

# Complete the SELECT statement
data = pd.read_sql("""
SELECT first_name, last_name FROM "Customer"
ORDER BY first_name, last_name
""", db_engine)

# Show the first 3 rows of the DataFrame
print(data.head(3))

# Show the info of the DataFrame
print(data.info())


>>>>> sample >>> inner join

# Complete the SELECT statement
data = pd.read_sql("""
SELECT * FROM "Customer"
INNER JOIN "Order"
ON "Order"."customer_id"="Customer"."id"
""", db_engine)

# Show the id column of data
print(data.id)


>>>>>>>>>>>>>>>>>Parallel computing

Idea behind parallel computing
1. memory
2. processing power

basis of modern data processing tools

idea
1. split a task into subtasks
2. distribute subtasks over several computers


Benefits of parallel computing
1. processing power
2. memory: partition the dataset

risk of parallel computing
1. overhead due to communication
2. tasks need to be large
3. need several processing units
4. parallel slow down means speed does not increase linearly

>>>>>>>>>>>>>>> multiprocessing.Pool

from multiprocessing import Pool

def take_mean_age(year_and_group):

	year,group= year_and_group
	return pd.DataFrame({"Age": group["Age"].mean()},index=[year])


with Pool(4) as p:
	results=p.map(take_mean_age, athlete_events.groupby('Year'))


the function runs in 4 separate processes and uses 4 cores.

results_df=df.concat(results)

>>>>>>>>>>>>>>>>>>>>Dask

import dask.dataframe as dd

athelete_events_dask= dd.from_pandas(athlete_events, npartitions=4)

result_df=athlete_events_dask.groupby('Year').Age.mean().compute()


>>>> sample  multiple cores

# Function to apply a function over multiple cores
@print_timing
def parallel_apply(apply_func, groups, nb_cores):
    with Pool(nb_cores) as p:
        results = p.map(apply_func, groups)
    return pd.concat(results)


# Parallel apply using 1 core
parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)


# Parallel apply using 2 cores
parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)

# Parallel apply using 4 cores
parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)


>>>>>>> sample  >>> dask

import dask.dataframe as dd

# Set the number of partitions
athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)

# Calculate the mean Age per Year
print(athlete_events_dask.groupby('Year').Age.mean().compute())


>>>>>>>>>>>>>>>>>parallel computation frameworks

hadoop (big data)
1. hdfs - file server
2. map reduce

hive
1. structured query language

select year, avg(age) from views.athlete_events
group by year

spark distributes computation between clusters
1. avoids disk writes
2. resilent distributed dataset (rdd)
3. list of tuples
4. transformations.map() or .filter()
5. actions .count() or .first()

pyspark is the python interface to spark
dataframe abstraction
looks similar to pandas

(athlete_events_spark.groupby('Year')
	.mean('Age')
	.show())

<<<<<<<<<<<< print dask


# Print the type of athlete_events_spark
print(type(athlete_events_spark))

# Print the schema of athlete_events_spark
print(athlete_events_spark.printSchema())

# Group by the Year, and find the mean Age
print(athlete_events_spark.groupBy('Year').mean('Age'))

# The same, but now show the results
print(athlete_events_spark.groupBy('Year').mean('Age').show())


>>>>>>>>>>>sample  >>> running pyspark files  >>>spark-submit 

spark-submit \
  --master local[4] \
  /home/repl/spark-script.py

if __name__ == "__main__":
    spark = SparkSession.builder.getOrCreate()
    athlete_events_spark = (spark
        .read
        .csv("/home/repl/datasets/athlete_events.csv",
             header=True,
             inferSchema=True,
             escape='"'))

    athlete_events_spark = (athlete_events_spark
        .withColumn("Height",
                    athlete_events_spark.Height.cast("inte
ger")))

    print(athlete_events_spark
        .groupBy('Year')
        .mean('Height')
        .orderBy('Year')
        .show())


>>>>>>>>>>>>>Workflow Scheduling frameworks


extract -> spark -> clean -> load

cron - scheduling tool

DAGs Direct Acyclic Graph
1. set of nodes
2. directed edges
3. no cycles

cron
luigi
airflow

dag=DAG(dag_id="example_dag", ..., schedule_intervale="0 * * * *")

start_cluster = StartClusterOperator(task_id="start_cluster", dag=dag)

ingest_customer=SparkJobOperator(task_id="ingest_customer_data",dag=dag)

ingest_product_data=SparkJobOperator(task_id="ingest_product_data",dag=dag)

enrich_customer_data= PythonOperator(task_id="enrich_customer_data",..., dag=dag)

start_cluster.set_dopwnstream(ingest_customer_data)
start_cluster.set_dopwnstream(ingest_product_data)
ingest_customer_data.set_downstream(enrich_customer_data)
ingetst_customer_data.set_downstream(enrich_customer_data)

>>>>>>>>>sample >>> set the the dag object

# Create the DAG object
dag = DAG(dag_id="car_factory_simulation",
          default_args={"owner": "airflow","start_date": airflow.utils.dates.days_ago(2)},
          schedule_interval="0 * * * *")


runs every 30 minutes


assemble_frame = BashOperator(task_id="assemble_frame", bash_command='echo "Assembling frame"', dag=dag)
place_tires = BashOperator(task_id="place_tires", bash_command='echo "Placing tires"', dag=dag)
assemble_body = BashOperator(task_id="assemble_body", bash_command='echo "Assembling body"', dag=dag)
apply_paint = BashOperator(task_id="apply_paint", bash_command='echo "Applying paint"', dag=dag)

# Complete the downstream flow
assemble_frame.set_downstream(place_tires)
assemble_frame.set_downstream(assemble_body)
assemble_body.set_downstream(apply_paint)


>>>>>>>>>>>>>>>>>>>>>ETL

extract, transfer and load

extracting data from persistent storage into memory
1. file
2. database
3. api

json
1. number
2. string
3. boolean
4. null

composite
1. array
2. object

import json

#convert json string to a dictionary

result=json.loads('{key_1': 'value_1','key_2':'value_2'})

print(results['key_1'])

data on the web
1. requests
2. response

api

twitter api
jackernews api

import requests

response=request.get("https://hacker-news.firstbaseio.com/v0/item/16222426.json")
print(response.json())

application database
1. transactions
2. insert or changes
3. oltp
4. row oriented

extracting from database
1. need a connection string


import sqlalchemy
connection_uri="postgresql://repl:password@localhost:5432/pagila"

db_engine=sqlalchemy.create_engine(connection_uri)



import pandas as pd

pd.read_sql("select * from customer", db_engine)



>>>>>>> sample >>> get response json access the dictionary element

import requests

# Fetch the Hackernews post
resp = requests.get("https://hacker-news.firebaseio.com/v0/item/16222426.json")

# Print the response parsed as JSON
print(resp.json())

# Assign the score of the test to post_score
post_score = resp.json()['score']
print(post_score)


>>>> sample connect to postgres and run a select query

# Function to extract table to a pandas DataFrame
def extract_table_to_pandas(tablename, db_engine):
    query = "SELECT * FROM {}".format(tablename)
    return pd.read_sql(query, db_engine)

# Connect to the database using the connection URI
connection_uri = "postgresql://repl:password@localhost:5432/pagila" 
db_engine = sqlalchemy.create_engine(connection_uri)

# Extract the film table into a pandas DataFrame
extract_table_to_pandas("film", db_engine)

# Extract the customer table into a pandas DataFrame
extract_table_to_pandas("customer", db_engine)

repl:password

user is repl
password is password


>>>>>>>>>>>>>>>>>>Transform

split_email = customer_df.email.str.split("@", expand=True)


customer_df = customer_df.assign(
	username=split_email[0],
	domain=split_email[1],
)


>>>>>>>>>>>>>>>>Transforming in pyspark
spark = pyspark.sql.SparkSession.builder.getOrCreate()

spark.read.jdbc("jdbc:postgresql://localhost:5432/pagila",
	"customer",
	properties={"user":"repl","password":"password"})

the table is customer


ratings_per_customer = ratings_df.groupBy("customer_id").mean("rating")

customer_df.join(
	ratings_per_customer,
	customer_df.customer_id=ratings_per_customer.customer_id
)

>>>>>>> sample >>>> pyspark split string in dollar and cents

# Get the rental rate column as a string
rental_rate_str = film_df.rental_rate.astype("str")

print(rental_rate_str)
print(film_df.columns)

# Split up and expand the column
rental_rate_expanded = rental_rate_str.str.split(".", expand=True)

# Assign the columns to film_df
film_df = film_df.assign(
    rental_rate_dollar=rental_rate_expanded[0],
    rental_rate_cents=rental_rate_expanded[1],
)
import matplotlib.pyplot as plt

plt.clf
print(film_df[['rental_rate_dollar','rental_rate_cents']])
film_df['rental_rate_dollar'].astype('int').plot()
plt.show()

>>>> sample  group and inner join

# Use groupBy and mean to aggregate the column
ratings_per_film_df = rating_df.groupBy('film_id').mean('rating')

print(rating_df.columns)

# Join the tables using the film_id column
film_df_with_ratings = film_df.join(
    ratings_per_film_df,
    film_df.film_id==ratings_per_film_df.film_id
)

# Show the 5 first results
print(film_df_with_ratings.show(5))

>>>>> sample  >>> group by film_id and mean rating

# Use groupBy and mean to aggregate the column
ratings_per_film_df = rating_df.groupBy('film_id').mean('rating')

# Join the tables using the film_id column
film_df_with_ratings = film_df.join(
    ratings_per_film_df,
    film_df.film_id==ratings_per_film_df.film_id
)

# Show the 5 first results
print(film_df_with_ratings.show(5))


>>>>>>>>>>>>>Load

aggregate queries
online analytical processing
store per record
queries about subset of columns
parallelization
massively parallel processing databases


a. amazon redshift
b. azure sql data warehouse
c. google big query

df.to_parquet("./s3://path/to/bucket/customer.parquet")
df.write.parquet("./s3://path/to/bucket/customer.parquet")


recommendations= transform_find_recommenation(ratings_df)

recommendations.to_sql("recommendations",
	db_engine,
	schema="store",
	if_exists="replace")

pandas.to_sql

1. tail
2. replace
3. append


>>>>>> sample

# Write the pandas DataFrame to parquet
film_pdf.to_parquet("films_pdf.parquet")

# Write the PySpark DataFrame to parquet
film_sdf.write.parquet("films_sdf.parquet")



>>>>>>> sample fetch data from dwh

# Finish the connection URI
connection_uri = "postgresql://repl:password@localhost:5432/dwh"
db_engine_dwh = sqlalchemy.create_engine(connection_uri)

# Transformation step, join with recommendations data
film_pdf_joined = film_pdf.join(recommendations)

# Finish the .to_sql() call to write to store.film
film_pdf_joined.to_sql("film", db_engine_dwh, schema="store", if_exists="replace")

# Run the query to fetch the data
pd.read_sql("SELECT film_id, recommended_film_ids FROM store.film", db_engine_dwh)

>>>>>>>>>putting it together

def extract_table_to_df(tablename, db_engine):
	return pd.read_sql("select * from {}".format(tablename),db_engine)


def split_columns_transform(df, column, pat, suffixes):
	#converts column into str and splits it on pat ...

def load_df_into_dwh(film,df, tablename, schema, db_engine):
	return pd.to_sql(tablename, db_engine, schema=schema, if_exists="replace")

db_engines={}

def etl():
	#Extract
	film_df= extract_table_to_df("film", db_engines["store"])

	#transform
	film_df=split_columns_transform(film_df,"rental_rate",".",["_dollar","_cents"])

	#load
	load_df_into_dwh(film_df, "film", "store", db_engines['dwh')


>>>> review of airflow
1. work scheduler
2. python
3. dags


Tasks defined in operators
1. BashOperator

from airflow.models import DAG

dag = DAG(dag_id="sample",
	...,
	schedule_interval="0 0 * * *")


cron expression for schedule interval
minute (0-59)
hour (0-23)
day of the month (1-31)
month(1-12)
day of the week (0-6)

* * * * *

>>>>>>>>>>>>>>>>>>>>>>>>>>>	
from airflow.models import DAG
from airflow.operators.python_operator import PythonOperator

dag = DAG(dag_id="sample",
	...,
	schedule_interval="0 0 * * *")


etl_task = PythonOperator(task_id="etl_task",
	python_callable=etl,
	dag=dag)


etl_task.set_upstream(wait_for_this_task)

save as etl_dag.py in ~/airflow/dags

air flow interface will then display the dag


>>>>>>>> sample >>> create the etl code

# Define the ETL function
def etl():
    film_df = extract_film_to_pandas()
    film_df = transform_rental_rate(film_df)
    load_dataframe_to_film(film_df)

# Define the ETL task using PythonOperator
etl_task = PythonOperator(task_id='etl_film',
                          python_callable=etl,
                          dag=dag)

# Set the upstream to wait_for_table and sample run etl()
etl_task.set_upstream(wait_for_table)
etl()

>>>>>>>>>>>>>>sample move the dag.py to airflow

echo $AIRFLOW_HOME

head airflow.cfg

shows airflow_home =/home/repl/airflow

dags_folder = /home/repl/airflow/dags

mv ./dag.py /home/repl/airflow/dags


>>>>>>>>>>>>>>Case study

course ratings: datacamp ratings
1. Get rating data
2. Clean and calculate top recommended courses
3. Recalculate daily

Course:
1. course_id
2. title
3. description
4. programming_language

Rating
1. user_id
2. course_id
3. rating


>>>>>>>>>sample query from ratings

# Complete the connection URI
connection_uri = "postgresql://repl:password@localhost:5432/datacamp_application"
db_engine = sqlalchemy.create_engine(connection_uri)

# Get user with id 4387
user1 = pd.read_sql("SELECT * FROM rating where user_id=4387", db_engine)

# Get user with id 18163
user2 = pd.read_sql("SELECT * FROM rating where user_id=18163", db_engine)

# Get user with id 8770
user3 = pd.read_sql("SELECT * FROM rating where user_id=8770", db_engine)

# Use the helper function to compare the 3 users
print_user_comparison(user1,user2,user3) 


>>>>>sample build function

# Complete the transformation function
def transform_avg_rating(rating_data):
  # Group by course_id and extract average rating per course
  avg_rating = rating_data.groupby('course_id').rating.mean()
  # Return sorted average ratings per course
  sort_rating = avg_rating.sort_values(ascending=False).reset_index()
  return sort_rating

# Extract the rating data into a DataFrame    
rating_data = extract_rating_data(db_engines)

# Use transform_avg_rating on the extracted data and print results
avg_rating_data = transform_avg_rating(rating_data)
print(avg_rating_data) 


>>>>>>>>>>>>from ratings to recommendations

recommendations
1. user_id
2. course_id
3. rating


>>>>> sample

course_data = extract_course_data(db_engines)

# Print out the number of missing values per column
print(course_data.isnull().sum())

# The transformation should fill in the missing values
def transform_fill_programming_language(course_data):
    imputed = course_data.fillna({"programming_language": "r"})
    return imputed

transformed = transform_fill_programming_language(course_data)

# Print out the number of missing values per column of transformed
print(transformed.isna().sum())


>>>>>>> sample >>> merge and group by and sort
# Complete the transformation function
def transform_recommendations(avg_course_ratings, courses_to_recommend):
    # Merge both DataFrames
    merged = courses_to_recommend.merge(avg_course_ratings) 
    # Sort values by rating and group by user_id
    grouped = merged.sort_values("rating", ascending = False).groupby('user_id')
    # Produce the top 3 values and sort by user_id
    recommendations = grouped.head(3).sort_values("user_id").reset_index()
    final_recommendations = recommendations[["user_id", "course_id","rating"]]
    # Return final recommendations
    return final_recommendations

# Use the function with the predefined DataFrame objects
recommendations = transform_recommendations(avg_course_ratings, courses_to_recommend)

print(recommendations)


>>>>>>> scheduling daily jobs

1. extract using extract_course_data() and extract_rating_data()
2. clean up using na using transform_fill_programming_language()
3. Average course ratings per course : transform_avg_rating()
4. get eligible user and course id pairs: transform_courses_to_recommend()
5. calculate the recommendations: transform_recommendations()

look at courses the users have not rated yet

>>>>>the loading phase

recommendations.to_sql(
	"recommendations",
	db_engine,
	if_exists="append",
)


def etl(db_engines):
	courses=extract_course_data(db_engines)
	rating = extract_rating_data(db_engines)
	courses=transform_fill_programming_language(courses)

	avg_course_ratings = transform_avg_rating(rating)

	courses_to_recommend=transform_courses_to_recommend(rating,courses)


recommendations=transform_recommendations(
	avg_course_rating,
	courses_to_recommend,
)

load_to_dwh(recommendations, db_engine))

from airflow.models import DAG
from airflow.operators.python_operator import PythonOperator

dag=DAG(dag_id="recommendations",
	schedule_interval="0 0 * * *")

task_recommendations=PythonOperator(
	task_id="recommendations_task",
	python_callable=etl
)

>>>>>>>>> case study

connection_uri = "postgresql://repl:password@localhost:5432/dwh"
db_engine = sqlalchemy.create_engine(connection_uri)

def load_to_dwh(recommendations):
    recommendations.to_sql("recommendations", db_engine, if_exists="replace")

# Define the DAG so it runs on a daily basis
dag = DAG(dag_id="recommendations",
          schedule_interval="0 0 * * *")

# Make sure `etl()` is called in the operator. Pass the correct kwargs.
task_recommendations = PythonOperator(
    task_id="recommendations_task",
    python_callable=etl,
    op_kwargs={"db_engines": db_engines},
)


def recommendations_for_user(user_id, threshold=4.5):
  # Join with the courses table
  query = """
  SELECT title, rating FROM recommendations
    INNER JOIN courses ON courses.course_id = recommendations.course_id
    WHERE user_id=%(user_id)s AND rating>%(threshold)s
    ORDER BY rating DESC
  """
  # Add the threshold parameter
  predictions_df = pd.read_sql(query, db_engine, params = {"user_id": user_id, 
                                                           "threshold": 4.5})
  return predictions_df.title.values

# Try the function you created
print(recommendations_for_user(12, 4.65))














