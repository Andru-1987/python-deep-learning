predict customer purchases
customer segmentation
agents maximizing rewards

predicting the target variable
predicting how much the customers will spend
customer churn
defaults on loans

customer segmentation by product purchase history


>>Preparation of data

explore the data sample

telco_raw.dtypes

custid=['customerID]
target=['Churn']

#Separate categorical and numeric column names as lists

categorical=telco_raw.nunique()[telcom.nunique()<10].keys().tolist()


#remove church
categorical.remove(target[0])

numerical = [ col for col in telco_raw.columns
	if col not in custid+target+categorical]


color
red
white
blue
red

one hot encode

color 	red  	white 	blue
red	1	0	0
white	0	1	0
blue	0	0	1


telco_raw=pd.get_dummies(data=telco_raw, columns=categorical, drop_first=True)

# drop_first column because it is redundant and can be inferred from the other columns


>>Scale the numerical features

divide by standard deviation 

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaled_numerical = scaler.fit_transform(telco_raw[numerical])

run on the numerical columns of the dataset

#build dataframe

scaled_numerical = pd.DataFrame(scaled_numerical, columns=numerical)

#drop non-scaled numerical columns
telco_raw = telco_raw.drop(columns=numerical, axis=1)

#merge the non-numerical with the scaled numerical data

telco= telco_raw.merge(right=scaled_nuemrical,
	how='left',
	left_index=True,
	right_index=True
	)

>>Sample


# Print the data types of telco_raw dataset
print(telco_raw.dtypes)

# Print the header of telco_raw dataset
print(telco_raw.head())

# Print the number of unique values in each telco_raw column
print(telco_raw.nunique())


>>Sample
#You will now separate categorical and numerical variables from the telco_raw DataFrame with a customized categorical vs. numerical unique value count threshold.

# Store customerID and Churn column names
custid = ['customerID']
target = ['Churn']

# Store categorical column names
categorical = telco_raw.nunique()[telco_raw.nunique() < 5].keys().tolist()

# Remove target from the list of categorical variables
categorical.remove(target[0])

# Store numerical column names
numerical = [x for x in telco_raw.columns if x not in custid + target + categorical]

>>Encoded categorical and scale numerical values

# Perform one-hot encoding to categorical variables 
telco_raw = pd.get_dummies(data = telco_raw, columns = categorical, drop_first=True)

# Initialize StandardScaler instance
scaler = StandardScaler()

# Fit and transform the scaler on numerical columns
scaled_numerical = scaler.fit_transform(telco_raw[numerical])

# Build a DataFrame from scaled_numerical
scaled_numerical = pd.DataFrame(scaled_numerical, columns=numerical)

>>ML modeling steps

1. split data to training and testing
2. initialize the model
3. fit the model on the testing data
4. predict values on the testing data
5. measure model performance on testing data


from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

train_X, test_X, train_Y, test_Y=train_test_split(X,Y, test_size=0.25)

mytree= tree.DecisionTreeClassifier()

treemodel=mytree.fit(train_X, train_Y)

pred_Y=treemodel.predict(test_X)

accuracy_score(test_Y,pred_Y)


>>Unsupervised learning steps

1. Initialize the model
2. Fit the model
3. Assign cluster values
4. Explore results

from sklearn.cluster import KMeans
import pandas as pd

kmeans=KMeans(n_cluster=3)

kmeans.fit(data)

data.assign(Cluster=kmeans.labels_)

data.groupby('Cluster').mean()


>>Sample

# Split X and Y into training and testing datasets
train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.25)

#print(X.columns)

# Ensure training dataset has only 75% of original X data
print(train_X.shape[0] / X.shape[0])

# Ensure testing dataset has only 25% of original X data
print(test_Y.shape[0] / Y.shape[0])


>>>Tree classifier

# Initialize the model with max_depth set at 5
mytree = tree.DecisionTreeClassifier(max_depth = 5)

# Fit the model on the training data
treemodel = mytree.fit(train_X, train_Y)

# Predict values on the testing data
pred_Y = treemodel.predict(test_X)

# Measure model performance on testing data
accuracy_score(test_Y,pred_Y)

>>>Predicting Churn

# Initialize the Decision Tree
clf = tree.DecisionTreeClassifier(max_depth = 7, 
               criterion = 'gini', 
               splitter  = 'best')

# Fit the model to the training data
clf = clf.fit(train_X, train_Y)

# Predict the values on test dataset
pred_Y = clf.predict(test_X)

# Print accuracy values
print("Training accuracy: ", np.round(clf.score(train_X, train_Y), 3)) 
print("Test accuracy: ", np.round(accuracy_score(test_Y, pred_Y), 3))


>> items to research

1. churn prediction fundamentals
2. exploring churn rates
3. predicting churn with logistic regression
4. fit logistic regression with L1 regularization
5. identify optimal L1 penalty coefficient
6. predict churn with decision trees
7. fit decision tree model
8. identify optimal tree depth
9. identify and interpret churn drivers
10. explore logistic regression coeffients
11. break down decision tree rules


>> Build customer and product segmentation
1. determine the optimal number of clusters
2. build segmentation using kmeans clustering
3. alternative segmentation with NMF
4. k-means segmentation averages
5. NMF segmentation averages









