budgets for school are complex

are spending more on text books

hundreds of hours each year are spent manually labeling

expense line
1. labels: Textbooks, Math, Middle School

Supervised learning problem

groupings of line items that go together

100 target variables
1. Expense for Pre kindergarden (different funding)
2. student_type

9 categories per column

prioritizing time
1. human in the loop (proability of likely)

df.info()

tells us the data types
tells us the missing values

df.describe()
gives us summary statistics


>>>>>>Project

df=pd.read_csv('TrainingData.csv',index_col=0)

df.info()

<class 'pandas.core.frame.DataFrame'>
Int64Index: 1560 entries, 198 to 101861
Data columns (total 25 columns):
Function                  1560 non-null object
Use                       1560 non-null object
Sharing                   1560 non-null object
Reporting                 1560 non-null object
Student_Type              1560 non-null object
Position_Type             1560 non-null object
Object_Type               1560 non-null object
Pre_K                     1560 non-null object
Operating_Status          1560 non-null object
Object_Description        1461 non-null object
Text_2                    382 non-null object
SubFund_Description       1183 non-null object
Job_Title_Description     1131 non-null object
Text_3                    296 non-null object
Text_4                    193 non-null object
Sub_Object_Description    364 non-null object
Location_Description      874 non-null object
FTE                       449 non-null float64
Function_Description      1340 non-null object
Facility_or_Department    252 non-null object
Position_Extra            1026 non-null object
Total                     1542 non-null float64
Program_Description       1192 non-null object
Fund_Description          819 non-null object
Text_1                    1132 non-null object
dtypes: float64(2), object(23)


df.describe()


FET Total
count  449.000000  1.542000e+03
mean     0.493532  1.446867e+04
std      0.452844  7.916752e+04
min     -0.002369 -1.044084e+06
25%      0.004310  1.108111e+02
50%      0.440000  7.060299e+02
75%      1.000000  5.347760e+03
max      1.047222  1.367500e+06


Job_Title_Description column. The values in this column tell us if a budget item is for a teacher, custodian, or other employee.

 For example, the Object_Type column describes whether the budget item is related classroom supplies, salary, travel expenses, etc. 


>>>> project  >> build a histogram

# Print the summary statistics
print(df.describe())

# Import matplotlib.pyplot as plt
import matplotlib.pyplot as plt

# Create the histogram
plt.hist(df['FTE'].dropna())

# Add title and labels
plt.title('Distribution of %full-time \n employee works')
plt.xlabel('% of full-time')
plt.ylabel('num employees')

# Display the histogram
plt.show()

FTE: Stands for "full-time equivalent". If the budget item is associated to an employee, this number tells us the percentage of full-time that the employee works. A value of 1 means the associated employee works for the school full-time. A value close to 0 means the item is associated to a part-time or contracted employee.

Total: Stands for the total cost of the expenditure. This number tells us how much the budget item cost.

There are some full time employees and some part time employees

>>>>>>>>>>>>>>Looking at the datatypes

ml algorithms work on numbers, not strings

strings are slow to compare versus numbers

category encodes categorical data

astype('category')

sample_df=sample_df.label.astype('category')

dummies = pd.get_dummies(sample[['label']],prefix_sep='_')

also called a binary indicator representation

lambda are simple online functions

square = lambda x: x*x

categorized_label = lambda x: x.astype('category')
sample_df.label=sample_df[['label']].apply(categorize_label,axis=0)
sample_df.info()


>>>>practice exploring datatypes

df.dtypes.value_counts()

LABELS: ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']

# Define the lambda function: categorize_label
categorize_label = lambda x: x.astype('category')

print(LABELS)
# Convert df[LABELS] to a categorical type
df[LABELS] = df[LABELS].apply(categorize_label, axis=0)

# Print the converted dtypes
print(df[LABELS].dtypes)

Function            category
Use                 category
Sharing             category
Reporting           category
Student_Type        category
Position_Type       category
Object_Type         category


>>>>> practice >> lambda to find unique labels and then plot their frequency

# Import matplotlib.pyplot
import matplotlib.pyplot as plt

# Calculate number of unique values for each label: num_unique_labels
num_unique_labels = df[LABELS].apply( lambda x: pd.Series.nunique(x))

# Plot number of unique values for each label
num_unique_labels.plot(kind='bar')

# Label the axes
plt.xlabel('Labels')
plt.ylabel('Number of unique values')

# Display the plot
plt.show()


>>>>>>>>>>>>>>>how do we measure success

how accurate is your model

legitimate email is 99% and spam 1%

a model must be useful

>>>>>>Log loss

log loss is the measurement of error
we want to minimize error

y=probability is between 0 and 1
p=actual value is binary (1=yes, 0=no)


logloss=-1/N sum(1-yi)log(1-pi)

where i is each row

The goal is to minimize logloss
1. better to be less confident than confident and wrong


import numpy as np

def compute_log_loss(predicted, actual, eps(1e-14):

""" computes the logarithmic loss between predicted and actual when these are 1D arrays
	:param predicted: The predicted probabilities as floats between 0-1
	:param actual: The actual binary labels. Either 0 or 1
	:param eps(optional) : log(0) is inf, so we need to offset our predicted values slightly by eps from 0 or 1
"""
	predicted=np.clip(predicted,eps,1-eps)
	loss=-1 * np.mean(actual*np.log(predicted)+(1-actual) * np.log(1-predicted))
	return loss


clip sets a maximum and minimum value of an array

compute_log_loss(predicted=0.9, actual=0)
2.3
compute_log_loss(predicted=0.5, actual=1)
0.69


>>>>practice

# Compute and print log loss for 1st case
correct_confident_loss = compute_log_loss(correct_confident, actual_labels)
print("Log loss, correct and confident: {}".format(correct_confident_loss)) 

# Compute log loss for 2nd case
correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)
print("Log loss, correct and not confident: {}".format(correct_not_confident_loss)) 

# Compute and print log loss for 3rd case
wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)
print("Log loss, wrong and not confident: {}".format(wrong_not_confident_loss)) 

# Compute and print log loss for 4th case
wrong_confident_loss = compute_log_loss(wrong_confident,actual_labels)
print("Log loss, wrong and confident: {}".format(wrong_confident_loss)) 

# Compute and print log loss for actual labels
actual_labels_loss = compute_log_loss(actual_labels, actual_labels)
print("Log loss, actual labels: {}".format(actual_labels_loss)) 


>>>>>>>>>>>>>Start with a simple model
how much signal you can pull out as quickly as possible

train basic model on numeric data 

multi-class logistic regression

StratifiedShuffleSplit
only works with a single target variable

multilabel_train_test_split()

data_to_train=df[NUMERIC_COLUMNS].fillna(-1000)

we want our model to respond to nan differently than real values


X_train, X_test, y_train,y_test= multilabel_train_test_split(data_to_train, label_to_use, size=0.2, seed=123)

from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

treats each column of y independently
fits a separate classifere for of the columns

clf=OneVsRestClassifier(LogisticRegression())
clf.fit(X_train,y_train)

>>>>sample >>> multi class classification to create train and test sets

print(df.columns)
# Create the new DataFrame: numeric_data_only
numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)

# Get labels and convert to dummy variables: label_dummies
label_dummies = pd.get_dummies(df[LABELS])

# Create training and test sets
X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,
                                                               label_dummies,
                                                               size=0.2, 
                                                               seed=123)

# Print the info
print("X_train info:")
print(X_train.info())
print("\nX_test info:")  
print(X_test.info())
print("\ny_train info:")  
print(y_train.info())
print("\ny_test info:")  
print(y_test.info()) 

output:
y_test info:
<class 'pandas.core.frame.DataFrame'>
Int64Index: 520 entries, 209 to 448628
Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating
dtypes: uint8(104)
memory usage: 56.9 KB
None

from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

# Instantiate the classifier: clf
clf=OneVsRestClassifier(LogisticRegression())

# Fit the classifier to the training data

clf.fit(X_train,y_train)
# Print the accuracy
print("Accuracy: {}".format(clf.score(X_test, y_test)))


>>>>>>>>>>>>>>Making predictions

predicting on holdout data

holdout=pd.read_csv('HoldoutData.csv',index_col=0)
holdout=holdout[NUMERIC_COLUMNS].fillna(-1000)

predictions=clf.predict_proba(holdout)

#calculate the probablity for each label
#log loss penalize being confident and wrong
#.predict would result in worst performance compared to .predict_proba()

#submit your predictions as a csv

#with to_csv function

#proba needs to be convert into a data frame

predictions_df=pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns,
index=holdout.index,
data=predictions)

predictions_df.to_csv('predictions.csv')

score=score_submission(pred_path='predictions.csv')


#The point of the holdout data is to provide a fair test for machine learning competitions.

>>>>> sample >>> creating predictions of probabilities of the features using holdout data

# Instantiate the classifier: clf
clf = OneVsRestClassifier(LogisticRegression())

# Fit it to the training data
clf.fit(X_train, y_train)

# Load the holdout data: holdout
holdout=pd.read_csv('HoldoutData.csv',index_col=0)
holdout=holdout[NUMERIC_COLUMNS].fillna(-1000)

print(holdout.columns)
# Generate predictions: predictions
predictions=clf.predict_proba(holdout)


labels output:
['Function',
 'Use',
 'Sharing',
 'Reporting',
 'Student_Type',
 'Position_Type',
 'Object_Type',
 'Pre_K',
 'Operating_Status']


# Format predictions in DataFrame: prediction_df
prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,
                             index=holdout.index,
                             data=predictions)


# Save prediction_df to csv
predictions_df.to_csv('predictions.csv')

# Submit the predictions for scoring: score
score = score_submission(pred_path='predictions.csv')

# Print score
print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))


Your model, trained with numeric data only, yields logloss score: 1.9067227623381413

>>>>>>>>>>>>>>Natural language processing

basic techniques for processing text data

Tokenization is splitting a long string into segments
store segments as lists

['natural','language','processing']

bag of word representation
1. count the number of times a particular token appears

bag of words

1-gram
petro vend fuel and fluids

2-gram
petro vend
vend fuel
fuel and
and fluids

3-gram
petro vend fuel
vend fuel and
fuel and fluids

>>>>>>> representing text numerically

represent text numerically

text as bag of words

CountVectorizer() tokenizes all the strings
builds a volcabulary

1. Converting text into vectors
2. bag of words model is extracting word tokens
a. computing the frequency of word tokens
b. computing a word vector out of these frequencies and volcabulary of corpus

from sklearn.feature_extraction.text import CountVectorizer

TOKEN_BASICS='\\\\s+(?=\\\\s+)'
df.Program_Description.fillna('',inplace=True)

vec_basic=CountVectorizer(token_pattern=TOKEN_BASICS)

vec_basic.fit(df.Program_Description)


print(vec_basics.get_feature_names())


>>>>>>>>>>>Representing text numerically


bag of words counts occurrences

CountVectorizer()



>>>>Practice >>> tokenizing position_extra

# Import CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Create the token pattern: TOKENS_ALPHANUMERIC
TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)'

# Fill missing values in df.Position_Extra
df.Position_Extra.fillna('',inplace=True)

# Instantiate the CountVectorizer: vec_alphanumeric
vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)

# Fit to the data
vec_alphanumeric.fit(df.Position_Extra)

# Print the number of tokens and first 15 tokens
msg = "There are {} tokens in Position_Extra if we split on non-alpha numeric"
print(msg.format(len(vec_alphanumeric.get_feature_names())))
print(vec_alphanumeric.get_feature_names()[:15])

output: ['1st', '2nd', '3rd', 'a', 'ab', 'additional', 'adm', 'administrative', 'and', 'any', 'art', 'assessment', 'assistant', 'asst', 'athletic']

# Define combine_text_columns()
def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):
    """ converts all text in each row of data_frame to single vector """
    
    # Drop non-text columns that are in the df
    to_drop = set(to_drop) & set(data_frame.columns.tolist())
    text_data =data_frame.drop(to_drop,axis=1)
    
    # Replace nans with blanks
    text_data.fillna("",inplace=True)
    
    # Join all text items in a row that have a space in between
    return text_data.apply(lambda x: " ".join(x), axis=1)


>>>>>combine text columns and tokenizing using regular expressions

# Import the CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Create the basic token pattern
TOKENS_BASIC = '\\S+(?=\\s+)'

# Create the alphanumeric token pattern
TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)'

# Instantiate basic CountVectorizer: vec_basic
vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)
# Instantiate alphanumeric CountVectorizer: vec_alphanumeric
vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)

# Create the text vector
text_vector = combine_text_columns(df)

# Fit and transform vec_basic
vec_basic.fit_transform(text_vector)

# Print number of tokens of vec_basic
print("There are {} tokens in the dataset".format(len(vec_basic.get_feature_names())))

# Fit and transform vec_alphanumeric
vec_alphanumeric.fit_transform(text_vector)

# Print number of tokens of vec_alphanumeric
print("There are {} alpha-numeric tokens in the dataset".format(len(vec_alphanumeric.get_feature_names())))

>>>>>Pipelines, features 
1. a pipelineis a repeatable way to go from raw data to trained model

2. pipeline object takes sequential list of steps where the output of one step is teh input to the next step.

3. each step is a tuple with two elements
tranform: obj implementing .fit() and .transform()

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

pl=Pipeline([
	(
'imp', Imputer()),
'clf', OneVsRestClassifier(LogisticRegression())
)
])


we pass the pipeline a series of named steps

sample data:
label, numeric, text, with_missing


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test= train_test_split(
	sample_df[['numeric','with_missing']],
	pd.get_dummies(sample_df['label']),
	random_state=2)

pl.fit(X_train, y_train)

accuracy = pl.score(X_test,y_test)

print('accuracy on numeric data, no nans',accuracy)

>>>> Practice  >>> build the pipeline

# Import Pipeline
from sklearn.pipeline import Pipeline
# Import other necessary modules
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

# Split and select numeric data only, no nans 
X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],
                                                    pd.get_dummies(sample_df['label']), 
                                                    random_state=22)

# Instantiate Pipeline object: pl
pl = Pipeline([
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])

# Fit the pipeline to the training data
pl.fit(X_train, y_train)

# Compute and print accuracy
accuracy = pl.score(X_test, y_test)
print("\nAccuracy on sample data - numeric, no nans: ", accuracy)

>>>> practice   >>> add the imputer

# Import the Imputer object
from sklearn.preprocessing import Imputer

# Create training and test sets using only numeric data
X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],
                                                    pd.get_dummies(sample_df['label']), 
                                                    random_state=456)

# Insantiate Pipeline object: pl
pl = Pipeline([
    
        ('imp', Imputer()),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])

# Fit the pipeline to the training data
pl.fit(X_train,y_train)

# Compute and print accuracy
accuracy =pl.score(X_test,y_test)
print("\nAccuracy on sample data - all numeric, incl nans: ", accuracy)

>>>>>>>>>>Adding text features to the pipeline


pl=Pipeline([
	('vec', CountVectorizer()),
	('clf', OneVsRestClassifier(LogisticRegression()))
])


Preprocessing multiple dtypes

want to use all available features in one pipeline

problem: pipeline steps for numeric and text preprocessing can't follow each other

the output of CountVectorizer can't be input to imputer

Solution: FunctionTransformer() and FeatureUnion()

FunctionTransformer turns a python function into an object that scikit-learn pipeline can understand

two functions for the pipeline preprocessing
1. function 1 return numeric columns
2. function 2 return text columns

can then preprocess numeric and text data in separate pipelines

from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import FeatureUnion


get_text_data=FunctionTransformer(lambda x: x['text'],validate=False)

get_numeric_data=FunctionTransformer(lambda x: x[['numeric','with_missing']],validate=False

validate=False means don't check for nans

union=FeatureUnion([
  	('numeric',numeric_pipeline),
	('text',text_pipeline)
]

>>>>>>>>>>>>Entire pipeline

numeric_pipeline=Pipeline([
    ('selector',get_numeric_data),
    ('imputer',SimpleImputer())
    
])

text_pipeline=Pipeline([
    ('selector',get_text_data),
    ('vectorizer',CountVectorizer())
])

pl = Pipeline([
        ('union',FeatureUnion([
            ('numeric',numeric_pipeline),
            ('text',text_pipeline)
        ])
        )
        ,
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])


>>>Practice  <<<< vectorize text and dummies


# Import the CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

print(sample_df['text'])
# Split out only the text data
X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],pd.get_dummies(sample_df['label']), 
                                                    random_state=456)

# Instantiate Pipeline object: pl
pl = Pipeline([
        ('vec', CountVectorizer()),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])

# Fit to the training data
pl.fit(X_train,y_train)

# Compute and print accuracy
accuracy = pl.score(X_test,y_test)
print("\nAccuracy on sample data - just text data: ", accuracy)


>>>>Practice >>> creating two pipelines using Function Transformer

# Import FunctionTransformer
from sklearn.preprocessing import FunctionTransformer

# Obtain the text data: get_text_data
get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)

# Obtain the numeric data: get_numeric_data
get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)

# Fit and transform the text data: just_text_data
just_text_data = get_text_data.fit_transform(sample_df)

# Fit and transform the numeric data: just_numeric_data
just_numeric_data = get_numeric_data.fit_transform(sample_df)

# Print head to check results
print('Text Data')
print(just_text_data.head())
print('\nNumeric Data')
print(just_numeric_data.head())


>>>>> Practice >>> create the union

# Import FeatureUnion
from sklearn.pipeline import FeatureUnion

# Split using ALL data in sample_df
X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],
                                                    pd.get_dummies(sample_df['label']), 
                                                    random_state=22)

# Create a FeatureUnion with nested pipeline: process_and_join_features
process_and_join_features = FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ('selector', get_text_data),
                    ('vectorizer', CountVectorizer())
                ]))
             ]
        )

# Instantiate nested pipeline: pl
pl = Pipeline([
        ('union', process_and_join_features),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])


# Fit pl to the training data
pl.fit(X_train, y_train)

# Compute and print accuracy
accuracy = pl.score(X_test, y_test)
print("\nAccuracy on sample data - all data: ", accuracy)


>>>>>>>>>>>>>>>>>>Choosing a classification model


def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):
    """ converts all text in each row of data_frame to single vector """
    
    # Drop non-text columns that are in the df
    to_drop = set(to_drop) & set(data_frame.columns.tolist())
    text_data =data_frame.drop(to_drop,axis=1)
    
    # Replace nans with blanks
    text_data.fillna("",inplace=True)
    
    # Join all text items in a row that have a space in between
    return text_data.apply(lambda x: " ".join(x), axis=1)

# Obtain the text data: get_text_data
get_text_data = FunctionTransformer(combine_text_columns, validate=False)

# Obtain the numeric data: get_numeric_data
get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)



>>>change the model

from sklearn.ensemble import RandomForestClassifier

pl = Pipeline([
        ('union', process_and_join_features),
        ('clf', OneVsRest(RandomForestClassifier()))
    ])


>>>>>>practice >>> setup the pipeline and add the combine text into one column function

# Import FunctionTransformer
from sklearn.preprocessing import FunctionTransformer

# Get the dummy encoding of the labels
dummy_labels = pd.get_dummies(df[LABELS])

# Get the columns that are features in the original df
NON_LABELS = [c for c in df.columns if c not in LABELS]

# Split into training and test sets
X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],
                                                               dummy_labels,
                                                               0.2, 
                                                               seed=123)

# Preprocess the text data: get_text_data
get_text_data = FunctionTransformer(combine_text_columns,validate=False)

# Preprocess the numeric data: get_numeric_data
get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)

>>> create the pipeline

# Complete the pipeline: pl
pl = Pipeline([
        ('union', FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ('selector', get_text_data),
                    ('vectorizer', CountVectorizer())
                ]))
             ]
        )),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])

# Fit to the training data
pl.fit(X_train,y_train)

# Compute and print accuracy
accuracy = pl.score(X_test, y_test)
print("\nAccuracy on budget dataset: ", accuracy)

>>>>switch to random forest

from sklearn.ensemble import RandomForestClassifier

# Import random forest classifer
from sklearn.ensemble import RandomForestClassifier

# Edit model step in pipeline
pl = Pipeline([
        ('union', FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ('selector', get_text_data),
                    ('vectorizer', CountVectorizer())
                ]))
             ]
        )),
        ('clf', RandomForestClassifier(n_estimators=15))
    ])

# Fit to the training data
pl.fit(X_train, y_train)

# Compute and print accuracy
accuracy = pl.score(X_test, y_test)
print("\nAccuracy on budget dataset: ", accuracy)


>>>>Learning from an expert


1. Text processing
2. statistical methods
3. computational efficiency

text preprocessing
Tokenize on punctuation to avoid hyphens, underscore, etc

include unigrams and bi-grams in the model to capture important information in the text.

vec = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,
ngram_range=(1,2))

accept alphanumeric tokens and use one-grams and bi-grams in the tokens


>>>>> getting predictions

holdout= pd.read_csv('HoldOutData.csv', index_col=0)

predictions=pl.predict_proba(holdout)
prediction_df=pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns, index=holdout.index, data=predictions)

prediction_df.to_csv('predictions.csv')
score=score_submission(pred_path='predictions.csv')


>>>>>>>>>>>Practice  >> combine text columns,  tokenize only alphanumeric values

# Import the CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Create the text vector
text_vector = combine_text_columns(X_train)

# Create the token pattern: TOKENS_ALPHANUMERIC
TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)'

# Instantiate the CountVectorizer: text_features
text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)

# Fit text_features to the text vector
text_features.fit(text_vector)

# Print the first 10 tokens
print(text_features.get_feature_names()[:10])


>>>> setup the pipeline  >>> countvectorizer with n-grams

# Import pipeline
from sklearn.pipeline import Pipeline

# Import classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

# Import CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Import other preprocessing modules
from sklearn.preprocessing import Imputer
from sklearn.feature_selection import chi2, SelectKBest

# Select 300 best features
chi_k = 300

# Import functional utilities
from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler
from sklearn.pipeline import FeatureUnion

# Perform preprocessing
get_text_data = FunctionTransformer(combine_text_columns, validate=False)
get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)

# Create the token pattern: TOKENS_ALPHANUMERIC
TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)'

# Instantiate pipeline: pl
pl = Pipeline([
        ('union', FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ('selector', get_text_data),
                    ('vectorizer', CountVectorizer   
                    (token_pattern=TOKENS_ALPHANUMERIC,
                    ngram_range=(1,2))),
                    ('dim_red', SelectKBest(chi2, chi_k))
                ]))
             ]
        )),
        ('scale', MaxAbsScaler()),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])

>>>>>>>>Statistical tools

interaction terms

B1X1+B2X2+B3(x1*x2)


beta are the coeffients
X are 0 or 1 in the row for a column


X1 and X2 are 0 or 1
X3 is X1*X2 and 1 if X1 and X2 are in the row

from sklearn.preprocessing import PolynomialFeatures


interaction=PolynomialFeatures(degree=2,
interaction_only=True,
include_bias=False)

degree=2 multiples two columns together
bias term is an offset for a model
bias term allows the model to have a non-zero y value when x value is zero

SparseInteractions(degree=2).fit_transform(x).toarray()

PolynomialFeatures does not support sparse matrices

SparseInteractions work the the sparse matrices

>>>>practice  >>> implement sparse interactions

# Instantiate pipeline: pl
pl = Pipeline([
        ('union', FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ('selector', get_text_data),
                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,
                                                   ngram_range=(1, 2))),  
                    ('dim_red', SelectKBest(chi2, chi_k))
                ]))
             ]
        )),
        ('int', SparseInteractions(degree=2)),
        ('scale', MaxAbsScaler()),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])


#https://gist.github.com/pjbull/063a9b4e4f9cfcc4d03cba18fee63de7
from sklearn.base import BaseEstimator, TransformerMixin
from scipy import sparse
from itertools import combinations

class SparseInteractions(BaseEstimator, TransformerMixin):
    def __init__(self, degree=2, feature_name_separator="_"):
        self.degree = degree
        self.feature_name_separator = feature_name_separator

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        if not sparse.isspmatrix_csc(X):
            X = sparse.csc_matrix(X)

        if hasattr(X, "columns"):
            self.orig_col_names = X.columns
        else:
            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])
          
        spi = self._create_sparse_interactions(X)
        return spi

    def get_feature_names(self):
        return self.feature_names

    def _create_sparse_interactions(self, X):
        out_mat = []
        self.feature_names = self.orig_col_names.tolist()

        for sub_degree in range(2, self.degree + 1):
            for col_ixs in combinations(range(X.shape[1]), sub_degree):
                # add name for new column
                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])
                self.feature_names.append(name)
                # get column multiplications value
                out = X[:, col_ixs[0]]    
                for j in col_ixs[1:]:
                    out = out.multiply(X[:, j])
                out_mat.append(out)
        return sparse.hstack([X] + out_mat)

>>>>>>>>>>>>>>>>Leaning from the expert

as the array grows we need more computational power

hashing is a way of increasing memory efficiency


hashing limits the size of the matrice

a hash out may be a integer

the hashing tokenizer maps the token to the value

dimensionality reduction

instead of using the countvectorizer for tokenizing words we change to the hashingvectorizer

vec=HashingVectorizer(norm=None,
	non_negative=True,
	token_pattern=TOKENS_ALPHANUMERIC,
	ngram_range=(1,2))




































