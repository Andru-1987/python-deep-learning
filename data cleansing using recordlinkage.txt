>>>>>>Generating pairs record linkage
record linkage is pairing data from different sources
1. generate pairs
2. score the pairs based on similarities
3. link the data


two data sources census_a and census_b
1. given_name
2. surname
3. date_of_birth
4. suburb
5. address_1

blocking is matching based on a matching column

conda install -c conda-forge r-recordlinkageactive
import recordlinkage

indexer=recordlinkage.Index()

indexer.block('state')
pairs=indexer.index(census_A,census_B)

#create a compare object
compare_c1=recordlinkage.Compare()

compare_c1.exact('date_of_birth','date_of_birth', label='date_of_birth')
compare_c1.exact('state','state',label='state')

#compare strings with fuzzy values

compare_c1.string('surname','surname', threshold=0.85, label='surname')
compare_c1.string('address_1','address_1',threshold=0.85, label='address_1'

potential_matches = compare_c1.compute(pairs, census_A, census_B)

#filtering for matches

potential_matches[potential_matches.sum(axis=1)=>2]


>>>Sample >>> recordlinkage using blocking

# Create an indexer and object and find possible pairs
indexer = recordlinkage.Index()


# Block pairing on cuisine_type
indexer.block('cuisine_type')

# Generate pairs
pairs = indexer.index(restaurants, restaurants_new)

# Create a comparison object
comp_cl = recordlinkage.Compare()

# Find exact matches on city, cuisine_types 
comp_cl.exact('city', 'city', label='city')
comp_cl.exact('cuisine_type', 'cuisine_type', label = 'cuisine_type')

# Find similar matches of rest_name
comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.85) 

# Get potential matches and print
potential_matches = comp_cl.compute(pairs, restaurants,restaurants_new)
print(potential_matches)


>>>Where n is the minimum number of columns you want matching to ensure a proper duplicate find, what do you think should the value of n be?

matches=potential_matches[potential_matches.sum(axis=1)=>3]

>>>>>>>>>Linking the data

matches = potential_matches[potential_matches.sum(axis = 1) >= 3]

#results in rows that is most likely duplicates

#get the indices

matches.index

duplicate_rows=matches.index.get_level_values(1)

print(census_B_index)

census_B_duplicates=census_B[census_B.index.isin(duplicate_rows)]

find the none duplicates  by filtering by subset

census_B_new=census_B[~census_B.index.isin(duplicate_rows)]

full_census = census_A.append(census_B_new)

>>>>>Sample >>>linking the data

# Isolate potential matches with row sum >=3
matches = potential_matches[potential_matches.sum(axis = 1) >= 3]

# Get values of second column index of matches
matching_indices = matches.index.get_level_values(1)

# Subset restaurants_new based on non-duplicate values
non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]

# Append non_dup to restaurants
full_restaurants = restaurants.append(non_dup)
print(full_restaurants)










































