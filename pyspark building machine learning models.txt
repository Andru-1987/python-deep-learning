https://github.com/conwayyao/Recipe-Analysis/blob/master/CuisineAnalyzer/cuisinedata/indian.csv

Indian food recipes (tangent)

more data is the objective

data is divided into partitions
the partition can fit into ram
spark does most processing in memory

the cluster composed of one or more nodes.

each node is a computer with ram and physical storage.

a cluster manager allocates.

resources and coordinates activity across the cluster. 

using the spark api the driver communicates with the cluster manager.

on each node, spark launches and executor tasks application.  Work is divided into tasks which are units of computation.  tasks run multiple threads across the cores in a node.

Interaction with spark can be written in java, scala, python, or r


import pyspark

pyspark.__version__
'2.4.1'

Structured Data - pyspark.sql
Streaming Data - pyspark.streaming
Machine Learning - pyspark.ml

connect to spark

remote cluster
spark://<IP address | DNS name>:<port>
spark://13.59.151.161:7077

7077 is the default port

Local cluster:
local
local[4]
local[*]

from pyspark.sql import SparkSession

spark= SparkSession.builder
	.master('local[*]')
	.appName('first_spark_application')
	.getOrCreate()



spark.stop()


>>>>sample create an spark session

# Import the PySpark module
from pyspark.sql import SparkSession

# Create SparkSession object
spark = SparkSession.builder \
                    .master('local[*]') \
                    .appName('test') \
                    .getOrCreate()

# What version of Spark?
# (Might be different to what you saw in the presentation!)
print(spark.version)

# Terminate the cluster
spark.stop()

>>>>>>>>>>>Loading data into a dataframe
1. count() #number of rows
2. show()
3. printSchema()
4. dtypes

cars.csv
1. mfr
2. mod
3. org
4. type
5. cyl
6. size
7. weight
8. len
9. rpm
10. cons

spark.read.csv parameters:

1. Header=True #tells if the first row is a header row
2. sep=','
3. schema - explicit column data types
4. inferSchema - deduced column data types from data (two passes over the data to infer the data types of the columns)
5. nullValue - placeholder for missing data


cars.printSchema()


read.csv treats all columns as strings by default

cars=spark.read.csv('cars.csv',header=True, inferSchema=True, nullValue='NA')


schema = StructType)[
	StructField("maker",StringType()),
	StructField("cyl",IntegerType()),
	StructField("size",DoubleType())
])

cars=spark.read.csv('cars.csv',header=True, schema=schema, nullValue='NA')


>>>>> read.csv

# Read data from CSV file
flights = spark.read.csv('flights.csv',
                         sep=',',
                         header=True,
                         inferSchema=True,
                         nullValue='NA')

# Get number of records
print("The data contain %d records." % flights.count())

# View the first five records
flights.show(5)

# Check column data types
print(flights.printSchema())
print(flights.dtype)

[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]

>>>>>sample >>> create a schema for the read.csv

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# Specify column names and types
schema = StructType([
    StructField("id", IntegerType()),
    StructField("text", StringType()),
    StructField("label", IntegerType())
])

# Load data from a delimited file
sms = spark.read.csv('sms.csv', sep=';', header=True, schema=schema)

# Print schema of DataFrame
sms.printSchema()


>>>>>>>>>>>>>>Data preparation>>>>>>>>>>>

cars.csv:

maker
model
origin
type
cyl
size
weight
length
rpm
consumption

cars=cars.drop('maker','model')
cars=cars.select('origin, 'type','cyl','size','weight','length','rpm','consumption')


cars.filter('cyl IS NULL').count()

or 

drop all records with missing values in any column

cars=cars.dropna()


from pyspark.sql.functions import round

#kilograms conversion
cars = cars.withColumn('mass',round(cars.weight/2.205,0))

#meters conversion
cars= cars.withColumn('length',round(cars.length * 0.0254,3))

>>>>>>Indexing categorical data

from pyspark.ml.feature import StringIndexer

indexer=StringIndexer(inputCol='type',
			outputCol='type_idx')

indexer=indexer.fit(cars)

cars=indexer.transform(cars)

use StringOrderType to change order

cars=StringIndexer(inputCol="origin", outputCol="label").fit(cars).transform(cars)

from pyspark.ml.feature import VectorAssembler

assembler=VectorAssembler(inputCols=['cyl','size'], outputCol='features')

assembler.transform(cars)

All the features are assembled into a single column


>>>>>>>> sample  >>>> determine delay vs missing flights

# Remove the 'flight' column
flights_drop_column = flights.drop('flight')

# Number of records with missing 'delay' values
flights_drop_column.filter('delay IS NULL').count()

# Remove records with missing 'delay' values
flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')

# Remove records with missing values in any column and get the number of remaining rows
flights_none_missing = flights_valid_delay.dropna()
print(flights_none_missing.count())


>>>> sample >>> create a new column and drop the old column

# Import the required function
from pyspark.sql.functions import round

# Convert 'mile' to 'km' and drop 'mile' column
flights_km = flights.withColumn('km', round(flights.mile * 1.60934, 0)) \
                    .drop('mile')

# Create 'label' column indicating whether flight delayed (1) or not (0)
flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))

# Check first five records
flights_km.show(5)


>>>>>> sample >>> transforming categorical string data

from pyspark.ml.feature import StringIndexer

# Create an indexer
indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')

# Indexer identifies categories in the data
indexer_model = indexer.fit(flights)

# Indexer creates a new column with numeric index values
flights_indexed = indexer_model.transform(flights)

# Repeat the process for the other categorical feature
flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)


>>>> create the vector assembler

# Import the necessary class
from pyspark.ml.feature import VectorAssembler

# Create an assembler object
assembler = VectorAssembler(inputCols=[
'mon', 'dom', 'dow',
'carrier_idx',
'org_idx', 
'km',
'depart',
'duration'
], outputCol='features')

# Consolidate predictor columns
flights_assembled = assembler.transform(flights)

# Check the resulting column
flights_assembled.select('features', 'delay').show(5, truncate=False)


>>>>>>>>>>>>>>>>>Decision tree

recursive partition

first split is a dominate class and the non dominate class

the depth of the tree along a branch need not be the same


0 for usa manufactured cars
1 for manufactured elsewhere

called label

cars_train, cars_test=cars.randomSplit([.8,.2], seed=23)

cars_train.count()
cars_test.count()


from pyspark.ml.classification import DecisionTreeClassifier

tree=DecisionTreeClassifier()


tree.fit(cars_train)

prediction=tree_model.transform(cars_test)
1. label
2. prediction
3. probability

>>>>>>> create the confusion matrix

prediction.groupBy("label","prediction").count().show()

True positive
False positive
False negative
True negative

Accuracy=(TN+TP)/(TN+TP+FN+FP)

>>>>sample >>> train test split

# Split into training and testing sets in a 80:20 ratio
flights_train, flights_test = flights.randomSplit([0.8,0.2],seed=17)

# Check that training set has around 80% of records
training_ratio = flights_train.count() / flights_test.count()
print(training_ratio)


>>>>>>>sample >>> Decision Tree Classifier

# Import the Decision Tree Classifier class
from pyspark.ml.classification import DecisionTreeClassifier

# Create a classifier object and fit to the training data
tree =DecisionTreeClassifier()
tree_model = tree.fit(flights_train)

# Create predictions for the testing data and take a look at the predictions
prediction = tree_model.transform(flights_test)
prediction.select('label', 'prediction', 'probability').show(5, False)


>>>>> sample >>> create a confusion matrix

# Create a confusion matrix
prediction.groupBy("label", 'prediction').count().show()

# Calculate the elements of the confusion matrix
TN = prediction.filter('prediction = 0 AND label = prediction').count()
TP = prediction.filter('prediction=1 AND label=prediction').count()
FN = prediction.filter('prediction=0 AND label!=prediction').count()
FP = prediction.filter('prediction=1 AND label!=prediction').count()

# Accuracy measures the proportion of correct predictions
accuracy = (TP+TN)/(TN+TP+FN+FP)
print(accuracy)


<<<<<<<<<<<<<<logistic regression

logistic curve models 0 or 1

threshhold above .5 then predictive state is 1 else 0

the curve can be more steep or more gradual or shift left or right


from pyspark.ml.classification import LogisticRegression


logistic=LogisticRegression()

logistic = logistic.fit(cars_train)

prediction=logistics.transform(car_test)

precision=  tp/ (tp+fp)

recall= tp/(tp+fn)

from pyspark.ml.evaluation import MulticlassClassficationEvaluator

evaluator=MulticlassClassificationEvaluator()
evaluator.evaluate(prediction,{evaluator.metricName:'weightedPrecision'})

1. weightedRecall
2. accuracy
3. f1

roc and auc
plots the true positive rate by the false positive rate

>>>>sample  >>> predict using logistic regression

# Import the logistic regression class
from pyspark.ml.classification import LogisticRegression


# Create a classifier object and train on training data
logistic = LogisticRegression().fit(flights_train)

# Create predictions for the testing data and show confusion matrix
prediction = logistic.transform(flights_test)
prediction.groupBy("label", "prediction").count().show()

>>>> sample >>> get the roc auc value

from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator

# Calculate the elements of the confusion matrix
TN = prediction.filter('prediction = 0 AND label = prediction').count()
TP = prediction.filter('prediction=1 AND label=prediction').count()
FN = prediction.filter('prediction=0 AND label!=prediction').count()
FP = prediction.filter('prediction=1 AND label!=prediction').count()

# Calculate precision and recall
precision = TP/(TP+FP)
recall = TP/(TP+FN)
print('precision = {:.2f}\nrecall    = {:.2f}'.format(precision, recall))

# Find weighted precision
multi_evaluator = MulticlassClassificationEvaluator()
weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: "weightedPrecision"})

# Find AUC
binary_evaluator = BinaryClassificationEvaluator()
auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName:'areaUnderROC'})

print(auc) 

>>>>>>>>>>>>>>Turning text into tables

80% of machine learning is data preparation

one record per document.

collection of documents

tokenize the documents as columns in the table

remove the stop words

The table indicates the frequency of the word

the table is known as a term document matrix

remove punctuation

>>>>>>>>>>>>regular expressions

from pyspark.sql.functions import regexp_replace

REGEX='[,\\-]'

escape the -

books=books.withColumn('text',regexp_replace(books.text, REGEX,' '))


books= Tokenizer(inputCol="text", outputCol="tokens").transform(books)

stop words are common words adding very little information

from pyspark.ml.feature import StopWordsRemover

stopwords=StopWordsRemover()

stopwords.getStopWords()

stopwords=stopwords.setInputCol('tokens').setOutputCol('words')

books=stopwords.transform(books)

from pyspark.ml.feature import HashingTF

hasher=HashingTF(inputCol="words", outputCol="hash", numFeatures=32)

books=hasher.transform(books)

from pyspark.ml.feature import IDF

books=IDF(inputCol="hash", outputCol="features").fit(books).transform(books)

IDF measure the frequency of the word across all documents

inverse document frequency

>>>>>> sample >>> Tokenize text

# Import the necessary functions
from pyspark.sql.functions import regexp_replace
from pyspark.ml.feature import Tokenizer

# Remove punctuation (REGEX provided) and numbers
wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\-]', ' '))
wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))

# Merge multiple spaces
wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))

# Split the text into words
wrangled = Tokenizer(inputCol='text', outputCol="words").transform(wrangled)

wrangled.show(4, truncate=False)


>>> sample >>> building the [[idf features]]

from pyspark.ml.feature import StopWordsRemover,HashingTF,IDF

# Remove stop words.
wrangled = StopWordsRemover(inputCol="words", outputCol="terms")\
      .transform(sms)

# Apply the hashing trick
wrangled = HashingTF(inputCol="terms", outputCol="hash", numFeatures=1024)\
      .transform(wrangled)

# Convert hashed symbols to TF-IDF
tf_idf = IDF(inputCol="hash", outputCol="features")\
      .fit(wrangled).transform(wrangled)
      
tf_idf.select('terms', 'features').show(4, truncate=False)

>>>>> sample >>> logistic regression prediction

# Split the data into training and testing sets
sms_train, sms_test = sms.randomSplit([0.8,0.2], seed=13)

# Fit a Logistic Regression model to the training data
logistic = LogisticRegression(regParam=0.2).fit(sms_train)

# Make predictions on the testing data
prediction = logistic.transform(sms_test)

# Create a confusion matrix, comparing predictions to known labels
prediction.groupBy("label","prediction").count().show()



>>>>>>>>>>>>>>>>>>One-Hot Encoding

categorical data

create a column for each of the categorical levels

dummy variables

the sparse form records the column number and the value 1 for the categorical data

the process of creating dummy variables is called one hot encoding because only one column is active or hot

from pyspark.ml.feature import OneHotEncoderEstimator


indexer=StringIndexer(inputCol='type',
			outputCol='type_idx')

indexer=indexer.fit(cars)

cars=indexer.transform(cars)

onehot = OneHotEncoderEstimator(inputCols=['type_idx'], outputCols=['type_dummy']

onehot=onehot.fit(cars)

onehot.categorySizes

cars=onehot.transform(cars)

cars.select('type,'type_idx','type_dummy').distinct().sort('type_idx').show


DenseVector([1,0,0,0,0,7,0,0])
represented as
SparseVector(8,[0,5],[1,7])


8 items, non zero in position 0 and 5 with values 1 and 7

>>>>>>>sample  OneHotEncoderEstimator

# Import the one hot encoder class
from pyspark.ml.feature import OneHotEncoderEstimator

# Create an instance of the one hot encoder
onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])

# Apply the one hot encoder to the flights data
onehot = onehot.fit(flights)
flights_onehot = onehot.transform(flights)

# Check the results
flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()


>>>>>>>>>>>>>>>>>>>>Regression

scatter plot to visualize consumption verus mass


residuals are the difference between the observed value and the corresponding value.  Vertical distance between the points and the model line

The best model is found by minimizing a loss function

Mean squared error 1/N (yi-y^i)**2

yi=observed values
y^i=model values

predict consumption using mass, cyl, and type_dummy

from pyspark.ml.regression import LinearRegression

regression = LinearRegression(labelCol="consumption")

regression=regression.fit(cars_train)

predictions = regression.transform(cars_test)


>>>>>>>>>>>>>>>>>RegressionEvaluator

from pyspark.ml.evalution import RegressionEvaluator

RegressionEvaluator(labelCol='consumption').evalulate(predictions)

the square root of the mean square error is the RMSE

RMSE is the standard deviation of the residuals

RegressionEvaluator
1. mae (mean absolute error)
2. r2
3. mse (mean squared error)


regression.intercept

slope with each mass and consumption combination
slope indicates how rapidly the model changes when mass and consumption change

regression.coefficients

there is a coefficient for each of the predictors
mass
cyl
midsize
small
compact
sporty
large

DenseVector([.0027,.1897,-1.309,-1.7933,-1.3594,-1.2917,-1.9693])


mass=.0027
cyl=.1897

midsize=-1.3
small=-1.79
compact=-1.35
sporty=-1.29
large=-1.9

large vehicles are the most fuel efficient for their mass.

all other types consume less fuel than a large vehicle

>>>> sample build a regressor

from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator

# Create a regression object and train on training data
regression =LinearRegression(labelCol="duration").fit(flights_train)

# Create predictions for the testing data and take a look at the predictions
predictions = regression.transform(flights_test)
predictions.select('duration', 'prediction').show(5, False)

# Calculate the RMSE
RegressionEvaluator(labelCol='duration').evaluate(predictions)


>>>> sample >>> making sense of the coefficients

# Intercept (average minutes on ground)
inter = regression.intercept
print(inter)

# Coefficients
coefs = regression.coefficients
print(coefs)

# Average minutes per km
minutes_per_km = regression.coefficients[0]
print(minutes_per_km)

# Average speed in km per hour
avg_speed = 60 / minutes_per_km
print(avg_speed)


regression
duration= intercept + coefficient * distance

>>> sample

from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator

# Create a regression object and train on training data
regression = LinearRegression(labelCol='duration').fit(flights_train)

# Create predictions for the testing data
predictions = regression.transform(flights_test)

# Calculate the RMSE on testing data
RegressionEvaluator(labelCol='duration').evaluate(predictions)


>>>>>>>>>>>>>>>>>>intrepreting coefficients

The coefficients attribute is a list, where the first element indicates how flight duration changes with flight distance

0 — km
1 — ORD
2 — SFO
3 — JFK
4 — LGA
5 — SMF
6 — SJC and
7 — TUS.

# Average speed in km per hour
avg_speed_hour = 60/regression.coefficients[0]
print(avg_speed_hour)

# Average minutes on ground at OGG
inter = regression.intercept
print(inter)

# Average minutes on ground at JFK
avg_ground_jfk = regression.intercept + regression.coefficients[3]
print(avg_ground_jfk)

# Average minutes on ground at LGA
avg_ground_lga = regression.intercept + regression.coefficients[4]
print(avg_ground_lga)


output:
807.3336599681242
15.856628374450773
68.53550999587868
62.56747182033072

intercept
15.856628374450773

coefficients
[0.07431871477075411,28.399568722791717,20.55190513998231,52.678881621427905,46.710843445879945,18.28741662016716,15.721837765620768,17.737941505895947]


>>>>>>>>>>>>>>>>carefully manipulating features

bucketing:  assigning features to buckets or bins with well defined boundaries

heights in meters

defined as short, average, tall

from pyspark.ml.feature import Bucketizer

bucketizer=Bucketizer(splits=[3500,4500,6000,6500],
	inputCol="rpm",
	outputCol="rpm_bin")

cars=bucketizer.transform(cars)

bucketed.select('rpm','rpm_bin').show(5)

cars.groupBy('rpm_bin).count().show()

low, medium, high [no sparse array]


regression.coefficients
[1.3814,0.1433])
regression.intercept
8.1835

low RPM
consumption=8.1835+1.3814

medium RPM
consumption=8.1835+0.1433

operations on a single column

log()
sqrt()
pow()

operations on two columns
product
ratio



bmi = mass / height**2

cars = cars.withColumn('density_line', cars.mass/cars.length)

cars = cars.withColumn('density_quad', cars.mass/cars.length**2)

cars = cars.withColumn('density_cube', cars.mass/cars.length**3)



>>>>>> sample

from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator

# Create buckets at 3 hour intervals through the day
buckets =Bucketizer(splits=[0,3,6,9,12,15,18,21,24], inputCol='depart', outputCol='depart_bucket')

# Bucket the departure times
bucketed = buckets.transform(flights)
bucketed.select('depart','depart_bucket').show(5)

# Create a one-hot encoder
onehot = OneHotEncoderEstimator(inputCols=['depart_bucket'], outputCols=['depart_dummy'])

# One-hot encode the bucketed departure times
flights_onehot = onehot.fit(bucketed).transform(bucketed)
flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').show(5)



>>>> adding departure time

# Find the RMSE on testing data
from pyspark.ml.evaluation import RegressionEvaluator

RegressionEvaluator(labelCol='duration').evaluate(predictions)

# Average minutes on ground at OGG for flights departing between 00:00 and 03:00
avg_night_ogg = regression.intercept + regression.coefficients[8]
print(avg_night_ogg)

# Average minutes on ground at JFK for flights departing between 00:00 and 03:00
avg_night_jfk = regression.intercept + regression.coefficients[8] + regression.coefficients[3]
print(avg_night_jfk)


>>>>>>>>>>>>>>>>Regularization


penalized regression: model is punished for having too many coefficients.

mse chooses coefficients that minimize the loss or the residual.

regularization term
Lasso - absolute value of the coefficients
Ridge - square of the coefficients

both will shrink the coefficients of non contributing coefficients
lasso moves those coefficients to 0

strength of regularization is determined by parameter alpha

alpha=0 - no regularization (standard regression)
alpha=infinity - complete regularation (all coefficients zero)


assembler = VectorAssembler(inputCols=[
	'mass','cyl','type_dummy','density_line','density_quad','density_cube'], outputCol='features')

cars=assembler.transform(cars)

regression = LinearRegression(labelCol='consumption').fit(cars_train)

regression.coefficients

DenseVector([-0.012,0.174,-0.897,-1.445,-0.985,-1.071,-1.335,0.189,-0.780,1.160])

every predictor is contributing to the model
however it is unlike that all the feature are equally important in predicting consumption

ridge= LinearRegression(labelCol='consumption', elasticNetParam=0, regParam=0.1)
ridge.fit(cars_train)

#RMSE
0.72453


lasso = LinearRegression(labelCol='consumption', elasticNetParam=1, regParam=0.1)
lasso.fit(cars_train)


DenseVector[0,0,0,-.056,0,0,0,0.026,0,0])

all but two features are 0

small type car and the linear density


>>>>features

km
org (origin airport, one-hot encoded, 8 levels)
depart (departure time, binned in 3 hour intervals, one-hot encoded, 8 levels)
dow (departure day of week, one-hot encoded, 7 levels) and
mon (departure month, one-hot encoded, 12 levels).


>>>>>>>Sample   >>> create a linear Regression and a Regression Evaluator

from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator


# Fit linear regression model to training data
regression = LinearRegression(labelCol="duration").fit(flights_train)

# Make predictions on testing data
predictions = regression.transform(flights_test)

# Calculate the RMSE on testing data
rmse = RegressionEvaluator(labelCol="duration").evaluate(predictions)
print("The test RMSE is", rmse)

# Look at the model coefficients
coeffs = regression.coefficients
print(coeffs)



>>>>>>> sample >>>> lasso

from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator

# Fit Lasso model (a = 1) to training data
regression = LinearRegression(labelCol='duration', regParam=1, elasticNetParam=1).fit(flights_train)

# Calculate the RMSE on testing data
rmse = RegressionEvaluator(labelCol='duration').evaluate(regression.transform(flights_test))
print("The test RMSE is", rmse)

# Look at the model coefficients
coeffs = regression.coefficients
print(coeffs)

# Number of zero coefficients
zero_coeff = sum([beta == 0 for beta in regression.coefficients])
print("Number of coefficients equal to 0:", zero_coeff)

output

The test RMSE is 11.221618112066176
[0.07326284332459325,0.26927242574175647,-4.213823507520847,23.31411303902282,16.924833465407964,-7.538366699625629,-5.04321753247765,-20.348693139176927,0.0,0.0,0.0,0.0,0.0,1.199161974782719,0.43548357163388335,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]
Number of coefficients equal to 0: 22


>>>>>>>>>>>>>>>>>>>Pipeline

make it better

leakage occurrs when fit is applied to any testing data
a pipeline combines a series of steps


manual sequence of steps

#one hot encode the type categorical data
indexer=StringIndexer(inputCol='type', outputCol='type_idx')

onehot = OneHotEncoderEstimator(inputCols=['type_idx'), outputCols=['type_dummy'])

#create a single features column

assemble=VectorAssembler(inputCols=['mass','cyl','type_dummy'],outputCol='features')

#build the regression model

regression=LinearRegression(labelCol='consumption')

indexer=indexer.fit(cars_train)
cars_train=indexer.transform(cars_train)
cars_test=indexer.transform(cars_test)

cars_train=onehot.transform(cars_train)
cars_tst=onehot.transform(cars_test)

cars_train=assemble.transform(cars_train)
cars_test=assemble.transform(cars_test)

regression=regression.fit(cars_train)

predictions=regression.transform(cars_test)


from pyspark.ml import Pipeline

#sequence of stages
pipeline= Pipeline(stages=[indexer, onehot, assemble, regression])

pipeline=pipeline.fit(cars_train)

predictions=pipeline.transform(cars_test)

pipeline.stages
pipeline.stages[3]

pipeline.stages[3].intercept
pipeline.stages[3].coefficients


>>>>>>>>>>>>>sample  >>> setup for the pipeline

# Convert categorical strings to index values
indexer =StringIndexer(inputCol='org',outputCol='org_idx')

# One-hot encode index values
onehot =OneHotEncoderEstimator(
    inputCols=['org_idx','dow'],
    outputCols=['org_dummy','dow_dummy']
)

# Assemble predictors into a single column
assembler = VectorAssembler(inputCols=['km','org_dummy','dow_dummy'], outputCol='features')

# A linear regression object
regression = LinearRegression(labelCol='duration')

>>>> Add the pipeline

# Import class for creating a pipeline
from pyspark.ml import Pipeline

# Construct a pipeline
pipeline = Pipeline(stages=[indexer,onehot,assembler,regression])

# Train the pipeline on the training data
pipeline = pipeline.fit(flights_train)

# Make predictions on the testing data
predictions = pipeline.transform(flights_test)


>>>>>>>>>> sample  >>>> setup for words for a pipeline

from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF

# Break text into tokens at non-word characters
tokenizer = Tokenizer(inputCol='text', outputCol='words')

# Remove stop words
remover = StopWordsRemover(inputCol="words", outputCol='terms')

# Apply the hashing trick and transform to TF-IDF
hasher = HashingTF(inputCol="terms", outputCol="hash")
idf = IDF(inputCol="hash", outputCol="features")

# Create a logistic regression object and add everything to a pipeline
logistic = LogisticRegression()
pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])


>>>>>>>>>>>>>>>>>Cross validation

data, training, testing

training is split into fold called cross validation

regression=LinearRegression(labelCol='consumption')

from pyspark.ml.tuning import CrossValidator, ParamGridBuilder


params = ParamGridBuilder().build()

cv=CrossValidator(estimator=regression,
	estimatorParamMaps = params,
	evaluator=evaluator,
	numFolds=10, seed=13)

folds default to 3

cv=cv.fit(cars_train)

cv.avgMetrics

predictions=cv.transform(cars_test)
rmse = evaluator.evaluate(predictions)

>>>> sample >>>>  setup Cross Validation with 5 fold

# Create an empty parameter grid
params = ParamGridBuilder().build()

# Create objects for building and evaluating a regression model
regression = LinearRegression(labelCol='duration')
evaluator = RegressionEvaluator(labelCol="duration")

# Create a cross validator
cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)

# Train and test model on multiple folds of the training data
cv = cv.fit(flights_train)

# NOTE: Since cross-valdiation builds multiple models, the fit() method can take a little while to complete.

predictions=cv.transform(flights_test)
rmse = evaluator.evaluate(predictions)
print(rmse)

output: 16



>>>>  sample cross validating with a pipeline

# Create an indexer for the org field
indexer = StringIndexer(inputCol='org',outputCol='org_idx')

# Create an one-hot encoder for the indexed org field
onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])

# Assemble the km and one-hot encoded fields
assembler = VectorAssembler(inputCols=['km','org_dummy'],outputCol='features')

# Create a pipeline and cross-validator.
pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])

cv = CrossValidator(estimator=pipeline,
          estimatorParamMaps=params,
          evaluator=evaluator)

>>>> sorting a groupby

res = g.apply(lambda x: x.sort_values(ascending=False).head(3))
>>>>>>>>>>>>>>>>>>>>>Grid Search


regression=LinearRegression(labelCol='consumption', fitIntercept=True)

regression = regression.fit(cars_train)

rmse = evaluator.evaluate(regression.transform(cars_test))

output.745

set fitInercept=False
output.65

from pyspark.ml.tuning import ParamGridBuilder

params=ParamGridBuilder()

#add grid points

params=params.addGrid(regression.fitIntercept,[True,False])

params=params.build()

print('Number of models to be tested:', len(params))

output:2


cv=CrossValidator(estimator=regression,
		estimatorParamsMaps=params,
		evaluator=evaluator)

cv=cv.setNumFolds(10).setSeed(13).fit(cars_train)

20 models

cv.avgMetrics

output:[.8006,0.9079] 

the model that includes an intercept does better than one without

cv.bestModel

predictions = cv.tranform(cars_test)

cv.bestModel.explainParam('fitIntercept')

output: current is True

#add multiple grid points

params=params.addGrid(regression.fitIntercept,[True,False]) \
	.addGrid(regression.regParam,[0.001,0.01,0.1,1,10])\
	.addGrid(regression.elasticNetParam,[0,0.25,0.5,0.75,1])\
	.build()


>>>>>>>>sample   >>> Grid builder  >> CrossValidator 5 folds

# Create parameter grid
params = ParamGridBuilder()

# Add grids for two parameters
params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0]) \
               .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])

# Build the parameter grid
params = params.build()
print('Number of models to be tested: ', len(params))

# Create cross-validator
cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)

output: number of models to be tested: 12


>>>>>>>>sample >>> make predictions with the best model

# Get the best model from cross validation
best_model = cv.bestModel

# Look at the stages in the best model
print(best_model.stages)

# Get the parameters for the LinearRegression object in the best model
best_model.stages[3].extractParamMap()

# Generate predictions on testing data using the best model then calculate RMSE
predictions = best_model.transform(flights_test)
evaluator.evaluate(predictions)

output:
[StringIndexer_14299b2d5472, OneHotEncoderEstimator_9a650c117f1d, VectorAssembler_933acae88a6e, LinearRegression_9f5a93965597]
In [1]:


>>>>>>>>>>>>>>>>>sample build the params grid for logistic regression

# Create parameter grid
params = ParamGridBuilder()

# Add grid for hashing trick parameters
params = params.addGrid(hasher.numFeatures,[1024, 4096,16384]) \
               .addGrid(hasher.binary, [True,False])

# Add grid for logistic regression parameters
params = params.addGrid(logistic.regParam,[0.01, 0.1, 1.0, 10.0]) \
               .addGrid(logistic.elasticNetParam, [0.0, 0.5, 1.0])

# Build parameter grid
params = params.build()

>>>>>>>>>>>>>>>>>Ensemble

it is a collection of models

collective opinion of a group is better

there must be diversity and independence 


Random Forest is a collection of trees

no two trees are the same

from pyspark.ml.classification import RandomForestClassifier

forest=RandomForestClassifier(numTrees=5)

forest=forest.fit(cars_train)

forest.trees


the model uses : cyl, size, mass, length, rpm, and consumption

forest.featureImportances

sparseVector(6,{0:0.0205,1:0.2701,2:0.108,3:0.1895,4:0.2939,5:0.1181})

rpm is the most important
cyl is the least important


Gradient-Boost Trees

trees working in series

1. build a decision tree and add to the ensemble
2. predict label for each training instance using ensemble
3. compare predictions with known labels
4. emphasize training instances with incorrect predictions

5. return to 1 and train another tree which works to improve the incorrect predictions.

each new tree attempts to correct the errors of the proceeding trees

from pyspark.ml.classification import GBTClassifier

gbt = GBTClassifier(maxIter=10)

gbt=gbt.fit(cars_train)


>>>> sample  compare the AUC for the Decision Tree, Random Forest, and Gradient Boosted Tree



# Import the classes required
from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Create model objects and train on training data
tree =DecisionTreeClassifier().fit(flights_train)
gbt = GBTClassifier().fit(flights_train)

# Compare AUC on testing data
evaluator = BinaryClassificationEvaluator()
evaluator.evaluate(tree.transform(flights_test))
evaluator.evaluate(gbt.transform(flights_test))

# Find the number of trees and the relative importance of features
print(gbt.getNumTrees)
print(gbt.featureImportances)


output
20 (trees)
(3,[0,1,2],[0.27857733519498645,0.3517987451488248,0.36962391965618874])




>>>>>>>>>> RandomForestClassifier

You'll find good values for the following parameters:

featureSubsetStrategy — the number of features to consider for splitting at each node and
maxDepth — the maximum number of splits along any branch.

# Create a random forest classifier
forest = RandomForestClassifier()

# Create a parameter grid
params = ParamGridBuilder() \
            .addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']) \
            .addGrid(forest.maxDepth, [2, 5, 10]) \
            .build()

# Create a binary classification evaluator
evaluator = BinaryClassificationEvaluator()

# Create a cross-validator
cv = CrossValidator(estimator=forest, estimatorParamMaps=params,evaluator=evaluator, numFolds=5)


>>>> random forest  >>> cross validation  >>> evaluate auc

# Average AUC for each parameter combination in grid
avg_auc = cv.avgMetrics

# Average AUC for the best model
best_model_auc =  max(avg_auc)

print(avg_auc)
print(best_model_auc)

# What's the optimal parameter value?
print(cv.bestModel.params)
opt_max_depth = cv.bestModel.explainParam("maxDepth")
opt_feat_substrat = cv.bestModel.explainParam('featureSubsetStrategy')

# AUC for best model on testing data
best_auc = evaluator.evaluate(cv.transform(flights_test))






























