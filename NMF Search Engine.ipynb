{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re \n",
    "import heapq\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@deangelaneves/how-to-build-a-search-engine-from-scratch-in-python-part-1-96eb240f9ecb\n",
    "def normalized_term_frequency(word, document):\n",
    "\n",
    "    raw_frequency = document.count(word)\n",
    "\n",
    "    if raw_frequency == 0:\n",
    "\n",
    "        return 0\n",
    "\n",
    "    return 1 + math.log(raw_frequency)\n",
    "\n",
    "def docs_contain_word(word, documents):\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for document in list_of_documents:\n",
    "\n",
    "        if word in document:\n",
    "\n",
    "            counter+=1\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "\n",
    "def get_vocabulary(documents):\n",
    "\n",
    "    vocabulary = set([word for document in documents for word in document])\t\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "def inverse_document_frequency(documents, vocabulary):\n",
    "\n",
    "    idf = {}\n",
    "\n",
    "    for word in vocabulary:\n",
    "\n",
    "        contains_word = docs_contain_word(word, documents)\n",
    "\n",
    "        idf[word] = 1 + math.log(len(documents)/(contains_word))\n",
    "    return idf\n",
    "\n",
    "def tf_idf(search_keys, dataframe, label):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    #fit_transform method converts the given text into TF-IDF scores for all the documents. \n",
    "\n",
    "    tfidf_weights_matrix = tfidf_vectorizer.fit_transform(dataframe.loc[:, label])\n",
    "\n",
    "    search_query_weights = tfidf_vectorizer.transform([search_keys])\n",
    "\n",
    "\n",
    "    return search_query_weights, tfidf_weights_matrix\n",
    "\n",
    "def cos_similarity(search_query_weights, tfidf_weights_matrix):\n",
    "\n",
    "\n",
    "    cosine_distance = cosine_similarity(query, tfidf_matrix)\n",
    "\n",
    "    similarity_list = cosine_distance[0]\n",
    "\n",
    "  \n",
    "    return similarity_list\n",
    "\n",
    "def most_similar(similarity_list, min_talks=1):\n",
    "\n",
    "    most_similar= []\n",
    "\n",
    "  \n",
    "\n",
    "    while min_talks > 0:\n",
    "\n",
    "        tmp_index = np.argmax(similarity_list)\n",
    "\n",
    "        most_similar.append(tmp_index)\n",
    "\n",
    "        similarity_list[tmp_index] = 0\n",
    "\n",
    "        min_talks -= 1\n",
    "\n",
    "    return most_similar\n",
    "\n",
    "def word_count(string) :\n",
    "    words= string.split()\n",
    "    return len(words)\n",
    "\n",
    "\n",
    "def avg_word_length(x):\n",
    "    words=x.split()\n",
    "    word_lengths= [len(word) for word in words]\n",
    "\n",
    "    avg_word_length= sum(word_lengths)/len(words)\n",
    "    return(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Herbert', 'Simon', 'research', 'and', 'concepts', 'increased', 'computer', 'scientist', 'understanding', 'of', 'reasoning', 'and', 'increased', 'the', 'computer', \"'s\", 'ability', 'too', 'solve', 'problems', 'and', 'proof', 'theorems', 'Herbert', 'Simon', ',', 'Al', 'Newell', ',', 'Clifford', 'Shaw', 'proposals', 'were', 'radical', 'and', 'affect', 'computer', 'scientist', 'today', 'In', 'Simon', '’', 's', 'book', ',', '“', 'Models', 'of', 'my', 'life', '”', ',', 'Simon', 'demonstrated', 'the', 'Logical', 'Theorem', 'algorithm', 'could', 'prove', 'certain', 'mathematical', 'theorems', 'Simon', 'said', ',', '“', 'This', 'was', 'the', 'task', 'to', 'get', 'a', 'system', 'to', 'discover', 'proof', 'for', 'a', 'theorem', ',', 'not', 'simply', 'to', 'test', 'the', 'proof', 'We', 'picked', 'logic', 'just', 'because', 'I', 'happened', 'to', 'have', 'Principia', 'Mathematica', 'sitting', 'on', 'my', 'shelf', 'and', 'I', 'was', 'using', 'it', 'to', 'see', 'what', 'was', 'involved', 'in', 'finding', 'a', 'proof', 'of', 'anything', '”', 'Alfred', 'North', 'Whitehead', 'and', 'Bertrand', 'Russell', 'book', 'Principia', 'Mathematica', 'contained', 'theorems', 'considered', 'to', 'form', 'the', 'foundation', 'of', 'mathematical', 'logic', 'Simeon', 'evolved', 'Logic', 'theorem', 'into', 'General', 'problem', 'solver', 'GPS', 'is', 'currently', 'used', 'in', 'robotics', 'and', 'gives', 'the', 'robot', 'amazing', 'problem', 'solving', 'capabilities', 'Many', 'mathematicians', 'considered', 'some', 'of', 'LTs', 'proofs', 'superior', 'to', 'those', 'previously']\n"
     ]
    }
   ],
   "source": [
    "paragraph=\"Herbert Simon research and concepts increased computer scientist understanding of reasoning and increased the computer's ability too solve problems and proof theorems . Herbert Simon , Al Newell , Clifford Shaw proposals were radical and affect computer scientist today . In Simon’s book , “Models of my life” , Simon demonstrated the Logical Theorem algorithm could prove certain mathematical theorems . Simon said , “This was the task to get a system to discover proof for a theorem , not simply to test the proof . We picked logic just because I happened to have Principia Mathematica sitting on my shelf and I was using it to see what was involved in finding a proof of anything . ” Alfred North Whitehead and Bertrand Russell book Principia Mathematica contained theorems considered to form the foundation of mathematical logic . Simeon evolved Logic theorem into General problem solver . GPS is currently used in robotics and gives the robot amazing problem solving capabilities . Many mathematicians considered some of LTs proofs superior to those previously published\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "words=[]\n",
    "for sentence in sentences:\n",
    "    word_list=nltk.word_tokenize(sentence)\n",
    "    \n",
    "    #print(word_list)\n",
    "    for i in range(0, len(word_list)-1):\n",
    "        words.append(word_list[i])\n",
    "    \n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herbert Simon research and concepts increased computer scientist understanding of reasoning and increased the computer's ability too solve problems and proof theorems . Herbert Simon , Al Newell , Clifford Shaw proposals were radical and affect computer scientist today . In Simon’s book , “Models of my life” , Simon demonstrated the Logical Theorem algorithm could prove certain mathematical theorems . Simon said , “This was the task to get a system to discover proof for a theorem , not simply to test the proof . We picked logic just because I happened to have Principia Mathematica sitting on my shelf and I was using it to see what was involved in finding a proof of anything . ” Alfred North Whitehead and Bertrand Russell book Principia Mathematica contained theorems considered to form the foundation of mathematical logic . Simeon evolved Logic theorem into General problem solver . GPS is currently used in robotics and gives the robot amazing problem solving capabilities . Many mathematicians considered some of LTs proofs superior to those previously published\n",
      "                                                   X         Target\n",
      "0  [[0.2158296070947768, 0.0, 0.0, 0.0, 0.0, 0.0,...  understanding\n",
      "1  [[0.0, 0.28992980412937575, 0.2899298041293757...           were\n",
      "2  [[0.0, 0.0, 0.0, 0.0, 0.2661936480775059, 0.0,...       theorems\n",
      "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...            was\n",
      "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.115875778570...           what\n",
      "9 9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-0011b1457496>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m#X_train,y_train,X_test,y_test=train_test_split(X,y,stratify=y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;31m#print(nb.score(X_test,y_test))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \"\"\"\n\u001b[1;32m--> 615\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    800\u001b[0m                     \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 802\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    803\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    795\u001b[0m               dtype='datetime64[ns]')\n\u001b[0;32m    796\u001b[0m         \"\"\"\n\u001b[1;32m--> 797\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\arrays\\numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "def return_weights(vocab, original_vocab, vector, vector_index):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False).index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "print(paragraph)\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "text_tfidf = tfidf_vec.fit_transform(sentences)\n",
    "\n",
    "shape=text_tfidf.get_shape()\n",
    "vocab= {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
    "\n",
    "df=pd.DataFrame(columns=['X','Target'])\n",
    "for index in np.arange(shape[0]):\n",
    "    weights=return_weights(vocab,tfidf_vec.vocabulary_,text_tfidf,index)\n",
    "    target=vocab.get(np.max(weights))\n",
    "    index=len(df)\n",
    "    df.loc[index]=[text_tfidf[index].toarray(),target]\n",
    "    \n",
    "print(df.head())    \n",
    "    \n",
    "\n",
    "\n",
    "#print(vocab)\n",
    "\n",
    "#zipped_row=dict(zip(text_tfidf.indices,\n",
    "#text_tfidf.data))\n",
    "\n",
    "#print(zipped_row)\n",
    "#for index,item in zipped_row.items():\n",
    "#    print(vocab.get(index),item)\n",
    "\n",
    "#X=text_tfidf.toarray()\n",
    "X=df['X']\n",
    "y=np.array(df['Target']).reshape(-1,1)\n",
    "print(len(X),len(y))    \n",
    "nb=MultinomialNB()\n",
    "#X_train,y_train,X_test,y_test=train_test_split(X,y,stratify=y)\n",
    "nb.fit(X,y)\n",
    "#print(nb.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "                                            sentence  word_count  \\\n",
      "0  Herbert Simon research and concepts increased ...          23   \n",
      "1  Herbert Simon , Al Newell , Clifford Shaw prop...          17   \n",
      "2  In Simon’s book , “Models of my life” , Simon ...          21   \n",
      "3  Simon said , “This was the task to get a syste...          25   \n",
      "4  We picked logic just because I happened to hav...          32   \n",
      "\n",
      "   avg_word_length  \n",
      "0         6.347826  \n",
      "1         5.058824  \n",
      "2         5.333333  \n",
      "3         3.520000  \n",
      "4         4.218750  \n",
      "(9, 3)\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences))\n",
    "index=np.arange(0, len(sentences))\n",
    "df=pd.DataFrame({'sentence':sentences})\n",
    "df.set_index(index)\n",
    "\n",
    "df['word_count']=df['sentence'].apply(word_count)\n",
    "df['avg_word_length']=df['sentence'].apply(avg_word_length)\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "#a,b=tf_idf (pd.Series(['Herbert']),df,'Herbert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(99, 70)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords=spacy.lang.en.stop_words.STOP_WORDS\n",
    "doc=nlp(paragraph)\n",
    "tokens=[token.text for token in doc]\n",
    "#print(tokens)\n",
    "lemmas =[token.lemma_ for token in doc]\n",
    "lemmas =[lemma for lemma in lemmas\n",
    "        if (lemma.isalpha() )\n",
    "         and lemma not in stopwords\n",
    "        ]\n",
    "#print(lemmas)\n",
    "\n",
    "paragraph2=' '.join(lemmas)\n",
    "#print(paragraph2)\n",
    "\n",
    "vectorizer= CountVectorizer()\n",
    "\n",
    "bow_lem= vectorizer.fit_transform(lemmas)\n",
    "\n",
    "#sparse array\n",
    "print(bow_lem.toarray())\n",
    "\n",
    "print(bow_lem.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECTED SENTENCE: Herbert Simon research and concepts increased computer scientist understanding of reasoning and increased the computer's ability too solve problems and proof theorems .\n",
      "SIMILIAR SENTENCES: \n",
      "(0, 'Herbert Simon , Al Newell , Clifford Shaw proposals were radical and affect computer scientist today .')\n",
      "(1, 'Simon said , “This was the task to get a system to discover proof for a theorem , not simply to test the proof .')\n",
      "(2, 'In Simon’s book , “Models of my life” , Simon demonstrated the Logical Theorem algorithm could prove certain mathematical theorems .')\n",
      "[(9528407286733565721, 1, 3)]\n"
     ]
    }
   ],
   "source": [
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar sentences\n",
    "    sim_scores = sim_scores[1:4]\n",
    "    # Get the movie indices\n",
    "    sentence_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return df['sentence'].iloc[sentence_indices]\n",
    "\n",
    "index=np.arange(0, len(sentences))\n",
    "df=pd.DataFrame({'sentence':sentences})\n",
    "df.set_index(index)\n",
    "\n",
    "#print(df)\n",
    "df['word_count']=df['sentence'].apply(word_count)\n",
    "df['avg_word_length']=df['sentence'].apply(avg_word_length)\n",
    "#print(df.head())\n",
    "#print(df.shape)\n",
    "\n",
    "a_sentence=sentences[0]\n",
    "print(\"SELECTED SENTENCE: \" + a_sentence)\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['sentence'])\n",
    "\n",
    "# Convert matrix into a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray())\n",
    "# Map the column names to vocabulary \n",
    "tfidf.columns = tfidf.get_feature_names()\n",
    "\n",
    "#print(\"FEATURES: \"+ str(tfidf.get_feature_names()[0:]))\n",
    "\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "indices = pd.Series(df.index, index=df['sentence']).drop_duplicates()\n",
    "\n",
    "results=get_recommendations(a_sentence,cosine_sim,indices)\n",
    "\n",
    "print(\"SIMILIAR SENTENCES: \")\n",
    "\n",
    "for result in enumerate(results):\n",
    "       print (result)\n",
    "\n",
    "doc=nlp(paragraph)\n",
    "\n",
    "matcher=Matcher(nlp.vocab)\n",
    "\n",
    "pattern=[\n",
    "    {'LOWER':'simon'}\n",
    "    ,{'LOWER':'research'}\n",
    "    ]\n",
    "\n",
    "matcher.add('IPHONE_PATTERN',None,pattern)\n",
    "\n",
    "matches=matcher(doc)\n",
    "\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Person found:  Herbert\n",
      " Person found:  Herbert\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(paragraph)\n",
    "for token in doc:\n",
    "    # Check if the next token's text equals '%'\n",
    "    if token.text == 'Herbert':\n",
    "        print(' Person found: ', token.text)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herbert PROPN compound Simon\n",
      "Simon PROPN compound research\n",
      "and CCONJ cc research\n",
      "concepts NOUN conj research\n",
      "increased VERB amod understanding\n",
      "computer NOUN compound scientist\n",
      "scientist NOUN compound understanding\n",
      "understanding NOUN dobj concepts\n",
      "of ADP prep understanding\n",
      "reasoning NOUN pobj of\n",
      "and CCONJ cc concepts\n",
      "increased VERB conj concepts\n",
      "the DET det computer\n",
      "computer NOUN poss ability\n",
      "'s PART case computer\n",
      "ability NOUN dobj increased\n",
      "too ADV advmod solve\n",
      "solve VERB advcl increased\n",
      "problems NOUN dobj solve\n",
      "and CCONJ cc problems\n",
      "proof NOUN compound theorems\n",
      "theorems NOUN conj problems\n",
      ". PUNCT punct research\n",
      "Herbert PROPN compound Simon\n",
      "Simon PROPN nsubj were\n",
      ", PUNCT punct Simon\n",
      "Al PROPN compound Newell\n",
      "Newell PROPN appos Simon\n",
      ", PUNCT punct Simon\n",
      "Clifford PROPN compound Shaw\n",
      "Shaw PROPN compound proposals\n",
      "proposals NOUN appos Simon\n",
      "radical ADJ acomp were\n",
      "and CCONJ cc were\n",
      "affect VERB conj were\n",
      "computer NOUN compound scientist\n",
      "scientist NOUN dobj affect\n",
      "today NOUN npadvmod affect\n",
      ". PUNCT punct were\n",
      "Simon PROPN poss book\n",
      "’s PART case Simon\n",
      "book NOUN pobj In\n",
      ", PUNCT punct In\n",
      "“ PUNCT compound Models\n",
      "Models NOUN intj ,\n",
      "of ADP prep Models\n",
      "my ADJ poss life\n",
      "life NOUN pobj of\n",
      ", PUNCT punct demonstrated\n",
      "Simon PROPN nsubj demonstrated\n",
      "the DET det Theorem\n",
      "Logical PROPN compound Theorem\n",
      "Theorem PROPN dobj demonstrated\n",
      "algorithm NOUN amod Theorem\n",
      "could VERB aux prove\n",
      "prove VERB ccomp demonstrated\n",
      "certain ADJ amod theorems\n",
      "mathematical ADJ amod theorems\n",
      "theorems NOUN dobj prove\n",
      ". PUNCT punct demonstrated\n",
      "Simon PROPN nsubj said\n",
      ", PUNCT punct said\n",
      "“ PUNCT punct said\n",
      "This DET nsubj was\n",
      "was VERB ccomp said\n",
      "the DET det task\n",
      "task NOUN attr was\n",
      "to PART aux get\n",
      "get VERB xcomp was\n",
      "a DET det system\n",
      "system NOUN dobj get\n",
      "to PART aux discover\n",
      "discover VERB relcl system\n",
      "proof NOUN dobj discover\n",
      "for ADP prep proof\n",
      "a DET det theorem\n",
      "theorem NOUN pobj for\n",
      ", PUNCT punct get\n",
      "not ADV neg test\n",
      "simply ADV advmod test\n",
      "to PART aux test\n",
      "test VERB advcl get\n",
      "the DET det proof\n",
      "proof NOUN dobj test\n",
      ". PUNCT punct said\n",
      "We PRON nsubj picked\n",
      "logic NOUN dobj picked\n",
      "just ADV advmod happened\n",
      "because ADP mark happened\n",
      "I PRON nsubj happened\n",
      "happened VERB advcl picked\n",
      "to PART aux have\n",
      "have VERB xcomp happened\n",
      "Principia PROPN compound Mathematica\n",
      "Mathematica PROPN nsubj sitting\n",
      "sitting VERB ccomp have\n",
      "on ADP prep sitting\n",
      "my ADJ poss shelf\n",
      "shelf NOUN pobj on\n",
      "and CCONJ cc using\n",
      "I PRON nsubj using\n",
      "was VERB aux using\n",
      "it PRON dobj using\n",
      "to PART aux see\n",
      "see VERB xcomp using\n",
      "what NOUN nsubjpass involved\n",
      "was VERB auxpass involved\n",
      "involved VERB ccomp see\n",
      "in ADP prep involved\n",
      "finding VERB pcomp in\n",
      "a DET det proof\n",
      "proof NOUN dobj finding\n",
      "of ADP prep proof\n",
      "anything NOUN pobj of\n",
      ". PUNCT punct using\n",
      "Alfred PROPN compound Whitehead\n",
      "North PROPN compound Whitehead\n",
      "Whitehead PROPN nsubj contained\n",
      "and CCONJ cc Whitehead\n",
      "Bertrand PROPN compound Russell\n",
      "Russell PROPN compound book\n",
      "book NOUN compound Mathematica\n",
      "Principia PROPN compound Mathematica\n",
      "Mathematica PROPN conj Whitehead\n",
      "theorems NOUN dobj contained\n",
      "considered VERB acl theorems\n",
      "to PART aux form\n",
      "form VERB xcomp considered\n",
      "the DET det foundation\n",
      "foundation NOUN dobj form\n",
      "of ADP prep foundation\n",
      "mathematical ADJ amod logic\n",
      "logic NOUN pobj of\n",
      ". PUNCT punct contained\n",
      "Simeon PROPN nsubj evolved\n",
      "Logic PROPN compound theorem\n",
      "theorem VERB dobj evolved\n",
      "into ADP prep evolved\n",
      "General PROPN compound problem\n",
      "problem NOUN pobj into\n",
      "solver NOUN advmod evolved\n",
      ". PUNCT punct evolved\n",
      "GPS PROPN nsubjpass used\n",
      "is VERB auxpass used\n",
      "currently ADV advmod used\n",
      "in ADP prep used\n",
      "robotics NOUN pobj in\n",
      "and CCONJ cc used\n",
      "gives VERB conj used\n",
      "the DET det problem\n",
      "robot NOUN nmod problem\n",
      "amazing ADJ amod problem\n",
      "problem NOUN dative gives\n",
      "solving VERB acl problem\n",
      "capabilities NOUN dobj solving\n",
      ". PUNCT punct used\n",
      "Many ADJ amod mathematicians\n",
      "mathematicians NOUN nsubj considered\n",
      "some DET nsubj proofs\n",
      "of ADP prep some\n",
      "LTs ADJ pobj of\n",
      "proofs NOUN ccomp considered\n",
      "superior ADJ advmod proofs\n",
      "to ADP prep superior\n",
      "those DET pobj to\n",
      "previously ADV advmod published\n",
      "published VERB acl those\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(paragraph)\n",
    "for token in doc:\n",
    "        #print(token.text, token.pos_)\n",
    "        if token.dep_ != 'ROOT' and token.dep_ != 'ART' and token.dep_ != 'VERB ROOT':\n",
    "            print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Herbert', 'PROPN'), ('Simon', 'PROPN'), ('research', 'NOUN'), ('and', 'CCONJ'), ('concepts', 'NOUN'), ('increased', 'VERB'), ('computer', 'NOUN'), ('scientist', 'NOUN'), ('understanding', 'NOUN'), ('of', 'ADP'), ('reasoning', 'NOUN'), ('and', 'CCONJ'), ('increased', 'VERB'), ('the', 'DET'), ('computer', 'NOUN'), (\"'s\", 'PART'), ('ability', 'NOUN'), ('too', 'ADV'), ('solve', 'VERB'), ('problems', 'NOUN'), ('and', 'CCONJ'), ('proof', 'NOUN'), ('theorems', 'NOUN'), ('.', 'PUNCT'), ('Herbert', 'PROPN'), ('Simon', 'PROPN'), (',', 'PUNCT'), ('Al', 'PROPN'), ('Newell', 'PROPN'), (',', 'PUNCT'), ('Clifford', 'PROPN'), ('Shaw', 'PROPN'), ('proposals', 'NOUN'), ('were', 'VERB'), ('radical', 'ADJ'), ('and', 'CCONJ'), ('affect', 'VERB'), ('computer', 'NOUN'), ('scientist', 'NOUN'), ('today', 'NOUN'), ('.', 'PUNCT'), ('In', 'ADP'), ('Simon', 'PROPN'), ('’s', 'PART'), ('book', 'NOUN'), (',', 'PUNCT'), ('“', 'PUNCT'), ('Models', 'NOUN'), ('of', 'ADP'), ('my', 'ADJ'), ('life', 'NOUN'), ('”', 'PUNCT'), (',', 'PUNCT'), ('Simon', 'PROPN'), ('demonstrated', 'VERB'), ('the', 'DET'), ('Logical', 'PROPN'), ('Theorem', 'PROPN'), ('algorithm', 'NOUN'), ('could', 'VERB'), ('prove', 'VERB'), ('certain', 'ADJ'), ('mathematical', 'ADJ'), ('theorems', 'NOUN'), ('.', 'PUNCT'), ('Simon', 'PROPN'), ('said', 'VERB'), (',', 'PUNCT'), ('“', 'PUNCT'), ('This', 'DET'), ('was', 'VERB'), ('the', 'DET'), ('task', 'NOUN'), ('to', 'PART'), ('get', 'VERB'), ('a', 'DET'), ('system', 'NOUN'), ('to', 'PART'), ('discover', 'VERB'), ('proof', 'NOUN'), ('for', 'ADP'), ('a', 'DET'), ('theorem', 'NOUN'), (',', 'PUNCT'), ('not', 'ADV'), ('simply', 'ADV'), ('to', 'PART'), ('test', 'VERB'), ('the', 'DET'), ('proof', 'NOUN'), ('.', 'PUNCT'), ('We', 'PRON'), ('picked', 'VERB'), ('logic', 'NOUN'), ('just', 'ADV'), ('because', 'ADP'), ('I', 'PRON'), ('happened', 'VERB'), ('to', 'PART'), ('have', 'VERB'), ('Principia', 'PROPN'), ('Mathematica', 'PROPN'), ('sitting', 'VERB'), ('on', 'ADP'), ('my', 'ADJ'), ('shelf', 'NOUN'), ('and', 'CCONJ'), ('I', 'PRON'), ('was', 'VERB'), ('using', 'VERB'), ('it', 'PRON'), ('to', 'PART'), ('see', 'VERB'), ('what', 'NOUN'), ('was', 'VERB'), ('involved', 'VERB'), ('in', 'ADP'), ('finding', 'VERB'), ('a', 'DET'), ('proof', 'NOUN'), ('of', 'ADP'), ('anything', 'NOUN'), ('.', 'PUNCT'), ('”', 'PUNCT'), ('Alfred', 'PROPN'), ('North', 'PROPN'), ('Whitehead', 'PROPN'), ('and', 'CCONJ'), ('Bertrand', 'PROPN'), ('Russell', 'PROPN'), ('book', 'NOUN'), ('Principia', 'PROPN'), ('Mathematica', 'PROPN'), ('contained', 'VERB'), ('theorems', 'NOUN'), ('considered', 'VERB'), ('to', 'PART'), ('form', 'VERB'), ('the', 'DET'), ('foundation', 'NOUN'), ('of', 'ADP'), ('mathematical', 'ADJ'), ('logic', 'NOUN'), ('.', 'PUNCT'), ('Simeon', 'PROPN'), ('evolved', 'VERB'), ('Logic', 'PROPN'), ('theorem', 'VERB'), ('into', 'ADP'), ('General', 'PROPN'), ('problem', 'NOUN'), ('solver', 'NOUN'), ('.', 'PUNCT'), ('GPS', 'PROPN'), ('is', 'VERB'), ('currently', 'ADV'), ('used', 'VERB'), ('in', 'ADP'), ('robotics', 'NOUN'), ('and', 'CCONJ'), ('gives', 'VERB'), ('the', 'DET'), ('robot', 'NOUN'), ('amazing', 'ADJ'), ('problem', 'NOUN'), ('solving', 'VERB'), ('capabilities', 'NOUN'), ('.', 'PUNCT'), ('Many', 'ADJ'), ('mathematicians', 'NOUN'), ('considered', 'VERB'), ('some', 'DET'), ('of', 'ADP'), ('LTs', 'ADJ'), ('proofs', 'NOUN'), ('superior', 'ADJ'), ('to', 'ADP'), ('those', 'DET'), ('previously', 'ADV'), ('published', 'VERB')]\n",
      "['Herbert', 'Simon', 'research', 'and', 'concepts', 'increased', 'computer', 'scientist', 'understanding', 'of', 'reasoning', 'and', 'increased', 'the', 'computer', \"'s\", 'ability', 'too', 'solve', 'problems', 'and', 'proof', 'theorems', '.', 'Herbert', 'Simon', ',', 'Al', 'Newell', ',', 'Clifford', 'Shaw', 'proposals', 'were', 'radical', 'and', 'affect', 'computer', 'scientist', 'today', '.', 'In', 'Simon', '’s', 'book', ',', '“', 'Models', 'of', 'my', 'life', '”', ',', 'Simon', 'demonstrated', 'the', 'Logical', 'Theorem', 'algorithm', 'could', 'prove', 'certain', 'mathematical', 'theorems', '.', 'Simon', 'said', ',', '“', 'This', 'was', 'the', 'task', 'to', 'get', 'a', 'system', 'to', 'discover', 'proof', 'for', 'a', 'theorem', ',', 'not', 'simply', 'to', 'test', 'the', 'proof', '.', 'We', 'picked', 'logic', 'just', 'because', 'I', 'happened', 'to', 'have', 'Principia', 'Mathematica', 'sitting', 'on', 'my', 'shelf', 'and', 'I', 'was', 'using', 'it', 'to', 'see', 'what', 'was', 'involved', 'in', 'finding', 'a', 'proof', 'of', 'anything', '.', '”', 'Alfred', 'North', 'Whitehead', 'and', 'Bertrand', 'Russell', 'book', 'Principia', 'Mathematica', 'contained', 'theorems', 'considered', 'to', 'form', 'the', 'foundation', 'of', 'mathematical', 'logic', '.', 'Simeon', 'evolved', 'Logic', 'theorem', 'into', 'General', 'problem', 'solver', '.', 'GPS', 'is', 'currently', 'used', 'in', 'robotics', 'and', 'gives', 'the', 'robot', 'amazing', 'problem', 'solving', 'capabilities', '.', 'Many', 'mathematicians', 'considered', 'some', 'of', 'LTs', 'proofs', 'superior', 'to', 'those', 'previously', 'published']\n"
     ]
    }
   ],
   "source": [
    "#part of speech\n",
    "doc=nlp(paragraph)\n",
    "pos=[(token.text, token.pos_) for token in doc]\n",
    "print(pos)\n",
    "\n",
    "#proper_noun=[word for word in pos if pos=='PROPN']\n",
    "#print (proper_noun)\n",
    "\n",
    "def find_nouns(text):\n",
    "    # Create Doc object\n",
    "    doc2 = nlp(text)\n",
    "  \n",
    "    # Identify the persons\n",
    "    nouns = [token.text for token in doc if token.pos_ == 'PROPN' or 'NOUN']\n",
    "  \n",
    "    # Return persons\n",
    "    return nouns\n",
    "\n",
    "nouns=find_nouns(paragraph)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Herbert Simon', 'PERSON'), ('Herbert Simon', 'PERSON'), ('Al Newell', 'PERSON'), ('Clifford Shaw', 'PERSON'), ('today', 'DATE'), ('Simon', 'PERSON'), ('“Models of my life', 'WORK_OF_ART'), ('Simon', 'PERSON'), ('Logical Theorem', 'NORP'), ('Simon', 'PERSON'), ('Principia Mathematica', 'ORG'), ('Alfred North Whitehead and Bertrand Russell', 'WORK_OF_ART'), ('Principia Mathematica', 'ORG'), ('Logic', 'PRODUCT'), ('GPS', 'PERSON')]\n",
      "['Herbert Simon', 'Herbert Simon', 'Al Newell', 'Clifford Shaw', 'Simon', 'Simon', 'Simon', 'GPS']\n"
     ]
    }
   ],
   "source": [
    "named_entity=[(entity.text,entity.label_) for entity in doc.ents]\n",
    "print(named_entity)\n",
    "\n",
    "def find_persons(text):\n",
    "    # Create Doc object\n",
    "    doc2 = nlp(text)\n",
    "  \n",
    "    # Identify the persons\n",
    "    persons = [ent.text for ent in doc2.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "    # Return persons\n",
    "    return persons\n",
    "\n",
    "persons=find_persons(paragraph)\n",
    "print(persons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Herbert Simon research and concepts increased ...\n",
      "1    Herbert Simon , Al Newell , Clifford Shaw prop...\n",
      "2    In Simon’s book , “Models of my life” , Simon ...\n",
      "3    Simon said , “This was the task to get a syste...\n",
      "4    We picked logic just because I happened to hav...\n",
      "5    ” Alfred North Whitehead and Bertrand Russell ...\n",
      "6    Simeon evolved Logic theorem into General prob...\n",
      "7    GPS is currently used in robotics and gives th...\n",
      "8    Many mathematicians considered some of LTs pro...\n",
      "dtype: object\n",
      "[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 2 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0\n",
      "  1 0 0 0 0 0 0 1 0 1 0 0]\n",
      " [1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      "  0 0 0 0 1 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1\n",
      "  1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sentences_series = pd.Series(sentences)\n",
    "\n",
    "print(sentences_series)\n",
    "\n",
    "#vectorizer= CountVectorizer()\n",
    "vectorizer=CountVectorizer(strip_accents='ascii', stop_words='english', lowercase=False)\n",
    "\n",
    "bow_matrix= vectorizer.fit_transform(sentences_series)\n",
    "\n",
    "print(bow_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'affect', 'al', 'alfred', 'algorithm', 'amazing', 'and', 'anything', 'because', 'bertrand', 'book', 'capabilities', 'certain', 'clifford', 'computer', 'concepts', 'considered', 'contained', 'could', 'currently', 'demonstrated', 'discover', 'evolved', 'finding', 'for', 'form', 'foundation', 'general', 'get', 'gives', 'gps', 'happened', 'have', 'herbert', 'in', 'increased', 'into', 'involved', 'is', 'it', 'just', 'life', 'logic', 'logical', 'lts', 'many', 'mathematica', 'mathematical', 'mathematicians', 'models', 'my', 'newell', 'north', 'not', 'of', 'on', 'picked', 'previously', 'principia', 'problem', 'problems', 'proof', 'proofs', 'proposals', 'prove', 'published', 'radical', 'reasoning', 'research', 'robot', 'robotics', 'russell', 'said', 'scientist', 'see', 'shaw', 'shelf', 'simeon', 'simon', 'simply', 'sitting', 'solve', 'solver', 'solving', 'some', 'superior', 'system', 'task', 'test', 'the', 'theorem', 'theorems', 'this', 'those', 'to', 'today', 'too', 'understanding', 'used', 'using', 'was', 'we', 'were', 'what', 'whitehead']\n",
      "[[0.21582961 0.         0.         0.         0.         0.\n",
      "  0.37488637 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.36458624 0.21582961 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.18229312 0.         0.43165921\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.12496212 0.         0.         0.         0.         0.\n",
      "  0.21582961 0.15849861 0.         0.         0.         0.\n",
      "  0.         0.21582961 0.21582961 0.         0.         0.\n",
      "  0.         0.18229312 0.         0.         0.         0.\n",
      "  0.14004215 0.         0.         0.21582961 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.12496212\n",
      "  0.         0.15849861 0.         0.         0.         0.\n",
      "  0.21582961 0.21582961 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.2899298  0.2899298  0.         0.         0.\n",
      "  0.16786503 0.         0.         0.         0.         0.\n",
      "  0.         0.2899298  0.24487933 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.24487933 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.2899298  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.2899298  0.         0.\n",
      "  0.2899298  0.         0.         0.         0.         0.\n",
      "  0.         0.24487933 0.         0.2899298  0.         0.\n",
      "  0.18812244 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2899298\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2899298  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.26619365 0.\n",
      "  0.         0.         0.         0.         0.22483139 0.\n",
      "  0.26619365 0.         0.         0.         0.         0.\n",
      "  0.26619365 0.         0.26619365 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.19548441 0.\n",
      "  0.         0.         0.         0.         0.         0.26619365\n",
      "  0.         0.26619365 0.         0.         0.         0.22483139\n",
      "  0.         0.26619365 0.22483139 0.         0.         0.\n",
      "  0.15412215 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26619365 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34544223 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.15412215\n",
      "  0.19548441 0.19548441 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.22965221 0.         0.\n",
      "  0.22965221 0.         0.         0.         0.22965221 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.22965221\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.337299   0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.22965221 0.         0.         0.         0.         0.\n",
      "  0.14901101 0.22965221 0.         0.         0.         0.\n",
      "  0.         0.         0.22965221 0.22965221 0.22965221 0.26593041\n",
      "  0.1686495  0.         0.22965221 0.         0.44703304 0.\n",
      "  0.         0.         0.         0.         0.19396791 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.11587578 0.20013603 0.20013603 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.20013603\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.20013603 0.20013603 0.         0.14697373 0.\n",
      "  0.         0.20013603 0.         0.20013603 0.20013603 0.\n",
      "  0.14697373 0.         0.         0.         0.16903808 0.\n",
      "  0.         0.         0.16903808 0.         0.         0.\n",
      "  0.11587578 0.20013603 0.20013603 0.         0.16903808 0.\n",
      "  0.         0.14697373 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.20013603 0.         0.20013603 0.\n",
      "  0.         0.         0.20013603 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25971859 0.\n",
      "  0.         0.         0.         0.20013603 0.33807616 0.20013603\n",
      "  0.         0.20013603 0.        ]\n",
      " [0.         0.         0.         0.26657512 0.         0.\n",
      "  0.15434302 0.         0.         0.26657512 0.22515359 0.\n",
      "  0.         0.         0.         0.         0.22515359 0.26657512\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.26657512 0.26657512 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.19576455 0.         0.         0.         0.22515359 0.22515359\n",
      "  0.         0.         0.         0.         0.26657512 0.\n",
      "  0.15434302 0.         0.         0.         0.22515359 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26657512\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.15434302\n",
      "  0.         0.19576455 0.         0.         0.17296863 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.26657512]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.38370906 0.\n",
      "  0.         0.         0.         0.38370906 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.38370906 0.         0.         0.         0.         0.\n",
      "  0.2817841  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.32408678\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.38370906\n",
      "  0.         0.         0.         0.         0.38370906 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2817841  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.28960431\n",
      "  0.16767657 0.         0.         0.         0.         0.28960431\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.28960431 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.28960431\n",
      "  0.28960431 0.         0.         0.         0.21267647 0.\n",
      "  0.         0.         0.28960431 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.24460441\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.28960431 0.28960431 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.28960431\n",
      "  0.         0.         0.         0.         0.         0.16767657\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.28960431 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26103212 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30905423 0.30905423 0.         0.\n",
      "  0.30905423 0.         0.         0.         0.         0.\n",
      "  0.17893779 0.         0.         0.30905423 0.         0.\n",
      "  0.         0.         0.30905423 0.         0.         0.30905423\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30905423 0.30905423 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.30905423 0.20053142 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Create a TfidfVectorizer: tfidf\n",
    "vectorizer = TfidfVectorizer() \n",
    "\n",
    "# Apply fit_transform to document: csr_mat\n",
    "csr_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "#print(csr_mat)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(csr_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99551016 1.         0.         0.00295036 0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[[1.         0.26783465 0.11787934 0.10756047 0.08121546 0.12746356\n",
      "  0.         0.08381288 0.02236045]\n",
      " [0.26783465 1.         0.06498544 0.02803232 0.01945149 0.0259088\n",
      "  0.         0.02814703 0.        ]\n",
      " [0.11787934 0.06498544 1.         0.12542881 0.08459516 0.18708746\n",
      "  0.0550844  0.06741761 0.02757828]\n",
      " [0.10756047 0.02803232 0.12542881 1.         0.23125281 0.1183672\n",
      "  0.04752275 0.0445903  0.08964417]\n",
      " [0.08121546 0.01945149 0.08459516 0.23125281 1.         0.18558371\n",
      "  0.04141486 0.05068751 0.07281629]\n",
      " [0.12746356 0.0259088  0.18708746 0.1183672  0.18558371 1.\n",
      "  0.05516334 0.05175942 0.12107576]\n",
      " [0.         0.         0.0550844  0.04752275 0.04141486 0.05516334\n",
      "  1.         0.07927305 0.        ]\n",
      " [0.08381288 0.02814703 0.06741761 0.0445903  0.05068751 0.05175942\n",
      "  0.07927305 1.         0.        ]\n",
      " [0.02236045 0.         0.02757828 0.08964417 0.07281629 0.12107576\n",
      "  0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "model = NMF(n_components=6)\n",
    "\n",
    "csr_matrix = vectorizer.fit_transform(sentences)\n",
    "# Fit the model to articles\n",
    "model.fit(csr_matrix)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = model.transform(csr_matrix)\n",
    "\n",
    "#print(len(nmf_features))\n",
    "# Print the NMF features\n",
    "#print(nmf_features)\n",
    "\n",
    "\n",
    "norm_features = normalize(nmf_features)\n",
    "\n",
    "current_sentence= norm_features[1,:]\n",
    "\n",
    "similarities=norm_features.dot(current_sentence)\n",
    "print(similarities)\n",
    "\n",
    "\n",
    "#print(similarities.nlargest())\n",
    "\n",
    "#cosine_sim=cosine_similarity(csr_matrix, csr_matrix)\n",
    "\n",
    "#print(cosine_sim)\n",
    "\n",
    "cosine_sim= linear_kernel(csr_matrix,csr_matrix)\n",
    "\n",
    "print(cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x19d009d7708>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokens = sum([word_tokenize(sentence) for sentence in sentences], [])\n",
    "\n",
    "words_frequency = FreqDist(tokens)\n",
    "\n",
    "words_frequency.plot(30, cumulative = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'herbert': 2, 'simon': 5, 'research': 1, 'and': 7, 'concepts': 1, 'increased': 2, 'computer': 3, 'scientist': 2, 'understanding': 1, 'of': 5, 'reasoning': 1, 'the': 6, 's': 2, 'ability': 1, 'too': 1, 'solve': 1, 'problems': 1, 'proof': 4, 'theorems': 3, 'al': 1, 'newell': 1, 'clifford': 1, 'shaw': 1, 'proposals': 1, 'were': 1, 'radical': 1, 'affect': 1, 'today': 1, 'in': 3, 'book': 2, 'models': 1, 'my': 2, 'life': 1, 'demonstrated': 1, 'logical': 1, 'theorem': 3, 'algorithm': 1, 'could': 1, 'prove': 1, 'certain': 1, 'mathematical': 2, 'said': 1, 'this': 1, 'was': 3, 'task': 1, 'to': 7, 'get': 1, 'a': 3, 'system': 1, 'discover': 1, 'for': 1, 'not': 1, 'simply': 1, 'test': 1, 'we': 1, 'picked': 1, 'logic': 3, 'just': 1, 'because': 1, 'i': 2, 'happened': 1, 'have': 1, 'principia': 2, 'mathematica': 2, 'sitting': 1, 'on': 1, 'shelf': 1, 'using': 1, 'it': 1, 'see': 1, 'what': 1, 'involved': 1, 'finding': 1, 'anything': 1, 'alfred': 1, 'north': 1, 'whitehead': 1, 'bertrand': 1, 'russell': 1, 'contained': 1, 'considered': 2, 'form': 1, 'foundation': 1, 'simeon': 1, 'evolved': 1, 'into': 1, 'general': 1, 'problem': 2, 'solver': 1, 'gps': 1, 'is': 1, 'currently': 1, 'used': 1, 'robotics': 1, 'gives': 1, 'robot': 1, 'amazing': 1, 'solving': 1, 'capabilities': 1, 'many': 1, 'mathematicians': 1, 'some': 1, 'lts': 1, 'proofs': 1, 'superior': 1, 'those': 1, 'previously': 1, 'published': 1}\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      "  0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#https://medium.com/analytics-vidhya/implementing-the-tf-idf-search-engine-5e9a42b1d30b\n",
    "\n",
    "   \n",
    "corpus = nltk.sent_tokenize(paragraph)    \n",
    "\n",
    "\n",
    "for i in range(len(corpus )):\n",
    "    corpus [i] = corpus [i].lower()\n",
    "    corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "    corpus [i] = re.sub(r'\\s+',' ',corpus [i])\n",
    "\n",
    "#print(corpus)\n",
    "\n",
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "\n",
    "print(wordfreq)\n",
    "\n",
    "#most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "\n",
    "sentence_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    #for token in most_freq:\n",
    "    for token in wordfreq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)\n",
    "\n",
    "sentence_vectors = np.asarray(sentence_vectors)\n",
    "\n",
    "print(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing Herbert 0.07276924\n",
      "computing Simon 0.05576835\n",
      "computing research 0.40343753\n",
      "computing and 0.1943571\n",
      "computing concepts 0.45711964\n",
      "computing increased 0.24925496\n",
      "computing computer 0.6224965\n",
      "computing scientist 0.2660795\n",
      "computing understanding 0.38134873\n",
      "computing of 0.24522913\n",
      "computing reasoning 0.30303115\n",
      "computing and 0.1943571\n",
      "computing increased 0.24925496\n",
      "computing the 0.22144683\n",
      "computing computer 0.6224965\n",
      "computing 's 0.1260481\n",
      "computing ability 0.30118415\n",
      "computing too 0.16354746\n",
      "computing solve 0.3485522\n",
      "computing problems 0.27601594\n",
      "computing and 0.1943571\n",
      "computing proof 0.18525296\n",
      "computing theorems 0.23384278\n",
      "computing . 0.1465077\n",
      "computing Herbert 0.07276924\n",
      "computing Simon 0.05576835\n",
      "computing , 0.18776377\n",
      "computing Al 0.059818372\n",
      "computing Newell -0.0047686454\n",
      "computing , 0.18776377\n",
      "computing Clifford 0.020007184\n",
      "computing Shaw 0.07874915\n",
      "computing proposals 0.1906419\n",
      "computing were 0.088380806\n",
      "computing radical 0.18658942\n",
      "computing and 0.1943571\n",
      "computing affect 0.26474202\n",
      "computing computer 0.6224965\n",
      "computing scientist 0.2660795\n",
      "computing today 0.2458577\n",
      "computing . 0.1465077\n",
      "computing In 0.20310555\n",
      "computing Simon 0.05576835\n",
      "computing ’s 0.1566072\n",
      "computing book 0.14006203\n",
      "computing , 0.18776377\n",
      "computing “ 0.16643858\n",
      "computing Models 0.293657\n",
      "computing of 0.24522913\n",
      "computing my 0.11952344\n",
      "computing life 0.2368741\n",
      "computing ” 0.18655829\n",
      "computing , 0.18776377\n",
      "computing Simon 0.05576835\n",
      "computing demonstrated 0.29319173\n",
      "computing the 0.22144683\n",
      "computing Logical 0.3891341\n",
      "computing Theorem 0.27457505\n",
      "computing algorithm 0.45087993\n",
      "computing could 0.2261679\n",
      "computing prove 0.23306897\n",
      "computing certain 0.26280278\n",
      "computing mathematical 0.47654593\n",
      "computing theorems 0.23384278\n",
      "computing . 0.1465077\n",
      "computing Simon 0.05576835\n",
      "computing said 0.17151013\n",
      "computing , 0.18776377\n",
      "computing “ 0.16643858\n",
      "computing This 0.17403455\n",
      "computing was 0.096630886\n",
      "computing the 0.22144683\n",
      "computing task 0.3699183\n",
      "computing to 0.19440654\n",
      "computing get 0.14464964\n",
      "computing a 0.21335497\n",
      "computing system 0.4341465\n",
      "computing to 0.19440654\n",
      "computing discover 0.21387707\n",
      "computing proof 0.18525296\n",
      "computing for 0.2270494\n",
      "computing a 0.21335497\n",
      "computing theorem 0.27457505\n",
      "computing , 0.18776377\n",
      "computing not 0.19226551\n",
      "computing simply 0.23587337\n",
      "computing to 0.19440654\n",
      "computing test 0.2567172\n",
      "computing the 0.22144683\n",
      "computing proof 0.18525296\n",
      "computing . 0.1465077\n",
      "computing We 0.24185108\n",
      "computing picked 0.024817133\n",
      "computing logic 0.40104094\n",
      "computing just 0.16287896\n",
      "computing because 0.20407403\n",
      "computing I 0.10273568\n",
      "computing happened 0.10298791\n",
      "computing to 0.19440654\n",
      "computing have 0.1529994\n",
      "computing Principia 0.028432935\n",
      "computing Mathematica 0.14816025\n",
      "computing sitting 0.10337253\n",
      "computing on 0.16639738\n",
      "computing my 0.11952344\n",
      "computing shelf 0.14861476\n",
      "computing and 0.1943571\n",
      "computing I 0.10273568\n",
      "computing was 0.096630886\n",
      "computing using 0.32855365\n",
      "computing it 0.22552036\n",
      "computing to 0.19440654\n",
      "computing see 0.15658993\n",
      "computing what 0.24160239\n",
      "computing was 0.096630886\n",
      "computing involved 0.2439669\n",
      "computing in 0.20310555\n",
      "computing finding 0.20014775\n",
      "computing a 0.21335497\n",
      "computing proof 0.18525296\n",
      "computing of 0.24522913\n",
      "computing anything 0.15932484\n",
      "computing . 0.1465077\n",
      "computing ” 0.18655829\n",
      "computing Alfred 0.08811794\n",
      "computing North 0.027524812\n",
      "computing Whitehead -0.027441887\n",
      "computing and 0.1943571\n",
      "computing Bertrand -0.018936047\n",
      "computing Russell 0.043770514\n",
      "computing book 0.14006203\n",
      "computing Principia 0.028432935\n",
      "computing Mathematica 0.14816025\n",
      "computing contained 0.15273635\n",
      "computing theorems 0.23384278\n",
      "computing considered 0.24339128\n",
      "computing to 0.19440654\n",
      "computing form 0.24674134\n",
      "computing the 0.22144683\n",
      "computing foundation 0.3020023\n",
      "computing of 0.24522913\n",
      "computing mathematical 0.47654593\n",
      "computing logic 0.40104094\n",
      "computing . 0.1465077\n",
      "computing Simeon -0.04268905\n",
      "computing evolved 0.32150316\n",
      "computing Logic 0.40104094\n",
      "computing theorem 0.27457505\n",
      "computing into 0.24331877\n",
      "computing General 0.35554928\n",
      "computing problem 0.26211986\n",
      "computing solver 0.23071279\n",
      "computing . 0.1465077\n",
      "computing GPS 0.20427263\n",
      "computing is 0.23551805\n",
      "computing currently 0.23484209\n",
      "computing used 0.23543057\n",
      "computing in 0.20310555\n",
      "computing robotics 0.4662865\n",
      "computing and 0.1943571\n",
      "computing gives 0.23953764\n",
      "computing the 0.22144683\n",
      "computing robot 0.28552082\n",
      "computing amazing 0.111162305\n",
      "computing problem 0.26211986\n",
      "computing solving 0.40362126\n",
      "computing capabilities 0.5456464\n",
      "computing . 0.1465077\n",
      "computing Many 0.20614779\n",
      "computing mathematicians 0.31407258\n",
      "computing considered 0.24339128\n",
      "computing some 0.19451094\n",
      "computing of 0.24522913\n",
      "computing LTs -0.003336904\n",
      "computing proofs 0.17753477\n",
      "computing superior 0.2543586\n",
      "computing to 0.19440654\n",
      "computing those 0.1991222\n",
      "computing previously 0.21355443\n",
      "computing published 0.116425596\n",
      "mathematics Herbert 0.15218541\n",
      "mathematics Simon 0.13137709\n",
      "mathematics research 0.44823438\n",
      "mathematics and 0.1919858\n",
      "mathematics concepts 0.5593571\n",
      "mathematics increased 0.1467636\n",
      "mathematics computer 0.3999747\n",
      "mathematics scientist 0.3824585\n",
      "mathematics understanding 0.4930703\n",
      "mathematics of 0.22132188\n",
      "mathematics reasoning 0.499432\n",
      "mathematics and 0.1919858\n",
      "mathematics increased 0.1467636\n",
      "mathematics the 0.17734843\n",
      "mathematics computer 0.3999747\n",
      "mathematics 's 0.094495125\n",
      "mathematics ability 0.2470493\n",
      "mathematics too 0.17345947\n",
      "mathematics solve 0.44170922\n",
      "mathematics problems 0.3159438\n",
      "mathematics and 0.1919858\n",
      "mathematics proof 0.30108315\n",
      "mathematics theorems 0.48887467\n",
      "mathematics . 0.1428826\n",
      "mathematics Herbert 0.15218541\n",
      "mathematics Simon 0.13137709\n",
      "mathematics , 0.16458644\n",
      "mathematics Al 0.04165649\n",
      "mathematics Newell -0.039203625\n",
      "mathematics , 0.16458644\n",
      "mathematics Clifford 0.12616213\n",
      "mathematics Shaw 0.080864504\n",
      "mathematics proposals 0.2353185\n",
      "mathematics were 0.14509474\n",
      "mathematics radical 0.2384969\n",
      "mathematics and 0.1919858\n",
      "mathematics affect 0.21838902\n",
      "mathematics computer 0.3999747\n",
      "mathematics scientist 0.3824585\n",
      "mathematics today 0.17317246\n",
      "mathematics . 0.1428826\n",
      "mathematics In 0.2502716\n",
      "mathematics Simon 0.13137709\n",
      "mathematics ’s 0.054871567\n",
      "mathematics book 0.27858353\n",
      "mathematics , 0.16458644\n",
      "mathematics “ 0.05520011\n",
      "mathematics Models 0.23841166\n",
      "mathematics of 0.22132188\n",
      "mathematics my 0.15176548\n",
      "mathematics life 0.26025322\n",
      "mathematics ” 0.07625117\n",
      "mathematics , 0.16458644\n",
      "mathematics Simon 0.13137709\n",
      "mathematics demonstrated 0.32376388\n",
      "mathematics the 0.17734843\n",
      "mathematics Logical 0.4115163\n",
      "mathematics Theorem 0.51952326\n",
      "mathematics algorithm 0.36506292\n",
      "mathematics could 0.17860435\n",
      "mathematics prove 0.32869932\n",
      "mathematics certain 0.26308656\n",
      "mathematics mathematical 0.8114012\n",
      "mathematics theorems 0.48887467\n",
      "mathematics . 0.1428826\n",
      "mathematics Simon 0.13137709\n",
      "mathematics said 0.097733065\n",
      "mathematics , 0.16458644\n",
      "mathematics “ 0.05520011\n",
      "mathematics This 0.1737804\n",
      "mathematics was 0.15852255\n",
      "mathematics the 0.17734843\n",
      "mathematics task 0.30316022\n",
      "mathematics to 0.17333423\n",
      "mathematics get 0.15163872\n",
      "mathematics a 0.15215312\n",
      "mathematics system 0.2486563\n",
      "mathematics to 0.17333423\n",
      "mathematics discover 0.23965321\n",
      "mathematics proof 0.30108315\n",
      "mathematics for 0.15437222\n",
      "mathematics a 0.15215312\n",
      "mathematics theorem 0.51952326\n",
      "mathematics , 0.16458644\n",
      "mathematics not 0.21616322\n",
      "mathematics simply 0.19955498\n",
      "mathematics to 0.17333423\n",
      "mathematics test 0.27182534\n",
      "mathematics the 0.17734843\n",
      "mathematics proof 0.30108315\n",
      "mathematics . 0.1428826\n",
      "mathematics We 0.19118974\n",
      "mathematics picked 0.043306027\n",
      "mathematics logic 0.48552948\n",
      "mathematics just 0.15451393\n",
      "mathematics because 0.22475593\n",
      "mathematics I 0.1450038\n",
      "mathematics happened 0.1378248\n",
      "mathematics to 0.17333423\n",
      "mathematics have 0.19579935\n",
      "mathematics Principia 0.13330363\n",
      "mathematics Mathematica 0.305936\n",
      "mathematics sitting 0.0924929\n",
      "mathematics on 0.13802332\n",
      "mathematics my 0.15176548\n",
      "mathematics shelf 0.06740684\n",
      "mathematics and 0.1919858\n",
      "mathematics I 0.1450038\n",
      "mathematics was 0.15852255\n",
      "mathematics using 0.20541163\n",
      "mathematics it 0.18828109\n",
      "mathematics to 0.17333423\n",
      "mathematics see 0.13032426\n",
      "mathematics what 0.2574315\n",
      "mathematics was 0.15852255\n",
      "mathematics involved 0.2709118\n",
      "mathematics in 0.2502716\n",
      "mathematics finding 0.21857777\n",
      "mathematics a 0.15215312\n",
      "mathematics proof 0.30108315\n",
      "mathematics of 0.22132188\n",
      "mathematics anything 0.20285393\n",
      "mathematics . 0.1428826\n",
      "mathematics ” 0.07625117\n",
      "mathematics Alfred 0.15882698\n",
      "mathematics North 0.11588196\n",
      "mathematics Whitehead 0.07808965\n",
      "mathematics and 0.1919858\n",
      "mathematics Bertrand 0.1010883\n",
      "mathematics Russell 0.11125292\n",
      "mathematics book 0.27858353\n",
      "mathematics Principia 0.13330363\n",
      "mathematics Mathematica 0.305936\n",
      "mathematics contained 0.14558052\n",
      "mathematics theorems 0.48887467\n",
      "mathematics considered 0.30557752\n",
      "mathematics to 0.17333423\n",
      "mathematics form 0.24019946\n",
      "mathematics the 0.17734843\n",
      "mathematics foundation 0.35152078\n",
      "mathematics of 0.22132188\n",
      "mathematics mathematical 0.8114012\n",
      "mathematics logic 0.48552948\n",
      "mathematics . 0.1428826\n",
      "mathematics Simeon 0.025687518\n",
      "mathematics evolved 0.27239838\n",
      "mathematics Logic 0.48552948\n",
      "mathematics theorem 0.51952326\n",
      "mathematics into 0.21459211\n",
      "mathematics General 0.39205575\n",
      "mathematics problem 0.29629782\n",
      "mathematics solver 0.4081052\n",
      "mathematics . 0.1428826\n",
      "mathematics GPS 0.074559174\n",
      "mathematics is 0.19002503\n",
      "mathematics currently 0.19156493\n",
      "mathematics used 0.21317719\n",
      "mathematics in 0.2502716\n",
      "mathematics robotics 0.3964132\n",
      "mathematics and 0.1919858\n",
      "mathematics gives 0.20057704\n",
      "mathematics the 0.17734843\n",
      "mathematics robot 0.14451122\n",
      "mathematics amazing 0.10834687\n",
      "mathematics problem 0.29629782\n",
      "mathematics solving 0.563514\n",
      "mathematics capabilities 0.23074819\n",
      "mathematics . 0.1428826\n",
      "mathematics Many 0.23618928\n",
      "mathematics mathematicians 0.62572014\n",
      "mathematics considered 0.30557752\n",
      "mathematics some 0.21192916\n",
      "mathematics of 0.22132188\n",
      "mathematics LTs -0.019567072\n",
      "mathematics proofs 0.4006276\n",
      "mathematics superior 0.18672936\n",
      "mathematics to 0.17333423\n",
      "mathematics those 0.21926636\n",
      "mathematics previously 0.19474582\n",
      "mathematics published 0.22297843\n",
      "theorm Herbert 0.11818329\n",
      "theorm Simon 0.105014116\n",
      "theorm research -0.08976414\n",
      "theorm and -0.3065261\n",
      "theorm concepts -0.03064575\n",
      "theorm increased -0.18423605\n",
      "theorm computer -0.1907887\n",
      "theorm scientist -0.0117785\n",
      "theorm understanding -0.08612586\n",
      "theorm of -0.21583192\n",
      "theorm reasoning 0.070340864\n",
      "theorm and -0.3065261\n",
      "theorm increased -0.18423605\n",
      "theorm the -0.23179406\n",
      "theorm computer -0.1907887\n",
      "theorm 's -0.19442289\n",
      "theorm ability -0.1519747\n",
      "theorm too -0.25065568\n",
      "theorm solve 0.061929695\n",
      "theorm problems -0.13245776\n",
      "theorm and -0.3065261\n",
      "theorm proof 0.040265605\n",
      "theorm theorems 0.34822604\n",
      "theorm . -0.23462069\n",
      "theorm Herbert 0.11818329\n",
      "theorm Simon 0.105014116\n",
      "theorm , -0.26550594\n",
      "theorm Al -0.054183308\n",
      "theorm Newell 0.14506607\n",
      "theorm , -0.26550594\n",
      "theorm Clifford 0.08445835\n",
      "theorm Shaw 0.14377542\n",
      "theorm proposals -0.072612345\n",
      "theorm were -0.20905404\n",
      "theorm radical -0.033799\n",
      "theorm and -0.3065261\n",
      "theorm affect -0.09056112\n",
      "theorm computer -0.1907887\n",
      "theorm scientist -0.0117785\n",
      "theorm today -0.23734725\n",
      "theorm . -0.23462069\n",
      "theorm In -0.21266408\n",
      "theorm Simon 0.105014116\n",
      "theorm ’s -0.13941796\n",
      "theorm book -0.2318929\n",
      "theorm , -0.26550594\n",
      "theorm “ -0.17818731\n",
      "theorm Models -0.12404612\n",
      "theorm of -0.21583192\n",
      "theorm my -0.2563133\n",
      "theorm life -0.2359273\n",
      "theorm ” -0.116432816\n",
      "theorm , -0.26550594\n",
      "theorm Simon 0.105014116\n",
      "theorm demonstrated -0.02737019\n",
      "theorm the -0.23179406\n",
      "theorm Logical -0.010818023\n",
      "theorm Theorem 0.39093226\n",
      "theorm algorithm 0.06194913\n",
      "theorm could -0.19535103\n",
      "theorm prove 0.03444274\n",
      "theorm certain -0.19363946\n",
      "theorm mathematical 0.11343587\n",
      "theorm theorems 0.34822604\n",
      "theorm . -0.23462069\n",
      "theorm Simon 0.105014116\n",
      "theorm said -0.16545586\n",
      "theorm , -0.26550594\n",
      "theorm “ -0.17818731\n",
      "theorm This -0.19484134\n",
      "theorm was -0.16673209\n",
      "theorm the -0.23179406\n",
      "theorm task -0.11063884\n",
      "theorm to -0.22422731\n",
      "theorm get -0.22994126\n",
      "theorm a -0.22609198\n",
      "theorm system -0.12630354\n",
      "theorm to -0.22422731\n",
      "theorm discover -0.11771403\n",
      "theorm proof 0.040265605\n",
      "theorm for -0.22994824\n",
      "theorm a -0.22609198\n",
      "theorm theorem 0.39093226\n",
      "theorm , -0.26550594\n",
      "theorm not -0.21552938\n",
      "theorm simply -0.1766701\n",
      "theorm to -0.22422731\n",
      "theorm test -0.031178804\n",
      "theorm the -0.23179406\n",
      "theorm proof 0.040265605\n",
      "theorm . -0.23462069\n",
      "theorm We -0.19386227\n",
      "theorm picked -0.19323385\n",
      "theorm logic -0.04283431\n",
      "theorm just -0.22426306\n",
      "theorm because -0.21146873\n",
      "theorm I -0.12149044\n",
      "theorm happened -0.14037731\n",
      "theorm to -0.22422731\n",
      "theorm have -0.27586418\n",
      "theorm Principia 0.26924914\n",
      "theorm Mathematica 0.3122619\n",
      "theorm sitting -0.22177035\n",
      "theorm on -0.20817377\n",
      "theorm my -0.2563133\n",
      "theorm shelf -0.114048585\n",
      "theorm and -0.3065261\n",
      "theorm I -0.12149044\n",
      "theorm was -0.16673209\n",
      "theorm using -0.14238404\n",
      "theorm it -0.19903475\n",
      "theorm to -0.22422731\n",
      "theorm see -0.2605721\n",
      "theorm what -0.17389978\n",
      "theorm was -0.16673209\n",
      "theorm involved -0.098828144\n",
      "theorm in -0.21266408\n",
      "theorm finding -0.10688144\n",
      "theorm a -0.22609198\n",
      "theorm proof 0.040265605\n",
      "theorm of -0.21583192\n",
      "theorm anything -0.16286898\n",
      "theorm . -0.23462069\n",
      "theorm ” -0.116432816\n",
      "theorm Alfred 0.10705631\n",
      "theorm North -0.09030698\n",
      "theorm Whitehead 0.2551702\n",
      "theorm and -0.3065261\n",
      "theorm Bertrand 0.2621902\n",
      "theorm Russell 0.13710104\n",
      "theorm book -0.2318929\n",
      "theorm Principia 0.26924914\n",
      "theorm Mathematica 0.3122619\n",
      "theorm contained -0.063795075\n",
      "theorm theorems 0.34822604\n",
      "theorm considered -0.1697552\n",
      "theorm to -0.22422731\n",
      "theorm form -0.16259058\n",
      "theorm the -0.23179406\n",
      "theorm foundation 0.02598746\n",
      "theorm of -0.21583192\n",
      "theorm mathematical 0.11343587\n",
      "theorm logic -0.04283431\n",
      "theorm . -0.23462069\n",
      "theorm Simeon 0.26194298\n",
      "theorm evolved -0.090613455\n",
      "theorm Logic -0.04283431\n",
      "theorm theorem 0.39093226\n",
      "theorm into -0.23217069\n",
      "theorm General -0.18473692\n",
      "theorm problem -0.07620558\n",
      "theorm solver 0.15412886\n",
      "theorm . -0.23462069\n",
      "theorm GPS -0.0417106\n",
      "theorm is -0.16401023\n",
      "theorm currently -0.211758\n",
      "theorm used -0.18147735\n",
      "theorm in -0.21266408\n",
      "theorm robotics 0.018042462\n",
      "theorm and -0.3065261\n",
      "theorm gives -0.1251743\n",
      "theorm the -0.23179406\n",
      "theorm robot -0.054104585\n",
      "theorm amazing -0.18826015\n",
      "theorm problem -0.07620558\n",
      "theorm solving 0.090270184\n",
      "theorm capabilities -0.12877503\n",
      "theorm . -0.23462069\n",
      "theorm Many -0.26827484\n",
      "theorm mathematicians 0.09056851\n",
      "theorm considered -0.1697552\n",
      "theorm some -0.24449508\n",
      "theorm of -0.21583192\n",
      "theorm LTs 0.20036662\n",
      "theorm proofs 0.090499416\n",
      "theorm superior -0.16764137\n",
      "theorm to -0.22422731\n",
      "theorm those -0.2633886\n",
      "theorm previously -0.14824788\n",
      "theorm published -0.19921798\n"
     ]
    }
   ],
   "source": [
    "#takes a long time to run\n",
    "nlp=spacy.load('en_core_web_lg')\n",
    "doc2=nlp(paragraph)\n",
    "doc=nlp('I am happy')\n",
    "\n",
    "#for token in doc2:\n",
    "#    print(token.vector)\n",
    "    \n",
    "    \n",
    "#doc3=nlp(\"happy joyous sad\")  \n",
    "         \n",
    "#for token1 in doc3:\n",
    "#    for token2 in doc2:\n",
    "#        print(token1.text,token2.text,token1.similarity(token2))\n",
    "\n",
    "doc3=nlp(\"computing mathematics theorm\")  \n",
    "\n",
    "for token1 in doc3:\n",
    "    for token2 in doc2:\n",
    "        print(token1.text,token2.text,token1.similarity(token2))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string   \n",
    "    return (str1.join(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=np.arange(0,len(sentences))\n",
    "wordfreq = dict()\n",
    "bagList=[]\n",
    "for i in index:\n",
    "    sentence=sentences[i]\n",
    "    if(len(sentence)>0):\n",
    "        tokens=sum([word_tokenize(sentence)],[])\n",
    "        #print(sentences[i])\n",
    "        #print(tokens)\n",
    "        #print(index[i])\n",
    "        words_frequency = FreqDist(tokens)\n",
    "        #words_frequency.plot(30, cumulative = False)\n",
    "        wordfreq=dict()\n",
    "        for token in tokens:\n",
    "            if token not in wordfreq.keys():\n",
    "                wordfreq[token] = 1\n",
    "            else:\n",
    "                wordfreq[token] += 1\n",
    "            #print(wordfreq)\n",
    "        bagList.append({'words': wordfreq})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon=dict()\n",
    "\n",
    "#breakfor i in np.arange(0,len(bagList)):\n",
    "for dictionaryItem in bagList:\n",
    "    words=dictionaryItem['words']\n",
    "    for key,(word,count) in enumerate(words.items()):\n",
    "        if word in uncommon:\n",
    "            uncommon[word]+=1\n",
    "        else:\n",
    "            uncommon[word]=1\n",
    "\n",
    "uncommon = {key: value for key, value in uncommon.items() if (value<=3 )}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herbert 0 Entity:True Uncommon:True [('Herbert Simon', 'PERSON')]\n",
      "Simon 0 Entity:True Uncommon:False [('Herbert Simon', 'PERSON')]\n",
      "research 0 Entity:False Uncommon:True []\n",
      "and 0 Entity:False Uncommon:False []\n",
      "concepts 0 Entity:False Uncommon:True []\n",
      "increased 0 Entity:False Uncommon:True []\n",
      "computer 0 Entity:False Uncommon:True []\n",
      "scientist 0 Entity:False Uncommon:True []\n",
      "understanding 0 Entity:False Uncommon:True []\n",
      "of 0 Entity:False Uncommon:False []\n",
      "reasoning 0 Entity:False Uncommon:True []\n",
      "the 0 Entity:False Uncommon:False []\n",
      "'s 0 Entity:False Uncommon:True []\n",
      "ability 0 Entity:False Uncommon:True []\n",
      "too 0 Entity:False Uncommon:True []\n",
      "solve 0 Entity:False Uncommon:True []\n",
      "problems 0 Entity:False Uncommon:True []\n",
      "proof 0 Entity:False Uncommon:True []\n",
      "theorems 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "Herbert 0 Entity:True Uncommon:True [('Herbert Simon', 'PERSON')]\n",
      "Simon 0 Entity:True Uncommon:False [('Herbert Simon', 'PERSON')]\n",
      ", 0 Entity:False Uncommon:True []\n",
      "Al 0 Entity:True Uncommon:True [('Al Newell Clifford Shaw', 'PERSON')]\n",
      "Newell 0 Entity:True Uncommon:True [('Al Newell Clifford Shaw', 'PERSON')]\n",
      "Clifford 0 Entity:True Uncommon:True [('Al Newell Clifford Shaw', 'PERSON')]\n",
      "Shaw 0 Entity:True Uncommon:True [('Al Newell Clifford Shaw', 'PERSON')]\n",
      "proposals 0 Entity:False Uncommon:True []\n",
      "were 0 Entity:False Uncommon:True []\n",
      "radical 0 Entity:False Uncommon:True []\n",
      "and 0 Entity:False Uncommon:False []\n",
      "affect 0 Entity:False Uncommon:True []\n",
      "computer 0 Entity:False Uncommon:True []\n",
      "scientist 0 Entity:False Uncommon:True []\n",
      "today 0 Entity:True Uncommon:True [('today', 'DATE')]\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "In 0 Entity:False Uncommon:True []\n",
      "Simon 0 Entity:True Uncommon:False [('Simon', 'PERSON')]\n",
      "’ 0 Entity:False Uncommon:True []\n",
      "s 0 Entity:True Uncommon:True [('Models', 'PERSON')]\n",
      "book 0 Entity:False Uncommon:True []\n",
      ", 0 Entity:False Uncommon:True []\n",
      "“ 0 Entity:False Uncommon:True []\n",
      "Models 0 Entity:True Uncommon:True [('Models', 'PERSON')]\n",
      "of 0 Entity:False Uncommon:False []\n",
      "my 0 Entity:False Uncommon:True []\n",
      "life 0 Entity:False Uncommon:True []\n",
      "” 0 Entity:False Uncommon:True []\n",
      "demonstrated 0 Entity:False Uncommon:True []\n",
      "the 0 Entity:False Uncommon:False []\n",
      "Logical 0 Entity:False Uncommon:True []\n",
      "Theorem 0 Entity:False Uncommon:True []\n",
      "algorithm 0 Entity:False Uncommon:True []\n",
      "could 0 Entity:False Uncommon:True []\n",
      "prove 0 Entity:False Uncommon:True []\n",
      "certain 0 Entity:False Uncommon:True []\n",
      "mathematical 0 Entity:False Uncommon:True []\n",
      "theorems 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "Simon 0 Entity:True Uncommon:False [('Simon', 'ORG')]\n",
      "said 0 Entity:False Uncommon:True []\n",
      ", 0 Entity:False Uncommon:True []\n",
      "“ 0 Entity:False Uncommon:True []\n",
      "This 0 Entity:False Uncommon:True []\n",
      "was 0 Entity:False Uncommon:True []\n",
      "the 0 Entity:False Uncommon:False []\n",
      "task 0 Entity:False Uncommon:True []\n",
      "to 0 Entity:False Uncommon:False []\n",
      "get 0 Entity:False Uncommon:True []\n",
      "a 0 Entity:False Uncommon:True []\n",
      "system 0 Entity:False Uncommon:True []\n",
      "discover 0 Entity:False Uncommon:True []\n",
      "proof 0 Entity:False Uncommon:True []\n",
      "for 0 Entity:False Uncommon:True []\n",
      "theorem 0 Entity:False Uncommon:True []\n",
      "not 0 Entity:False Uncommon:True []\n",
      "simply 0 Entity:False Uncommon:True []\n",
      "test 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "We 0 Entity:False Uncommon:True []\n",
      "picked 0 Entity:False Uncommon:True []\n",
      "logic 0 Entity:False Uncommon:True []\n",
      "just 0 Entity:False Uncommon:True []\n",
      "because 0 Entity:False Uncommon:True []\n",
      "I 0 Entity:False Uncommon:True []\n",
      "happened 0 Entity:False Uncommon:True []\n",
      "to 0 Entity:False Uncommon:False []\n",
      "have 0 Entity:False Uncommon:True []\n",
      "Principia 0 Entity:True Uncommon:True [('Principia Mathematica', 'ORG')]\n",
      "Mathematica 0 Entity:True Uncommon:True [('Principia Mathematica', 'ORG')]\n",
      "sitting 0 Entity:False Uncommon:True []\n",
      "on 0 Entity:False Uncommon:True []\n",
      "my 0 Entity:False Uncommon:True []\n",
      "shelf 0 Entity:False Uncommon:True []\n",
      "and 0 Entity:False Uncommon:False []\n",
      "was 0 Entity:False Uncommon:True []\n",
      "using 0 Entity:False Uncommon:True []\n",
      "it 0 Entity:False Uncommon:True []\n",
      "see 0 Entity:False Uncommon:True []\n",
      "what 0 Entity:False Uncommon:True []\n",
      "involved 0 Entity:False Uncommon:True []\n",
      "in 0 Entity:True Uncommon:True [('Principia Mathematica', 'ORG')]\n",
      "finding 0 Entity:False Uncommon:True []\n",
      "a 0 Entity:True Uncommon:True [('Principia Mathematica', 'ORG')]\n",
      "proof 0 Entity:False Uncommon:True []\n",
      "of 0 Entity:False Uncommon:False []\n",
      "anything 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "” 0 Entity:False Uncommon:True []\n",
      "Alfred 0 Entity:False Uncommon:True []\n",
      "North 0 Entity:False Uncommon:True []\n",
      "Whitehead 0 Entity:False Uncommon:True []\n",
      "and 0 Entity:False Uncommon:False []\n",
      "Bertrand 0 Entity:False Uncommon:True []\n",
      "Russell 0 Entity:False Uncommon:True []\n",
      "book 0 Entity:False Uncommon:True []\n",
      "Principia 0 Entity:True Uncommon:True [('Principia Mathematica', 'ORG')]\n",
      "Mathematica 0 Entity:True Uncommon:True [('Principia Mathematica', 'ORG')]\n",
      "contained 0 Entity:False Uncommon:True []\n",
      "theorems 0 Entity:False Uncommon:True []\n",
      "considered 0 Entity:False Uncommon:True []\n",
      "to 0 Entity:False Uncommon:False []\n",
      "form 0 Entity:False Uncommon:True []\n",
      "the 0 Entity:True Uncommon:False [('Principia Mathematica', 'ORG')]\n",
      "foundation 0 Entity:False Uncommon:True []\n",
      "of 0 Entity:False Uncommon:False []\n",
      "mathematical 0 Entity:False Uncommon:True []\n",
      "logic 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "Simeon 0 Entity:False Uncommon:True []\n",
      "evolved 0 Entity:False Uncommon:True []\n",
      "Logic 0 Entity:False Uncommon:True []\n",
      "theorem 0 Entity:False Uncommon:True []\n",
      "into 0 Entity:False Uncommon:True []\n",
      "General 0 Entity:False Uncommon:True []\n",
      "problem 0 Entity:False Uncommon:True []\n",
      "solver 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "GPS 0 Entity:False Uncommon:True []\n",
      "is 0 Entity:False Uncommon:True []\n",
      "currently 0 Entity:False Uncommon:True []\n",
      "used 0 Entity:False Uncommon:True []\n",
      "in 0 Entity:False Uncommon:True []\n",
      "robotics 0 Entity:False Uncommon:True []\n",
      "and 0 Entity:False Uncommon:False []\n",
      "gives 0 Entity:False Uncommon:True []\n",
      "the 0 Entity:False Uncommon:False []\n",
      "robot 0 Entity:False Uncommon:True []\n",
      "amazing 0 Entity:False Uncommon:True []\n",
      "problem 0 Entity:False Uncommon:True []\n",
      "solving 0 Entity:False Uncommon:True []\n",
      "capabilities 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "Many 0 Entity:False Uncommon:True []\n",
      "mathematicians 0 Entity:False Uncommon:True []\n",
      "considered 0 Entity:False Uncommon:True []\n",
      "some 0 Entity:False Uncommon:True []\n",
      "of 0 Entity:False Uncommon:False []\n",
      "LTs 0 Entity:False Uncommon:True []\n",
      "proofs 0 Entity:False Uncommon:True []\n",
      "superior 0 Entity:False Uncommon:True []\n",
      "to 0 Entity:False Uncommon:False []\n",
      "those 0 Entity:False Uncommon:True []\n",
      "previously 0 Entity:False Uncommon:True []\n",
      "published 0 Entity:False Uncommon:True []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dictionaryItem in bagList:\n",
    "    words=[]\n",
    "    wordPair=dictionaryItem['words']\n",
    "    for key, value in enumerate(wordPair):\n",
    "        #if(value<3):\n",
    "        words.append(value)\n",
    "\n",
    "    paragraph=listToString(words)\n",
    "    #print(paragraph)\n",
    "    \n",
    "    nlp=spacy.load('en_core_web_sm')\n",
    "  \n",
    "    doc = nlp(paragraph)\n",
    "    output=[]\n",
    "    isEntity=False\n",
    "    isUncommon=False\n",
    "    for token in doc:\n",
    "        flag=0\n",
    "        entity_match=[(ent.text,ent.label_) for ent in doc.ents if token.text in ent.text]\n",
    "        isEntity=(len(entity_match)>0)\n",
    "        isUncommon=token.text in uncommon.keys()\n",
    "        if  isEntity==False and isUncommon==False and len(token.text)>=4 and token.pos_=='PROPN':\n",
    "            flag=1\n",
    "                \n",
    "        print(token.text,flag,\"Entity:\"+str(isEntity),\"Uncommon:\"+str(isUncommon),entity_match)\n",
    "    \n",
    "        if flag==1 :\n",
    "            output.append('and \\' \\' + [SentenceValue]+\\' \\' like \\'%'+token.text+' %\\'')\n",
    "    \n",
    "    print (listToString(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar sentences\n",
    "    sim_scores = sim_scores[1:10]\n",
    "    # Get the movie indices\n",
    "    sentence_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return df['Sentence'].iloc[sentence_indices]\n",
    "    #printreturn sentences[sentence_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Simon’s book , “Models of my life” , Simon demonstrated the Logical Theorem algorithm could prove certain mathematical theorems .\n",
      "(0, '” Alfred North Whitehead and Bertrand Russell book Principia Mathematica contained theorems considered to form the foundation of mathematical logic .')\n",
      "(1, 'Simon said , “This was the task to get a system to discover proof for a theorem , not simply to test the proof .')\n",
      "(2, \"Herbert Simon research and concepts increased computer scientist understanding of reasoning and increased the computer's ability too solve problems and proof theorems .\")\n",
      "(3, 'Herbert Simon , Al Newell , Clifford Shaw proposals were radical and affect computer scientist today .')\n",
      "(4, 'Simeon evolved Logic theorem into General problem solver .')\n",
      "(5, 'We picked logic just because I happened to have Principia Mathematica sitting on my shelf and I was using it to see what was involved in finding a proof of anything .')\n",
      "(6, 'GPS is currently used in robotics and gives the robot amazing problem solving capabilities .')\n",
      "(7, 'Many mathematicians considered some of LTs proofs superior to those previously published')\n"
     ]
    }
   ],
   "source": [
    "a_sentence=sentences[2] \n",
    "print(a_sentence)\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(sentences)\n",
    "# Convert matrix into a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray())\n",
    "# Map the column names to vocabulary \n",
    "tfidf.columns = tfidf.get_feature_names()\n",
    "\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "\n",
    "df=pd.DataFrame()\n",
    "#indices=np.arange(0,len(sentences))\n",
    "df['Sentence']=sentences\n",
    "indices = pd.Series(df.index, index=df['Sentence']).drop_duplicates()\n",
    "results=get_recommendations(a_sentence,cosine_sim,indices)\n",
    "for result in enumerate(results):\n",
    "       print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_score(new_series, train_series, tokenizer):\n",
    "    \"\"\"\n",
    "    return the tf idf score of each possible pairs of documents\n",
    "    Args:\n",
    "        new_series (pd.Series): new data (To compare against train data)\n",
    "        train_series (pd.Series): train data (To fit the tf-idf transformer)\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    train_tfidf = tokenizer.fit_transform(train_series)\n",
    "    new_tfidf = tokenizer.transform(new_series)\n",
    "    X = pd.DataFrame(cosine_similarity(new_tfidf, train_tfidf), columns=train_series.index)\n",
    "    X['ix_new'] = new_series.index\n",
    "    score = pd.melt(\n",
    "        X,\n",
    "        id_vars='ix_new',\n",
    "        var_name='ix_train',\n",
    "        value_name='score'\n",
    "    )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    We picked logic just because I happened to hav...\n",
      "dtype: object\n",
      "0 0.08121546496588868\n",
      "1 0.019451491211043708\n",
      "2 0.08459516431514912\n",
      "3 0.23125281077112542\n",
      "4 1.0\n",
      "5 0.1855837102074369\n",
      "6 0.04141486064131285\n",
      "7 0.05068750805304069\n",
      "8 0.07281629311343475\n",
      "[3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "match_index=4\n",
    "train_set=pd.Series(sentences)\n",
    "test_set=pd.Series(sentences[match_index])\n",
    "#print(train_set)\n",
    "print(test_set)\n",
    "tokenizer = TfidfVectorizer() # initiate here your own tokenizer (TfidfVectorizer, CountVectorizer, with stopwords...)\n",
    "score = create_tokenizer_score(train_series=train_set, new_series=test_set, tokenizer=tokenizer)\n",
    "#print(score)\n",
    "index=0\n",
    "matches=[]\n",
    "\n",
    "for index in np.arange(0,len(score)):\n",
    "\n",
    "    value=score.loc[index,'score']\n",
    "    print (index,value)\n",
    "    if value>0.10:\n",
    "        matches.append(index)\n",
    "        #matches.append(index)\n",
    "        \n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
