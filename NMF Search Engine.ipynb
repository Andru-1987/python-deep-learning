{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re \n",
    "import heapq\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@deangelaneves/how-to-build-a-search-engine-from-scratch-in-python-part-1-96eb240f9ecb\n",
    "def normalized_term_frequency(word, document):\n",
    "\n",
    "    raw_frequency = document.count(word)\n",
    "\n",
    "    if raw_frequency == 0:\n",
    "\n",
    "        return 0\n",
    "\n",
    "    return 1 + math.log(raw_frequency)\n",
    "\n",
    "def docs_contain_word(word, documents):\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for document in list_of_documents:\n",
    "\n",
    "        if word in document:\n",
    "\n",
    "            counter+=1\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "\n",
    "def get_vocabulary(documents):\n",
    "\n",
    "    vocabulary = set([word for document in documents for word in document])\t\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "def inverse_document_frequency(documents, vocabulary):\n",
    "\n",
    "    idf = {}\n",
    "\n",
    "    for word in vocabulary:\n",
    "\n",
    "        contains_word = docs_contain_word(word, documents)\n",
    "\n",
    "        idf[word] = 1 + math.log(len(documents)/(contains_word))\n",
    "    return idf\n",
    "\n",
    "def tf_idf(search_keys, dataframe, label):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    #fit_transform method converts the given text into TF-IDF scores for all the documents. \n",
    "\n",
    "    tfidf_weights_matrix = tfidf_vectorizer.fit_transform(dataframe.loc[:, label])\n",
    "\n",
    "    search_query_weights = tfidf_vectorizer.transform([search_keys])\n",
    "\n",
    "\n",
    "    return search_query_weights, tfidf_weights_matrix\n",
    "\n",
    "def cos_similarity(search_query_weights, tfidf_weights_matrix):\n",
    "\n",
    "\n",
    "    cosine_distance = cosine_similarity(query, tfidf_matrix)\n",
    "\n",
    "    similarity_list = cosine_distance[0]\n",
    "\n",
    "  \n",
    "    return similarity_list\n",
    "\n",
    "def most_similar(similarity_list, min_talks=1):\n",
    "\n",
    "    most_similar= []\n",
    "\n",
    "  \n",
    "\n",
    "    while min_talks > 0:\n",
    "\n",
    "        tmp_index = np.argmax(similarity_list)\n",
    "\n",
    "        most_similar.append(tmp_index)\n",
    "\n",
    "        similarity_list[tmp_index] = 0\n",
    "\n",
    "        min_talks -= 1\n",
    "\n",
    "    return most_similar\n",
    "\n",
    "def word_count(string) :\n",
    "    words= string.split()\n",
    "    return len(words)\n",
    "\n",
    "\n",
    "def avg_word_length(x):\n",
    "    words=x.split()\n",
    "    word_lengths= [len(word) for word in words]\n",
    "\n",
    "    avg_word_length= sum(word_lengths)/len(words)\n",
    "    return(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Herbert', 'Simon', 'research', 'and', 'concepts', 'increased', 'computer', 'scientist', 'understanding', 'of', 'reasoning', 'and', 'increased', 'the', 'computer', \"'s\", 'ability', 'too', 'solve', 'problems', 'and', 'proof', 'theorems', 'Herbert', 'Simon', ',', 'Al', 'Newell', ',', 'Clifford', 'Shaw', 'proposals', 'were', 'radical', 'and', 'affect', 'computer', 'scientist', 'today', 'In', 'Simon', '’', 's', 'book', ',', '“', 'Models', 'of', 'my', 'life', '”', ',', 'Simon', 'demonstrated', 'the', 'Logical', 'Theorem', 'algorithm', 'could', 'prove', 'certain', 'mathematical', 'theorems', 'Simon', 'said', ',', '“', 'This', 'was', 'the', 'task', 'to', 'get', 'a', 'system', 'to', 'discover', 'proof', 'for', 'a', 'theorem', ',', 'not', 'simply', 'to', 'test', 'the', 'proof', 'We', 'picked', 'logic', 'just', 'because', 'I', 'happened', 'to', 'have', 'Principia', 'Mathematica', 'sitting', 'on', 'my', 'shelf', 'and', 'I', 'was', 'using', 'it', 'to', 'see', 'what', 'was', 'involved', 'in', 'finding', 'a', 'proof', 'of', 'anything', '”', 'Alfred', 'North', 'Whitehead', 'and', 'Bertrand', 'Russell', 'book', 'Principia', 'Mathematica', 'contained', 'theorems', 'considered', 'to', 'form', 'the', 'foundation', 'of', 'mathematical', 'logic', 'Simeon', 'evolved', 'Logic', 'theorem', 'into', 'General', 'problem', 'solver', 'GPS', 'is', 'currently', 'used', 'in', 'robotics', 'and', 'gives', 'the', 'robot', 'amazing', 'problem', 'solving', 'capabilities', 'Many', 'mathematicians', 'considered', 'some', 'of', 'LTs', 'proofs', 'superior', 'to', 'those', 'previously']\n"
     ]
    }
   ],
   "source": [
    "paragraph=\"Herbert Simon research and concepts increased computer scientist understanding of reasoning and increased the computer's ability too solve problems and proof theorems . Herbert Simon , Al Newell , Clifford Shaw proposals were radical and affect computer scientist today . In Simon’s book , “Models of my life” , Simon demonstrated the Logical Theorem algorithm could prove certain mathematical theorems . Simon said , “This was the task to get a system to discover proof for a theorem , not simply to test the proof . We picked logic just because I happened to have Principia Mathematica sitting on my shelf and I was using it to see what was involved in finding a proof of anything . ” Alfred North Whitehead and Bertrand Russell book Principia Mathematica contained theorems considered to form the foundation of mathematical logic . Simeon evolved Logic theorem into General problem solver . GPS is currently used in robotics and gives the robot amazing problem solving capabilities . Many mathematicians considered some of LTs proofs superior to those previously published\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "words=[]\n",
    "for sentence in sentences:\n",
    "    word_list=nltk.word_tokenize(sentence)\n",
    "    \n",
    "    #print(word_list)\n",
    "    for i in range(0, len(word_list)-1):\n",
    "        words.append(word_list[i])\n",
    "    \n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_weights(vocab, original_vocab, vector, vector_index):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False).index\n",
    "\n",
    "    return [original_vocab[i] for i in zipped_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS=[]\n",
    "LABELS=[]\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    \n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data =data_frame.drop(to_drop,axis=1)\n",
    "    \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)\n",
    "\n",
    "\n",
    "get_text_data=FunctionTransformer(combine_text_columns,validate=False)\n",
    "\n",
    "#(\"tfidf1\", TfidfTransformer(use_idf=True,smooth_idf=True)),\n",
    "#('clf', MultinomialNB(alpha=1)) #Laplace smoothing\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english',lowercase=True)),\n",
    "    (\"tfidf1\", TfidfTransformer()),\n",
    "    ##('vectorizer',TfidfVectorizer(stop_words='english')),\n",
    "    ##('chi', SelectKBest()),\n",
    "    \n",
    "     ('scale', MaxAbsScaler()),\n",
    "    ('clf', LogisticRegression(C=1e5)),\n",
    "    #('clf', MultinomialNB()) \n",
    "    #('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index                                               Text  \\\n",
      "0     0  Herbert Simon research and concepts increased ...   \n",
      "1     1  Herbert Simon , Al Newell , Clifford Shaw prop...   \n",
      "2     2  In Simon’s book , “Models of my life” , Simon ...   \n",
      "3     3  Simon said , “This was the task to get a syste...   \n",
      "4     4  We picked logic just because I happened to hav...   \n",
      "5     5  ” Alfred North Whitehead and Bertrand Russell ...   \n",
      "6     6  Simeon evolved Logic theorem into General prob...   \n",
      "7     7  GPS is currently used in robotics and gives th...   \n",
      "8     8  Many mathematicians considered some of LTs pro...   \n",
      "\n",
      "                                               Tfidf         Target  \n",
      "0  [[0.24412135013075287, 0.0, 0.0, 0.0, 0.0, 0.0...  understanding  \n",
      "1  [[0.0, 0.30771210180770386, 0.3077121018077038...          today  \n",
      "2  [[0.0, 0.0, 0.0, 0.0, 0.29894865038661705, 0.0...       theorems  \n",
      "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        theorem  \n",
      "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...          using  \n",
      "5  [[0.0, 0.0, 0.0, 0.28121106318651534, 0.0, 0.0...      whitehead  \n",
      "6  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        theorem  \n",
      "7  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.3208594972084965,...           used  \n",
      "8  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...       superior  \n",
      "7    5\n",
      "1    3\n",
      "5    7\n",
      "0    4\n",
      "8    0\n",
      "2    2\n",
      "Name: Target, dtype: int32 [1 1 6 1 1 1]\n",
      "score of Naive Bayes algo is : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnishimoto.BOISE\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1270: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#print(paragraph)\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english')\n",
    "text_tfidf = tfidf_vec.fit_transform(sentences)\n",
    "\n",
    "shape=text_tfidf.get_shape()\n",
    "vocab= {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
    "\n",
    "df=pd.DataFrame(columns=['Index','Text','Tfidf','Target'])\n",
    "for index in np.arange(shape[0]):\n",
    "    weights=return_weights(vocab,tfidf_vec.vocabulary_,text_tfidf,index)\n",
    "    target=vocab.get(np.max(weights))\n",
    "    index=len(df)\n",
    "    #df.loc[index]=[text_tfidf[index].toarray(),target]\n",
    "    df.loc[index]=[index,sentences[index],text_tfidf[index].toarray(),target]\n",
    "\n",
    "df.set_index('Index')\n",
    "print(df.head(10))    \n",
    "\n",
    "#X=df[['Index','Text']].values\n",
    "#y=df['Target'].values.astype(str)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['Target']=encoder.fit_transform(df['Target'])\n",
    "\n",
    "train,test=train_test_split(df,test_size=.6,random_state=42, shuffle=True)\n",
    "pipeline.fit(train['Text'],train['Target'])\n",
    "\n",
    "predictions=pipeline.predict(test['Text'])\n",
    "print(test['Target'],predictions)\n",
    "\n",
    "score = f1_score(test['Target'],predictions,pos_label='positive',average='micro')\n",
    "print(\"score of Naive Bayes algo is :\" , score)\n",
    "\n",
    "#k_fold = KFold(n_splits=6)    \n",
    "#scores=[]\n",
    "#for train_indices,test_indices in k_fold.split(df):\n",
    "#    train_text=df.iloc[train_indices]['Text'].values\n",
    "#    train_y=df.iloc[train_indices]['Target'].values.astype(str)\n",
    "    \n",
    "#    test_text=df.iloc[test_indices]['Text'].values\n",
    "#    test_y=df.iloc[test_indices]['Target'].values.astype(str)\n",
    "#    pipeline.fit(train_text,train_y)\n",
    "#    predictions=pipeline.predict(test_text)\n",
    "    #print(predictions)\n",
    "    #score = pipeline['clf'].score(test_text, test_y)\n",
    "#    print(test_y,predictions)\n",
    "#    score = f1_score(test_y,predictions,pos_label='positive',average='micro')\n",
    "#    print(\"score of Naive Bayes algo is :\" , score)\n",
    "#    scores.append(score)\n",
    "\n",
    "#X=df['X']\n",
    "#y=pd.get_dummies(df['Target'])\n",
    "#print(X,y)    \n",
    "\n",
    "#nb=MultinomialNB()\n",
    "#X_train,y_train,X_test,y_test=train_test_split(X,y,stratify=y)\n",
    "#pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "                                            sentence  word_count  \\\n",
      "0  Herbert Simon research and concepts increased ...          23   \n",
      "1  Herbert Simon , Al Newell , Clifford Shaw prop...          17   \n",
      "2  In Simon’s book , “Models of my life” , Simon ...          21   \n",
      "3  Simon said , “This was the task to get a syste...          25   \n",
      "4  We picked logic just because I happened to hav...          32   \n",
      "\n",
      "   avg_word_length  \n",
      "0         6.347826  \n",
      "1         5.058824  \n",
      "2         5.333333  \n",
      "3         3.520000  \n",
      "4         4.218750  \n",
      "(9, 3)\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences))\n",
    "index=np.arange(0, len(sentences))\n",
    "df=pd.DataFrame({'sentence':sentences})\n",
    "df.set_index(index)\n",
    "\n",
    "df['word_count']=df['sentence'].apply(word_count)\n",
    "df['avg_word_length']=df['sentence'].apply(avg_word_length)\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "#a,b=tf_idf (pd.Series(['Herbert']),df,'Herbert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(99, 71)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords=spacy.lang.en.stop_words.STOP_WORDS\n",
    "doc=nlp(paragraph)\n",
    "tokens=[token.text for token in doc]\n",
    "#print(tokens)\n",
    "lemmas =[token.lemma_ for token in doc]\n",
    "lemmas =[lemma for lemma in lemmas\n",
    "        if (lemma.isalpha() )\n",
    "         and lemma not in stopwords\n",
    "        ]\n",
    "#print(lemmas)\n",
    "\n",
    "paragraph2=' '.join(lemmas)\n",
    "#print(paragraph2)\n",
    "\n",
    "vectorizer= CountVectorizer()\n",
    "\n",
    "bow_lem= vectorizer.fit_transform(lemmas)\n",
    "\n",
    "#sparse array\n",
    "print(bow_lem.toarray())\n",
    "\n",
    "print(bow_lem.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECTED SENTENCE: Herbert Simon research and concepts increased computer scientist understanding of reasoning and increased the computer's ability too solve problems and proof theorems .\n",
      "SIMILIAR SENTENCES: \n",
      "(0, 'Herbert Simon , Al Newell , Clifford Shaw proposals were radical and affect computer scientist today .')\n",
      "(1, 'Simon said , “This was the task to get a system to discover proof for a theorem , not simply to test the proof .')\n",
      "(2, 'In Simon’s book , “Models of my life” , Simon demonstrated the Logical Theorem algorithm could prove certain mathematical theorems .')\n",
      "[(9528407286733565721, 1, 3)]\n"
     ]
    }
   ],
   "source": [
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar sentences\n",
    "    sim_scores = sim_scores[1:4]\n",
    "    # Get the movie indices\n",
    "    sentence_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return df['sentence'].iloc[sentence_indices]\n",
    "\n",
    "index=np.arange(0, len(sentences))\n",
    "df=pd.DataFrame({'sentence':sentences})\n",
    "df.set_index(index)\n",
    "\n",
    "#print(df)\n",
    "df['word_count']=df['sentence'].apply(word_count)\n",
    "df['avg_word_length']=df['sentence'].apply(avg_word_length)\n",
    "#print(df.head())\n",
    "#print(df.shape)\n",
    "\n",
    "a_sentence=sentences[0]\n",
    "print(\"SELECTED SENTENCE: \" + a_sentence)\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['sentence'])\n",
    "\n",
    "# Convert matrix into a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray())\n",
    "# Map the column names to vocabulary \n",
    "tfidf.columns = tfidf.get_feature_names()\n",
    "\n",
    "#print(\"FEATURES: \"+ str(tfidf.get_feature_names()[0:]))\n",
    "\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "indices = pd.Series(df.index, index=df['sentence']).drop_duplicates()\n",
    "\n",
    "results=get_recommendations(a_sentence,cosine_sim,indices)\n",
    "\n",
    "print(\"SIMILIAR SENTENCES: \")\n",
    "\n",
    "for result in enumerate(results):\n",
    "       print (result)\n",
    "\n",
    "doc=nlp(paragraph)\n",
    "\n",
    "matcher=Matcher(nlp.vocab)\n",
    "\n",
    "pattern=[\n",
    "    {'LOWER':'simon'}\n",
    "    ,{'LOWER':'research'}\n",
    "    ]\n",
    "\n",
    "matcher.add('IPHONE_PATTERN',None,pattern)\n",
    "\n",
    "matches=matcher(doc)\n",
    "\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Person found:  Herbert\n",
      " Person found:  Herbert\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(paragraph)\n",
    "for token in doc:\n",
    "    # Check if the next token's text equals '%'\n",
    "    if token.text == 'Herbert':\n",
    "        print(' Person found: ', token.text)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herbert PROPN compound Simon\n",
      "Simon PROPN compound research\n",
      "research NOUN nsubj increased\n",
      "and CCONJ cc research\n",
      "concepts NOUN conj research\n",
      "computer NOUN compound scientist\n",
      "scientist NOUN compound understanding\n",
      "understanding NOUN dobj increased\n",
      "of ADP prep understanding\n",
      "reasoning NOUN pobj of\n",
      "and CCONJ cc increased\n",
      "increased VERB conj increased\n",
      "the DET det computer\n",
      "computer NOUN poss ability\n",
      "'s PART case computer\n",
      "ability NOUN dobj increased\n",
      "too ADV advmod solve\n",
      "solve VERB amod problems\n",
      "problems NOUN dobj increased\n",
      "and CCONJ cc problems\n",
      "proof NOUN amod theorems\n",
      "theorems NOUN conj problems\n",
      ". PUNCT punct increased\n",
      "Herbert PROPN compound Simon\n",
      "Simon PROPN nsubj were\n",
      ", PUNCT punct Simon\n",
      "Al PROPN compound Newell\n",
      "Newell PROPN appos Simon\n",
      ", PUNCT punct were\n",
      "Clifford PROPN compound Shaw\n",
      "Shaw PROPN compound proposals\n",
      "proposals NOUN nsubj were\n",
      "radical ADJ acomp were\n",
      "and CCONJ cc were\n",
      "affect VERB conj were\n",
      "computer NOUN compound scientist\n",
      "scientist NOUN dobj affect\n",
      "today NOUN npadvmod affect\n",
      ". PUNCT punct were\n",
      "In ADP prep demonstrated\n",
      "Simon PROPN pobj In\n",
      "’s PART punct book\n",
      "book NOUN pobj In\n",
      ", PUNCT punct book\n",
      "“ PUNCT punct book\n",
      "Models NOUN appos book\n",
      "of ADP prep Models\n",
      "my DET poss life\n",
      "life NOUN pobj of\n",
      "” PUNCT punct Models\n",
      ", PUNCT punct demonstrated\n",
      "Simon PROPN nsubj demonstrated\n",
      "the DET det algorithm\n",
      "Logical PROPN compound Theorem\n",
      "Theorem PROPN compound algorithm\n",
      "algorithm NOUN nsubj prove\n",
      "could VERB aux prove\n",
      "prove VERB ccomp demonstrated\n",
      "certain ADJ amod theorems\n",
      "mathematical ADJ amod theorems\n",
      "theorems NOUN dobj prove\n",
      ". PUNCT punct demonstrated\n",
      "Simon PROPN nsubj said\n",
      ", PUNCT punct said\n",
      "“ PUNCT punct said\n",
      "This DET nsubj was\n",
      "was AUX ccomp said\n",
      "the DET det task\n",
      "task NOUN attr was\n",
      "to PART aux get\n",
      "get AUX advcl was\n",
      "a DET det system\n",
      "system NOUN nsubj discover\n",
      "to PART aux discover\n",
      "discover VERB ccomp get\n",
      "proof NOUN dobj discover\n",
      "for ADP prep proof\n",
      "a DET det theorem\n",
      "theorem NOUN pobj for\n",
      ", PUNCT punct discover\n",
      "not PART neg simply\n",
      "simply ADV advmod test\n",
      "to PART aux test\n",
      "test VERB advcl discover\n",
      "the DET det proof\n",
      "proof NOUN dobj test\n",
      ". PUNCT punct said\n",
      "We PRON nsubj picked\n",
      "logic NOUN dobj picked\n",
      "just ADV advmod happened\n",
      "because SCONJ mark happened\n",
      "I PRON nsubj happened\n",
      "happened VERB advcl picked\n",
      "to PART aux have\n",
      "have AUX xcomp happened\n",
      "Principia PROPN compound Mathematica\n",
      "Mathematica PROPN nsubj sitting\n",
      "sitting VERB ccomp have\n",
      "on ADP prep sitting\n",
      "my DET poss shelf\n",
      "shelf NOUN pobj on\n",
      "and CCONJ cc using\n",
      "I PRON nsubj using\n",
      "was AUX aux using\n",
      "it PRON dobj using\n",
      "to PART aux see\n",
      "see VERB xcomp using\n",
      "what PRON nsubjpass involved\n",
      "was AUX auxpass involved\n",
      "involved VERB ccomp see\n",
      "in ADP prep involved\n",
      "finding VERB pcomp in\n",
      "a DET det proof\n",
      "proof NOUN dobj finding\n",
      "of ADP prep proof\n",
      "anything PRON pobj of\n",
      ". PUNCT punct using\n",
      "” PUNCT punct using\n",
      "Alfred PROPN compound Whitehead\n",
      "North PROPN compound Whitehead\n",
      "Whitehead PROPN nsubj contained\n",
      "and CCONJ cc Whitehead\n",
      "Bertrand PROPN compound Russell\n",
      "Russell PROPN compound book\n",
      "book NOUN compound Mathematica\n",
      "Principia PROPN compound Mathematica\n",
      "Mathematica PROPN conj Whitehead\n",
      "theorems NOUN dobj contained\n",
      "considered VERB acl theorems\n",
      "to PART aux form\n",
      "form VERB xcomp considered\n",
      "the DET det foundation\n",
      "foundation NOUN dobj form\n",
      "of ADP prep foundation\n",
      "mathematical ADJ amod logic\n",
      "logic NOUN pobj of\n",
      ". PUNCT punct contained\n",
      "Simeon PROPN nsubj evolved\n",
      "Logic PROPN compound theorem\n",
      "theorem NOUN dobj evolved\n",
      "into ADP prep evolved\n",
      "General ADJ amod solver\n",
      "problem NOUN compound solver\n",
      "solver NOUN pobj into\n",
      ". PUNCT punct evolved\n",
      "GPS PROPN nsubjpass used\n",
      "is AUX auxpass used\n",
      "currently ADV advmod used\n",
      "in ADP prep used\n",
      "robotics NOUN pobj in\n",
      "and CCONJ cc used\n",
      "gives VERB conj used\n",
      "the DET det robot\n",
      "robot NOUN dative gives\n",
      "amazing ADJ amod problem\n",
      "problem NOUN dobj gives\n",
      "solving VERB acl problem\n",
      "capabilities NOUN dobj solving\n",
      ". PUNCT punct used\n",
      "Many ADJ amod mathematicians\n",
      "mathematicians NOUN nsubj considered\n",
      "some DET oprd considered\n",
      "of ADP prep some\n",
      "LTs NOUN compound proofs\n",
      "proofs PROPN compound superior\n",
      "superior ADJ pobj of\n",
      "to ADP prep some\n",
      "those DET pobj to\n",
      "previously ADV advmod published\n",
      "published VERB acl those\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(paragraph)\n",
    "for token in doc:\n",
    "        #print(token.text, token.pos_)\n",
    "        if token.dep_ != 'ROOT' and token.dep_ != 'ART' and token.dep_ != 'VERB ROOT':\n",
    "            print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Herbert', 'PROPN'), ('Simon', 'PROPN'), ('research', 'NOUN'), ('and', 'CCONJ'), ('concepts', 'NOUN'), ('increased', 'VERB'), ('computer', 'NOUN'), ('scientist', 'NOUN'), ('understanding', 'NOUN'), ('of', 'ADP'), ('reasoning', 'NOUN'), ('and', 'CCONJ'), ('increased', 'VERB'), ('the', 'DET'), ('computer', 'NOUN'), (\"'s\", 'PART'), ('ability', 'NOUN'), ('too', 'ADV'), ('solve', 'VERB'), ('problems', 'NOUN'), ('and', 'CCONJ'), ('proof', 'NOUN'), ('theorems', 'NOUN'), ('.', 'PUNCT'), ('Herbert', 'PROPN'), ('Simon', 'PROPN'), (',', 'PUNCT'), ('Al', 'PROPN'), ('Newell', 'PROPN'), (',', 'PUNCT'), ('Clifford', 'PROPN'), ('Shaw', 'PROPN'), ('proposals', 'NOUN'), ('were', 'AUX'), ('radical', 'ADJ'), ('and', 'CCONJ'), ('affect', 'VERB'), ('computer', 'NOUN'), ('scientist', 'NOUN'), ('today', 'NOUN'), ('.', 'PUNCT'), ('In', 'ADP'), ('Simon', 'PROPN'), ('’s', 'PART'), ('book', 'NOUN'), (',', 'PUNCT'), ('“', 'PUNCT'), ('Models', 'NOUN'), ('of', 'ADP'), ('my', 'DET'), ('life', 'NOUN'), ('”', 'PUNCT'), (',', 'PUNCT'), ('Simon', 'PROPN'), ('demonstrated', 'VERB'), ('the', 'DET'), ('Logical', 'PROPN'), ('Theorem', 'PROPN'), ('algorithm', 'NOUN'), ('could', 'VERB'), ('prove', 'VERB'), ('certain', 'ADJ'), ('mathematical', 'ADJ'), ('theorems', 'NOUN'), ('.', 'PUNCT'), ('Simon', 'PROPN'), ('said', 'VERB'), (',', 'PUNCT'), ('“', 'PUNCT'), ('This', 'DET'), ('was', 'AUX'), ('the', 'DET'), ('task', 'NOUN'), ('to', 'PART'), ('get', 'AUX'), ('a', 'DET'), ('system', 'NOUN'), ('to', 'PART'), ('discover', 'VERB'), ('proof', 'NOUN'), ('for', 'ADP'), ('a', 'DET'), ('theorem', 'NOUN'), (',', 'PUNCT'), ('not', 'PART'), ('simply', 'ADV'), ('to', 'PART'), ('test', 'VERB'), ('the', 'DET'), ('proof', 'NOUN'), ('.', 'PUNCT'), ('We', 'PRON'), ('picked', 'VERB'), ('logic', 'NOUN'), ('just', 'ADV'), ('because', 'SCONJ'), ('I', 'PRON'), ('happened', 'VERB'), ('to', 'PART'), ('have', 'AUX'), ('Principia', 'PROPN'), ('Mathematica', 'PROPN'), ('sitting', 'VERB'), ('on', 'ADP'), ('my', 'DET'), ('shelf', 'NOUN'), ('and', 'CCONJ'), ('I', 'PRON'), ('was', 'AUX'), ('using', 'VERB'), ('it', 'PRON'), ('to', 'PART'), ('see', 'VERB'), ('what', 'PRON'), ('was', 'AUX'), ('involved', 'VERB'), ('in', 'ADP'), ('finding', 'VERB'), ('a', 'DET'), ('proof', 'NOUN'), ('of', 'ADP'), ('anything', 'PRON'), ('.', 'PUNCT'), ('”', 'PUNCT'), ('Alfred', 'PROPN'), ('North', 'PROPN'), ('Whitehead', 'PROPN'), ('and', 'CCONJ'), ('Bertrand', 'PROPN'), ('Russell', 'PROPN'), ('book', 'NOUN'), ('Principia', 'PROPN'), ('Mathematica', 'PROPN'), ('contained', 'VERB'), ('theorems', 'NOUN'), ('considered', 'VERB'), ('to', 'PART'), ('form', 'VERB'), ('the', 'DET'), ('foundation', 'NOUN'), ('of', 'ADP'), ('mathematical', 'ADJ'), ('logic', 'NOUN'), ('.', 'PUNCT'), ('Simeon', 'PROPN'), ('evolved', 'VERB'), ('Logic', 'PROPN'), ('theorem', 'NOUN'), ('into', 'ADP'), ('General', 'ADJ'), ('problem', 'NOUN'), ('solver', 'NOUN'), ('.', 'PUNCT'), ('GPS', 'PROPN'), ('is', 'AUX'), ('currently', 'ADV'), ('used', 'VERB'), ('in', 'ADP'), ('robotics', 'NOUN'), ('and', 'CCONJ'), ('gives', 'VERB'), ('the', 'DET'), ('robot', 'NOUN'), ('amazing', 'ADJ'), ('problem', 'NOUN'), ('solving', 'VERB'), ('capabilities', 'NOUN'), ('.', 'PUNCT'), ('Many', 'ADJ'), ('mathematicians', 'NOUN'), ('considered', 'VERB'), ('some', 'DET'), ('of', 'ADP'), ('LTs', 'NOUN'), ('proofs', 'PROPN'), ('superior', 'ADJ'), ('to', 'ADP'), ('those', 'DET'), ('previously', 'ADV'), ('published', 'VERB')]\n",
      "['Herbert', 'Simon', 'research', 'and', 'concepts', 'increased', 'computer', 'scientist', 'understanding', 'of', 'reasoning', 'and', 'increased', 'the', 'computer', \"'s\", 'ability', 'too', 'solve', 'problems', 'and', 'proof', 'theorems', '.', 'Herbert', 'Simon', ',', 'Al', 'Newell', ',', 'Clifford', 'Shaw', 'proposals', 'were', 'radical', 'and', 'affect', 'computer', 'scientist', 'today', '.', 'In', 'Simon', '’s', 'book', ',', '“', 'Models', 'of', 'my', 'life', '”', ',', 'Simon', 'demonstrated', 'the', 'Logical', 'Theorem', 'algorithm', 'could', 'prove', 'certain', 'mathematical', 'theorems', '.', 'Simon', 'said', ',', '“', 'This', 'was', 'the', 'task', 'to', 'get', 'a', 'system', 'to', 'discover', 'proof', 'for', 'a', 'theorem', ',', 'not', 'simply', 'to', 'test', 'the', 'proof', '.', 'We', 'picked', 'logic', 'just', 'because', 'I', 'happened', 'to', 'have', 'Principia', 'Mathematica', 'sitting', 'on', 'my', 'shelf', 'and', 'I', 'was', 'using', 'it', 'to', 'see', 'what', 'was', 'involved', 'in', 'finding', 'a', 'proof', 'of', 'anything', '.', '”', 'Alfred', 'North', 'Whitehead', 'and', 'Bertrand', 'Russell', 'book', 'Principia', 'Mathematica', 'contained', 'theorems', 'considered', 'to', 'form', 'the', 'foundation', 'of', 'mathematical', 'logic', '.', 'Simeon', 'evolved', 'Logic', 'theorem', 'into', 'General', 'problem', 'solver', '.', 'GPS', 'is', 'currently', 'used', 'in', 'robotics', 'and', 'gives', 'the', 'robot', 'amazing', 'problem', 'solving', 'capabilities', '.', 'Many', 'mathematicians', 'considered', 'some', 'of', 'LTs', 'proofs', 'superior', 'to', 'those', 'previously', 'published']\n"
     ]
    }
   ],
   "source": [
    "#part of speech\n",
    "doc=nlp(paragraph)\n",
    "pos=[(token.text, token.pos_) for token in doc]\n",
    "print(pos)\n",
    "\n",
    "#proper_noun=[word for word in pos if pos=='PROPN']\n",
    "#print (proper_noun)\n",
    "\n",
    "def find_nouns(text):\n",
    "    # Create Doc object\n",
    "    doc2 = nlp(text)\n",
    "  \n",
    "    # Identify the persons\n",
    "    nouns = [token.text for token in doc if token.pos_ == 'PROPN' or 'NOUN']\n",
    "  \n",
    "    # Return persons\n",
    "    return nouns\n",
    "\n",
    "nouns=find_nouns(paragraph)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Herbert Simon', 'PERSON'), ('Herbert Simon', 'PERSON'), ('Al Newell', 'PERSON'), ('Clifford Shaw', 'PERSON'), ('today', 'DATE'), ('Simon', 'PERSON'), ('Simon', 'PERSON'), ('the Logical Theorem', 'ORG'), ('Simon', 'PERSON'), ('Principia Mathematica', 'PERSON'), ('Alfred North Whitehead', 'PERSON'), ('Bertrand Russell', 'PERSON'), ('Principia Mathematica', 'PERSON')]\n",
      "['Herbert Simon', 'Herbert Simon', 'Al Newell', 'Clifford Shaw', 'Simon', 'Simon', 'Simon', 'Principia Mathematica', 'Alfred North Whitehead', 'Bertrand Russell', 'Principia Mathematica']\n"
     ]
    }
   ],
   "source": [
    "named_entity=[(entity.text,entity.label_) for entity in doc.ents]\n",
    "print(named_entity)\n",
    "\n",
    "def find_persons(text):\n",
    "    # Create Doc object\n",
    "    doc2 = nlp(text)\n",
    "  \n",
    "    # Identify the persons\n",
    "    persons = [ent.text for ent in doc2.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "    # Return persons\n",
    "    return persons\n",
    "\n",
    "persons=find_persons(paragraph)\n",
    "print(persons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Herbert Simon research and concepts increased ...\n",
      "1    Herbert Simon , Al Newell , Clifford Shaw prop...\n",
      "2    In Simon’s book , “Models of my life” , Simon ...\n",
      "3    Simon said , “This was the task to get a syste...\n",
      "4    We picked logic just because I happened to hav...\n",
      "5    ” Alfred North Whitehead and Bertrand Russell ...\n",
      "6    Simeon evolved Logic theorem into General prob...\n",
      "7    GPS is currently used in robotics and gives th...\n",
      "8    Many mathematicians considered some of LTs pro...\n",
      "dtype: object\n",
      "[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 2 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0\n",
      "  1 0 0 0 0 0 0 1 0 1 0 0]\n",
      " [1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      "  0 0 0 0 1 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1\n",
      "  1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sentences_series = pd.Series(sentences)\n",
    "\n",
    "print(sentences_series)\n",
    "\n",
    "#vectorizer= CountVectorizer()\n",
    "vectorizer=CountVectorizer(strip_accents='ascii', stop_words='english', lowercase=False)\n",
    "\n",
    "bow_matrix= vectorizer.fit_transform(sentences_series)\n",
    "\n",
    "print(bow_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'affect', 'al', 'alfred', 'algorithm', 'amazing', 'and', 'anything', 'because', 'bertrand', 'book', 'capabilities', 'certain', 'clifford', 'computer', 'concepts', 'considered', 'contained', 'could', 'currently', 'demonstrated', 'discover', 'evolved', 'finding', 'for', 'form', 'foundation', 'general', 'get', 'gives', 'gps', 'happened', 'have', 'herbert', 'in', 'increased', 'into', 'involved', 'is', 'it', 'just', 'life', 'logic', 'logical', 'lts', 'many', 'mathematica', 'mathematical', 'mathematicians', 'models', 'my', 'newell', 'north', 'not', 'of', 'on', 'picked', 'previously', 'principia', 'problem', 'problems', 'proof', 'proofs', 'proposals', 'prove', 'published', 'radical', 'reasoning', 'research', 'robot', 'robotics', 'russell', 'said', 'scientist', 'see', 'shaw', 'shelf', 'simeon', 'simon', 'simply', 'sitting', 'solve', 'solver', 'solving', 'some', 'superior', 'system', 'task', 'test', 'the', 'theorem', 'theorems', 'this', 'those', 'to', 'today', 'too', 'understanding', 'used', 'using', 'was', 'we', 'were', 'what', 'whitehead']\n",
      "[[0.21582961 0.         0.         0.         0.         0.\n",
      "  0.37488637 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.36458624 0.21582961 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.18229312 0.         0.43165921\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.12496212 0.         0.         0.         0.         0.\n",
      "  0.21582961 0.15849861 0.         0.         0.         0.\n",
      "  0.         0.21582961 0.21582961 0.         0.         0.\n",
      "  0.         0.18229312 0.         0.         0.         0.\n",
      "  0.14004215 0.         0.         0.21582961 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.12496212\n",
      "  0.         0.15849861 0.         0.         0.         0.\n",
      "  0.21582961 0.21582961 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.2899298  0.2899298  0.         0.         0.\n",
      "  0.16786503 0.         0.         0.         0.         0.\n",
      "  0.         0.2899298  0.24487933 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.24487933 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.2899298  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.2899298  0.         0.\n",
      "  0.2899298  0.         0.         0.         0.         0.\n",
      "  0.         0.24487933 0.         0.2899298  0.         0.\n",
      "  0.18812244 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2899298\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2899298  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.26619365 0.\n",
      "  0.         0.         0.         0.         0.22483139 0.\n",
      "  0.26619365 0.         0.         0.         0.         0.\n",
      "  0.26619365 0.         0.26619365 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.19548441 0.\n",
      "  0.         0.         0.         0.         0.         0.26619365\n",
      "  0.         0.26619365 0.         0.         0.         0.22483139\n",
      "  0.         0.26619365 0.22483139 0.         0.         0.\n",
      "  0.15412215 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26619365 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34544223 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.15412215\n",
      "  0.19548441 0.19548441 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.22965221 0.         0.\n",
      "  0.22965221 0.         0.         0.         0.22965221 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.22965221\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.337299   0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.22965221 0.         0.         0.         0.         0.\n",
      "  0.14901101 0.22965221 0.         0.         0.         0.\n",
      "  0.         0.         0.22965221 0.22965221 0.22965221 0.26593041\n",
      "  0.1686495  0.         0.22965221 0.         0.44703304 0.\n",
      "  0.         0.         0.         0.         0.19396791 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.11587578 0.20013603 0.20013603 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.20013603\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.20013603 0.20013603 0.         0.14697373 0.\n",
      "  0.         0.20013603 0.         0.20013603 0.20013603 0.\n",
      "  0.14697373 0.         0.         0.         0.16903808 0.\n",
      "  0.         0.         0.16903808 0.         0.         0.\n",
      "  0.11587578 0.20013603 0.20013603 0.         0.16903808 0.\n",
      "  0.         0.14697373 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.20013603 0.         0.20013603 0.\n",
      "  0.         0.         0.20013603 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25971859 0.\n",
      "  0.         0.         0.         0.20013603 0.33807616 0.20013603\n",
      "  0.         0.20013603 0.        ]\n",
      " [0.         0.         0.         0.26657512 0.         0.\n",
      "  0.15434302 0.         0.         0.26657512 0.22515359 0.\n",
      "  0.         0.         0.         0.         0.22515359 0.26657512\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.26657512 0.26657512 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.19576455 0.         0.         0.         0.22515359 0.22515359\n",
      "  0.         0.         0.         0.         0.26657512 0.\n",
      "  0.15434302 0.         0.         0.         0.22515359 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26657512\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.15434302\n",
      "  0.         0.19576455 0.         0.         0.17296863 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.26657512]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.38370906 0.\n",
      "  0.         0.         0.         0.38370906 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.38370906 0.         0.         0.         0.         0.\n",
      "  0.2817841  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.32408678\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.38370906\n",
      "  0.         0.         0.         0.         0.38370906 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2817841  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.28960431\n",
      "  0.16767657 0.         0.         0.         0.         0.28960431\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.28960431 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.28960431\n",
      "  0.28960431 0.         0.         0.         0.21267647 0.\n",
      "  0.         0.         0.28960431 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.24460441\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.28960431 0.28960431 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.28960431\n",
      "  0.         0.         0.         0.         0.         0.16767657\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.28960431 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26103212 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30905423 0.30905423 0.         0.\n",
      "  0.30905423 0.         0.         0.         0.         0.\n",
      "  0.17893779 0.         0.         0.30905423 0.         0.\n",
      "  0.         0.         0.30905423 0.         0.         0.30905423\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30905423 0.30905423 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.30905423 0.20053142 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Create a TfidfVectorizer: tfidf\n",
    "vectorizer = TfidfVectorizer() \n",
    "\n",
    "# Apply fit_transform to document: csr_mat\n",
    "csr_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "#print(csr_mat)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(csr_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99551016 1.         0.         0.00295036 0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[[1.         0.26783465 0.11787934 0.10756047 0.08121546 0.12746356\n",
      "  0.         0.08381288 0.02236045]\n",
      " [0.26783465 1.         0.06498544 0.02803232 0.01945149 0.0259088\n",
      "  0.         0.02814703 0.        ]\n",
      " [0.11787934 0.06498544 1.         0.12542881 0.08459516 0.18708746\n",
      "  0.0550844  0.06741761 0.02757828]\n",
      " [0.10756047 0.02803232 0.12542881 1.         0.23125281 0.1183672\n",
      "  0.04752275 0.0445903  0.08964417]\n",
      " [0.08121546 0.01945149 0.08459516 0.23125281 1.         0.18558371\n",
      "  0.04141486 0.05068751 0.07281629]\n",
      " [0.12746356 0.0259088  0.18708746 0.1183672  0.18558371 1.\n",
      "  0.05516334 0.05175942 0.12107576]\n",
      " [0.         0.         0.0550844  0.04752275 0.04141486 0.05516334\n",
      "  1.         0.07927305 0.        ]\n",
      " [0.08381288 0.02814703 0.06741761 0.0445903  0.05068751 0.05175942\n",
      "  0.07927305 1.         0.        ]\n",
      " [0.02236045 0.         0.02757828 0.08964417 0.07281629 0.12107576\n",
      "  0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "model = NMF(n_components=6)\n",
    "\n",
    "csr_matrix = vectorizer.fit_transform(sentences)\n",
    "# Fit the model to articles\n",
    "model.fit(csr_matrix)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = model.transform(csr_matrix)\n",
    "\n",
    "#print(len(nmf_features))\n",
    "# Print the NMF features\n",
    "#print(nmf_features)\n",
    "\n",
    "\n",
    "norm_features = normalize(nmf_features)\n",
    "\n",
    "current_sentence= norm_features[1,:]\n",
    "\n",
    "similarities=norm_features.dot(current_sentence)\n",
    "print(similarities)\n",
    "\n",
    "\n",
    "#print(similarities.nlargest())\n",
    "\n",
    "#cosine_sim=cosine_similarity(csr_matrix, csr_matrix)\n",
    "\n",
    "#print(cosine_sim)\n",
    "\n",
    "cosine_sim= linear_kernel(csr_matrix,csr_matrix)\n",
    "\n",
    "print(cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokens = sum([word_tokenize(sentence) for sentence in sentences], [])\n",
    "\n",
    "words_frequency = FreqDist(tokens)\n",
    "\n",
    "words_frequency.plot(30, cumulative = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'herbert': 2, 'simon': 5, 'research': 1, 'and': 7, 'concepts': 1, 'increased': 2, 'computer': 3, 'scientist': 2, 'understanding': 1, 'of': 5, 'reasoning': 1, 'the': 6, 's': 2, 'ability': 1, 'too': 1, 'solve': 1, 'problems': 1, 'proof': 4, 'theorems': 3, 'al': 1, 'newell': 1, 'clifford': 1, 'shaw': 1, 'proposals': 1, 'were': 1, 'radical': 1, 'affect': 1, 'today': 1, 'in': 3, 'book': 2, 'models': 1, 'my': 2, 'life': 1, 'demonstrated': 1, 'logical': 1, 'theorem': 3, 'algorithm': 1, 'could': 1, 'prove': 1, 'certain': 1, 'mathematical': 2, 'said': 1, 'this': 1, 'was': 3, 'task': 1, 'to': 7, 'get': 1, 'a': 3, 'system': 1, 'discover': 1, 'for': 1, 'not': 1, 'simply': 1, 'test': 1, 'we': 1, 'picked': 1, 'logic': 3, 'just': 1, 'because': 1, 'i': 2, 'happened': 1, 'have': 1, 'principia': 2, 'mathematica': 2, 'sitting': 1, 'on': 1, 'shelf': 1, 'using': 1, 'it': 1, 'see': 1, 'what': 1, 'involved': 1, 'finding': 1, 'anything': 1, 'alfred': 1, 'north': 1, 'whitehead': 1, 'bertrand': 1, 'russell': 1, 'contained': 1, 'considered': 2, 'form': 1, 'foundation': 1, 'simeon': 1, 'evolved': 1, 'into': 1, 'general': 1, 'problem': 2, 'solver': 1, 'gps': 1, 'is': 1, 'currently': 1, 'used': 1, 'robotics': 1, 'gives': 1, 'robot': 1, 'amazing': 1, 'solving': 1, 'capabilities': 1, 'many': 1, 'mathematicians': 1, 'some': 1, 'lts': 1, 'proofs': 1, 'superior': 1, 'those': 1, 'previously': 1, 'published': 1}\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      "  0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#https://medium.com/analytics-vidhya/implementing-the-tf-idf-search-engine-5e9a42b1d30b\n",
    "\n",
    "   \n",
    "corpus = nltk.sent_tokenize(paragraph)    \n",
    "\n",
    "\n",
    "for i in range(len(corpus )):\n",
    "    corpus [i] = corpus [i].lower()\n",
    "    corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "    corpus [i] = re.sub(r'\\s+',' ',corpus [i])\n",
    "\n",
    "#print(corpus)\n",
    "\n",
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "\n",
    "print(wordfreq)\n",
    "\n",
    "#most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "\n",
    "sentence_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    #for token in most_freq:\n",
    "    for token in wordfreq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)\n",
    "\n",
    "sentence_vectors = np.asarray(sentence_vectors)\n",
    "\n",
    "print(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing Herbert 0.07276924\n",
      "computing Simon 0.05576835\n",
      "computing research 0.40343753\n",
      "computing and 0.1943571\n",
      "computing concepts 0.45711964\n",
      "computing increased 0.24925496\n",
      "computing computer 0.6224965\n",
      "computing scientist 0.2660795\n",
      "computing understanding 0.38134873\n",
      "computing of 0.24522913\n",
      "computing reasoning 0.30303115\n",
      "computing and 0.1943571\n",
      "computing increased 0.24925496\n",
      "computing the 0.22144683\n",
      "computing computer 0.6224965\n",
      "computing 's 0.1260481\n",
      "computing ability 0.30118415\n",
      "computing too 0.16354746\n",
      "computing solve 0.3485522\n",
      "computing problems 0.27601594\n",
      "computing and 0.1943571\n",
      "computing proof 0.18525296\n",
      "computing theorems 0.23384278\n",
      "computing . 0.1465077\n",
      "computing Herbert 0.07276924\n",
      "computing Simon 0.05576835\n",
      "computing , 0.18776377\n",
      "computing Al 0.059818372\n",
      "computing Newell -0.0047686454\n",
      "computing , 0.18776377\n",
      "computing Clifford 0.020007184\n",
      "computing Shaw 0.07874915\n",
      "computing proposals 0.1906419\n",
      "computing were 0.088380806\n",
      "computing radical 0.18658942\n",
      "computing and 0.1943571\n",
      "computing affect 0.26474202\n",
      "computing computer 0.6224965\n",
      "computing scientist 0.2660795\n",
      "computing today 0.2458577\n",
      "computing . 0.1465077\n",
      "computing In 0.20310555\n",
      "computing Simon 0.05576835\n",
      "computing ’s 0.1566072\n",
      "computing book 0.14006203\n",
      "computing , 0.18776377\n",
      "computing “ 0.16643858\n",
      "computing Models 0.293657\n",
      "computing of 0.24522913\n",
      "computing my 0.11952344\n",
      "computing life 0.2368741\n",
      "computing ” 0.18655829\n",
      "computing , 0.18776377\n",
      "computing Simon 0.05576835\n",
      "computing demonstrated 0.29319173\n",
      "computing the 0.22144683\n",
      "computing Logical 0.3891341\n",
      "computing Theorem 0.27457505\n",
      "computing algorithm 0.45087993\n",
      "computing could 0.2261679\n",
      "computing prove 0.23306897\n",
      "computing certain 0.26280278\n",
      "computing mathematical 0.47654593\n",
      "computing theorems 0.23384278\n",
      "computing . 0.1465077\n",
      "computing Simon 0.05576835\n",
      "computing said 0.17151013\n",
      "computing , 0.18776377\n",
      "computing “ 0.16643858\n",
      "computing This 0.17403455\n",
      "computing was 0.096630886\n",
      "computing the 0.22144683\n",
      "computing task 0.3699183\n",
      "computing to 0.19440654\n",
      "computing get 0.14464964\n",
      "computing a 0.21335497\n",
      "computing system 0.4341465\n",
      "computing to 0.19440654\n",
      "computing discover 0.21387707\n",
      "computing proof 0.18525296\n",
      "computing for 0.2270494\n",
      "computing a 0.21335497\n",
      "computing theorem 0.27457505\n",
      "computing , 0.18776377\n",
      "computing not 0.19226551\n",
      "computing simply 0.23587337\n",
      "computing to 0.19440654\n",
      "computing test 0.2567172\n",
      "computing the 0.22144683\n",
      "computing proof 0.18525296\n",
      "computing . 0.1465077\n",
      "computing We 0.24185108\n",
      "computing picked 0.024817133\n",
      "computing logic 0.40104094\n",
      "computing just 0.16287896\n",
      "computing because 0.20407403\n",
      "computing I 0.10273568\n",
      "computing happened 0.10298791\n",
      "computing to 0.19440654\n",
      "computing have 0.1529994\n",
      "computing Principia 0.028432935\n",
      "computing Mathematica 0.14816025\n",
      "computing sitting 0.10337253\n",
      "computing on 0.16639738\n",
      "computing my 0.11952344\n",
      "computing shelf 0.14861476\n",
      "computing and 0.1943571\n",
      "computing I 0.10273568\n",
      "computing was 0.096630886\n",
      "computing using 0.32855365\n",
      "computing it 0.22552036\n",
      "computing to 0.19440654\n",
      "computing see 0.15658993\n",
      "computing what 0.24160239\n",
      "computing was 0.096630886\n",
      "computing involved 0.2439669\n",
      "computing in 0.20310555\n",
      "computing finding 0.20014775\n",
      "computing a 0.21335497\n",
      "computing proof 0.18525296\n",
      "computing of 0.24522913\n",
      "computing anything 0.15932484\n",
      "computing . 0.1465077\n",
      "computing ” 0.18655829\n",
      "computing Alfred 0.08811794\n",
      "computing North 0.027524812\n",
      "computing Whitehead -0.027441887\n",
      "computing and 0.1943571\n",
      "computing Bertrand -0.018936047\n",
      "computing Russell 0.043770514\n",
      "computing book 0.14006203\n",
      "computing Principia 0.028432935\n",
      "computing Mathematica 0.14816025\n",
      "computing contained 0.15273635\n",
      "computing theorems 0.23384278\n",
      "computing considered 0.24339128\n",
      "computing to 0.19440654\n",
      "computing form 0.24674134\n",
      "computing the 0.22144683\n",
      "computing foundation 0.3020023\n",
      "computing of 0.24522913\n",
      "computing mathematical 0.47654593\n",
      "computing logic 0.40104094\n",
      "computing . 0.1465077\n",
      "computing Simeon -0.04268905\n",
      "computing evolved 0.32150316\n",
      "computing Logic 0.40104094\n",
      "computing theorem 0.27457505\n",
      "computing into 0.24331877\n",
      "computing General 0.35554928\n",
      "computing problem 0.26211986\n",
      "computing solver 0.23071279\n",
      "computing . 0.1465077\n",
      "computing GPS 0.20427263\n",
      "computing is 0.23551805\n",
      "computing currently 0.23484209\n",
      "computing used 0.23543057\n",
      "computing in 0.20310555\n",
      "computing robotics 0.4662865\n",
      "computing and 0.1943571\n",
      "computing gives 0.23953764\n",
      "computing the 0.22144683\n",
      "computing robot 0.28552082\n",
      "computing amazing 0.111162305\n",
      "computing problem 0.26211986\n",
      "computing solving 0.40362126\n",
      "computing capabilities 0.5456464\n",
      "computing . 0.1465077\n",
      "computing Many 0.20614779\n",
      "computing mathematicians 0.31407258\n",
      "computing considered 0.24339128\n",
      "computing some 0.19451094\n",
      "computing of 0.24522913\n",
      "computing LTs -0.003336904\n",
      "computing proofs 0.17753477\n",
      "computing superior 0.2543586\n",
      "computing to 0.19440654\n",
      "computing those 0.1991222\n",
      "computing previously 0.21355443\n",
      "computing published 0.116425596\n",
      "mathematics Herbert 0.15218541\n",
      "mathematics Simon 0.13137709\n",
      "mathematics research 0.44823438\n",
      "mathematics and 0.1919858\n",
      "mathematics concepts 0.5593571\n",
      "mathematics increased 0.1467636\n",
      "mathematics computer 0.3999747\n",
      "mathematics scientist 0.3824585\n",
      "mathematics understanding 0.4930703\n",
      "mathematics of 0.22132188\n",
      "mathematics reasoning 0.499432\n",
      "mathematics and 0.1919858\n",
      "mathematics increased 0.1467636\n",
      "mathematics the 0.17734843\n",
      "mathematics computer 0.3999747\n",
      "mathematics 's 0.094495125\n",
      "mathematics ability 0.2470493\n",
      "mathematics too 0.17345947\n",
      "mathematics solve 0.44170922\n",
      "mathematics problems 0.3159438\n",
      "mathematics and 0.1919858\n",
      "mathematics proof 0.30108315\n",
      "mathematics theorems 0.48887467\n",
      "mathematics . 0.1428826\n",
      "mathematics Herbert 0.15218541\n",
      "mathematics Simon 0.13137709\n",
      "mathematics , 0.16458644\n",
      "mathematics Al 0.04165649\n",
      "mathematics Newell -0.039203625\n",
      "mathematics , 0.16458644\n",
      "mathematics Clifford 0.12616213\n",
      "mathematics Shaw 0.080864504\n",
      "mathematics proposals 0.2353185\n",
      "mathematics were 0.14509474\n",
      "mathematics radical 0.2384969\n",
      "mathematics and 0.1919858\n",
      "mathematics affect 0.21838902\n",
      "mathematics computer 0.3999747\n",
      "mathematics scientist 0.3824585\n",
      "mathematics today 0.17317246\n",
      "mathematics . 0.1428826\n",
      "mathematics In 0.2502716\n",
      "mathematics Simon 0.13137709\n",
      "mathematics ’s 0.054871567\n",
      "mathematics book 0.27858353\n",
      "mathematics , 0.16458644\n",
      "mathematics “ 0.05520011\n",
      "mathematics Models 0.23841166\n",
      "mathematics of 0.22132188\n",
      "mathematics my 0.15176548\n",
      "mathematics life 0.26025322\n",
      "mathematics ” 0.07625117\n",
      "mathematics , 0.16458644\n",
      "mathematics Simon 0.13137709\n",
      "mathematics demonstrated 0.32376388\n",
      "mathematics the 0.17734843\n",
      "mathematics Logical 0.4115163\n",
      "mathematics Theorem 0.51952326\n",
      "mathematics algorithm 0.36506292\n",
      "mathematics could 0.17860435\n",
      "mathematics prove 0.32869932\n",
      "mathematics certain 0.26308656\n",
      "mathematics mathematical 0.8114012\n",
      "mathematics theorems 0.48887467\n",
      "mathematics . 0.1428826\n",
      "mathematics Simon 0.13137709\n",
      "mathematics said 0.097733065\n",
      "mathematics , 0.16458644\n",
      "mathematics “ 0.05520011\n",
      "mathematics This 0.1737804\n",
      "mathematics was 0.15852255\n",
      "mathematics the 0.17734843\n",
      "mathematics task 0.30316022\n",
      "mathematics to 0.17333423\n",
      "mathematics get 0.15163872\n",
      "mathematics a 0.15215312\n",
      "mathematics system 0.2486563\n",
      "mathematics to 0.17333423\n",
      "mathematics discover 0.23965321\n",
      "mathematics proof 0.30108315\n",
      "mathematics for 0.15437222\n",
      "mathematics a 0.15215312\n",
      "mathematics theorem 0.51952326\n",
      "mathematics , 0.16458644\n",
      "mathematics not 0.21616322\n",
      "mathematics simply 0.19955498\n",
      "mathematics to 0.17333423\n",
      "mathematics test 0.27182534\n",
      "mathematics the 0.17734843\n",
      "mathematics proof 0.30108315\n",
      "mathematics . 0.1428826\n",
      "mathematics We 0.19118974\n",
      "mathematics picked 0.043306027\n",
      "mathematics logic 0.48552948\n",
      "mathematics just 0.15451393\n",
      "mathematics because 0.22475593\n",
      "mathematics I 0.1450038\n",
      "mathematics happened 0.1378248\n",
      "mathematics to 0.17333423\n",
      "mathematics have 0.19579935\n",
      "mathematics Principia 0.13330363\n",
      "mathematics Mathematica 0.305936\n",
      "mathematics sitting 0.0924929\n",
      "mathematics on 0.13802332\n",
      "mathematics my 0.15176548\n",
      "mathematics shelf 0.06740684\n",
      "mathematics and 0.1919858\n",
      "mathematics I 0.1450038\n",
      "mathematics was 0.15852255\n",
      "mathematics using 0.20541163\n",
      "mathematics it 0.18828109\n",
      "mathematics to 0.17333423\n",
      "mathematics see 0.13032426\n",
      "mathematics what 0.2574315\n",
      "mathematics was 0.15852255\n",
      "mathematics involved 0.2709118\n",
      "mathematics in 0.2502716\n",
      "mathematics finding 0.21857777\n",
      "mathematics a 0.15215312\n",
      "mathematics proof 0.30108315\n",
      "mathematics of 0.22132188\n",
      "mathematics anything 0.20285393\n",
      "mathematics . 0.1428826\n",
      "mathematics ” 0.07625117\n",
      "mathematics Alfred 0.15882698\n",
      "mathematics North 0.11588196\n",
      "mathematics Whitehead 0.07808965\n",
      "mathematics and 0.1919858\n",
      "mathematics Bertrand 0.1010883\n",
      "mathematics Russell 0.11125292\n",
      "mathematics book 0.27858353\n",
      "mathematics Principia 0.13330363\n",
      "mathematics Mathematica 0.305936\n",
      "mathematics contained 0.14558052\n",
      "mathematics theorems 0.48887467\n",
      "mathematics considered 0.30557752\n",
      "mathematics to 0.17333423\n",
      "mathematics form 0.24019946\n",
      "mathematics the 0.17734843\n",
      "mathematics foundation 0.35152078\n",
      "mathematics of 0.22132188\n",
      "mathematics mathematical 0.8114012\n",
      "mathematics logic 0.48552948\n",
      "mathematics . 0.1428826\n",
      "mathematics Simeon 0.025687518\n",
      "mathematics evolved 0.27239838\n",
      "mathematics Logic 0.48552948\n",
      "mathematics theorem 0.51952326\n",
      "mathematics into 0.21459211\n",
      "mathematics General 0.39205575\n",
      "mathematics problem 0.29629782\n",
      "mathematics solver 0.4081052\n",
      "mathematics . 0.1428826\n",
      "mathematics GPS 0.074559174\n",
      "mathematics is 0.19002503\n",
      "mathematics currently 0.19156493\n",
      "mathematics used 0.21317719\n",
      "mathematics in 0.2502716\n",
      "mathematics robotics 0.3964132\n",
      "mathematics and 0.1919858\n",
      "mathematics gives 0.20057704\n",
      "mathematics the 0.17734843\n",
      "mathematics robot 0.14451122\n",
      "mathematics amazing 0.10834687\n",
      "mathematics problem 0.29629782\n",
      "mathematics solving 0.563514\n",
      "mathematics capabilities 0.23074819\n",
      "mathematics . 0.1428826\n",
      "mathematics Many 0.23618928\n",
      "mathematics mathematicians 0.62572014\n",
      "mathematics considered 0.30557752\n",
      "mathematics some 0.21192916\n",
      "mathematics of 0.22132188\n",
      "mathematics LTs -0.019567072\n",
      "mathematics proofs 0.4006276\n",
      "mathematics superior 0.18672936\n",
      "mathematics to 0.17333423\n",
      "mathematics those 0.21926636\n",
      "mathematics previously 0.19474582\n",
      "mathematics published 0.22297843\n",
      "theorm Herbert 0.11818329\n",
      "theorm Simon 0.105014116\n",
      "theorm research -0.08976414\n",
      "theorm and -0.3065261\n",
      "theorm concepts -0.03064575\n",
      "theorm increased -0.18423605\n",
      "theorm computer -0.1907887\n",
      "theorm scientist -0.0117785\n",
      "theorm understanding -0.08612586\n",
      "theorm of -0.21583192\n",
      "theorm reasoning 0.070340864\n",
      "theorm and -0.3065261\n",
      "theorm increased -0.18423605\n",
      "theorm the -0.23179406\n",
      "theorm computer -0.1907887\n",
      "theorm 's -0.19442289\n",
      "theorm ability -0.1519747\n",
      "theorm too -0.25065568\n",
      "theorm solve 0.061929695\n",
      "theorm problems -0.13245776\n",
      "theorm and -0.3065261\n",
      "theorm proof 0.040265605\n",
      "theorm theorems 0.34822604\n",
      "theorm . -0.23462069\n",
      "theorm Herbert 0.11818329\n",
      "theorm Simon 0.105014116\n",
      "theorm , -0.26550594\n",
      "theorm Al -0.054183308\n",
      "theorm Newell 0.14506607\n",
      "theorm , -0.26550594\n",
      "theorm Clifford 0.08445835\n",
      "theorm Shaw 0.14377542\n",
      "theorm proposals -0.072612345\n",
      "theorm were -0.20905404\n",
      "theorm radical -0.033799\n",
      "theorm and -0.3065261\n",
      "theorm affect -0.09056112\n",
      "theorm computer -0.1907887\n",
      "theorm scientist -0.0117785\n",
      "theorm today -0.23734725\n",
      "theorm . -0.23462069\n",
      "theorm In -0.21266408\n",
      "theorm Simon 0.105014116\n",
      "theorm ’s -0.13941796\n",
      "theorm book -0.2318929\n",
      "theorm , -0.26550594\n",
      "theorm “ -0.17818731\n",
      "theorm Models -0.12404612\n",
      "theorm of -0.21583192\n",
      "theorm my -0.2563133\n",
      "theorm life -0.2359273\n",
      "theorm ” -0.116432816\n",
      "theorm , -0.26550594\n",
      "theorm Simon 0.105014116\n",
      "theorm demonstrated -0.02737019\n",
      "theorm the -0.23179406\n",
      "theorm Logical -0.010818023\n",
      "theorm Theorem 0.39093226\n",
      "theorm algorithm 0.06194913\n",
      "theorm could -0.19535103\n",
      "theorm prove 0.03444274\n",
      "theorm certain -0.19363946\n",
      "theorm mathematical 0.11343587\n",
      "theorm theorems 0.34822604\n",
      "theorm . -0.23462069\n",
      "theorm Simon 0.105014116\n",
      "theorm said -0.16545586\n",
      "theorm , -0.26550594\n",
      "theorm “ -0.17818731\n",
      "theorm This -0.19484134\n",
      "theorm was -0.16673209\n",
      "theorm the -0.23179406\n",
      "theorm task -0.11063884\n",
      "theorm to -0.22422731\n",
      "theorm get -0.22994126\n",
      "theorm a -0.22609198\n",
      "theorm system -0.12630354\n",
      "theorm to -0.22422731\n",
      "theorm discover -0.11771403\n",
      "theorm proof 0.040265605\n",
      "theorm for -0.22994824\n",
      "theorm a -0.22609198\n",
      "theorm theorem 0.39093226\n",
      "theorm , -0.26550594\n",
      "theorm not -0.21552938\n",
      "theorm simply -0.1766701\n",
      "theorm to -0.22422731\n",
      "theorm test -0.031178804\n",
      "theorm the -0.23179406\n",
      "theorm proof 0.040265605\n",
      "theorm . -0.23462069\n",
      "theorm We -0.19386227\n",
      "theorm picked -0.19323385\n",
      "theorm logic -0.04283431\n",
      "theorm just -0.22426306\n",
      "theorm because -0.21146873\n",
      "theorm I -0.12149044\n",
      "theorm happened -0.14037731\n",
      "theorm to -0.22422731\n",
      "theorm have -0.27586418\n",
      "theorm Principia 0.26924914\n",
      "theorm Mathematica 0.3122619\n",
      "theorm sitting -0.22177035\n",
      "theorm on -0.20817377\n",
      "theorm my -0.2563133\n",
      "theorm shelf -0.114048585\n",
      "theorm and -0.3065261\n",
      "theorm I -0.12149044\n",
      "theorm was -0.16673209\n",
      "theorm using -0.14238404\n",
      "theorm it -0.19903475\n",
      "theorm to -0.22422731\n",
      "theorm see -0.2605721\n",
      "theorm what -0.17389978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theorm was -0.16673209\n",
      "theorm involved -0.098828144\n",
      "theorm in -0.21266408\n",
      "theorm finding -0.10688144\n",
      "theorm a -0.22609198\n",
      "theorm proof 0.040265605\n",
      "theorm of -0.21583192\n",
      "theorm anything -0.16286898\n",
      "theorm . -0.23462069\n",
      "theorm ” -0.116432816\n",
      "theorm Alfred 0.10705631\n",
      "theorm North -0.09030698\n",
      "theorm Whitehead 0.2551702\n",
      "theorm and -0.3065261\n",
      "theorm Bertrand 0.2621902\n",
      "theorm Russell 0.13710104\n",
      "theorm book -0.2318929\n",
      "theorm Principia 0.26924914\n",
      "theorm Mathematica 0.3122619\n",
      "theorm contained -0.063795075\n",
      "theorm theorems 0.34822604\n",
      "theorm considered -0.1697552\n",
      "theorm to -0.22422731\n",
      "theorm form -0.16259058\n",
      "theorm the -0.23179406\n",
      "theorm foundation 0.02598746\n",
      "theorm of -0.21583192\n",
      "theorm mathematical 0.11343587\n",
      "theorm logic -0.04283431\n",
      "theorm . -0.23462069\n",
      "theorm Simeon 0.26194298\n",
      "theorm evolved -0.090613455\n",
      "theorm Logic -0.04283431\n",
      "theorm theorem 0.39093226\n",
      "theorm into -0.23217069\n",
      "theorm General -0.18473692\n",
      "theorm problem -0.07620558\n",
      "theorm solver 0.15412886\n",
      "theorm . -0.23462069\n",
      "theorm GPS -0.0417106\n",
      "theorm is -0.16401023\n",
      "theorm currently -0.211758\n",
      "theorm used -0.18147735\n",
      "theorm in -0.21266408\n",
      "theorm robotics 0.018042462\n",
      "theorm and -0.3065261\n",
      "theorm gives -0.1251743\n",
      "theorm the -0.23179406\n",
      "theorm robot -0.054104585\n",
      "theorm amazing -0.18826015\n",
      "theorm problem -0.07620558\n",
      "theorm solving 0.090270184\n",
      "theorm capabilities -0.12877503\n",
      "theorm . -0.23462069\n",
      "theorm Many -0.26827484\n",
      "theorm mathematicians 0.09056851\n",
      "theorm considered -0.1697552\n",
      "theorm some -0.24449508\n",
      "theorm of -0.21583192\n",
      "theorm LTs 0.20036662\n",
      "theorm proofs 0.090499416\n",
      "theorm superior -0.16764137\n",
      "theorm to -0.22422731\n",
      "theorm those -0.2633886\n",
      "theorm previously -0.14824788\n",
      "theorm published -0.19921798\n"
     ]
    }
   ],
   "source": [
    "#takes a long time to run\n",
    "nlp=spacy.load('en_core_web_lg')\n",
    "doc2=nlp(paragraph)\n",
    "doc=nlp('I am happy')\n",
    "\n",
    "#for token in doc2:\n",
    "#    print(token.vector)\n",
    "    \n",
    "    \n",
    "#doc3=nlp(\"happy joyous sad\")  \n",
    "         \n",
    "#for token1 in doc3:\n",
    "#    for token2 in doc2:\n",
    "#        print(token1.text,token2.text,token1.similarity(token2))\n",
    "\n",
    "doc3=nlp(\"computing mathematics theorm\")  \n",
    "\n",
    "for token1 in doc3:\n",
    "    for token2 in doc2:\n",
    "        print(token1.text,token2.text,token1.similarity(token2))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string   \n",
    "    return (str1.join(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=np.arange(0,len(sentences))\n",
    "wordfreq = dict()\n",
    "bagList=[]\n",
    "for i in index:\n",
    "    sentence=sentences[i]\n",
    "    if(len(sentence)>0):\n",
    "        tokens=sum([word_tokenize(sentence)],[])\n",
    "        #print(sentences[i])\n",
    "        #print(tokens)\n",
    "        #print(index[i])\n",
    "        words_frequency = FreqDist(tokens)\n",
    "        #words_frequency.plot(30, cumulative = False)\n",
    "        wordfreq=dict()\n",
    "        for token in tokens:\n",
    "            if token not in wordfreq.keys():\n",
    "                wordfreq[token] = 1\n",
    "            else:\n",
    "                wordfreq[token] += 1\n",
    "            #print(wordfreq)\n",
    "        bagList.append({'words': wordfreq})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon=dict()\n",
    "\n",
    "#breakfor i in np.arange(0,len(bagList)):\n",
    "for dictionaryItem in bagList:\n",
    "    words=dictionaryItem['words']\n",
    "    for key,(word,count) in enumerate(words.items()):\n",
    "        if word in uncommon:\n",
    "            uncommon[word]+=1\n",
    "        else:\n",
    "            uncommon[word]=1\n",
    "\n",
    "uncommon = {key: value for key, value in uncommon.items() if (value<=3 )}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herbert 0 Entity:True Uncommon:True [('Herbert Simon', 'PERSON')]\n",
      "Simon 0 Entity:True Uncommon:False [('Herbert Simon', 'PERSON')]\n",
      "research 0 Entity:False Uncommon:True []\n",
      "and 0 Entity:False Uncommon:False []\n",
      "concepts 0 Entity:False Uncommon:True []\n",
      "increased 0 Entity:False Uncommon:True []\n",
      "computer 0 Entity:False Uncommon:True []\n",
      "scientist 0 Entity:False Uncommon:True []\n",
      "understanding 0 Entity:False Uncommon:True []\n",
      "of 0 Entity:False Uncommon:False []\n",
      "reasoning 0 Entity:False Uncommon:True []\n",
      "the 0 Entity:False Uncommon:False []\n",
      "'s 0 Entity:False Uncommon:True []\n",
      "ability 0 Entity:False Uncommon:True []\n",
      "too 0 Entity:False Uncommon:True []\n",
      "solve 0 Entity:False Uncommon:True []\n",
      "problems 0 Entity:False Uncommon:True []\n",
      "proof 0 Entity:False Uncommon:True []\n",
      "theorems 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "Herbert 0 Entity:True Uncommon:True [('Herbert Simon', 'PERSON')]\n",
      "Simon 0 Entity:True Uncommon:False [('Herbert Simon', 'PERSON')]\n",
      ", 0 Entity:False Uncommon:True []\n",
      "Al 0 Entity:True Uncommon:True [('Al Newell Clifford Shaw', 'PERSON')]\n",
      "Newell 0 Entity:True Uncommon:True [('Al Newell Clifford Shaw', 'PERSON')]\n",
      "Clifford 0 Entity:True Uncommon:True [('Al Newell Clifford Shaw', 'PERSON')]\n",
      "Shaw 0 Entity:True Uncommon:True [('Al Newell Clifford Shaw', 'PERSON')]\n",
      "proposals 0 Entity:False Uncommon:True []\n",
      "were 0 Entity:False Uncommon:True []\n",
      "radical 0 Entity:False Uncommon:True []\n",
      "and 0 Entity:False Uncommon:False []\n",
      "affect 0 Entity:False Uncommon:True []\n",
      "computer 0 Entity:False Uncommon:True []\n",
      "scientist 0 Entity:False Uncommon:True []\n",
      "today 0 Entity:True Uncommon:True [('today', 'DATE')]\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "In 0 Entity:False Uncommon:True []\n",
      "Simon 0 Entity:True Uncommon:False [('Simon', 'PERSON')]\n",
      "’ 0 Entity:False Uncommon:True []\n",
      "s 0 Entity:False Uncommon:True []\n",
      "book 0 Entity:False Uncommon:True []\n",
      ", 0 Entity:False Uncommon:True []\n",
      "“ 0 Entity:False Uncommon:True []\n",
      "Models 0 Entity:False Uncommon:True []\n",
      "of 0 Entity:False Uncommon:False []\n",
      "my 0 Entity:False Uncommon:True []\n",
      "life 0 Entity:False Uncommon:True []\n",
      "” 0 Entity:False Uncommon:True []\n",
      "demonstrated 0 Entity:False Uncommon:True []\n",
      "the 0 Entity:True Uncommon:False [('the Logical Theorem', 'ORG')]\n",
      "Logical 0 Entity:True Uncommon:True [('the Logical Theorem', 'ORG')]\n",
      "Theorem 0 Entity:True Uncommon:True [('the Logical Theorem', 'ORG')]\n",
      "algorithm 0 Entity:False Uncommon:True []\n",
      "could 0 Entity:False Uncommon:True []\n",
      "prove 0 Entity:False Uncommon:True []\n",
      "certain 0 Entity:False Uncommon:True []\n",
      "mathematical 0 Entity:False Uncommon:True []\n",
      "theorems 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "Simon 0 Entity:True Uncommon:False [('Simon', 'ORG')]\n",
      "said 0 Entity:False Uncommon:True []\n",
      ", 0 Entity:False Uncommon:True []\n",
      "“ 0 Entity:False Uncommon:True []\n",
      "This 0 Entity:False Uncommon:True []\n",
      "was 0 Entity:False Uncommon:True []\n",
      "the 0 Entity:False Uncommon:False []\n",
      "task 0 Entity:False Uncommon:True []\n",
      "to 0 Entity:False Uncommon:False []\n",
      "get 0 Entity:False Uncommon:True []\n",
      "a 0 Entity:False Uncommon:True []\n",
      "system 0 Entity:False Uncommon:True []\n",
      "discover 0 Entity:False Uncommon:True []\n",
      "proof 0 Entity:False Uncommon:True []\n",
      "for 0 Entity:False Uncommon:True []\n",
      "theorem 0 Entity:False Uncommon:True []\n",
      "not 0 Entity:False Uncommon:True []\n",
      "simply 0 Entity:False Uncommon:True []\n",
      "test 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "We 0 Entity:False Uncommon:True []\n",
      "picked 0 Entity:False Uncommon:True []\n",
      "logic 0 Entity:False Uncommon:True []\n",
      "just 0 Entity:False Uncommon:True []\n",
      "because 0 Entity:False Uncommon:True []\n",
      "I 0 Entity:False Uncommon:True []\n",
      "happened 0 Entity:False Uncommon:True []\n",
      "to 0 Entity:False Uncommon:False []\n",
      "have 0 Entity:False Uncommon:True []\n",
      "Principia 0 Entity:True Uncommon:True [('Principia Mathematica', 'PERSON')]\n",
      "Mathematica 0 Entity:True Uncommon:True [('Principia Mathematica', 'PERSON')]\n",
      "sitting 0 Entity:False Uncommon:True []\n",
      "on 0 Entity:False Uncommon:True []\n",
      "my 0 Entity:False Uncommon:True []\n",
      "shelf 0 Entity:False Uncommon:True []\n",
      "and 0 Entity:False Uncommon:False []\n",
      "was 0 Entity:False Uncommon:True []\n",
      "using 0 Entity:False Uncommon:True []\n",
      "it 0 Entity:False Uncommon:True []\n",
      "see 0 Entity:False Uncommon:True []\n",
      "what 0 Entity:False Uncommon:True []\n",
      "involved 0 Entity:False Uncommon:True []\n",
      "in 0 Entity:True Uncommon:True [('Principia Mathematica', 'PERSON')]\n",
      "finding 0 Entity:False Uncommon:True []\n",
      "a 0 Entity:True Uncommon:True [('Principia Mathematica', 'PERSON')]\n",
      "proof 0 Entity:False Uncommon:True []\n",
      "of 0 Entity:False Uncommon:False []\n",
      "anything 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "” 0 Entity:False Uncommon:True []\n",
      "Alfred 0 Entity:True Uncommon:True [('Alfred North Whitehead', 'PERSON')]\n",
      "North 0 Entity:True Uncommon:True [('Alfred North Whitehead', 'PERSON')]\n",
      "Whitehead 0 Entity:True Uncommon:True [('Alfred North Whitehead', 'PERSON')]\n",
      "and 0 Entity:True Uncommon:False [('Bertrand Russell', 'PERSON')]\n",
      "Bertrand 0 Entity:True Uncommon:True [('Bertrand Russell', 'PERSON')]\n",
      "Russell 0 Entity:True Uncommon:True [('Bertrand Russell', 'PERSON')]\n",
      "book 0 Entity:False Uncommon:True []\n",
      "Principia 0 Entity:True Uncommon:True [('Principia Mathematica', 'PERSON')]\n",
      "Mathematica 0 Entity:True Uncommon:True [('Principia Mathematica', 'PERSON')]\n",
      "contained 0 Entity:False Uncommon:True []\n",
      "theorems 0 Entity:False Uncommon:True []\n",
      "considered 0 Entity:False Uncommon:True []\n",
      "to 0 Entity:False Uncommon:False []\n",
      "form 0 Entity:False Uncommon:True []\n",
      "the 0 Entity:True Uncommon:False [('Principia Mathematica', 'PERSON')]\n",
      "foundation 0 Entity:False Uncommon:True []\n",
      "of 0 Entity:False Uncommon:False []\n",
      "mathematical 0 Entity:False Uncommon:True []\n",
      "logic 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "Simeon 0 Entity:False Uncommon:True []\n",
      "evolved 0 Entity:False Uncommon:True []\n",
      "Logic 0 Entity:False Uncommon:True []\n",
      "theorem 0 Entity:False Uncommon:True []\n",
      "into 0 Entity:False Uncommon:True []\n",
      "General 0 Entity:False Uncommon:True []\n",
      "problem 0 Entity:False Uncommon:True []\n",
      "solver 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "GPS 0 Entity:True Uncommon:True [('GPS', 'ORG')]\n",
      "is 0 Entity:False Uncommon:True []\n",
      "currently 0 Entity:False Uncommon:True []\n",
      "used 0 Entity:False Uncommon:True []\n",
      "in 0 Entity:False Uncommon:True []\n",
      "robotics 0 Entity:False Uncommon:True []\n",
      "and 0 Entity:False Uncommon:False []\n",
      "gives 0 Entity:False Uncommon:True []\n",
      "the 0 Entity:False Uncommon:False []\n",
      "robot 0 Entity:False Uncommon:True []\n",
      "amazing 0 Entity:False Uncommon:True []\n",
      "problem 0 Entity:False Uncommon:True []\n",
      "solving 0 Entity:False Uncommon:True []\n",
      "capabilities 0 Entity:False Uncommon:True []\n",
      ". 0 Entity:False Uncommon:False []\n",
      "\n",
      "Many 0 Entity:False Uncommon:True []\n",
      "mathematicians 0 Entity:False Uncommon:True []\n",
      "considered 0 Entity:False Uncommon:True []\n",
      "some 0 Entity:False Uncommon:True []\n",
      "of 0 Entity:False Uncommon:False []\n",
      "LTs 0 Entity:False Uncommon:True []\n",
      "proofs 0 Entity:False Uncommon:True []\n",
      "superior 0 Entity:False Uncommon:True []\n",
      "to 0 Entity:False Uncommon:False []\n",
      "those 0 Entity:False Uncommon:True []\n",
      "previously 0 Entity:False Uncommon:True []\n",
      "published 0 Entity:False Uncommon:True []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dictionaryItem in bagList:\n",
    "    words=[]\n",
    "    wordPair=dictionaryItem['words']\n",
    "    for key, value in enumerate(wordPair):\n",
    "        #if(value<3):\n",
    "        words.append(value)\n",
    "\n",
    "    paragraph=listToString(words)\n",
    "    #print(paragraph)\n",
    "    \n",
    "    nlp=spacy.load('en_core_web_sm')\n",
    "  \n",
    "    doc = nlp(paragraph)\n",
    "    output=[]\n",
    "    isEntity=False\n",
    "    isUncommon=False\n",
    "    for token in doc:\n",
    "        flag=0\n",
    "        entity_match=[(ent.text,ent.label_) for ent in doc.ents if token.text in ent.text]\n",
    "        isEntity=(len(entity_match)>0)\n",
    "        isUncommon=token.text in uncommon.keys()\n",
    "        if  isEntity==False and isUncommon==False and len(token.text)>=4 and token.pos_=='PROPN':\n",
    "            flag=1\n",
    "                \n",
    "        print(token.text,flag,\"Entity:\"+str(isEntity),\"Uncommon:\"+str(isUncommon),entity_match)\n",
    "    \n",
    "        if flag==1 :\n",
    "            output.append('and \\' \\' + [SentenceValue]+\\' \\' like \\'%'+token.text+' %\\'')\n",
    "    \n",
    "    print (listToString(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar sentences\n",
    "    sim_scores = sim_scores[1:10]\n",
    "    # Get the movie indices\n",
    "    sentence_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return df['Sentence'].iloc[sentence_indices]\n",
    "    #printreturn sentences[sentence_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Simon’s book , “Models of my life” , Simon demonstrated the Logical Theorem algorithm could prove certain mathematical theorems .\n",
      "(0, '” Alfred North Whitehead and Bertrand Russell book Principia Mathematica contained theorems considered to form the foundation of mathematical logic .')\n",
      "(1, 'Simon said , “This was the task to get a system to discover proof for a theorem , not simply to test the proof .')\n",
      "(2, \"Herbert Simon research and concepts increased computer scientist understanding of reasoning and increased the computer's ability too solve problems and proof theorems .\")\n",
      "(3, 'Herbert Simon , Al Newell , Clifford Shaw proposals were radical and affect computer scientist today .')\n",
      "(4, 'Simeon evolved Logic theorem into General problem solver .')\n",
      "(5, 'We picked logic just because I happened to have Principia Mathematica sitting on my shelf and I was using it to see what was involved in finding a proof of anything .')\n",
      "(6, 'GPS is currently used in robotics and gives the robot amazing problem solving capabilities .')\n",
      "(7, 'Many mathematicians considered some of LTs proofs superior to those previously published')\n"
     ]
    }
   ],
   "source": [
    "a_sentence=sentences[2] \n",
    "print(a_sentence)\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(sentences)\n",
    "# Convert matrix into a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray())\n",
    "# Map the column names to vocabulary \n",
    "tfidf.columns = tfidf.get_feature_names()\n",
    "\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "\n",
    "df=pd.DataFrame()\n",
    "#indices=np.arange(0,len(sentences))\n",
    "df['Sentence']=sentences\n",
    "indices = pd.Series(df.index, index=df['Sentence']).drop_duplicates()\n",
    "results=get_recommendations(a_sentence,cosine_sim,indices)\n",
    "for result in enumerate(results):\n",
    "       print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_score(new_series, train_series, tokenizer):\n",
    "    \"\"\"\n",
    "    return the tf idf score of each possible pairs of documents\n",
    "    Args:\n",
    "        new_series (pd.Series): new data (To compare against train data)\n",
    "        train_series (pd.Series): train data (To fit the tf-idf transformer)\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    train_tfidf = tokenizer.fit_transform(train_series)\n",
    "    new_tfidf = tokenizer.transform(new_series)\n",
    "    X = pd.DataFrame(cosine_similarity(new_tfidf, train_tfidf), columns=train_series.index)\n",
    "    X['ix_new'] = new_series.index\n",
    "    score = pd.melt(\n",
    "        X,\n",
    "        id_vars='ix_new',\n",
    "        var_name='ix_train',\n",
    "        value_name='score'\n",
    "    )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    We picked logic just because I happened to hav...\n",
      "dtype: object\n",
      "0 0.08121546496588868\n",
      "1 0.019451491211043708\n",
      "2 0.08459516431514912\n",
      "3 0.23125281077112542\n",
      "4 1.0\n",
      "5 0.18558371020743689\n",
      "6 0.04141486064131286\n",
      "7 0.050687508053040675\n",
      "8 0.07281629311343475\n",
      "[3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "match_index=4\n",
    "train_set=pd.Series(sentences)\n",
    "test_set=pd.Series(sentences[match_index])\n",
    "#print(train_set)\n",
    "print(test_set)\n",
    "tokenizer = TfidfVectorizer() # initiate here your own tokenizer (TfidfVectorizer, CountVectorizer, with stopwords...)\n",
    "score = create_tokenizer_score(train_series=train_set, new_series=test_set, tokenizer=tokenizer)\n",
    "#print(score)\n",
    "index=0\n",
    "matches=[]\n",
    "\n",
    "for index in np.arange(0,len(score)):\n",
    "\n",
    "    value=score.loc[index,'score']\n",
    "    print (index,value)\n",
    "    if value>0.10:\n",
    "        matches.append(index)\n",
    "        #matches.append(index)\n",
    "        \n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
